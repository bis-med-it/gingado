[
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Data augmentation",
    "section": "",
    "text": "gingado provides data augmentation functionalities that can help users to augment their datasets with a time series dimension. This can be done both on a stand-alone basis as the user incorporates new data on top of the original dataset, or as part of a scikit-learn Pipeline that also includes other steps like data transformation and model estimation."
  },
  {
    "objectID": "augmentation.html#compatibility-with-scikit-learn",
    "href": "augmentation.html#compatibility-with-scikit-learn",
    "title": "Data augmentation",
    "section": "Compatibility with scikit-learn",
    "text": "Compatibility with scikit-learn\nAs mentioned above, gingado’s transformers are built to be compatible with scikit-learn. The code below demonstrates this compatibility.\nFirst, we create the example dataset. In this case, it comprises the daily foreign exchange rate of selected currencies to the Euro. The Brazilian Real (BRL) is chosen for this example as the dependent variable.\n\nfrom gingado.utils import load_SDMX_data, Lag\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\nX = load_SDMX_data(\n    sources={'ECB': 'EXR'}, \n    keys={'FREQ': 'D', 'CURRENCY': ['EUR', 'AUD', 'BRL', 'CAD', 'CHF', 'GBP', 'JPY', 'SGD', 'USD']},\n    params={\"startPeriod\": 2003}\n    )\n# drop rows with empty values\nX.dropna(inplace=True)\n# adjust column names in this simple example for ease of understanding:\n# remove parts related to source and dataflow names\nX.columns = X.columns.str.replace(\"ECB__EXR_D__\", \"\").str.replace(\"__EUR__SP00__A\", \"\")\nX = Lag(lags=1, jump=0, keep_contemporaneous_X=True).fit_transform(X)\ny = X.pop('BRL')\n# retain only the lagged variables in the X variable\nX = X[X.columns[X.columns.str.contains('_lag_')]]\n\nQuerying data from ECB's dataflow 'EXR' - Exchange Rates...\n\n\n\nX_train, X_test = X.iloc[:-1], X.tail(1)\ny_train, y_test = y.iloc[:-1], y.tail(1)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n((5497, 8), (5497,), (1, 8), (1,))\n\n\nNext, the data augmentation object provided by gingado adds more data. In this case, for brevity only one dataflow from one source is listed. If users want to add more SDMX sources, simply add more keys to the dictionary. And if users want data from all dataflows from a given source provided the keys and parameters such as frequency and dates match, the value should be set to 'all', as in {'ECB': ['CISS'], 'BIS': 'all'}.\n\ntest_src = {'ECB': ['CISS'], 'BIS': ['WS_CBPOL_D']}\n\nX_train__fit_transform = AugmentSDMX(sources=test_src).fit_transform(X=X_train)\nX_train__fit_then_transform = AugmentSDMX(sources=test_src).fit(X=X_train).transform(X=X_train, training=True)\n\nassert X_train__fit_transform.shape == X_train__fit_then_transform.shape\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\nThis is the dataset now after this particular augmentation:\n\nprint(f\"No of columns: {len(X_train__fit_transform.columns)} {X_train__fit_transform.columns}\")\nX_train__fit_transform\n\nNo of columns: 30 Index(['AUD_lag_1', 'BRL_lag_1', 'CAD_lag_1', 'CHF_lag_1', 'GBP_lag_1',\n       'JPY_lag_1', 'SGD_lag_1', 'USD_lag_1',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\n\nAUD_lag_1\nBRL_lag_1\nCAD_lag_1\nCHF_lag_1\nGBP_lag_1\nJPY_lag_1\nSGD_lag_1\nUSD_lag_1\nECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX\n...\nECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX\nECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON\nECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX\nECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-01-03\n1.8554\n3.6770\n1.6422\n1.4528\n0.65200\n124.40\n1.8188\n1.0446\n0.021899\n0.043292\n...\n0.033001\n0.192056\n0.088186\n-0.243567\n0.150383\n0.139864\n0.036917\n0.075458\n0.249733\n0.133170\n\n\n2003-01-06\n1.8440\n3.6112\n1.6264\n1.4555\n0.65000\n124.56\n1.8132\n1.0392\n0.020801\n0.039924\n...\n0.033001\n0.192056\n0.091836\n-0.243567\n0.150383\n0.139864\n0.036917\n0.075458\n0.249733\n0.145225\n\n\n2003-01-07\n1.8281\n3.5145\n1.6383\n1.4563\n0.64950\n124.40\n1.8210\n1.0488\n0.019738\n0.038084\n...\n0.033001\n0.192056\n0.094332\n-0.243567\n0.150383\n0.139864\n0.036917\n0.075458\n0.249733\n0.151298\n\n\n2003-01-08\n1.8160\n3.5139\n1.6257\n1.4565\n0.64960\n124.82\n1.8155\n1.0425\n0.019947\n0.040338\n...\n0.033001\n0.192056\n0.098529\n-0.243567\n0.150383\n0.139864\n0.036917\n0.075458\n0.249733\n0.160782\n\n\n2003-01-09\n1.8132\n3.4405\n1.6231\n1.4586\n0.64950\n124.90\n1.8102\n1.0377\n0.017026\n0.040535\n...\n0.033001\n0.192056\n0.101165\n-0.243567\n0.150383\n0.139864\n0.036917\n0.075458\n0.249733\n0.178999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-06-13\n1.6280\n5.7912\n1.4795\n0.9641\n0.84365\n169.35\n1.4553\n1.0765\n0.067120\n0.044007\n...\n0.047971\n0.062167\n0.025347\n-0.175750\n0.042737\n0.097854\n0.025751\n0.023605\n0.076653\n0.028596\n\n\n2024-06-14\n1.6232\n5.8261\n1.4823\n0.9668\n0.84468\n169.58\n1.4557\n1.0784\n0.065084\n0.046021\n...\n0.058594\n0.069012\n0.049797\n-0.222608\n0.068237\n0.106395\n0.032912\n0.025483\n0.065012\n0.026324\n\n\n2024-06-17\n1.6156\n5.7321\n1.4704\n0.9534\n0.84205\n167.80\n1.4464\n1.0686\n0.067882\n0.050889\n...\n0.058594\n0.069012\n0.058923\n-0.222608\n0.068237\n0.106395\n0.032912\n0.025483\n0.065012\n0.021923\n\n\n2024-06-18\n1.6246\n5.7841\n1.4726\n0.9561\n0.84573\n169.11\n1.4497\n1.0712\n0.071435\n0.053666\n...\n0.058594\n0.069012\n0.064863\n-0.222608\n0.068237\n0.106395\n0.032912\n0.025483\n0.065012\n0.018536\n\n\n2024-06-19\n1.6207\n5.8275\n1.4731\n0.9512\n0.84540\n169.41\n1.4510\n1.0715\n0.068695\n0.052024\n...\n0.058594\n0.069012\n0.067859\n-0.222608\n0.068237\n0.106395\n0.032912\n0.025483\n0.065012\n0.015635\n\n\n\n\n5497 rows × 30 columns\n\n\n\n\n\nPipeline\nAugmentSDMX can also be part of a Pipeline object, which minimises operational errors during modelling and avoids using testing data during training:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\npipeline = Pipeline([\n    ('augmentation', AugmentSDMX(sources={'BIS': 'WS_CBPOL_D'})),\n    ('imp', IterativeImputer(max_iter=10)),\n    ('forest', RandomForestRegressor())\n], verbose=True)\n\n\n\nTuning the data augmentation to enhance model performance\nAnd since AugmentSDMX can be included in a Pipeline, it can also be fine-tuned by parameter search techniques (such as grid search), further helping users make the best of available data to enhance performance of their models.\n\n\n\n\n\n\nTip\n\n\n\nUsers can cache the data augmentation step to avoid repeating potentially lengthy data downloads. See the memory argument in the sklearn.pipeline.Pipeline documentation.\n\n\n\ngrid = GridSearchCV(\n    estimator=pipeline,\n    param_grid={\n        'augmentation': ['passthrough', AugmentSDMX(sources={'ECB': 'CISS'})]\n    },\n    verbose=2,\n    cv=TimeSeriesSplit(n_splits=2)\n    )\n\ny_pred_grid = grid.fit(X_train, y_train).predict(X_test)\n\nFitting 2 folds for each of 2 candidates, totalling 4 fits\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.6s\n[CV] END ...........................augmentation=passthrough; total time=   1.6s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   3.4s\n[CV] END ...........................augmentation=passthrough; total time=   3.5s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   8.1s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.4s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   4.9s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time=  20.5s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  15.9s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.6s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=  11.0s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time=  41.9s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   5.3s\n\n\n\ngrid.best_params_\n\n{'augmentation': 'passthrough'}\n\n\n\nprint(f\"In this particular case, the best model was achieved by {'not ' if grid.best_params_['augmentation'] == 'passthrough' else ''}using the data augmentation.\")\n\nIn this particular case, the best model was achieved by not using the data augmentation.\n\n\n\nprint(f\"The last value in the training dataset was {y_train.tail(1).to_numpy()}. The predicted value was {y_pred_grid}, and the actual value was {y_test.to_numpy()}.\")\n\nThe last value in the training dataset was [5.8479]. The predicted value was [5.907591], and the actual value was [5.7921]."
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Automatic benchmark model",
    "section": "",
    "text": "A Benchmark object has a similar API to a sciki-learn estimator: you build an instance with the desired arguments, and fit it to the data at a later moment. Benchmarks is a convenience wrapper for reading the training data, passing it through a simplified pipeline consisting of data imputation and a standard scalar, and then the benchmark function calibrated with a grid search.\nA gingado Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a compare method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This compare method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators.\nBenchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark’s components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning."
  },
  {
    "objectID": "benchmark.html#scoring",
    "href": "benchmark.html#scoring",
    "title": "Automatic benchmark model",
    "section": "Scoring",
    "text": "Scoring\nClassificationBenchmark and RegressionBenchmark use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with gingado Benchmark objects can be found here."
  },
  {
    "objectID": "benchmark.html#data-split",
    "href": "benchmark.html#data-split",
    "title": "Automatic benchmark model",
    "section": "Data split",
    "text": "Data split\ngingado benchmarks rely on hyperparameter tuning to discover the benchmark specification that is most likely to perform better with the user data. This tuning in turn depends on a data splitting strategy for the cross-validation. By default, gingado uses StratifiedShuffleSplit (in classification problems) or ShuffleSplit (in regression problems) if the data is not time series and TimeSeriesSplit otherwise.\nThe user may overrun these defaults either by directly setting the parameter cv or default_cv when instanciating the gingado benchmark class. The difference is that default_cv is only used after gingado checks that the data is not a time series (if a time dimension exists, then TimeSeriesSplit is used).\n\nX, y = make_classification()\nbm_cls = ClassificationBenchmark(cv=TimeSeriesSplit(n_splits=3)).fit(X, y)\nassert bm_cls.benchmark.n_splits_ == 3\n\nX, y = make_regression()\nbm_reg = RegressionBenchmark(default_cv=ShuffleSplit(n_splits=7)).fit(X, y)\nassert bm_reg.benchmark.n_splits_ == 7\n\nPlease refer to this page for more information on the different Splitter classes available on scikit-learn, and this page for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a Benchmark object.\nUsers that wish to use specific parameters should include the actual Splitter object as the parameter, as done with the n_splits parameter in the chunk above."
  },
  {
    "objectID": "benchmark.html#custom-benchmarks",
    "href": "benchmark.html#custom-benchmarks",
    "title": "Automatic benchmark model",
    "section": "Custom benchmarks",
    "text": "Custom benchmarks\ngingado provides users with two Benchmark objects out of the box: ClassificationBenchmark and RegressionBenchmark, to be used depending on the task at hand. Both classes derive from a base class ggdBenchmark, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n\nthe simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a Benchmark object.\nif the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from ggdBenchmark and either implements custom fit, predict and score methods, or also subclasses from scikit-learn’s BaseEstimator.\n\nIn any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the fit method should call self._fit on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended)."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets for economic research",
    "section": "",
    "text": "load_BarroLee_1994 (return_tuple: 'bool' = True)\n\nLoads the dataset used in R. Barro and J.-W. Lee's \"Sources of Economic Growth\" (1994).\n\nArgs:\n    return_tuple (bool):  Whether to return the data in a tuple or jointly in a single pandas\n                          DataFrame.\n\nReturns:\n    pandas.DataFrame or tuple: If `return_tuple` is True, returns a tuple of (X, y), where `X` is a\n    DataFrame of independent variables and `y` is a Series of the dependent variable. If False,\n    returns a single DataFrame with both independent and dependent variables.\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\nIf you use this data in your work, please cite Barro and Lee (1994). A BibTeX code for convenience is below:\n@article{BARRO19941,\ntitle = {Sources of economic growth},\njournal = {Carnegie-Rochester Conference Series on Public Policy},\nvolume = {40},\npages = {1-46},\nyear = {1994},\nissn = {0167-2231},\ndoi = {10.1016/0167-2231(94)90002-7},\nurl = {https://www.sciencedirect.com/science/article/pii/0167223194900027},\nauthor = {Robert J. Barro and Jong-Wha Lee},\nabstract = {For 116 countries from 1965 to 1985, the lowest quintile had an average growth rate of real per capita GDP of - 1.3pct, whereas the highest quintile had an average of 4.8pct. We isolate five influences that discriminate reasonably well between the slow-and fast-growers: a conditional convergence effect, whereby a country grows faster if it begins with lower real per-capita GDP relative to its initial level of human capital in the forms of educational attainment and health; a positive effect on growth from a high ratio of investment to GDP (although this effect is weaker than that reported in some previous studies); a negative effect from overly large government; a negative effect from government-induced distortions of markets; and a negative effect from political instability. Overall, the fitted growth rates for 85 countries for 1965–1985 had a correlation of 0.8 with the actual values. We also find that female educational attainment has a pronounced negative effect on fertility, whereas female and male attainment are each positively related to life expectancy and negatively related to infant mortality. Male attainment plays a positive role in primary-school enrollment ratios, and male and female attainment relate positively to enrollment at the secondary level.}\n}\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n&lt;AxesSubplot:title={'center':'GDP growth'}, ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\nload_CB_speeches (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad Central Bankers speeches dataset from \nBank for International Settlements (2024). Central bank speeches, all years,\nhttps://www.bis.org/cbspeeches/download.htm.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n\nReturns:\n    A pandas DataFrame containing the speeches dataset.\n\nUsage:\n    &gt;&gt;&gt; load_CB_speeches()\n\n    &gt;&gt;&gt; load_CB_speeches('2020')\n\n    &gt;&gt;&gt; load_CB_speeches([2020, 2021, 2022])\nThis function downloads the Central bankers speeches dataset (2024) from the BIS website (www.bis.org). More information on the dataset can be found on the BIS website.\nIf you use this data in your work, please cite the BIS central bank speeches dataset, as follows (Please substitute YYYY for the relevant years):\n@misc{biscbspeeches\n    author = {{Bank for International Settlements}},\n    title = {Central bank speeches, YYYY-YYYY},\n    year = {2024},\n    url = {https://www.bis.org/cbspeeches/download.htm}\n}\n\n# Load speeches for 2020\nspeeches = load_CB_speeches(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\nurl\ntitle\ndescription\ndate\ntext\nauthor\n\n\n\n\n0\nhttps://www.bis.org/review/r200107a.htm\nShaktikanta Das: Journey towards inclusive gro...\nOpening remarks by Mr Shaktikanta Das, Governo...\n2020-01-07 00:00:00\nShaktikanta Das: Journey towards inclusive gro...\nShaktikanta Das\n\n\n1\nhttps://www.bis.org/review/r200108a.htm\nLuis de Guindos: Europe's role in the global f...\nKeynote speech by Mr Luis de Guindos, Vice-Pre...\n2020-01-08 00:00:00\nLuis de Guindos: Europe's role in the global f...\nLuis de Guindos\n\n\n2\nhttps://www.bis.org/review/r200108b.htm\nKlaas Knot: Interests and alliances\nOpening remarks by Mr Klaas Knot, President of...\n2020-01-08 00:00:00\n\"Interests and alliances\"\\nOpening remarks by ...\nKlaas Knot\n\n\n3\nhttps://www.bis.org/review/r200108c.htm\nLael Brainard: Strengthening the Community Rei...\nSpeech by Ms Lael Brainard, Member of the Boar...\n2020-01-08 00:00:00\nFor release on delivery\\n10:00 a.m. EST\\nJanua...\nLael Brainard\n\n\n4\nhttps://www.bis.org/review/r200108d.htm\nChristine Lagarde: Interview in \"Challenges\" m...\nInterview with Ms Christine Lagarde, President...\n2020-01-08 00:00:00\nChristine Lagarde: Interview in \"Challenges\" m...\nChristine Lagarde\n\n\n\n\n\n\n\n\n\n\n\n\nload_monpol_statements (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad monetary policy statements from multiple central banks.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n    \nReturns:\n    A pandas DataFrame containing the dataset.\n\nUsage:\n    &gt;&gt;&gt; load_monpol_statements()\n\n    &gt;&gt;&gt; load_monpol_statements('2020')\n\n    &gt;&gt;&gt; load_monpol_statements([2020, 2021, 2022])\nThis function downloads monetary policy statements from 26 emerging market central banks (Armenia, Brazil, Chile, Colombia, Czech Republic, Egypt, Georgia, Hungary, Israel, India, Kazakhstan, Malaysia, Mongolia, Mexico, Nigeria, Pakistan, Peru, Philippines, Poland, Romania, Russia, South Africa, South Korea, Thailand, Türkiye, Ukraine) as well as the Fed and the ECB (press-conference introductory statements). The dataset includes official English versions of statements for 1998-2023 (starting date varies depending on data availability). If you use this data in your work, please cite the dataset, as follows:\n@article{emcbcom,\n    author = {Tatiana Evdokimova and Piroska Nagy Mohácsi and Olga Ponomarenko and Elina Ribakova},\n    title = {Central banks and policy communication: How emerging markets have outperformed the Fed and ECB},\n    year = {2023},\n    institution = {Peterson Institute for International Economics}\n    url = {https://www.piie.com/publications/working-papers/central-banks-and-policy-communication-how-emerging-markets-have}\n}\n\n# Load monpol statements for 2020\nspeeches = load_monpol_statements(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\ncountry\ndate\nstatement\n\n\n\n\n0\nPoland\n2020-01-08\nWarsaw, 8 January 2020 Information from the me...\n\n\n1\nRomania\n2020-01-08\nIn its meeting of 8 January 2020, the Board of...\n\n\n2\nIsrael\n2020-01-09\nThe inflation environment remains low. The Nov...\n\n\n3\nPeru\n2020-01-09\nPRESS RELEASE MONETARY POLICY STATEMENT JANUAR...\n\n\n4\nEgypt\n2020-01-16\nCentral Bank of Egypt Press Release January 16..."
  },
  {
    "objectID": "datasets.html#real-datasets",
    "href": "datasets.html#real-datasets",
    "title": "Datasets for economic research",
    "section": "",
    "text": "load_BarroLee_1994 (return_tuple: 'bool' = True)\n\nLoads the dataset used in R. Barro and J.-W. Lee's \"Sources of Economic Growth\" (1994).\n\nArgs:\n    return_tuple (bool):  Whether to return the data in a tuple or jointly in a single pandas\n                          DataFrame.\n\nReturns:\n    pandas.DataFrame or tuple: If `return_tuple` is True, returns a tuple of (X, y), where `X` is a\n    DataFrame of independent variables and `y` is a Series of the dependent variable. If False,\n    returns a single DataFrame with both independent and dependent variables.\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\nIf you use this data in your work, please cite Barro and Lee (1994). A BibTeX code for convenience is below:\n@article{BARRO19941,\ntitle = {Sources of economic growth},\njournal = {Carnegie-Rochester Conference Series on Public Policy},\nvolume = {40},\npages = {1-46},\nyear = {1994},\nissn = {0167-2231},\ndoi = {10.1016/0167-2231(94)90002-7},\nurl = {https://www.sciencedirect.com/science/article/pii/0167223194900027},\nauthor = {Robert J. Barro and Jong-Wha Lee},\nabstract = {For 116 countries from 1965 to 1985, the lowest quintile had an average growth rate of real per capita GDP of - 1.3pct, whereas the highest quintile had an average of 4.8pct. We isolate five influences that discriminate reasonably well between the slow-and fast-growers: a conditional convergence effect, whereby a country grows faster if it begins with lower real per-capita GDP relative to its initial level of human capital in the forms of educational attainment and health; a positive effect on growth from a high ratio of investment to GDP (although this effect is weaker than that reported in some previous studies); a negative effect from overly large government; a negative effect from government-induced distortions of markets; and a negative effect from political instability. Overall, the fitted growth rates for 85 countries for 1965–1985 had a correlation of 0.8 with the actual values. We also find that female educational attainment has a pronounced negative effect on fertility, whereas female and male attainment are each positively related to life expectancy and negatively related to infant mortality. Male attainment plays a positive role in primary-school enrollment ratios, and male and female attainment relate positively to enrollment at the secondary level.}\n}\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n&lt;AxesSubplot:title={'center':'GDP growth'}, ylabel='Frequency'&gt;\n\n\n\n\n\n\n\n\n\n\nload_CB_speeches (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad Central Bankers speeches dataset from \nBank for International Settlements (2024). Central bank speeches, all years,\nhttps://www.bis.org/cbspeeches/download.htm.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n\nReturns:\n    A pandas DataFrame containing the speeches dataset.\n\nUsage:\n    &gt;&gt;&gt; load_CB_speeches()\n\n    &gt;&gt;&gt; load_CB_speeches('2020')\n\n    &gt;&gt;&gt; load_CB_speeches([2020, 2021, 2022])\nThis function downloads the Central bankers speeches dataset (2024) from the BIS website (www.bis.org). More information on the dataset can be found on the BIS website.\nIf you use this data in your work, please cite the BIS central bank speeches dataset, as follows (Please substitute YYYY for the relevant years):\n@misc{biscbspeeches\n    author = {{Bank for International Settlements}},\n    title = {Central bank speeches, YYYY-YYYY},\n    year = {2024},\n    url = {https://www.bis.org/cbspeeches/download.htm}\n}\n\n# Load speeches for 2020\nspeeches = load_CB_speeches(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\nurl\ntitle\ndescription\ndate\ntext\nauthor\n\n\n\n\n0\nhttps://www.bis.org/review/r200107a.htm\nShaktikanta Das: Journey towards inclusive gro...\nOpening remarks by Mr Shaktikanta Das, Governo...\n2020-01-07 00:00:00\nShaktikanta Das: Journey towards inclusive gro...\nShaktikanta Das\n\n\n1\nhttps://www.bis.org/review/r200108a.htm\nLuis de Guindos: Europe's role in the global f...\nKeynote speech by Mr Luis de Guindos, Vice-Pre...\n2020-01-08 00:00:00\nLuis de Guindos: Europe's role in the global f...\nLuis de Guindos\n\n\n2\nhttps://www.bis.org/review/r200108b.htm\nKlaas Knot: Interests and alliances\nOpening remarks by Mr Klaas Knot, President of...\n2020-01-08 00:00:00\n\"Interests and alliances\"\\nOpening remarks by ...\nKlaas Knot\n\n\n3\nhttps://www.bis.org/review/r200108c.htm\nLael Brainard: Strengthening the Community Rei...\nSpeech by Ms Lael Brainard, Member of the Boar...\n2020-01-08 00:00:00\nFor release on delivery\\n10:00 a.m. EST\\nJanua...\nLael Brainard\n\n\n4\nhttps://www.bis.org/review/r200108d.htm\nChristine Lagarde: Interview in \"Challenges\" m...\nInterview with Ms Christine Lagarde, President...\n2020-01-08 00:00:00\nChristine Lagarde: Interview in \"Challenges\" m...\nChristine Lagarde\n\n\n\n\n\n\n\n\n\n\n\n\nload_monpol_statements (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad monetary policy statements from multiple central banks.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n    \nReturns:\n    A pandas DataFrame containing the dataset.\n\nUsage:\n    &gt;&gt;&gt; load_monpol_statements()\n\n    &gt;&gt;&gt; load_monpol_statements('2020')\n\n    &gt;&gt;&gt; load_monpol_statements([2020, 2021, 2022])\nThis function downloads monetary policy statements from 26 emerging market central banks (Armenia, Brazil, Chile, Colombia, Czech Republic, Egypt, Georgia, Hungary, Israel, India, Kazakhstan, Malaysia, Mongolia, Mexico, Nigeria, Pakistan, Peru, Philippines, Poland, Romania, Russia, South Africa, South Korea, Thailand, Türkiye, Ukraine) as well as the Fed and the ECB (press-conference introductory statements). The dataset includes official English versions of statements for 1998-2023 (starting date varies depending on data availability). If you use this data in your work, please cite the dataset, as follows:\n@article{emcbcom,\n    author = {Tatiana Evdokimova and Piroska Nagy Mohácsi and Olga Ponomarenko and Elina Ribakova},\n    title = {Central banks and policy communication: How emerging markets have outperformed the Fed and ECB},\n    year = {2023},\n    institution = {Peterson Institute for International Economics}\n    url = {https://www.piie.com/publications/working-papers/central-banks-and-policy-communication-how-emerging-markets-have}\n}\n\n# Load monpol statements for 2020\nspeeches = load_monpol_statements(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\ncountry\ndate\nstatement\n\n\n\n\n0\nPoland\n2020-01-08\nWarsaw, 8 January 2020 Information from the me...\n\n\n1\nRomania\n2020-01-08\nIn its meeting of 8 January 2020, the Board of...\n\n\n2\nIsrael\n2020-01-09\nThe inflation environment remains low. The Nov...\n\n\n3\nPeru\n2020-01-09\nPRESS RELEASE MONETARY POLICY STATEMENT JANUAR...\n\n\n4\nEgypt\n2020-01-16\nCentral Bank of Egypt Press Release January 16..."
  },
  {
    "objectID": "datasets.html#simulated-datasets",
    "href": "datasets.html#simulated-datasets",
    "title": "Datasets for economic research",
    "section": "Simulated datasets",
    "text": "Simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nAll of the functions creating simulated datasets have a parameter random_state that allow for reproducible random numbers.\n\n\n\nfrom gingado.datasets import make_causal_effect\n\n\nmake_causal_effect\n\nmake_causal_effect (n_samples: 'int' = 100, n_features: 'int' = 100, pretreatment_outcome=&lt;function &lt;lambda&gt; at 0x0000021DAF1F8820&gt;, treatment_propensity=&lt;function &lt;lambda&gt; at 0x0000021DAF1F88B0&gt;, treatment_assignment=&lt;function &lt;lambda&gt; at 0x0000021DAF1F8940&gt;, treatment=&lt;function &lt;lambda&gt; at 0x0000021DAF1F89D0&gt;, treatment_effect=&lt;function &lt;lambda&gt; at 0x0000021DAF1F8A60&gt;, bias: 'float' = 0, noise: 'float' = 0, random_state=None, return_propensity: 'bool' = False, return_assignment: 'bool' = False, return_treatment_value: 'bool' = False, return_treatment_effect: 'bool' = True, return_pretreatment_y: 'bool' = False, return_as_dict: 'bool' = False)\n\nGenerates a simulated dataset to analyze causal effects of a treatment on an outcome variable.\n\nArgs:\n    n_samples (int): Number of observations in the dataset.\n    n_features (int): Number of covariates for each observation.\n    pretreatment_outcome (function): Function to generate outcome variable before any treatment.\n    treatment_propensity (function or float): Function to generate treatment propensity or a fixed value for each observation.\n    treatment_assignment (function): Function to determine treatment assignment based on propensity.\n    treatment (function): Function to determine the magnitude of treatment for each treated observation.\n    treatment_effect (function): Function to calculate the effect of treatment on the outcome variable.\n    bias (float): Constant value added to the outcome variable.\n    noise (float): Standard deviation of the noise added to the outcome variable. If 0, no noise is added.\n    random_state (int, RandomState instance, or None): Seed or numpy random state instance for reproducibility.\n    return_propensity (bool): If True, returns the treatment propensity for each observation.\n    return_assignment (bool): If True, returns the treatment assignment status for each observation.\n    return_treatment_value (bool): If True, returns the treatment value for each observation.\n    return_treatment_effect (bool): If True, returns the treatment effect for each observation.\n    return_pretreatment_y (bool): If True, returns the outcome variable of each observation before treatment effect.\n    return_as_dict (bool): If True, returns the results as a dictionary; otherwise, returns as a list.\n    \nReturns:\n    A dictionary or list containing the simulated dataset components specified by the return flags.\nmake_causal_effect creates a dataset for when the question of interest is related to the causal effects of a treatment. For example, for a simulated dataset, we can check that \\(Y_i\\) corresponds to the sum of the treatment effects plus the component that does not depend on the treatment:\n\n causal_sim = make_causal_effect(\n    n_samples=2000,\n    n_features=100,\n    return_propensity=True,\n    return_treatment_effect=True, \n    return_pretreatment_y=True, \n    return_as_dict=True)\n\n assert not np.any(np.round(causal_sim['y'] - causal_sim['pretreatment_y'] - causal_sim['treatment_effect'], decimals=13))\n\n\nPre-treatment outcome\nThe pre-treatment outcome \\(Y_i|X_i\\) (the part of the outcome variable that is not dependent on the treatment) might be defined by the user. This corresponds to the value of the outcome for any untreated observations. The function should always take at least two arguments: X and bias, even if one of them is unused; bias is the constant. The argument is zero by default but can be set by the user to be another value.\n\ncausal_sim = make_causal_effect(\n    bias=0.123,\n    pretreatment_outcome=lambda X, bias: bias,\n    return_assignment=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim['y'][causal_sim['treatment_assignment'] == 0] == 0.123)\n\nIf the outcome depends on specific columns of \\(X\\), this can be implemented as shown below.\n\ncausal_sim = make_causal_effect(\n    pretreatment_outcome=lambda X, bias: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias\n)\n\nAnd of course, the outcome might also have a random component.\nIn these cases (and in other parts of this function), when the user wants to use the same random number generator as the other parts of the function, the function must have an argment rng for the NumPy random number generator used in other parts of the function.\n\ncausal_sim_1 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\ncausal_sim_2 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim_1['X'].reshape(-1, 1) == causal_sim_2['X'].reshape(-1, 1))\nassert all(causal_sim_1['y'] == causal_sim_2['y'])\nassert all(causal_sim_1['pretreatment_y'] == causal_sim_2['pretreatment_y'])\n\n\n\nTreatment propensity\nThe treatment propensity of observations may all be the same, in which case treatment_propensity is a floating number between 0 and 1.\n\nsame_propensity_sim = make_causal_effect(\n    n_samples=485,\n    treatment_propensity=0.3,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nassert np.unique(same_propensity_sim['propensity']) == 0.3\nassert len(same_propensity_sim['propensity']) == 485\n\nOr it might depend on the observation’s covariates, with the user passing a function with an argument ‘X’.\n\nheterogenous_propensities_sim = make_causal_effect(\n    n_samples=1000,\n    treatment_propensity=lambda X: 0.3 + (X[:, 0] &gt; 0) * 0.2,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Heterogenously distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(heterogenous_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\nThe propensity can also be randomly allocated, together with covariate dependence or not. Note that even if the propensity is completely random and does not depend on covariates, the function must still use the argument X to calculate a random vector with the appropriate size.\n\nrandom_propensities_sim = make_causal_effect(\n    n_samples=50000,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Randomly distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(random_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\n\n\nTreatment assignment\nAs seen above, every observation has a given treatment propensity - the chance that they are treated. Users can define how this propensity translates into actual treatment with the argument treatment_assignment. This argument takes a function, which must have an argument called propensity.\nThe default value for this argument is a function returning 1s with probability propensity and 0s otherwise. Any other function should always return either 0s or 1s for the data simulator to work as expected.\n\ncausal_sim = make_causal_effect(\n    treatment_assignment=lambda propensity: np.random.binomial(1, propensity)\n)\n\nWhile the case above is likely to be the most useful in practice, this argument accepts more complex relationships between an observation’s propensity and the actual treatment assignment.\nFor example, if treatment is subject to rationing, then one could simulate data with 10 observations where only the samples with the highest (say, 3) propensity scores get treated, as below:\n\nrationed_treatment_sim = make_causal_effect(\n    n_samples=10,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    treatment_assignment=lambda propensity: propensity &gt;= propensity[np.argsort(propensity)][-3],\n    return_propensity=True,\n    return_assignment=True,\n    return_as_dict=True\n)\n\n\nrationed_treatment = pd.DataFrame(\n    np.column_stack((rationed_treatment_sim['propensity'], rationed_treatment_sim['treatment_assignment'])),\n    columns = ['propensity', 'assignment']\n    )\n\n\nrationed_treatment.sort_values('propensity')\n\n\n\n\n\n\n\n\npropensity\nassignment\n\n\n\n\n1\n0.096859\n0.0\n\n\n9\n0.214662\n0.0\n\n\n5\n0.274574\n0.0\n\n\n2\n0.327766\n0.0\n\n\n3\n0.340469\n0.0\n\n\n7\n0.448527\n0.0\n\n\n6\n0.692757\n0.0\n\n\n8\n0.699199\n1.0\n\n\n4\n0.699470\n1.0\n\n\n0\n0.752914\n1.0\n\n\n\n\n\n\n\n\n\nTreatment value\nThe treatment argument indicates the magnitude of the treatment for each observation assigned for treatment. Its value is always a function that must have an argument called assignment, as in the first example below.\nIn the simplest case, the treatment is a binary variable indicating whether or not a variable was treated. In other words, the treatment is the same as the assignment, as in the default value.\nBut users can also simulate data with heterogenous treatment, conditional on assignment. This is done by including a pararemeter X in the function, as shown in the second example below.\n\nbinary_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment: assignment,\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nassert sum(binary_treatment_sim['treatment_assignment'] - binary_treatment_sim['treatment_value'][0]) == 0\n\nHeterogenous treatments may occur in settings where treatment intensity, conditional on assignment, varies across observations. Please note the following:\n\nthe heterogenous treatment amount may or may not depend on covariates, but either way, if treatment values are heterogenous, then X needs to be an argument of the function passed to treatment, if nothing else to make sure the shapes match; and\nif treatments are heterogenous, then it is important to multiply the treatment value with the assignment argument to ensure that observations that are not assigned to be treated are indeed not treated (the function will return an AssertionError otherwise).\n\n\nhetereogenous_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment, X: assignment * np.random.uniform(size=X.shape[0]),\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nIn contrast to the function above, in the chunk below the function make_causal_effect fails because a treatment value is also assigned to observations that were not assigned for treatment.\n\ntry:\n    make_causal_effect(\n        treatment=lambda assignment, X: assignment + np.random.uniform(size=X.shape[0])\n    )\nexcept ValueError as e:\n    print(e)\n\nArgument `treatment` must be a function that returns 0 for observations with `assignment` == 0.\nOne suggestion is to multiply the desired treatment value with `assignment`.\n\n\n\n\nTreatment effect\nThe treatment effect can be homogenous, ie, is doesn’t depend on any other characteristic of the individual observations (in other words, does not depend on \\(X_i\\)), or heterogenous (where the treatment effect on \\(Y_i\\) does depend on each observation’s \\(X_i\\)). This can be done by specifying the causal relationship through a lambda function, as below:\n\nhomogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value: treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (homogenous_effects_sim['treatment_effect'] == homogenous_effects_sim['treatment_value']).all()\n\nheterogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value, X: np.maximum(X[:, 1], 0) * treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (heterogenous_effects_sim['treatment_effect'] != heterogenous_effects_sim['treatment_value']).any()"
  },
  {
    "objectID": "datasets.html#references",
    "href": "datasets.html#references",
    "title": "Datasets for economic research",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37.\n\n\nInternational Settlements, Bank for. 2024. “Central Bank Speeches.” https://www.bis.org/cbspeeches/download.htm."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Model documentation",
    "section": "",
    "text": "Each user has a specific documentation need, ranging from simply logging the model training to a more complex description of the model pipeline with a discusson of the model outcomes. gingado addresses this variety of needs by offering a class of objects, “Documenters”, that facilitate model documentation. A base class facilitates the creation of generic ways to document models, and gingado includes two specific model documentation templates off-the-shelf as described below.\nThe model documentation is performed by Documenters, objects that subclass from the base class ggdModelDocumentation. This base class offers code that can be used by any Documenter to read the model in question, format the information according to a template and save the resulting documentation in a JSON format. Documenters save the underlying information using the JSON format. With the JSON documentation file at hand, the user can then deploy existing third-party libraries to transform the information stored in JSON into a variety of formats (eg, HTML, PDF) as needed.\nOne current area of development is the automatic filing of some fields related to the model. The objective is to automatise documentation of the information that can be fetched automatically from the model, leaving time for the analyst to concentrate on other tasks, such as considering the ethical implications of the machine learning model being trained."
  },
  {
    "objectID": "documentation.html#modelcard",
    "href": "documentation.html#modelcard",
    "title": "Model documentation",
    "section": "ModelCard",
    "text": "ModelCard\nModelCard - the model documentation template inspired by the work of Mitchell et al. (2018) already comes with gingado. Its template can be used by users as is, or tweaked according to each need. The ModelCard template can also serve as inspiration for any custom documentation needs. Users with documentation needs beyond the out-of-the-box solutions provided by gingado can create their own class of Documenters (more information on that below), and compatibility with these custom documentation routines with the rest of the code is ensured. Users are encouraged to submit a pull request with their own documentation models subclassing ggdModelDocumentation if these custom templates can also benefit other users.\nLike all gingado Documenters, a ModelCard is can be easily created on a standalone basis as shown below, or as part of a gingado.ggdBenchmark object.\n\nmodel_doc = ModelCard()\n\nBy default, it autofills the template with the current date and time. Users can add other information to be automatically added by a customised Documenter object.\n\nmodel_doc_with_autofill = ModelCard(autofill=True)\nmodel_doc_no_autofill = ModelCard(autofill=False)\n\nBelow is a comparison of the model_details section of the model document, with and without the autofill.\n\nmodel_doc_with_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': '2024-02-27 08:47:49 ',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nmodel_doc_no_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': 'Model date',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nModelCard\n\nModelCard (file_path: 'str' = '', autofill: 'bool' = True, indent_level: 'int | None' = 2)\n\nA gingado Documenter based on @ModelCards\n\nautofill_template\n\nautofill_template (self)\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#forecastcard",
    "href": "documentation.html#forecastcard",
    "title": "Model documentation",
    "section": "ForecastCard",
    "text": "ForecastCard\nForecastCard is a model documentation template inspired by Mitchell et al. (2018), but with fields that are more specifically targeted towards forecasting or nowcasting use cases.\nBecause a ForecastCard Documenter object is targeted to forecasting and nowcasting models, it contains some specialised fields, as illustrated below.\n\nmodel_doc = ForecastCard()\n\nmodel_doc.show_template()\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"variable\": \"Variable(s) being forecasted or nowcasted\",\n    \"jurisdiction\": \"Jurisdiction(s) of the variable being forecasted or nowcasted\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"pipeline\": \"Description of the pipeline steps being used\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"econometric_model\": \"Information about the econometric model or technique\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"estimation_approaches\": \"How are the evaluation metrics calculated? Include information on the cross-validation approach, if used\"\n  },\n  \"data\": {\n    \"field_description\": \"Details on the dataset(s) used for the training and evaluation of the model\",\n    \"datasets\": \"Datasets\",\n    \"preprocessing\": \"Preprocessing\",\n    \"cutoff_date\": \"Cut-off date that separates training from evaluation data\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"Ethical considerations that went into model development, surfacing ethical challenges and solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"risks_and_harms\": \"What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n\n\nmodel_doc.show_json()\n\n{'model_details': {'variable': 'Variable(s) being forecasted or nowcasted',\n  'jurisdiction': 'Jurisdiction(s) of the variable being forecasted or nowcasted',\n  'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-27 08:47:50 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'pipeline': 'Description of the pipeline steps being used',\n  'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n  'econometric_model': 'Information about the econometric model or technique',\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'estimation_approaches': 'How are the evaluation metrics calculated? Include information on the cross-validation approach, if used'},\n 'data': {'datasets': 'Datasets',\n  'preprocessing': 'Preprocessing',\n  'cutoff_date': 'Cut-off date that separates training from evaluation data'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\n\nForecastCard\n\nForecastCard (file_path: 'str' = '', autofill: 'bool' = True, indent_level: 'int | None' = 2)\n\nA gingado Documenter for forecasting or nowcasting use cases\n\nautofill_template\n\nautofill_template (self)\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#preliminaries",
    "href": "documentation.html#preliminaries",
    "title": "Model documentation",
    "section": "Preliminaries",
    "text": "Preliminaries\nThe mock dataset below is used to construct models using different libraries, to demonstrate how they are read by Documenters.\n\nfrom sklearn.datasets import make_classification\n\n\n# some mock up data\nX, y = make_classification()\n\nX.shape, y.shape\n\n((100, 20), (100,))"
  },
  {
    "objectID": "documentation.html#gingado-benchmark",
    "href": "documentation.html#gingado-benchmark",
    "title": "Model documentation",
    "section": "gingado Benchmark",
    "text": "gingado Benchmark\n\nfrom gingado.benchmark import ClassificationBenchmark\n\n\n# the gingado benchmark\ngingado_clf = ClassificationBenchmark(verbose_grid=1).fit(X, y)\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n\n\n\n# a new instance of ModelCard is created and used to document the model\nmodel_doc_gingado = ModelCard()\nmodel_doc_gingado.read_model(gingado_clf.benchmark)\nprint(model_doc_gingado.show_json()['model_details']['info'])\n\n# but given that gingado Benchmark objects already document the best model at every fit, we can check that they are equal:\nassert model_doc_gingado.show_json()['model_details']['info'] == gingado_clf.model_documentation.show_json()['model_details']['info']\n\n{'_estimator_type': 'classifier', 'best_estimator_': RandomForestClassifier(max_features=None, n_estimators=250, oob_score=True), 'best_index_': 5, 'best_params_': {'max_features': None, 'n_estimators': 250}, 'best_score_': 0.93, 'classes_': array([0, 1]), 'cv_results_': {'mean_fit_time': array([0.17820907, 0.48922832, 0.19631364, 0.42252822, 0.24131639,\n       0.57173564]), 'std_fit_time': array([0.01348286, 0.03802951, 0.02532672, 0.01887258, 0.02557295,\n       0.06991078]), 'mean_score_time': array([0.0051146 , 0.01220322, 0.00839796, 0.01019723, 0.00599849,\n       0.01011045]), 'std_score_time': array([0.00056082, 0.00302682, 0.00915481, 0.00059421, 0.00319521,\n       0.00068275]), 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'max_features': 'sqrt', 'n_estimators': 100}, {'max_features': 'sqrt', 'n_estimators': 250}, {'max_features': 'log2', 'n_estimators': 100}, {'max_features': 'log2', 'n_estimators': 250}, {'max_features': None, 'n_estimators': 100}, {'max_features': None, 'n_estimators': 250}], 'split0_test_score': array([0.8, 0.8, 0.9, 0.9, 0.9, 0.9]), 'split1_test_score': array([0.9, 0.9, 0.9, 0.9, 0.9, 1. ]), 'split2_test_score': array([0.8, 0.8, 0.9, 0.8, 0.9, 0.9]), 'split3_test_score': array([1. , 0.9, 1. , 1. , 1. , 1. ]), 'split4_test_score': array([0.9, 1. , 0.9, 1. , 1. , 1. ]), 'split5_test_score': array([0.8, 0.8, 0.9, 0.8, 0.8, 0.8]), 'split6_test_score': array([1., 1., 1., 1., 1., 1.]), 'split7_test_score': array([0.8, 0.8, 0.8, 0.7, 0.8, 0.8]), 'split8_test_score': array([1. , 1. , 1. , 1. , 0.9, 1. ]), 'split9_test_score': array([1. , 0.9, 0.9, 0.9, 0.9, 0.9]), 'mean_test_score': array([0.9 , 0.89, 0.92, 0.9 , 0.91, 0.93]), 'std_test_score': array([0.08944272, 0.08306624, 0.06      , 0.1       , 0.07      ,\n       0.0781025 ]), 'rank_test_score': array([4, 6, 2, 4, 3, 1])}, 'multimetric_': False, 'n_features_in_': 20, 'n_splits_': 10, 'refit_time_': 0.6470375061035156, 'scorer_': &lt;sklearn.metrics._scorer._PassthroughScorer object at 0x000001D1015FB190&gt;}"
  },
  {
    "objectID": "documentation.html#scikit-learn",
    "href": "documentation.html#scikit-learn",
    "title": "Model documentation",
    "section": "scikit-learn",
    "text": "scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nsklearn_clf = RandomForestClassifier().fit(X, y)\n\n\nmodel_doc_sklearn = ModelCard()\nmodel_doc_sklearn.read_model(sklearn_clf)\nprint(model_doc_sklearn.show_json()['model_details']['info'])\n\n{'_estimator_type': 'classifier', 'classes_': array([0, 1]), 'estimator_': DecisionTreeClassifier(), 'estimators_': [DecisionTreeClassifier(max_features='sqrt', random_state=984336080), DecisionTreeClassifier(max_features='sqrt', random_state=387935964), DecisionTreeClassifier(max_features='sqrt', random_state=1119873145), DecisionTreeClassifier(max_features='sqrt', random_state=1050144277), DecisionTreeClassifier(max_features='sqrt', random_state=941639386), DecisionTreeClassifier(max_features='sqrt', random_state=844869905), DecisionTreeClassifier(max_features='sqrt', random_state=1084376571), DecisionTreeClassifier(max_features='sqrt', random_state=838680530), DecisionTreeClassifier(max_features='sqrt', random_state=2131844637), DecisionTreeClassifier(max_features='sqrt', random_state=281664374), DecisionTreeClassifier(max_features='sqrt', random_state=387195694), DecisionTreeClassifier(max_features='sqrt', random_state=1900725751), DecisionTreeClassifier(max_features='sqrt', random_state=1398287329), DecisionTreeClassifier(max_features='sqrt', random_state=107007245), DecisionTreeClassifier(max_features='sqrt', random_state=851863750), DecisionTreeClassifier(max_features='sqrt', random_state=1383879282), DecisionTreeClassifier(max_features='sqrt', random_state=217952844), DecisionTreeClassifier(max_features='sqrt', random_state=320258304), DecisionTreeClassifier(max_features='sqrt', random_state=653524562), DecisionTreeClassifier(max_features='sqrt', random_state=547335341), DecisionTreeClassifier(max_features='sqrt', random_state=65298601), DecisionTreeClassifier(max_features='sqrt', random_state=90961819), DecisionTreeClassifier(max_features='sqrt', random_state=1684018311), DecisionTreeClassifier(max_features='sqrt', random_state=655454563), DecisionTreeClassifier(max_features='sqrt', random_state=1786516343), DecisionTreeClassifier(max_features='sqrt', random_state=994298205), DecisionTreeClassifier(max_features='sqrt', random_state=1293597554), DecisionTreeClassifier(max_features='sqrt', random_state=1133785409), DecisionTreeClassifier(max_features='sqrt', random_state=847212468), DecisionTreeClassifier(max_features='sqrt', random_state=1194070920), DecisionTreeClassifier(max_features='sqrt', random_state=1506960310), DecisionTreeClassifier(max_features='sqrt', random_state=705803780), DecisionTreeClassifier(max_features='sqrt', random_state=1531279072), DecisionTreeClassifier(max_features='sqrt', random_state=1780828084), DecisionTreeClassifier(max_features='sqrt', random_state=815253514), DecisionTreeClassifier(max_features='sqrt', random_state=935926288), DecisionTreeClassifier(max_features='sqrt', random_state=19607747), DecisionTreeClassifier(max_features='sqrt', random_state=2111856427), DecisionTreeClassifier(max_features='sqrt', random_state=784991697), DecisionTreeClassifier(max_features='sqrt', random_state=1241164616), DecisionTreeClassifier(max_features='sqrt', random_state=1387079999), DecisionTreeClassifier(max_features='sqrt', random_state=168074365), DecisionTreeClassifier(max_features='sqrt', random_state=1788266150), DecisionTreeClassifier(max_features='sqrt', random_state=1189226461), DecisionTreeClassifier(max_features='sqrt', random_state=991777677), DecisionTreeClassifier(max_features='sqrt', random_state=242884733), DecisionTreeClassifier(max_features='sqrt', random_state=157311037), DecisionTreeClassifier(max_features='sqrt', random_state=2055890918), DecisionTreeClassifier(max_features='sqrt', random_state=1727577809), DecisionTreeClassifier(max_features='sqrt', random_state=269422783), DecisionTreeClassifier(max_features='sqrt', random_state=1389820690), DecisionTreeClassifier(max_features='sqrt', random_state=1929534429), DecisionTreeClassifier(max_features='sqrt', random_state=1414737041), DecisionTreeClassifier(max_features='sqrt', random_state=1247576130), DecisionTreeClassifier(max_features='sqrt', random_state=1353547520), DecisionTreeClassifier(max_features='sqrt', random_state=894390039), DecisionTreeClassifier(max_features='sqrt', random_state=158653442), DecisionTreeClassifier(max_features='sqrt', random_state=243266945), DecisionTreeClassifier(max_features='sqrt', random_state=455577999), DecisionTreeClassifier(max_features='sqrt', random_state=1371394730), DecisionTreeClassifier(max_features='sqrt', random_state=1451808483), DecisionTreeClassifier(max_features='sqrt', random_state=1589801337), DecisionTreeClassifier(max_features='sqrt', random_state=1380089697), DecisionTreeClassifier(max_features='sqrt', random_state=1541728708), DecisionTreeClassifier(max_features='sqrt', random_state=1730746642), DecisionTreeClassifier(max_features='sqrt', random_state=1569100702), DecisionTreeClassifier(max_features='sqrt', random_state=87894264), DecisionTreeClassifier(max_features='sqrt', random_state=182251102), DecisionTreeClassifier(max_features='sqrt', random_state=1598824537), DecisionTreeClassifier(max_features='sqrt', random_state=1566880189), DecisionTreeClassifier(max_features='sqrt', random_state=2036645957), DecisionTreeClassifier(max_features='sqrt', random_state=69788070), DecisionTreeClassifier(max_features='sqrt', random_state=725498370), DecisionTreeClassifier(max_features='sqrt', random_state=886772744), DecisionTreeClassifier(max_features='sqrt', random_state=1619512775), DecisionTreeClassifier(max_features='sqrt', random_state=847881750), DecisionTreeClassifier(max_features='sqrt', random_state=505640702), DecisionTreeClassifier(max_features='sqrt', random_state=338379070), DecisionTreeClassifier(max_features='sqrt', random_state=1789388801), DecisionTreeClassifier(max_features='sqrt', random_state=184042237), DecisionTreeClassifier(max_features='sqrt', random_state=796977047), DecisionTreeClassifier(max_features='sqrt', random_state=1147247523), DecisionTreeClassifier(max_features='sqrt', random_state=2106557009), DecisionTreeClassifier(max_features='sqrt', random_state=1638844864), DecisionTreeClassifier(max_features='sqrt', random_state=1373057286), DecisionTreeClassifier(max_features='sqrt', random_state=1356063932), DecisionTreeClassifier(max_features='sqrt', random_state=1318119814), DecisionTreeClassifier(max_features='sqrt', random_state=1196983038), DecisionTreeClassifier(max_features='sqrt', random_state=132896044), DecisionTreeClassifier(max_features='sqrt', random_state=152120878), DecisionTreeClassifier(max_features='sqrt', random_state=2100501339), DecisionTreeClassifier(max_features='sqrt', random_state=197690499), DecisionTreeClassifier(max_features='sqrt', random_state=1765855662), DecisionTreeClassifier(max_features='sqrt', random_state=1579978128), DecisionTreeClassifier(max_features='sqrt', random_state=1581716817), DecisionTreeClassifier(max_features='sqrt', random_state=1439289735), DecisionTreeClassifier(max_features='sqrt', random_state=1808084503), DecisionTreeClassifier(max_features='sqrt', random_state=1398215665), DecisionTreeClassifier(max_features='sqrt', random_state=2058382938), DecisionTreeClassifier(max_features='sqrt', random_state=304859939)], 'estimators_samples_': [array([73, 68, 87, 11, 98, 49, 84, 67,  6, 53, 16, 59, 41, 13, 23, 79, 30,\n        1, 62, 40, 36, 25, 46, 66, 30, 71, 26, 72, 22, 69, 68, 10, 60, 72,\n       96, 59, 77, 67, 77, 89, 59, 81, 49, 84, 11, 44, 40, 29, 46, 33, 43,\n       78, 48, 82, 77, 60, 67, 24, 59,  3, 33, 28, 62, 36,  0, 16, 88, 85,\n       44, 88,  9, 67, 83, 27, 41, 41, 24,  0, 35, 59, 82, 99, 66, 72,  2,\n       27, 61, 54, 61, 55,  3, 97, 38, 12,  8, 48, 99, 35, 61,  8]), array([58, 87,  7, 15, 26, 50, 50, 23, 37, 50, 13, 53, 61, 56, 76, 42, 52,\n       64, 32,  1, 50,  9, 94, 56, 44, 93,  6, 47, 61, 87, 33, 50, 37,  5,\n       58, 68, 27, 33, 42, 86, 71, 11, 14, 60, 78, 11, 86, 97, 27, 95, 38,\n       85, 66, 43, 85, 58, 10, 66, 62, 17, 76, 42, 43,  8, 85,  8, 97, 41,\n       46, 68, 28, 71, 18, 21, 33, 66, 92, 24, 65, 91,  6, 21, 39, 99, 78,\n       68, 57, 28,  3, 56, 82, 82, 68, 21, 85, 39, 37, 19, 18,  4]), array([98, 52, 38, 15, 45, 45, 59, 63, 63,  5,  0, 95, 88, 22, 34,  9, 79,\n       49, 15,  8, 80, 44, 45, 20, 16, 92, 51, 94, 76, 20, 42, 65, 98, 52,\n       19, 19, 38, 75, 50, 24,  7, 34, 91,  5, 18, 15, 95, 60, 90, 94, 42,\n       16, 54, 44, 28, 80, 85,  3, 17,  3, 12, 29, 20, 29, 28, 87, 10, 21,\n       98, 17, 78, 90, 59, 60, 20,  0, 95, 13, 16,  3, 93, 14, 26, 38, 17,\n       58, 37, 81, 47, 92,  6, 32, 55,  1, 76,  3, 81, 33, 53, 63]), array([21, 55, 28, 35, 51, 79, 78, 70, 19, 66, 19, 20, 67, 21, 96, 18, 20,\n       79, 51, 87, 69, 54, 99,  3, 99, 35, 94, 42, 97, 37, 64, 91, 64, 75,\n       73,  2, 53, 10, 59, 97, 42, 21, 84, 61, 65, 29,  2,  6, 26, 46, 37,\n       66, 78, 17, 17, 83, 63, 27, 64, 53, 25, 10, 18, 97, 84,  7, 95, 59,\n       54, 55, 33, 56,  5,  5, 65,  1,  1, 91, 67, 99, 22, 14, 13, 14, 88,\n       84,  8, 74, 39, 26, 33, 88, 81, 56, 61, 49, 76, 40, 14, 73]), array([68, 98, 66, 11, 17, 79, 81, 54, 46, 92, 82,  9, 91, 98, 61, 77, 16,\n       23, 68, 58, 36, 98, 43, 74, 51, 69, 78, 41, 56, 57, 70, 84, 39, 52,\n       93,  0, 96,  9, 55, 54, 68,  9, 34, 94, 28, 13, 63, 28, 58, 18, 46,\n       56, 42, 74, 45, 77, 43, 68, 35, 55,  9,  5,  3, 49, 59, 27,  6, 42,\n       49, 23, 61, 13, 50, 19, 41, 30, 28, 23, 27, 20, 63, 57, 68, 82,  4,\n       72, 66, 70, 86, 95, 60, 78,  9, 37, 65, 13, 88,  0, 20, 73]), array([56, 30, 68, 59, 97, 18, 96, 63, 78, 42, 32, 51, 16, 51, 92, 85, 74,\n       26, 42, 43, 65, 34, 55, 40, 97, 48, 87, 38, 93, 79, 11, 94, 84, 11,\n       58, 10, 90, 79, 62,  3, 46, 22,  0, 53, 54, 45, 75, 71, 45, 15, 95,\n       13, 84, 58, 35, 48, 12, 48, 99, 71, 44, 98, 86, 28, 89, 51,  4,  0,\n       68, 28, 66, 81, 93, 64, 37, 50, 48, 90, 68, 77, 56, 73, 87, 60, 57,\n       88, 36, 87, 28, 71, 50, 25, 75, 27, 54,  5, 76,  6,  0, 70]), array([29, 87, 45, 48,  6, 96, 86, 96,  0, 83, 18, 61,  7, 70, 37, 29, 33,\n       50, 45, 85,  4, 57, 67, 19, 57, 84,  8, 58, 36, 11,  5, 60, 88, 75,\n       65, 79, 15, 67, 87, 34, 11, 83, 66, 55, 99,  7, 17, 53, 68, 23, 64,\n       90, 41, 43, 88, 71,  8, 30, 94, 74, 92, 48, 97, 76,  1, 27, 47, 43,\n        4, 41, 27, 74,  5,  4, 50, 43, 80, 75, 72, 30, 70, 74, 53, 64, 69,\n       40, 15, 81, 65, 61, 68,  8, 54, 43,  0, 63, 54, 74, 71, 93]), array([32, 45, 35, 17, 32, 88,  7, 56, 31, 10, 75, 58, 41, 29, 32, 10, 32,\n       22, 99, 17, 74, 55,  1,  3, 95, 82,  7, 79, 86, 90, 80, 33, 89, 30,\n       33, 60, 33, 20, 24, 54,  5,  3, 97, 77, 36, 53, 54, 22, 46, 88,  1,\n       98, 50, 54,  4, 58, 77, 57, 15, 38, 19, 59, 91, 90,  8, 68, 89, 20,\n       84, 77, 79, 42, 35, 72, 93,  5, 33, 52, 50, 63, 92, 73,  7, 72, 45,\n       76, 58, 78, 50, 31, 16,  8, 20, 73, 48,  4, 76, 61, 19, 41]), array([32, 21, 68, 10, 21, 53, 29, 64, 76, 40, 57, 36, 67, 44, 28, 96, 67,\n       37, 85, 17, 60, 78, 74, 96, 28, 70, 60, 75, 61, 71, 41, 14, 40, 61,\n       96, 67, 58, 19, 81, 14, 90, 52, 26, 47, 10, 77, 82,  0, 29, 27,  8,\n       87, 23, 65, 65, 66, 32, 97,  7, 65, 59, 91, 84, 11, 32, 53, 48, 93,\n       76, 94, 55, 68, 56, 98, 61, 89, 85, 86, 73, 71, 81,  0, 93, 53, 68,\n       83, 31,  9, 42, 63, 40, 22, 53, 18, 87, 56, 90, 46, 87, 81]), array([85, 15, 91, 50, 60, 63, 66, 80, 80, 94, 62, 39, 90, 86, 85, 62, 28,\n       12, 42, 78, 21, 32, 99, 92, 72, 24, 56, 93, 43, 61,  6, 23, 34, 28,\n       29, 13, 49, 96, 22, 65, 73, 81, 87, 46, 37, 21, 31, 55, 42, 20, 44,\n       84, 13, 52, 56,  7, 68, 53, 65, 40, 95, 40,  8, 28,  5, 50, 53, 71,\n       84, 51, 79, 74, 14, 57, 58, 81, 38, 60, 27, 32, 30, 35, 29, 70, 11,\n       96, 28, 46, 22, 14, 83, 14, 37, 73, 30, 70,  4, 54, 15, 43]), array([23, 56, 70,  4, 69, 20, 43,  4, 16, 16, 60, 70, 82, 78, 37, 68, 29,\n       47,  7, 25, 59, 16, 99, 50, 52, 90, 52,  3, 11, 71, 96, 14, 94, 48,\n       73, 49, 43, 70, 65, 99,  8, 66, 37, 61, 91, 32, 35, 74, 14, 82, 79,\n       68, 18, 75, 30, 83, 77, 10, 57, 74, 48, 53,  6, 65, 62, 57, 39, 46,\n       59, 76, 86, 95, 79,  4, 51, 67, 14, 40, 31, 75, 44, 61, 83,  9, 14,\n       94, 57, 44,  6, 74, 95,  3,  9, 12, 53, 87,  6, 23, 96, 27]), array([60,  0, 43, 16, 83, 67, 14, 51, 66, 89, 19, 69, 12, 33, 30, 45,  4,\n       55, 99, 65, 29, 82, 17,  9, 47, 54, 26, 94, 28, 12, 12, 76, 53, 10,\n       31, 63, 83, 47, 55, 34, 86, 91, 18, 72, 87,  4, 26, 87,  1, 97, 40,\n       98, 75, 71, 38, 53, 84, 61, 66,  5, 25, 67, 55, 39, 98, 11, 71, 89,\n       36, 64, 79, 80, 42, 37, 51, 64, 67, 97, 85, 61,  9, 84, 82, 60, 46,\n        4, 70, 51, 39, 90, 66, 90,  5, 66, 28, 59, 34, 79, 91, 95]), array([29,  0,  8, 93, 93, 85, 60, 63, 60, 94,  9, 28, 93, 14, 52, 96, 24,\n       28, 14, 86,  7, 95,  1, 48,  0, 58, 45, 82, 59, 37,  1, 70,  3, 17,\n        2, 88, 56, 68, 60, 59,  7, 28, 60, 49, 59, 78, 60, 32, 99, 50, 97,\n       93, 55, 90, 65, 30, 88, 11, 34, 76,  4, 85, 50, 77, 47, 15, 98, 28,\n       26, 97, 55, 84, 53, 88, 19, 28, 12, 44, 68, 99, 11, 94, 76, 18, 52,\n       76, 52, 56, 39, 74, 16, 38, 52,  1, 12, 59,  5, 16, 86, 74]), array([54, 40, 31, 33, 71, 43, 46, 71, 69, 23, 29, 77, 34, 18, 47, 62, 73,\n       97, 84, 84, 41, 32, 89,  2, 94, 20,  0, 65, 65,  8, 26, 95, 75, 77,\n       35, 92, 25, 23, 62, 44, 59, 18,  3, 32, 65, 43, 24, 19, 26,  0, 31,\n       63, 20, 66, 93, 24, 56, 13, 95, 70, 22, 96, 55, 41, 34, 30, 93, 92,\n       66, 93, 60, 39, 76, 36, 68, 93,  0, 15, 19, 96, 93, 65, 86, 69, 78,\n       39, 19, 76, 75, 96, 55, 41, 99, 26,  5, 69, 34,  6, 91, 30]), array([16,  2, 21, 98, 71,  0, 50, 62, 14, 36, 72, 56, 12, 52, 60, 26, 17,\n       51, 81, 33, 76, 20, 60, 45,  2, 72, 66, 10, 54, 16, 77,  7, 50,  3,\n        9, 68, 91, 34, 24, 38, 84, 10, 89, 14, 68, 49, 81, 64, 17, 72,  8,\n       35, 84, 80, 99, 57, 67, 30, 16, 73, 22, 71, 64,  4, 22, 56,  6, 38,\n       36, 70, 89, 81, 88, 88, 99, 91, 94, 60, 10, 82, 79, 94, 33, 34, 12,\n       52, 52, 64, 40, 73, 85, 85, 19, 26, 42, 69, 86,  0, 67,  4]), array([67, 92, 11, 45, 50, 97, 30, 11, 96, 96, 46,  0, 78, 43, 40, 16, 80,\n       73, 46,  4, 95, 40, 60, 17, 17,  6, 10, 96, 24,  7, 99, 71, 52, 35,\n       30, 82, 97, 49, 21, 58, 12, 77, 76, 98, 79, 20,  6,  2, 27, 25, 82,\n       24,  9, 19, 70, 82, 27, 61, 78, 23, 76, 44,  7, 23, 21, 85, 50, 92,\n       76, 40,  0, 73, 55, 51, 47, 65, 15, 64, 14, 29, 84, 82, 91, 97, 84,\n       78, 63, 34, 45, 97, 70, 10, 13, 69, 37, 49, 70, 48,  7,  5]), array([71, 80, 94, 34, 87, 33, 12, 88, 61, 92, 74, 88, 38, 11, 20, 17, 14,\n        2, 69, 43, 38, 24, 42, 81, 17, 75, 23, 49, 15, 46, 89, 70,  7,  3,\n       44, 28, 49, 35, 78, 22, 60, 30, 57, 97, 10, 60, 10, 65, 52, 14, 87,\n       91, 87, 44, 78, 22,  9, 21, 19, 63, 23, 41, 98, 70, 12, 66, 33,  1,\n       51, 95, 55, 10, 52,  2,  2, 98, 67, 82, 30, 43, 90, 46, 88, 78, 45,\n       82, 86, 49, 15, 66,  4, 32, 64, 53,  2, 94, 56, 19, 97, 53]), array([56, 88,  4,  4, 40, 43, 43, 75, 60, 37,  0,  8, 40, 24, 24, 15,  2,\n       79, 12, 81, 26, 70, 61,  6, 67, 88, 26, 23, 49, 86, 36, 98, 56,  3,\n       26, 40, 76, 50, 58, 55, 83, 88, 41, 51, 60, 86, 88, 48, 36, 19, 43,\n       69, 20, 80, 42, 74, 93, 94, 55, 82, 79, 12, 81, 37, 98, 33, 12, 44,\n       44, 86, 79, 63, 66, 32, 66,  5, 89, 27, 12, 53, 64, 29, 24, 78, 86,\n       82, 44, 59, 87, 59, 66, 49, 95, 44, 20, 78, 94, 24,  2, 18]), array([33, 44, 27, 12,  5, 62, 31, 17, 12, 74,  7, 37, 38, 40, 48, 50, 68,\n       80, 45, 77, 16,  5, 65,  0, 24, 88, 78, 57, 73, 46, 65, 59, 87,  5,\n       24, 80, 96, 25, 95, 95, 65, 60, 41, 57, 24, 98, 38, 63, 12, 10, 28,\n       20, 71, 17, 53, 94,  7, 11, 74, 39, 32, 14, 64,  9, 77, 47, 73,  4,\n       31, 73, 29,  6, 50, 26, 98, 51, 10, 53, 50, 37, 90, 50, 67, 12, 71,\n       79, 44, 29, 14, 88, 14, 27, 91, 99, 89, 52,  6, 28, 46, 73]), array([38, 85, 25, 18, 53, 18, 68,  8, 41, 75, 87, 38, 80, 43, 54, 92, 74,\n       10, 81, 32, 74, 98, 93,  7, 10, 69, 63,  0, 74, 32, 27, 28, 83,  5,\n       81, 55, 80, 56, 95, 86, 79, 63, 11, 53, 87, 87, 21,  1, 99, 95, 74,\n       69, 83, 65, 38,  1, 65, 49, 67, 98, 43, 19, 77, 42, 40, 43, 76, 19,\n       29, 26, 98, 88, 67,  5, 35, 67, 56, 89, 98, 45, 72, 51, 43, 95, 31,\n       26, 81, 77, 58, 19,  3, 30,  6, 15, 69, 72, 28, 28, 64, 67]), array([64, 26, 75, 91, 87, 78, 16, 14, 85, 95, 84, 94, 54, 62, 71, 51, 77,\n       17, 92, 84, 18, 56, 37, 34, 63, 13, 26, 39, 24, 83, 61, 49, 28, 19,\n       14,  3, 11, 15, 13, 28, 11, 69, 73, 36, 77, 91, 19,  3, 29, 32, 38,\n       24,  9, 73, 16, 48, 26, 36, 62, 27, 60, 57, 73, 64, 15, 19, 18, 47,\n       90, 71, 32, 55, 65, 47, 81, 10, 64, 82, 67, 59, 41, 59, 36, 92, 46,\n       87, 88, 54, 55,  3, 64, 22, 85, 18, 71, 80, 16, 58, 52, 96]), array([57, 85, 23, 40, 66, 75, 57, 77, 30, 58, 74, 24, 64, 22, 55, 34, 66,\n        5, 86, 15, 31, 50, 89, 83, 11, 88, 75, 52, 93, 78, 91, 39, 38, 23,\n       66, 36,  5, 95, 82, 69, 90, 93, 50, 16, 60, 44, 97, 59, 27, 11, 16,\n        9, 59, 18, 73, 12, 82, 34,  0,  3, 51, 78, 96, 40, 41, 23, 53, 80,\n       31, 12,  7, 31, 72,  0, 36, 78,  3, 81, 34, 48, 97,  0, 53, 22, 32,\n       56,  2, 37, 18, 27, 42, 46,  0, 68, 95, 43, 59, 46, 68,  6]), array([ 3, 35, 99, 10, 52, 57, 12, 70,  3, 18, 23, 34, 26, 76, 97, 55, 92,\n       54, 53, 50, 70, 85, 60, 66, 15, 61, 14, 69, 99, 93, 43, 55, 95, 16,\n       61, 14, 84, 65, 17, 63,  0, 44, 74, 27, 44, 75, 31, 68, 94, 32, 45,\n       88, 32, 39, 80, 21, 15, 63, 41, 41,  7, 21,  6, 76, 30, 92, 59, 64,\n       53, 24, 56, 33, 89, 48, 64, 79, 55, 37, 22, 85, 81, 19, 14, 53, 61,\n       74, 27, 12, 72, 52, 37, 68, 88, 63, 84,  3, 93, 29,  9, 90]), array([79,  4, 55,  6,  2, 48, 40, 56, 47, 17, 27, 85, 79, 92, 68,  0, 21,\n        2, 78, 65, 74, 12, 92, 67, 13, 86, 60, 90, 48, 20, 15, 59, 30,  2,\n       36, 81, 76, 87, 76, 64, 45, 43, 27, 48, 17, 15, 61, 27, 52, 69, 32,\n       44, 39, 76,  4, 23, 82, 12, 37, 15, 94, 82, 90,  5, 12, 55,  4, 21,\n       73, 94, 37, 21, 39, 25, 99, 93, 29, 93, 16,  3, 51,  4, 54, 45, 32,\n       23, 93, 25, 45, 85, 77,  5, 79, 98,  8, 57, 91, 47, 90, 59]), array([67, 73, 32, 90, 72, 19,  1, 64, 19, 49, 55, 84, 95, 56, 65, 63, 73,\n       23, 50, 22, 43, 17, 32, 19, 66, 65, 62,  3, 88, 40, 44, 80,  0, 55,\n       23,  1, 50, 68, 22,  7, 46, 58, 71, 71,  7, 96,  4, 29, 71,  3, 32,\n       49, 98, 69, 65,  6, 62, 43, 40, 74, 20, 25, 94, 25, 71, 12,  2, 41,\n       73, 70, 73, 62, 71, 73,  4, 34, 41,  4, 37,  5, 58, 63, 52, 14, 87,\n       65, 53, 90, 52, 97, 52, 95, 80, 19, 17,  9, 98, 40, 87, 29]), array([38, 31,  0, 89, 85, 86, 99, 45, 92, 35,  0, 68, 82, 85, 28, 85, 12,\n        3, 80, 34,  7, 39, 22, 62, 73, 12,  4, 13, 31, 90, 85, 88, 61, 54,\n       84, 80,  0, 64, 47, 99, 50, 93, 35, 92, 24,  4, 11, 22, 21,  1, 96,\n       85,  3, 59, 36, 39, 52, 99, 43, 69, 81, 19, 16, 43, 38, 30, 14, 77,\n       68, 79, 47, 51, 90, 67, 89, 51, 28, 95, 82,  3, 33,  7,  4, 67, 70,\n       57, 34, 64, 47, 21, 85, 97,  1, 36, 22, 36, 36, 66, 26, 59]), array([74, 93, 27, 12, 95, 28, 45, 93, 96, 11, 77, 43, 66, 31, 23, 11, 52,\n        6, 20, 53, 53, 95, 41, 35, 70, 68, 96, 41, 71, 86,  1,  8, 56, 47,\n       81, 65, 99,  7, 93, 95, 89, 68, 86, 68, 65, 37,  1, 16, 59,  2,  1,\n       58, 50, 98, 53, 58, 16, 62, 87, 56, 18, 38, 32, 86, 35, 50, 10, 33,\n       40,  3, 37, 68,  5, 92, 78, 84, 24, 68,  9, 89, 43, 21, 55,  4, 34,\n       12, 73, 43, 70, 55, 39, 61, 55, 69, 40, 15,  5, 52, 27, 53]), array([ 2, 59, 35, 11, 82, 39, 32, 81, 23, 33, 52, 27, 31,  3, 32,  7, 79,\n        9, 45, 69, 53, 77, 56, 14, 83, 50, 76, 41, 52, 10, 91, 74, 37, 33,\n       43, 49, 46, 38, 85, 24, 19, 47, 87, 80, 93, 61, 81, 41, 77,  8, 47,\n       89, 47, 45, 88, 31, 96, 19, 41, 36, 16, 47, 76, 73, 74, 84, 30, 40,\n       92, 27, 34, 68, 65, 99, 75, 30, 26, 66, 43, 58, 15, 19, 12, 16,  2,\n       20, 98, 74, 65, 45, 55,  8, 31, 84, 47,  8, 55, 78, 91, 42]), array([39, 14, 19, 95, 31, 92,  3, 22, 19, 59, 25, 78, 85, 44, 36, 82, 38,\n        1, 39, 67, 49, 63, 24, 67,  4, 10, 66, 76, 66, 87, 37, 14, 57, 39,\n       17, 69, 46,  3, 42, 74, 24, 74, 68,  7, 81, 81, 13,  6, 10, 51, 82,\n       61, 98, 60, 37, 27, 44,  4, 21, 13, 31, 91,  7, 36, 12, 84, 85, 11,\n       16, 17, 52, 23, 98, 18, 70, 47, 72, 45, 34, 56, 76, 80, 66,  1, 56,\n       89, 51, 56, 53, 38, 71, 10, 57, 88, 50, 60, 38, 28, 33, 78]), array([68, 31, 77, 50, 19, 49, 87, 59, 32, 10, 52, 40, 42,  9, 49, 32, 91,\n       91, 30, 92, 88, 45, 85, 17, 85, 95, 33, 60, 30, 76, 71,  6, 38, 30,\n       74, 83, 89, 39, 38, 30, 13, 42, 89, 87, 79, 77, 69, 53, 34,  3, 13,\n       32, 98, 80, 99, 50, 33, 38,  8, 62, 93, 79, 97, 46, 64, 72, 37, 52,\n       79,  3, 27, 72, 39, 39, 12, 32, 42, 28, 78, 39, 76, 11, 60, 57, 85,\n        1, 96, 69,  5,  5, 55, 14, 85, 89, 40, 51, 74, 57, 58, 50]), array([17, 42, 16, 57, 12, 17, 69, 84, 57, 74, 30, 47, 21, 44, 23, 50, 76,\n       86, 92, 84, 53, 49, 54, 81, 84, 23, 89, 13, 52, 65, 81, 77, 93, 25,\n       91, 81, 17, 42, 14, 32, 65, 83,  2, 86, 52, 64, 23, 82, 74, 82, 17,\n       97, 87, 72, 73, 77, 87,  6, 87,  6, 46, 76,  0, 30, 79, 32, 43, 98,\n       42, 51, 71, 69, 57, 45, 30, 12, 18, 54, 90, 11, 32, 92, 88, 56, 26,\n       43, 62, 57, 90, 96, 61, 76, 79, 55, 76, 58, 64,  1, 23, 59]), array([74, 54, 90, 19, 62, 90, 40, 28, 29, 68, 13, 80, 49, 65, 98, 20, 61,\n       71, 71, 55, 22, 74, 13, 17,  1, 58, 57, 66, 80, 70, 86, 42, 44, 88,\n       62, 36, 79,  4, 92, 44, 65, 11, 63, 47, 91, 58, 45, 44, 75, 27, 47,\n        4, 36, 43, 17, 20, 10,  8, 30, 67, 60, 74, 52, 48, 49, 76, 12, 31,\n       18, 43, 85, 53, 41, 46,  0, 31, 81, 75, 52, 77, 33, 52, 39, 77, 67,\n        6, 51, 90, 91, 63, 40, 69, 79, 14, 51, 35,  4, 89, 62, 47]), array([20, 35, 99, 91, 15, 45, 78, 27, 68, 25, 98,  1, 40, 16,  7, 52, 44,\n        5, 65, 43, 27, 25, 61,  5, 54,  5, 72, 79, 71,  8, 34, 45, 61, 82,\n       95, 18, 56, 84, 70,  7, 42, 35, 95, 65, 37, 63, 96, 73, 39, 44, 64,\n       99, 55, 69, 45, 94, 55, 83, 86, 64, 41, 67, 65, 64, 75, 80, 48, 87,\n       98,  5, 83, 42, 12, 51,  3, 56, 70, 32, 70, 90, 86, 19, 52, 22, 11,\n        9, 46, 61, 72, 75, 39,  8, 39, 72, 82, 56, 12, 60, 55, 33]), array([64, 97, 63, 50, 28, 93, 62, 97, 24, 57, 91, 19, 99, 81, 48, 40, 39,\n       46, 36, 87, 20, 66, 30, 71, 96, 54, 54, 41, 59, 19, 42, 89, 38, 42,\n       20, 69, 62, 46,  8, 93, 91, 92, 12, 72, 10, 86, 92, 30, 38, 73, 61,\n       99, 53, 56, 80, 88,  3, 79, 28, 95, 27,  7, 73, 83, 52, 67, 55, 59,\n       43, 61, 80, 73, 45, 57,  4, 96, 92, 26, 92, 29, 42, 63, 51, 74, 72,\n       86, 68, 17, 85, 13, 24, 82, 26, 96, 23, 41, 54, 50,  4, 68]), array([56, 32, 39,  8, 36, 49, 51, 68, 48, 50, 51, 48, 29, 94, 78, 95, 79,\n       18, 91, 17, 90, 95, 59, 70, 26, 68, 83, 88, 13, 61, 84, 98, 76, 87,\n       73, 35, 83, 39, 34, 14, 80,  6, 44, 58, 82, 25, 62, 69, 42, 72, 46,\n       97, 36, 46, 39, 33, 68, 48, 98,  4, 33, 85, 88, 29, 68, 95, 42, 11,\n       83,  3,  0, 54, 44, 36, 34, 48, 96, 37, 37, 38, 18, 71, 41, 67, 46,\n       93, 73, 74, 81, 93, 58, 99, 48, 51, 75,  7, 33, 45, 38, 61]), array([67, 91, 21, 53, 95, 93, 66, 77, 43, 34, 35,  5, 87, 58,  4, 12,  1,\n       40, 93, 26, 36, 37, 95, 34, 31, 44, 58, 16,  1, 99, 20, 83, 46, 16,\n       26, 71, 19,  7, 55, 57, 32, 57, 97, 48, 59,  0, 82, 15, 90, 27, 71,\n       99, 69, 59, 93, 99,  3, 64, 20,  4, 62, 17,  8, 23, 47,  8, 23, 45,\n       38, 35, 53, 90, 55, 47, 72, 67, 90, 23, 76, 67, 57, 97, 40, 28, 15,\n       26, 81,  5, 99, 15, 23, 35, 45, 24, 68, 71, 72, 20, 50, 34]), array([52, 43, 20,  4, 48, 89,  4, 19, 31, 40,  3,  3, 20, 42, 81, 27, 30,\n        4, 80,  8,  1, 24,  2, 99, 17, 20, 18, 28, 43, 96, 23, 78, 38, 88,\n       10, 87, 97, 77, 83, 68, 22,  5,  8,  2, 43, 25,  1, 65, 76, 79, 11,\n        2, 45, 33, 92,  6,  9, 54,  8, 29, 66, 11, 79, 79, 82,  8, 43, 22,\n        5, 92, 32, 71, 60, 44, 10, 66, 35, 36, 10, 10, 75,  2, 32,  0, 27,\n       69, 65, 93, 45, 71, 42, 73, 23, 91, 12, 15, 22, 60, 82, 31]), array([58, 51, 73, 14, 19, 61, 63, 96, 22, 51, 11, 43, 19,  7,  9, 36, 62,\n       11, 61, 60, 85, 30, 12, 63, 56, 56, 48, 52, 20, 22, 50, 78, 58, 82,\n       71, 96, 44, 76, 54, 64, 98, 26, 65, 22, 91, 35, 29,  9,  0, 67, 88,\n       77, 83,  6, 21, 24,  8, 51, 38, 10, 22, 71, 89, 90, 39, 10, 96, 51,\n       63, 30, 98, 50, 66, 74, 18, 52, 67, 98, 98, 56, 99, 82, 52, 79, 38,\n       58, 98, 96,  6, 76, 33, 27,  8, 79, 40, 29, 72, 61, 63, 26]), array([83, 47, 88, 61, 37, 12, 56, 40, 64, 48, 58, 98, 22, 21,  4, 19, 86,\n       66, 22, 25, 60, 35, 48, 54, 18, 43, 53, 42, 54, 48, 49, 44, 14, 38,\n       19, 86, 52, 85, 12, 17,  8, 51, 42, 47, 26, 99, 66, 77, 97, 21, 53,\n       25, 72, 87, 44,  4,  5, 22, 14, 46, 50, 67, 63, 28, 55, 38, 77, 61,\n       64,  7, 96, 68, 47, 40, 52, 27,  1, 39, 44, 70, 36, 71, 75, 31, 15,\n       66, 15, 46,  8, 94, 90, 75, 13, 87, 12, 75, 64, 43, 18, 25]), array([27, 53, 21,  7, 28, 95, 74,  6,  9, 72, 63, 41, 19, 71, 62, 29, 93,\n       14, 80, 21, 29, 67, 28,  0,  1, 10, 99, 27, 23, 17, 11, 99,  0, 65,\n       40, 68, 42, 47, 93, 34, 19,  3, 35, 43, 13, 44, 68, 30, 74, 94, 31,\n       77, 21, 41, 65,  9, 24, 77, 54, 54, 30, 12,  7, 91, 27, 24, 29, 13,\n       73, 88, 10, 93, 65, 59, 76, 22,  4,  6, 45, 16, 98, 70, 82, 64, 23,\n       18, 39,  8, 67, 19,  5, 34, 49, 26, 73, 28,  5, 67, 35, 14]), array([60, 72, 55, 77, 95, 65, 49, 87, 32, 83, 96,  1, 60, 42, 31, 26, 65,\n       47, 46, 15, 26, 42, 87, 48, 70, 61, 57, 74, 33, 16, 23, 41, 52, 46,\n       46, 19, 51, 51, 76, 57, 34,  2, 28, 91, 88,  1, 39, 36, 70, 99, 86,\n       33, 11, 90, 66, 38, 15, 50, 88, 43, 77, 70, 76, 75, 74, 24, 16, 39,\n       50, 46, 39, 68, 61, 45, 22, 50, 32, 93, 66, 62,  6, 43, 97, 39, 87,\n       89, 71, 75, 19, 98,  6, 69, 88, 23, 62, 28, 68, 11, 35, 30]), array([88, 60,  8, 59, 74, 64, 60, 27,  0, 50,  0, 33, 34, 48, 62, 39, 50,\n       23, 35, 19, 14, 33, 54, 95, 37, 83, 21, 89, 88,  2, 58,  7, 75, 91,\n       64, 79, 16, 84, 49, 91, 98, 76, 77, 38, 29, 21, 54,  5, 72, 41, 25,\n       45, 50, 35, 28,  3, 84, 84, 91, 49, 33, 68, 96, 27,  4, 13, 53, 31,\n        2, 90, 77, 48, 73, 25, 65, 80, 80, 79,  8, 50,  3, 18, 82,  8, 92,\n       58, 27, 24, 26, 95,  2, 15,  4, 33, 27, 78, 91, 76, 87, 29]), array([44, 65, 62, 18, 59, 85, 45, 27, 61, 70, 42,  1,  3, 34, 86,  6, 82,\n       68, 66, 77, 34, 11,  5,  2, 81, 93, 76, 44, 89, 76, 86, 82, 59, 50,\n       56, 15, 60, 94, 82, 63, 16, 93,  4, 67, 48, 67, 14, 35,  0, 99, 53,\n       62, 25, 85, 37, 87, 26,  3, 98, 26, 40, 39, 70, 63, 91, 32, 90, 26,\n       60, 44,  5, 55, 37, 61,  6, 98, 15,  7, 15, 47, 69, 72, 98, 27, 98,\n       65, 70, 27, 56, 66, 99, 28,  3, 27, 35, 20, 32, 43, 91, 37]), array([ 2, 50, 10, 16, 14, 99, 92, 35, 30, 47, 48,  4, 79, 49, 89, 29, 28,\n       11, 91, 34, 16,  2, 50, 96, 20, 76,  8, 27, 31, 58,  7, 84, 39, 24,\n       28, 66, 55, 18, 14, 24, 69, 11, 63, 47, 67, 99, 13, 94, 19, 46, 55,\n       58, 62, 97, 29, 47,  8, 38, 53, 51,  2, 38, 11, 70, 36, 40, 88,  3,\n       77,  1, 72, 67, 22, 51, 22, 41, 73, 18, 84, 96, 77, 62, 46, 55, 10,\n       95, 31, 32, 34, 49, 46, 46, 82, 13, 71, 32, 24, 55, 10, 34]), array([55, 13,  3, 80, 54, 41, 17, 86, 35, 56, 46, 44, 39, 79, 63, 62, 94,\n       26, 43, 77, 85, 74, 32, 67, 57,  0, 29, 74, 45, 20, 48, 99, 35, 91,\n       47,  3, 26, 52,  4, 89, 90,  7,  3, 32, 50, 33,  7, 97, 98, 91, 88,\n       28, 18, 41,  0, 34, 49, 40, 46, 53, 62, 68, 19, 77, 57,  2, 45, 14,\n       54,  5, 70, 33, 46, 47, 66, 60, 39, 39, 77, 99, 68, 33, 66, 14, 73,\n       66, 68, 40, 85, 24, 81, 66, 54, 68, 24, 63, 64,  5, 32, 35]), array([15, 59, 71, 53, 30, 15, 16, 75, 60, 86, 46, 76, 90, 21, 66, 33, 73,\n       54, 50, 99, 93, 51, 67, 82, 41,  5, 89, 51, 67, 23, 70, 97, 47, 33,\n       23, 46, 80, 44, 43, 52, 51, 83, 84, 33, 27, 98, 99, 91, 10, 61, 92,\n       39, 28, 71, 26,  3, 21, 14, 32, 53, 94, 86, 91, 38, 82, 39, 15, 18,\n       75, 35,  5, 84, 91,  7, 70, 42, 11, 61,  3, 35, 67, 44, 21, 94, 26,\n       29, 72, 52, 44, 97, 14, 24, 83, 65, 24,  2, 32, 35, 21, 32]), array([ 9, 68, 62, 70, 52, 89, 91, 35,  5, 70, 44, 47, 11, 15, 66,  7, 87,\n       58, 36, 39, 73, 85, 12,  1, 64, 46, 37, 95, 14, 75, 80, 64, 88, 75,\n       64,  2,  0, 16, 57,  5,  7, 80, 74, 55, 17, 43, 72, 10, 75, 58, 95,\n       38, 89, 18, 11, 12, 69, 65, 90, 61, 35, 36, 15, 45, 52, 22, 89, 28,\n       63, 28, 71, 58, 73, 78, 37, 90, 56, 69, 98,  4, 25, 91, 45, 86, 96,\n       74, 91, 40, 80, 12,  8, 38, 15, 64, 95, 62, 27, 66, 63, 89]), array([54, 82, 84, 80, 27,  9, 74, 67,  1, 25, 79, 50, 24, 45, 32, 89, 14,\n       98, 41, 72, 35, 56, 68, 46, 32, 64,  8, 41, 72, 16, 67, 13, 75, 27,\n       42,  5, 42, 60, 53, 22, 27, 48, 65,  6, 86, 39, 60, 74, 85, 23, 76,\n       17, 95, 20, 41, 60,  7, 27, 93, 33, 95, 26,  7, 98, 57, 92, 96, 38,\n       94,  7, 83, 98, 71, 27, 13, 96,  4, 55, 46, 77, 41, 91, 35, 23, 44,\n       13,  6, 61, 24, 36, 31, 68,  6, 37, 66, 97, 98, 29, 76, 92]), array([ 3, 65, 89, 14, 43, 68, 38, 22, 48, 97, 20, 38, 67, 84, 53, 22, 36,\n       77, 29, 50, 58, 48, 65, 29, 41, 45, 60, 49, 60, 61, 53,  5, 65, 76,\n       83, 23, 86, 22, 37,  3, 31, 12, 91, 28, 86, 84, 28, 49, 39, 96, 60,\n       87, 30, 51, 78,  5, 84, 97, 43, 10, 82, 49, 45, 12, 87, 25, 51, 46,\n       10,  5,  2, 15, 74, 10,  2, 12, 91, 19, 78, 57, 48, 60, 49, 94, 70,\n       31, 30,  3, 67, 67, 70, 22, 12, 90, 31, 60, 53, 30, 49, 99]), array([21, 12, 99, 58, 95, 21, 64, 36, 88,  3, 66, 87, 65, 74, 64, 70, 88,\n       26, 63, 34, 47,  2, 49,  7, 78, 67, 17, 61, 95, 85, 27, 50, 94, 10,\n       96,  1, 70, 93, 38, 56, 96, 93, 94, 87, 58, 22,  5, 11,  7, 45, 79,\n       21, 36, 90,  9, 43, 57, 69, 76,  2, 19, 29, 35, 92, 16, 49, 24, 88,\n       30, 20, 72, 95, 41, 43, 75, 28, 61, 72, 46, 25, 10, 10, 25,  5, 66,\n       79, 39, 25, 71, 35, 33, 87, 78, 72, 37, 29, 20, 90,  2, 92]), array([65, 99, 12, 95, 57, 89, 24,  7, 40, 99, 31, 49, 25, 37, 47, 38, 22,\n       95, 61, 58, 61,  4, 93, 40,  2, 61, 97, 53, 38, 12, 12, 66, 45, 71,\n       88, 36, 71, 55, 86, 53, 49,  0, 25,  1, 28, 10, 65,  2, 76, 15,  8,\n       65,  9, 82, 44, 30, 80, 84,  8, 85, 53, 95, 51,  4, 87, 66, 67, 31,\n       43, 98, 22,  0, 64, 78, 32, 13, 36,  5, 87, 41, 70, 89, 92, 77, 88,\n       11, 19, 80, 53, 14, 96, 64, 21, 30, 12, 79,  5, 85, 90, 73]), array([12, 77,  4, 60, 78, 44, 83, 77, 71, 70, 76, 49, 88, 30,  6, 48, 45,\n       26, 44, 23, 52, 47, 41, 72, 94, 16, 45, 93, 17, 42,  0, 11, 77, 23,\n       12, 67, 22, 37,  9, 48, 50, 74,  5, 54, 81, 75, 76, 14, 56, 70, 14,\n       39, 19, 88, 70, 83, 98, 71,  1, 23, 89, 52,  3, 78, 14, 69, 79, 42,\n       11, 29,  5, 47, 77, 99, 69, 43, 91, 11, 93, 29, 84, 20, 75, 22, 43,\n       75, 32, 97, 79, 57, 69,  2,  2, 42,  7, 47, 49, 15, 41, 43]), array([ 2, 88, 65, 34,  1, 85, 40, 22, 77, 96, 47, 12, 47, 37,  5,  7, 62,\n       42, 96, 95, 51, 12, 47, 78, 19, 66, 17, 33,  7, 75, 97, 53, 22, 53,\n        5, 36, 33, 33, 46, 24, 31, 46, 92, 32, 18, 68,  7, 10, 87, 35,  7,\n       51, 17, 78, 98, 39, 32, 21, 38, 22, 41, 74, 71, 64, 24, 17, 40, 22,\n       97, 57, 15, 12, 33, 32, 84, 32, 22, 34, 91, 93, 76, 88, 24, 34,  3,\n       66, 51,  4, 23, 22, 38, 81,  0, 51, 59, 21, 99, 47, 48, 48]), array([46, 77, 97, 66, 73, 71, 82, 54, 19, 28, 91, 38, 88, 94, 32, 23, 85,\n       52, 27, 17, 26, 66, 82,  9, 68, 78, 75, 31, 45, 96, 23, 30, 51, 76,\n       95, 22, 42, 53, 77, 13, 73, 19, 99, 57, 87, 29, 16, 72, 86, 84, 40,\n       37, 78, 76, 11, 67, 86, 88, 32, 82, 64, 12, 94, 95,  2, 33, 31, 33,\n       23, 33, 48, 40,  4, 76, 40, 65, 56, 15, 76, 36, 91, 41, 88, 69, 71,\n       86,  8, 58,  7, 94, 41, 80,  2, 64, 34, 72, 99, 29, 76, 56]), array([ 3, 56, 61, 92, 33, 22, 37, 88, 97,  1, 57, 98, 65, 16,  9, 14, 61,\n       82, 47, 25, 92, 65, 56, 57, 44, 83, 85, 58, 61, 56, 50, 76,  4, 51,\n       77, 12, 49, 33, 44, 48, 67, 39, 68, 49, 82, 69, 39, 84, 34, 66, 38,\n       30, 78, 75, 96, 63, 51, 10, 77, 63, 90, 75, 48, 18, 38, 27, 79, 72,\n       89, 27, 19, 24, 37, 97, 61, 86, 30, 81, 99, 40, 20, 96, 17, 13, 72,\n       70, 60, 75, 76, 50, 41,  0, 29, 49, 57, 96, 15, 92, 17, 93]), array([23, 39, 59, 91, 65, 34, 14, 82, 77, 60, 32, 63, 71, 88, 86, 89, 27,\n       19, 29, 86, 46, 99, 43, 29, 75, 17, 90, 74, 32, 81,  3, 82, 58, 63,\n       85, 80, 74,  4, 39, 94, 98, 74, 86, 18, 65, 66, 85, 36,  1, 98, 23,\n       70, 71, 86, 32, 37,  1, 73,  8, 87,  9, 51, 68, 22, 79, 44, 71, 18,\n       80, 48, 14,  7, 17,  9, 75, 79, 67, 22, 35, 99, 74, 65, 73,  1, 91,\n       87, 42, 85,  2, 91, 82,  6, 46, 33, 34, 76,  8,  0, 42, 70]), array([52, 69, 44, 75, 31,  1, 90, 27, 61, 20, 94, 65, 59, 59, 73, 92, 39,\n       34, 49,  8, 63, 94, 88, 95, 16, 61, 85, 57, 92, 73, 28, 90, 39, 41,\n       83, 15, 22, 62, 39, 37, 19, 67, 87, 47, 90, 25, 75, 38, 28, 65, 58,\n       81, 40, 18, 20, 66, 21, 90, 44, 90, 61, 26, 52, 44, 35, 31,  1, 86,\n       34, 36, 68, 19, 95, 50, 51, 84,  6, 19, 87, 54, 94, 44, 72, 89, 87,\n       51, 39, 27, 18, 64, 91, 88, 63, 33,  3, 75, 45, 25, 99, 18]), array([15, 27, 16, 71, 47, 86, 41,  2, 77, 38, 18, 27, 64, 80, 75, 53,  7,\n       20, 18, 98, 95, 43, 88, 20, 60, 30, 38, 74, 86, 77, 45, 13, 41, 89,\n        2, 72, 68, 92, 84, 99, 21, 55, 76, 55, 28, 70, 70, 33, 84,  0, 70,\n       96, 84, 31, 34, 11,  6, 90, 60, 36, 27, 85, 16,  8, 52, 55, 97, 37,\n       80, 16,  2, 66, 49, 68, 69, 69, 56,  4, 23, 92, 24,  0, 80, 15, 88,\n       59, 88, 22, 64, 77, 66, 35, 23, 99, 37, 37,  4, 59, 79, 98]), array([89, 28, 17, 57, 95, 77, 12, 98, 39, 86,  5, 26, 58, 81, 83, 35, 55,\n       22, 18,  4,  8, 19, 27, 47,  4, 16, 15, 44, 73, 27, 99, 34, 84, 71,\n       74, 94, 23, 80,  0, 35, 42, 65, 60, 31, 84, 55, 24, 91, 11,  7, 55,\n       93, 79, 66,  3,  1, 27, 21, 52, 29,  4, 35, 93, 57, 85, 27, 83, 86,\n       43, 92, 98, 80, 41, 11, 58, 99, 88, 14, 45, 81, 27, 89, 98, 63, 49,\n        8, 84, 21, 63, 62, 50,  9, 67,  1, 62, 19, 72, 21, 25, 78]), array([14, 70, 58, 39, 97, 67, 46, 75, 91, 74,  2, 91, 97, 61, 70, 12,  7,\n        6, 82, 97, 24, 90, 95, 58, 29, 34,  6, 77, 27, 88, 47, 37, 39, 38,\n       42,  6, 87, 30,  2, 86, 66, 73, 29, 44, 54, 95, 44, 22, 19, 55, 90,\n        6, 60, 46, 13, 31, 42, 34, 48, 22, 72, 57, 75, 85, 96, 88, 15,  4,\n       40,  9, 99, 62, 19, 59, 54,  3,  1, 26, 91, 66, 35, 36, 11, 51, 12,\n       93, 18, 77, 72, 70, 85, 95, 95, 68, 55, 90, 21, 11, 34, 54]), array([80,  0,  4,  1, 21, 37, 25, 31, 64, 84, 34, 91, 78, 47, 12, 40,  5,\n       23, 25, 95, 41, 50, 71, 89,  5, 85, 48, 31, 92, 98, 81,  7, 67, 37,\n       67, 27, 66, 86, 28, 47, 96, 47, 98, 31, 17, 89, 78, 13, 66, 61, 57,\n        8, 60, 31, 30, 72, 25, 69, 55, 78, 67, 18, 62, 54, 26, 32, 72, 75,\n       25, 64, 58, 51, 47, 41, 38, 52, 30, 32,  5, 13,  3, 95, 37, 64, 43,\n       71, 87,  2, 40, 95, 46, 34,  7, 20, 85, 87, 56,  7, 95,  9]), array([32, 12, 94,  7, 64, 17,  2, 69, 93,  3, 97, 15, 50, 55, 40, 90, 98,\n       50, 75, 80, 21,  1, 99, 70, 67, 44,  7, 99, 77,  4, 12, 99, 84, 59,\n       28, 70, 75, 42, 87,  8,  4, 74,  6, 55, 42, 84, 78, 51, 57, 55, 78,\n       10, 28, 96, 25, 61,  1,  7, 87, 55, 16,  9, 23, 42, 18, 59, 14, 80,\n       33, 28,  5, 21, 46, 65, 61, 97, 30, 50, 64, 78, 99, 26, 16, 45, 20,\n       24, 41, 52, 98, 78,  6, 73, 21, 43, 37, 45,  9, 49, 47, 23]), array([25, 58, 85, 56, 96, 68, 98, 30, 13, 65, 27, 23,  7, 48, 17, 82, 63,\n       98, 26, 19, 49,  6, 40, 88,  3, 17, 31, 99, 28, 51, 57,  9, 49, 26,\n       25, 63, 53, 40, 85, 54, 39, 50, 12, 68, 33, 93, 38, 98, 29, 64, 16,\n       97, 78, 16, 36, 51, 91, 11, 91, 71, 89, 29, 61, 26, 98, 92, 92, 85,\n       61, 39,  1,  1, 81,  9, 50, 27,  2, 84, 64, 76, 26, 55,  9, 52, 43,\n       49, 44, 62, 43, 25, 93, 34, 61, 60,  4, 47, 15, 88, 79, 92]), array([85, 96,  7, 68, 39, 50, 18, 60, 38, 50, 89, 84, 83, 75, 22, 39, 51,\n       66, 63,  0, 99, 56, 95, 29, 69,  9, 24, 81, 88, 89, 94,  8, 30, 94,\n        5, 37, 84, 43, 63, 33,  2, 30, 92, 94, 49,  8, 51, 99, 43, 57, 35,\n       23, 78, 90, 37, 71, 28, 70, 41, 20, 87,  5,  4, 23, 44, 89, 67, 32,\n       13, 90, 53, 43, 48, 30, 53, 57, 96, 41, 69, 58, 24, 32, 47, 92,  2,\n       24, 99, 52, 78, 47, 90,  5, 84, 70,  2, 37, 15, 64, 59, 55]), array([ 8, 58, 33, 29, 59, 91, 37, 70, 62, 50, 14, 76, 22,  5, 67, 90, 99,\n       10, 45, 27, 51, 15, 90, 65, 69, 50, 72, 97, 56, 75, 79, 36,  6, 33,\n       82, 77,  8, 26, 29, 80, 54,  3, 72,  8, 45, 45, 93, 62, 90, 86, 84,\n       81, 41,  8,  0, 15, 72, 24, 93, 59, 78, 78, 27,  7, 99, 46, 63, 53,\n       56, 42, 63, 70, 25, 51, 40, 42, 36, 80,  6, 44, 87, 27, 97, 30, 23,\n       13, 41, 48, 68, 39, 79, 22, 48, 30, 48, 29,  8,  0, 59,  6]), array([92, 40, 45, 14, 89, 27, 21, 14, 51, 67, 57, 55, 11, 66, 53, 39, 69,\n       44, 51, 79,  2, 41, 77, 82, 19, 57, 34, 31, 67, 76, 16, 81, 57, 49,\n       17, 92, 48, 33, 26,  3, 18, 17, 54, 10, 42, 56, 55, 66, 25, 76, 47,\n       53, 27, 33, 52, 29, 67, 98,  3, 62, 95, 76, 69, 91, 29, 57, 44,  2,\n       63, 57, 82, 25, 88,  6,  2, 61, 14, 41, 27, 24, 93, 46, 98, 99, 49,\n       43, 78,  1, 60, 54, 97, 74, 33, 67, 96, 13, 89, 50, 50, 33]), array([64, 76, 81, 89, 25, 36, 56, 20, 65, 43, 21, 88, 35, 11, 52, 95, 41,\n       41, 22,  4, 69, 20,  1, 45, 24,  6,  7, 49, 40, 20, 52, 20, 22, 95,\n       79,  7, 68, 43, 16, 50, 89, 41, 99, 52, 42, 70, 50, 73, 90, 72,  3,\n       24, 50,  7, 42, 20, 83, 27, 32, 58, 76, 99, 89, 45, 78, 43, 88, 26,\n       62, 23, 10, 12, 89, 73, 78,  3, 36, 43, 64, 41, 42, 94, 40, 57,  4,\n       76, 49, 21, 53, 17,  0, 51,  5, 27, 40, 98, 48, 92, 56, 48]), array([33, 29, 27, 77, 20,  1, 22, 16, 59, 83,  3,  7, 45, 84, 48, 80,  6,\n       18, 66, 92, 10, 40,  2, 96, 11, 17, 77, 73, 80, 58, 87,  4, 21, 16,\n       37, 38, 38,  8, 33, 88, 54, 18, 24,  8, 75, 94, 55, 24, 98, 25, 82,\n       30, 95, 55, 88, 71, 55, 83, 11,  5, 75, 83,  9, 33, 26, 39, 66, 20,\n       88, 60, 41, 84, 65, 47, 89, 87, 99, 41, 29, 34, 96, 49, 67, 14, 80,\n       15, 47, 31, 25, 50, 98, 73, 43, 86, 25, 54, 82,  2, 26, 97]), array([75, 83, 81, 80, 99,  5, 39, 45, 74, 67, 54, 10, 82, 82, 27, 29, 47,\n       25, 94, 54, 57, 42, 91, 79,  9, 82, 63, 58, 94, 28, 72, 93, 16, 73,\n       99, 35, 36,  3, 90, 56, 88, 99, 97, 68,  7, 27, 49, 85, 89, 86, 45,\n       21, 53, 33, 95, 70, 27, 26, 70, 79, 79, 49, 73, 28, 24, 70, 15, 44,\n       71, 20, 40, 91, 64,  3, 43, 71,  7, 43, 88, 16, 25, 85, 27, 62, 85,\n       25, 27, 31, 98, 62, 41, 44, 59, 73, 65, 18,  5, 91, 10, 90]), array([41, 19, 25, 39, 42,  6, 25, 53, 61, 48, 79, 64, 47, 38, 34,  1, 47,\n        4, 48, 19,  9, 88, 42, 68, 10, 63, 99,  9, 92, 44, 50, 75, 88, 29,\n       81, 51, 45, 29, 26, 71, 88, 30, 48, 58, 14, 32, 50, 83, 14, 67, 65,\n       18, 72,  1, 42,  5, 76, 10, 78, 77, 83, 29, 11, 84, 58, 51, 55, 25,\n       91, 37, 56, 38, 46,  9, 97, 93, 38,  0, 93, 26,  3, 86, 60, 62, 69,\n       26, 30, 95, 77, 48, 36, 23, 78, 18, 13, 98, 46, 19, 28, 59]), array([30, 56, 29,  3, 50, 40, 26, 16, 11, 40, 77, 17, 49, 64, 77, 81,  1,\n       82,  8,  7, 57, 74, 60, 17, 32, 88, 59, 75, 60, 45, 21, 27, 42, 42,\n       55, 65, 71, 68, 42,  3,  7, 54, 86, 97, 42, 24, 51, 56,  0, 23, 39,\n        2, 68, 89, 40, 92, 47, 90, 16, 13, 69,  2, 55, 28, 69, 93, 46, 61,\n       65, 64,  1, 19, 27, 91,  6, 88, 83, 21, 94, 47, 31, 18, 32, 90, 39,\n       53, 83, 35, 80,  3, 23, 63, 65, 97, 55, 67, 72, 30, 94, 66]), array([38, 65, 93, 91, 76, 76, 48, 42, 82, 83, 80,  4, 79, 28, 25, 10, 78,\n       86, 86,  3, 86, 62, 44, 95, 85, 16, 26, 47, 30, 44, 78, 85, 10, 67,\n       94, 44, 31, 38, 72, 30,  9, 54, 41, 31, 63, 95, 94, 42, 48,  8, 69,\n        1, 60, 78, 23, 26, 10, 17, 35, 84, 42, 50, 41, 53, 51, 83,  5, 68,\n       15, 36, 16, 91, 12, 36, 29, 58, 92, 83, 28, 41, 10,  3, 27, 22, 88,\n       28, 12, 78, 32, 50, 92, 65,  3, 82, 73,  7, 60, 32, 48, 38]), array([78,  5, 85, 82, 38, 95, 58, 71, 91, 91, 38,  5, 42, 38, 84, 44, 44,\n       67, 73, 98, 15, 78, 35, 43, 87, 19, 50, 90, 94, 12, 39,  2, 24, 55,\n       39,  2,  5, 22,  9, 95, 54, 20, 24, 67, 84, 37, 98, 80, 34, 84, 70,\n        2, 37, 37, 62, 40, 40, 26, 84, 20, 79, 48, 99, 95, 87, 45, 34, 82,\n       17, 52, 50, 36, 89, 26, 22, 18, 58, 73, 52, 25, 93, 28,  9, 32, 67,\n        7, 76, 35, 95, 82, 79, 36,  6, 13, 58, 97, 92, 91, 81, 29]), array([ 0, 67, 37, 47, 54, 64, 42, 73, 53, 75,  9,  5, 78, 64, 98, 55, 10,\n       84, 85,  9, 90, 63, 34, 49, 66, 70, 51, 55, 29, 59, 57, 92,  2, 79,\n        5,  8, 78, 22, 95, 10, 37, 73, 13, 77, 51, 53, 50, 50, 24, 24, 87,\n        2, 53, 42, 96, 11, 46, 85, 83, 25, 29, 17, 18, 79, 45, 28, 19, 13,\n       87,  7, 44, 98, 70, 65, 89, 15, 80, 71, 67, 40,  9, 64, 42, 65, 76,\n        4,  2, 37,  3, 82,  2,  9, 13, 48,  3, 40,  3, 34, 31, 56]), array([29, 15, 64, 97, 37, 25, 20,  2,  0, 41, 57,  1, 36, 91, 97, 98, 95,\n       37, 33, 40, 55, 14, 44, 72, 99, 82, 20, 16, 89, 40, 27, 26,  1, 26,\n       40, 95, 31, 82, 33,  6, 19, 98, 93, 27, 74, 23, 32,  7, 52, 95, 71,\n       59, 42, 67, 29, 13, 41, 90, 83, 47, 76,  0, 84,  8, 36, 32, 39, 35,\n       84, 42, 10, 95, 58, 11, 52, 75, 56, 63,  0, 60, 35, 78, 24, 79, 60,\n       62, 47, 18, 98, 22, 91, 71, 51, 18, 37, 53, 61,  1, 72, 34]), array([84, 12, 19,  7, 60, 60, 97, 68, 77, 10, 60, 13, 22, 32, 96, 13,  3,\n       25,  1, 32, 65, 79, 73, 78, 86, 85, 59, 87, 20, 59, 15, 27, 21, 14,\n       79, 87, 73, 27, 15, 20, 97, 38, 30, 61, 74, 77, 27, 28, 14, 68, 24,\n        2, 93, 60, 98, 17, 57, 52, 79, 32, 19, 79, 34, 77, 41, 49,  5, 14,\n       90, 29, 39, 92, 16, 45, 84, 99, 90, 11, 66, 43, 82, 88, 45, 82, 68,\n       96, 79, 76, 51, 85, 20,  9, 64, 30, 44, 58,  2, 88, 64, 52]), array([95, 23, 94, 45, 67, 51, 83, 38, 79, 73, 47, 47, 42, 48, 72, 36, 97,\n       17, 58, 75, 14, 87, 66, 12, 37, 43, 92, 18, 57, 76, 34, 91, 83, 27,\n       92, 48, 59, 66,  4, 55, 46, 30, 88, 19, 43, 91, 12, 62, 74, 20, 17,\n       30,  1, 35, 72, 19, 82, 13,  8, 72, 40, 81, 56, 71, 23,  9, 39, 82,\n       86, 75, 41,  4, 39, 83, 63, 58, 51, 20, 95, 83, 30, 82, 80, 74, 93,\n       10, 71, 78, 60, 33, 46, 22, 78, 71, 13, 41, 77, 55, 78, 51]), array([ 1,  8,  1, 16, 70,  5, 39, 31, 79, 12, 84, 51, 43, 47, 85, 73, 42,\n       11, 31, 33, 41, 82,  3, 72, 43, 25, 38, 20, 22, 40, 73, 40, 30, 23,\n       57, 71, 22, 40, 85, 96,  5, 24, 77, 57, 11, 63, 62, 32, 99, 44, 70,\n       11, 43, 60, 71, 65, 95, 62, 29, 42,  7, 23, 83,  6, 72, 50, 34, 22,\n       70, 40, 11, 51, 85, 57, 41, 20, 43, 27, 67, 95, 59,  9, 43, 97, 27,\n       39, 23, 90, 50, 43, 21, 68,  0, 17, 35, 41, 41, 31, 42,  2]), array([90, 78, 70, 32,  0, 53,  4, 60,  5, 73,  5, 69, 59, 55, 61, 34, 99,\n       92, 68, 54, 23, 91, 26, 28, 42, 70, 83, 58, 26, 40, 48, 52,  9,  3,\n       14, 53,  5, 75, 72, 79, 19, 23, 61, 21, 60, 87, 35, 67, 30, 36, 73,\n       50, 86, 15, 65, 48, 34, 50, 59, 53, 84, 59, 56, 25, 23, 91, 71, 29,\n       43,  4,  0, 14, 18, 65,  1,  6, 94,  8, 81, 85, 61, 77, 40, 54, 38,\n       92, 34, 92, 32, 38, 47, 91,  7,  4, 58,  0, 38, 78, 85, 40]), array([25, 24, 63, 35, 31, 79,  6, 47,  8, 76, 79, 51, 16, 88, 92, 16, 61,\n       73, 59, 34, 18, 53, 59, 30, 31, 60, 49, 52, 37, 61, 91, 30, 66, 70,\n       35,  7, 67, 50, 75, 21, 34, 67, 68, 85, 21, 39, 48, 44, 56, 77, 74,\n       43, 55, 61, 44, 44,  7, 56, 99, 91, 83, 88, 26,  0, 17, 70, 84, 72,\n        2, 89,  0, 44, 36,  4, 58, 16, 84, 20, 87, 57, 74, 39, 30, 49, 20,\n        3, 48, 76, 62, 52, 33,  1,  1, 64, 47, 39, 74,  3, 23, 13]), array([24, 16, 26, 27, 48, 11, 25, 22, 35, 62, 13, 41, 43, 74, 34, 15, 30,\n       70, 13, 87, 39, 66, 11, 37, 98, 12, 38, 85, 51, 28, 33, 58,  0, 56,\n       70, 28, 19, 88, 47, 17,  6, 96, 99, 72, 95, 78, 13, 18, 70, 53, 60,\n       62,  0, 35, 35, 34, 69, 91, 53, 41, 95, 24, 57, 21,  3, 91, 21, 74,\n        3,  9, 73, 30,  6, 65, 36, 38, 36, 30, 15,  0, 39, 64,  8, 21, 96,\n       82, 54, 45, 49, 26, 18, 67, 89, 27, 76, 51,  2, 77, 56, 48]), array([76,  2, 17, 83, 71, 15, 38,  2, 62, 50, 75, 47, 95, 55, 48, 88, 73,\n       53, 86, 78, 84, 86, 96, 46, 42, 84, 73, 52, 84, 59, 88, 11, 35,  6,\n       56, 66, 60, 53, 29, 28, 97, 37,  8, 35,  7, 90, 29, 80, 48, 62, 53,\n       27, 90, 63,  7, 53,  5, 68, 42, 31, 52, 42, 83, 63, 62, 41, 80,  6,\n       16, 36, 13, 49, 48, 66, 45, 25, 88, 52, 36, 30, 99, 16, 48, 64, 64,\n       65, 12, 12, 50, 67, 77, 53, 47, 18, 73, 55, 46, 30, 63, 24]), array([96, 70,  4, 46, 67, 29, 73, 28, 42, 82, 98, 20, 56, 71, 30, 53, 68,\n       74, 29, 47, 85,  4, 49, 57, 45, 16, 97, 16, 17, 30, 13, 77, 34, 17,\n       14, 29, 85, 22, 10, 82, 79, 77, 18, 32, 34, 94, 32, 41, 33, 71,  8,\n       55, 23, 55, 39, 80, 58, 75, 87, 48,  4, 18, 96, 45,  3, 81,  5, 92,\n       61, 23, 98,  9,  0, 28, 44, 85, 26, 17, 39, 72, 79,  2, 19, 60, 82,\n       14,  7, 41, 52, 17, 49, 71, 53, 51, 26, 77, 68, 20, 40, 57]), array([61,  9, 24, 28, 56, 63, 63, 17, 47,  0, 38, 67, 46, 55, 11, 57, 46,\n       70, 14, 35, 36, 84, 50, 21, 21, 48, 28, 47, 71,  7, 94, 25, 96, 37,\n       23, 98, 39,  0, 36,  2, 41, 95, 46, 44, 10, 67, 61, 92, 84, 49, 89,\n       22, 13, 53,  6, 56, 17,  6, 61, 52,  4, 41, 34, 50, 14, 73,  2, 36,\n       82, 38, 48, 62, 77, 83, 44, 26, 89, 31, 49,  4, 78, 45, 11, 12, 71,\n       23, 90, 97, 88, 29, 97, 12, 81, 99, 43, 85, 62, 76, 75, 41]), array([58, 94,  6,  7, 99, 65, 18, 17, 42, 99, 93, 48, 68,  6, 34, 50, 30,\n       42, 18, 40,  6,  6,  1, 17, 79, 99, 55, 65, 91, 32, 19, 16, 64, 93,\n       42,  7, 78, 29, 25, 64, 93, 35, 22, 98,  4, 80, 21, 96, 61, 48, 86,\n       31, 83, 59, 89, 42, 75, 95, 67, 40, 87, 12, 64,  0,  5, 17, 69, 99,\n        9, 72, 11, 65, 40, 86, 68, 71, 11, 86, 67, 64, 77, 53, 42, 57, 85,\n       92, 53, 98,  6, 59, 51, 99, 24, 71, 55, 64, 38, 52, 95, 79]), array([10, 95, 87, 17, 83,  9, 31, 47, 24, 50, 72, 73, 48, 45, 58, 24, 49,\n       91, 20, 32, 62, 15, 30, 80, 57, 33, 26, 54, 58, 35,  4, 26, 94, 27,\n       55, 96, 28, 24, 25, 57, 80,  5, 98,  6, 10, 59,  0, 97, 75, 69, 97,\n       20, 23, 49, 98, 93, 54, 17, 52, 41, 69, 44, 95, 27, 42, 18, 82, 17,\n       48, 70, 56, 84, 86, 87, 59, 68, 58,  2, 46, 41, 15, 51, 12, 94, 54,\n       18, 40, 50,  7, 97,  7, 36, 16, 94, 87, 77, 58, 98,  6, 66]), array([ 7, 12, 95, 55, 94,  4, 26,  3, 68, 74, 15, 25,  2, 23, 16, 80, 11,\n       18, 88,  0, 32, 34, 76, 11, 49, 96, 20, 75, 11, 37, 43, 74, 90, 78,\n       51, 42,  4, 77, 40, 51, 73, 11, 16, 82, 70, 56, 94, 54,  3, 17, 37,\n       95, 13, 13, 94, 65,  6, 10, 92, 16, 15, 42, 10, 11, 11, 74,  3, 33,\n        5, 83,  2, 26, 31, 67, 88, 90,  6, 57, 44, 34, 70, 20, 24, 27, 41,\n       65, 42, 94, 25, 78, 31, 48, 75, 53,  7, 11, 90, 11, 40, 99]), array([71, 99, 89, 27,  4, 97, 67, 75, 99,  7, 80, 28, 71, 67, 67, 27, 88,\n       16, 53, 95, 49, 53, 56, 89, 25,  1, 45, 86, 23, 15, 46, 28, 75,  8,\n       11, 57, 67, 42, 21, 78, 49, 51, 68,  0, 98, 36, 76, 87,  6, 45, 94,\n       52,  5, 29, 72, 92, 92, 41, 30, 33, 37, 53, 80, 16, 55, 45, 89, 62,\n       84, 12, 21, 92, 59,  6, 39, 54,  4, 22, 29, 44, 19, 92, 81, 52, 73,\n       86, 41, 76, 82, 22, 51, 54, 66, 99, 25, 32, 39, 77, 26, 13]), array([43, 11, 47, 44, 57, 40, 81,  0, 37, 73, 27, 62, 29,  5, 78, 39, 30,\n       13,  8, 12, 16, 57, 54, 21, 72,  0, 36, 67, 51, 66, 81, 99, 26, 41,\n       38, 39, 90, 92, 87, 52, 89, 40, 16, 96,  1,  3, 38, 86, 32, 35, 82,\n       57, 47,  7, 36, 73, 88, 48, 15, 19, 85, 30, 28, 76, 45, 93, 91, 83,\n        6, 85, 37, 38, 76, 21, 57, 93, 58, 63, 78, 16, 69, 16, 71, 51, 27,\n       20, 94, 85, 68, 71, 95, 38,  8, 29,  6, 55, 36, 59, 37, 87]), array([70, 16,  0, 88, 37, 76, 98, 16, 46, 20, 96, 76, 94, 46, 46, 73, 17,\n       29, 98, 76, 96, 83, 80, 17, 40, 69, 27, 66, 54, 85, 87, 37, 58, 82,\n       25, 76, 27, 99,  2, 60, 89, 80, 93,  7, 16, 27, 38, 77, 35, 91, 76,\n       64, 54, 46, 27, 51, 85, 31, 94, 18, 37, 79, 21, 10, 91, 28, 37, 59,\n       30, 46, 43,  2, 97,  5, 18, 43, 85, 77, 52,  7, 89, 46, 33, 87, 24,\n        6, 35, 60, 80, 97, 52, 11, 32, 47,  5,  6, 16, 85, 12, 52]), array([40, 13, 57, 35, 71, 49, 37, 60, 87, 22, 81, 39, 52, 92,  3, 34, 37,\n       18, 28, 36, 25, 40, 45, 22, 33, 88, 65, 30, 63, 45, 13,  0,  2, 62,\n        3, 43, 47, 54,  2, 24, 35, 91, 53, 56, 34, 30, 43, 11, 56, 27, 82,\n       19,  9, 70, 56, 99, 62, 32, 31, 26,  7, 54, 18, 98, 82, 22, 75, 62,\n       32,  5, 99, 75, 52, 84, 28, 94,  3, 53, 94, 45, 60, 61, 48, 55, 14,\n       76, 99, 46, 93, 42, 61, 44, 84, 16, 30, 19, 22, 93, 26, 23]), array([17, 85,  8, 10, 16, 30, 51, 61, 42, 70,  5, 36,  2, 55, 97, 77, 29,\n       13, 98, 64, 74, 21, 70, 58, 30, 83,  1, 27, 67, 25,  1, 48, 31, 90,\n       48, 40, 80, 45, 59, 98,  5, 39, 91, 28, 53, 33, 64, 40, 38, 51, 94,\n       65, 97, 32, 41, 78, 71, 28, 49, 54,  4, 86, 79, 78, 88, 96, 59, 74,\n       39, 75, 28, 30, 70,  2, 57,  1, 54, 96, 38, 62, 86, 66, 98, 87, 27,\n       52, 74, 73, 48, 77, 73, 24, 30, 75, 77, 52, 75, 30, 61,  0]), array([56, 11, 67,  2, 62, 52,  2, 73, 94, 65, 94, 88, 73, 30, 44, 44, 75,\n       17, 22, 69, 78, 25,  5, 73,  2, 91, 81, 52, 66, 97, 13, 88, 68, 33,\n       56, 29, 34, 59, 53, 15, 90, 87, 30, 59, 12, 54, 25, 86, 13, 56,  0,\n        1, 47, 60, 49, 86, 72, 49,  6,  1,  7, 43, 89, 77, 78, 57, 82, 42,\n       13, 84, 34,  4, 39, 22, 36, 31, 30, 43, 83, 24, 78, 63, 80, 96, 22,\n       83,  3, 28, 73, 91, 20,  3, 73, 26, 58, 66, 84, 82, 12, 89]), array([18, 85, 69, 91, 37,  6, 65, 82, 50,  3, 40, 84, 31, 30, 19, 73, 31,\n        1, 50, 51, 92, 91, 51, 14, 59, 57, 65, 32, 50, 65, 31, 98, 13, 74,\n       21, 39, 33, 63, 21, 88, 36, 37, 62, 35,  8, 86,  6, 97, 88,  0, 60,\n       41, 85, 72,  5, 98,  8, 77, 60,  9, 33, 85,  1, 86, 22, 89, 31, 88,\n       90, 28, 91, 67, 80, 57, 89, 95, 67, 17,  3, 44, 40, 70, 69, 63, 84,\n        7, 69, 44, 34, 26, 13, 52,  3, 58, 23, 98, 58, 10, 84, 21]), array([57, 18, 16, 80, 70,  8, 34, 40, 73, 52, 34, 25, 64, 24, 26, 59, 44,\n       70, 93, 57, 78, 16, 91, 65,  1, 91, 74, 71, 67, 67, 32, 21, 53, 10,\n       67, 31, 47, 73, 48, 22, 54,  9, 93, 53, 56, 63, 24, 98, 15, 12, 91,\n       29, 86,  5, 25, 21, 61, 20, 20, 95, 31, 38, 20, 13, 51, 47, 61, 26,\n       85, 46, 69, 60, 74, 64, 25, 92, 77, 35,  3, 64, 39, 56, 22, 86,  2,\n       91, 23, 43, 94, 38, 93, 29, 91, 61, 71, 56, 46, 92,  3, 52]), array([96, 22, 30, 91, 71, 26, 20, 94, 42, 72, 63, 43, 25, 87, 40, 44, 41,\n       10, 29, 54, 95, 97, 59, 63, 92, 49, 97, 68, 31, 35, 59, 20, 45, 64,\n        9, 69, 81, 93, 88, 32, 90, 76, 84, 85, 34, 48, 29, 36, 24, 51, 80,\n       89, 79, 15, 63, 53, 49, 20, 41,  0, 20, 39, 21, 99, 77,  9, 34, 96,\n       29, 89, 80, 98, 58, 98, 23, 83, 53, 27, 90, 92, 94, 95, 65, 42, 33,\n       35,  2,  5, 64, 79, 61,  4, 81, 72, 67, 43, 96, 59, 77, 16]), array([95, 98, 63, 57, 74, 47, 48, 74, 41, 88, 93, 59, 56,  4, 53, 24, 44,\n       17, 58, 45, 41, 43, 32, 26, 22, 92, 75, 49, 94,  1, 42, 84, 64, 77,\n       73, 43, 38,  5, 59, 38, 95, 52, 70, 55, 49, 74, 97, 95, 39, 19, 25,\n       31, 74, 87, 21, 43,  5, 10, 29, 44, 25, 39, 74, 57, 19, 58, 22, 22,\n        6, 36, 91, 69, 39, 41, 11, 20, 77, 62, 78, 38, 21, 54,  3,  6, 99,\n       12, 30, 33,  5, 57, 18, 36, 61, 66, 71, 16, 23,  7, 85,  9]), array([96, 30,  5, 84, 91, 64, 39,  9, 95, 36, 56, 25, 53, 75, 47, 54, 67,\n       61, 87, 87,  0, 71, 46, 64, 87, 88, 45, 83, 83, 98, 93, 75, 61, 74,\n       46, 77, 26, 91, 60, 23, 21, 61,  8,  1,  7, 23, 20,  6, 54, 33, 28,\n       56, 23, 54, 85, 93, 59, 75, 29, 40, 43, 18, 67, 94, 63, 32, 75, 17,\n       93,  3, 28, 72, 20, 47, 11, 92, 18, 70, 61, 22, 25, 20, 97, 18, 87,\n        2, 48, 53, 99, 24, 86, 85, 36, 50, 49, 52, 64, 41, 79, 45]), array([51, 11, 98, 23, 94, 94, 19, 22, 51,  2, 37, 15,  2, 46, 19, 31, 10,\n       72, 92, 26, 59, 37, 35, 33, 38, 99, 66, 80,  8, 71, 25, 62, 47, 53,\n       21, 64, 98, 64, 39, 15, 19,  1, 52, 16, 49, 92, 10, 40, 43, 93,  5,\n       70, 33, 54, 86, 17, 42, 33, 32, 47, 51, 14, 79, 35, 20, 75, 88, 62,\n       99, 86, 71, 27,  8, 47, 23, 65, 86, 52, 72, 97, 49, 94, 13, 75, 32,\n       68, 15, 27, 13, 15, 59, 78, 83, 32, 68, 22, 31, 92,  0, 80]), array([48, 73, 51, 54, 69, 54,  1, 54, 66, 42, 65, 87, 18, 35, 15, 29, 77,\n       96, 42, 96, 10, 63, 93, 50, 83, 58, 96, 71, 10, 37, 66, 55, 17,  3,\n       50, 59, 32, 17, 71, 16, 49, 10, 52, 21, 61, 12, 61, 73, 60, 55, 80,\n       87, 16, 75, 59, 26, 13, 22,  1, 79, 73, 59, 73, 36, 97, 17, 12, 95,\n       28, 65, 97,  7, 81, 91, 92, 97, 59, 59, 34, 12, 29, 25, 27, 19, 31,\n       19, 26, 29, 60, 19, 48, 73, 44,  8, 37, 34, 13, 51, 23, 68])], 'feature_importances_': array([0.02255645, 0.02748645, 0.01537776, 0.03050232, 0.0208906 ,\n       0.02155879, 0.02768038, 0.03659055, 0.03612084, 0.09769932,\n       0.07244548, 0.01221973, 0.28028859, 0.0344751 , 0.01857089,\n       0.14272386, 0.01528442, 0.01722933, 0.0509372 , 0.01936194]), 'n_classes_': 2, 'n_features_in_': 20, 'n_outputs_': 1}"
  },
  {
    "objectID": "documentation.html#keras",
    "href": "documentation.html#keras",
    "title": "Model documentation",
    "section": "Keras",
    "text": "Keras\n\nfrom tensorflow import keras\n\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\nkeras_clf = keras.Sequential()\nkeras_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(20,)))\nkeras_clf.add(keras.layers.Dense(8, activation='relu'))\nkeras_clf.add(keras.layers.Dense(1, activation='sigmoid'))\nkeras_clf.compile(optimizer='sgd', loss='binary_crossentropy')\nkeras_clf.fit(X, y, batch_size=10, epochs=10)\n\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nEpoch 1/10\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n 1/10 [==&gt;...........................] - ETA: 8s - loss: 0.7216\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 1s 3ms/step - loss: 0.7106\nEpoch 2/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6545\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 4ms/step - loss: 0.6968\nEpoch 3/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6870\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6850\nEpoch 4/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7126\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6756\nEpoch 5/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6654\nEpoch 6/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6585\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6569\nEpoch 7/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6481\nEpoch 8/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5906\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6399\nEpoch 9/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6335\nEpoch 10/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4890\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6267\n\n\n&lt;keras.src.callbacks.History at 0x1d10aea88b0&gt;\n\n\n\nmodel_doc_keras = ModelCard()\nmodel_doc_keras.read_model(keras_clf)\nmodel_doc_keras.show_json()['model_details']['info']\n\n'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"batch_input_shape\": [null, 20], \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 20]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 16]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 8]}}]}, \"keras_version\": \"2.15.0\", \"backend\": \"tensorflow\"}'"
  },
  {
    "objectID": "documentation.html#other-models",
    "href": "documentation.html#other-models",
    "title": "Model documentation",
    "section": "Other models",
    "text": "Other models\nNative support for automatic documentation of other model types, such as from fastai, pytorch is expected to be available in future versions. Until then, any models coded form scratch by the user as well as any other model can be documented by passing the information as an argument to the Documenter’s fill_model_info method. This can be done with a string or dictionary. For example:\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.len = self.X.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass PytorchNet(torch.nn.Module):\n    def __init__(self):\n        super(PytorchNet, self).__init__()\n        self.layer1 = torch.nn.Linear(20, 16)\n        self.layer2 = torch.nn.Linear(16, 8)\n        self.layer3 = torch.nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        x = torch.sigmoid(self.layer3(x))\n        return x\n\npytorch_clf = PytorchNet()\n\ndataloader = MockDataset(X, y)\n\n\nloss_func = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(pytorch_clf.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        _X, _y = data\n        optimizer.zero_grad()\n        y_pred_epoch = pytorch_clf(_X)\n        loss = loss_func(y_pred_epoch, _y.reshape(1))\n        loss.backward()\n        optimizer.step()\n\n\nmodel_doc_pytorch = ModelCard()\nmodel_doc_pytorch.fill_model_info(\"This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation\")\nmodel_doc_pytorch.show_json()['model_details']['info']\n\n'This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation'"
  },
  {
    "objectID": "estimators.html",
    "href": "estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "In many instances, economists are interested in using machine learning models for specific purposes that go beyond their ability to predict variables to a good accuracy. For example:\nThe gingado.estimators module contains machine learning algorithms adapted to enable the types of analyses described above. More estimators can be expected over time.\nFor more academic discussions of machine learning methods in economics covering a broad range of topics, see Athey and Imbens (2019)."
  },
  {
    "objectID": "estimators.html#clustering",
    "href": "estimators.html#clustering",
    "title": "Estimators",
    "section": "Clustering",
    "text": "Clustering\nThe clustering algorithms used below are not themselves adapted from the general use methods. Rather, the functions offer convenience functionalities to find and retain the other variables in the same cluster.\nThese variables are usually entities (individuals, countries, stocks, etc) in a larger population.\nThe gingado clustering routines are designed to allow users standalone usage, or a seamless integration as part of a pipeline.\nThere are three levels of sophistication that users can choose from:\n\nusing the off-the-shelf clustering routines provided by gingado, which were selected to be applied cross various use cases;\nselecting an existing clustering routine from the scikit-learn.cluster module; or\ndesigning their own clustering algorithm.\n\n\nFindCluster\n\nFindCluster (cluster_alg: '[BaseEstimator, ClusterMixin]' = AffinityPropagation(), auto_document: 'ggdModelDocumentation' = &lt;class 'gingado.model_documentation.ModelCard'&gt;, random_state: 'int | None' = None)\n\nRetain only the columns of `X` that are in the same cluster as `y`.\n\nArgs:\n    cluster_alg (BaseEstimator|ClusterMixin): An instance of the clustering algorithm to use.\n    auto_document (ggdModelDocumentation): gingado Documenter template to facilitate model documentation.\n    random_state (int|None): The random seed to be used by the algorithm, if relevant. Defaults to None.\n\nfit\n\nfit (self, X, y)\n\nFit `FindCluster`.\n\nArgs:\n    X: The population of entities, organized in columns.\n    y: The entity of interest.\n\n\ntransform\n\ntransform (self, X) -&gt; 'np.array'\n\nKeep only the entities in `X` that belong to the same cluster as `y`.\n\nArgs:\n    X: The population of entities, organized in columns.\n\nReturns:\n    np.array: Columns of `X` that are in the same cluster as `y`.\n\n\nfit_transform\n\nfit_transform (self, X, y) -&gt; 'np.array'\n\nFit a `FindCluster` object and keep only the entities in `X` that belong to the same cluster as `y`.\n\nArgs:\n    X: The population of entities, organized in columns.\n    y: The entity of interest.\n\nReturns:\n    np.array: Columns of `X` that are in the same cluster as `y`.\n\n\ndocument\n\ndocument (self, documenter: 'ggdModelDocumentation | None' = None)\n\nDocument the `FindCluster` model using the template in `documenter`.\n\nArgs:\n    documenter (ggdModelDocumentation|None): A gingado Documenter or the documenter set in `auto_document` if None.\n        Defaults to None.\n\n\nExample: finding similar countries\nThe Barro and Lee (1994) dataset is used to illustrate the use of FindCluster. It is a country-level dataset. Let’s use it to answer the following question: for some specific country, what other countries are the closest to it considering the data available?\nFirst, we import the data:\n\nfrom gingado.datasets import load_BarroLee_1994\n\nThe data is organized by rows: each row is a different country, and the variables are organised in columns.\nThe dataset is originally organised for a regression of GDP growth (here denoted y) on the covariates (X). This is not what we want to do in this case. So instead of keeping GDP as a separate variable, the next step is to include it in the X DataFrame.\n\nX, y = load_BarroLee_1994()\nX['gdp'] = y\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\ngdp\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n-0.024336\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n0.100473\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n0.067051\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n0.064089\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n0.027930\n\n\n\n\n5 rows × 63 columns\n\n\n\nNow we remove the first column (an identifier) and transpose the DataFrame, so that countries are organized in columns.\nEach country is identified by a number: 0, 1, …\n\nX = X.iloc[:, 1:]\ncountries = X.T\ncountries.columns = ['country_' + str(c) for c in countries.columns]\ncountries.head()\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n\n\n5 rows × 90 columns\n\n\n\nSuppose we are interested in country No 13. What other countries are similar to it?\nFirst, country No 13 needs to be carved out of the DataFrame with the other countries.\nSecond, we can now pass the larger DataFrame and country 13’s data separately to an instance of FindCluster.\n\ncountry_of_interest = countries.pop('country_13')\n\n\nsimilar = FindCluster(AffinityPropagation(convergence_iter=5000))\nsimilar\n\nFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FindClusteriNot fittedFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000)) cluster_alg: AffinityPropagationAffinityPropagation(convergence_iter=5000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(convergence_iter=5000) \n\n\n\nsame_cluster = similar.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster.equals(similar.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster\n\n\n\n\n\n\n\n\ncountry_2\ncountry_9\ncountry_41\ncountry_48\ncountry_49\ncountry_52\ncountry_60\ncountry_64\ncountry_66\n\n\n\n\ngdpsh465\n8.895082\n8.151910\n7.360740\n6.469250\n5.762051\n9.224933\n8.346168\n7.655864\n7.830028\n\n\nbmp1l\n0.000000\n0.148400\n0.418100\n0.538800\n0.600500\n0.000000\n0.319900\n0.134500\n0.488000\n\n\nfreeop\n0.204244\n0.110885\n0.218471\n0.153491\n0.151848\n0.204244\n0.110885\n0.164598\n0.136287\n\n\nfreetar\n0.009186\n0.028579\n0.027087\n0.043888\n0.024100\n0.009186\n0.028579\n0.044446\n0.046730\n\n\nh65\n0.260000\n0.145000\n0.032000\n0.015000\n0.002000\n0.393000\n0.272000\n0.080000\n0.146000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.174100\n0.052400\n0.190500\n0.069200\n0.148400\n0.255800\n0.062500\n0.052500\n0.076400\n\n\nim1\n0.175000\n0.052300\n0.225700\n0.074800\n0.186400\n0.241200\n0.057800\n0.057200\n0.086600\n\n\nxr65\n1.082000\n2.119000\n3.949000\n0.348000\n7.367000\n1.017000\n36.603000\n30.929000\n40.500000\n\n\ntot1\n-0.010040\n0.007584\n0.205768\n0.035226\n0.007548\n0.018636\n0.014286\n-0.004592\n-0.007018\n\n\ngdp\n0.067051\n0.039147\n0.016775\n-0.048712\n0.024477\n0.050757\n-0.034045\n0.046010\n-0.011384\n\n\n\n\n62 rows × 9 columns\n\n\n\nThe default clustering algorithm used by FindCluster is affinity propagation (Frey and Dueck 2007). It is the algorithm of choice because of it combines several desireable characteristics, in particular: - the number of clusters is data-driven instad of set by the user, - the number of entities in each cluster is also chosen by the model, - all entities are part of a cluster, and - each cluster might have a different number of entities.\nHowever, we may want to try different clustering algorithms. Let’s compare the result above with the same analyses using DBSCAN (Ester et al. 1996).\n\nfrom sklearn.cluster import DBSCAN\n\n\nsimilar_dbscan = FindCluster(cluster_alg=DBSCAN())\nsimilar_dbscan\n\nFindCluster(cluster_alg=DBSCAN())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FindClusteriNot fittedFindCluster(cluster_alg=DBSCAN()) cluster_alg: DBSCANDBSCAN()  DBSCAN?Documentation for DBSCANDBSCAN() \n\n\n\nsame_cluster_dbscan = similar_dbscan.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster_dbscan.equals(similar_dbscan.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster_dbscan\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n0.063400\n0.034200\n0.086400\n0.059400\n0.052400\n...\n0.166200\n0.259700\n0.104400\n0.286600\n0.129600\n0.440700\n0.166900\n0.323800\n0.184500\n0.187600\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n0.076200\n0.042800\n0.093100\n0.046000\n0.052300\n...\n0.161700\n0.228800\n0.179600\n0.350000\n0.145800\n0.425700\n0.220100\n0.313400\n0.194000\n0.200700\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n1.000000\n12.499000\n7.000000\n1.000000\n2.119000\n...\n4.286000\n2.460000\n32.051000\n0.452000\n652.850000\n2.529000\n25.553000\n4.152000\n0.452000\n0.886000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n-0.001747\n0.009092\n0.011630\n0.008169\n0.007584\n...\n-0.006642\n-0.003241\n-0.034352\n-0.001660\n-0.046278\n-0.011883\n-0.039080\n0.005175\n-0.029551\n-0.036482\n\n\ngdp\n-0.024336\n0.100473\n0.067051\n0.064089\n0.027930\n0.046407\n0.067332\n0.020978\n0.033551\n0.039147\n...\n0.038095\n0.034213\n0.052759\n0.038416\n0.031895\n0.031196\n0.034096\n0.046900\n0.039773\n0.040642\n\n\n\n\n62 rows × 89 columns\n\n\n\nAs illustrated above, the results can be quite different. In this case, affinity propagation converged to more tightly defined clusters, while DBSCAN selected a cluster that contains almost all other countries (therefore, not useful in this particular case).\nNote that model documentation is already jumpstarted when the cluster is fit. A glimpse of the current template, including the questions in the documentation template that have been automatically filled, are shown below.\n\nsimilar.model_documentation.show_json()\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-27 08:49:13 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'clusterer',\n   'affinity_matrix_': array([[-4.23373922e+08, -5.97375771e+07, -5.35974361e+07, ...,\n           -1.92434215e+09, -8.60822083e+07, -3.77976931e+07],\n          [-5.97375771e+07, -4.23373922e+08, -2.26471602e+08, ...,\n           -2.66217555e+09, -2.43057326e+06, -1.92555486e+08],\n          [-5.35974361e+07, -2.26471602e+08, -4.23373922e+08, ...,\n           -1.33575671e+09, -2.75395788e+08, -1.37934978e+06],\n          ...,\n          [-1.92434215e+09, -2.66217555e+09, -1.33575671e+09, ...,\n           -4.23373922e+08, -2.82418157e+09, -1.42280304e+09],\n          [-8.60822083e+07, -2.43057326e+06, -2.75395788e+08, ...,\n           -2.82418157e+09, -4.23373922e+08, -2.37881124e+08],\n          [-3.77976931e+07, -1.92555486e+08, -1.37934978e+06, ...,\n           -1.42280304e+09, -2.37881124e+08, -4.23373922e+08]]),\n   'cluster_centers_': array([[ 6.82979374e+00,  6.14100000e-01,  3.13509000e-01, ...,\n            5.25000000e-01,  5.75000000e-03,  1.00472567e-01],\n          [ 8.89508153e+00,  0.00000000e+00,  2.04244000e-01, ...,\n            1.08200000e+00, -1.00400000e-02,  6.70514822e-02],\n          [ 7.56527528e+00,  1.99700000e-01,  2.48714000e-01, ...,\n            6.62500000e+00, -2.19500000e-03,  6.40891662e-02],\n          ...,\n          [ 8.33254894e+00,  0.00000000e+00,  3.45485000e-01, ...,\n            4.52000000e-01, -1.66000000e-03,  3.84156381e-02],\n          [ 8.86531163e+00,  0.00000000e+00,  2.82939000e-01, ...,\n            4.52000000e-01, -2.95510000e-02,  3.97733722e-02],\n          [ 8.91233857e+00,  0.00000000e+00,  1.50366000e-01, ...,\n            8.86000000e-01, -3.64820000e-02,  4.06415381e-02]]),\n   'cluster_centers_indices_': array([ 1,  2,  3,  4,  5,  7,  8, 10, 13, 14, 16, 18, 19, 25, 27, 32, 35,\n          39, 42, 45, 46, 49, 50, 52, 53, 55, 57, 58, 60, 62, 67, 68, 69, 71,\n          76, 82, 87, 88], dtype=int64),\n   'feature_names_in_': array(['gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65', 'hf65',\n          'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65', 'mort65',\n          'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n          'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65',\n          'highm65', 'highf65', 'highc65', 'highcm65', 'highcf65', 'human65',\n          'humanm65', 'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65',\n          'nom65', 'nof65', 'pinstab1', 'pop65', 'worker65', 'pop1565',\n          'pop6565', 'sec65', 'secm65', 'secf65', 'secc65', 'seccm65',\n          'seccf65', 'syr65', 'syrm65', 'syrf65', 'teapri65', 'teasec65',\n          'ex1', 'im1', 'xr65', 'tot1', 'gdp'], dtype=object),\n   'labels_': array([29,  0,  1,  2,  3,  4, 18,  5,  6,  1,  7, 30, 14,  8,  9, 29, 10,\n          29, 11, 12, 12, 18, 29, 36, 18, 13, 18, 14, 29, 36, 36, 14, 15, 36,\n          29, 16, 18, 14, 36, 17,  1, 14, 18, 29, 29, 19, 20,  1,  1, 21, 22,\n           1, 23, 24, 21, 25, 36, 26, 27,  1, 28, 12, 29,  1, 14,  1, 29, 30,\n          31, 32, 12, 33, 18, 29, 30, 18, 34, 14, 18, 36, 36, 29, 35, 36, 29,\n          29, 14, 36, 37,  1], dtype=int64),\n   'n_features_in_': 62,\n   'n_iter_': 200},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nFindCluster can also be used as part of a pipeline. In this case, only the entities in the same cluster as the entity of interest will continue on to the next steps of the estimation.\n\nfrom gingado.benchmark import RegressionBenchmark\nfrom sklearn.pipeline import Pipeline\n\n\npipe = Pipeline([\n    ('cluster', FindCluster(AffinityPropagation(convergence_iter=5000))),\n    ('rf', RegressionBenchmark())\n])\n\n\npipe.fit(X=countries, y=country_of_interest)\n\nPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))]) cluster: FindClusterFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000)) cluster_alg: AffinityPropagationAffinityPropagation(convergence_iter=5000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(convergence_iter=5000) rf: RegressionBenchmarkRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True)"
  },
  {
    "objectID": "estimators.html#comparative-case-studies",
    "href": "estimators.html#comparative-case-studies",
    "title": "Estimators",
    "section": "Comparative case studies",
    "text": "Comparative case studies\n\nMachineControl\n\nMachineControl (cluster_alg: '[BaseEstimator, ClusterMixin] | None' = AffinityPropagation(), estimator: 'BaseEstimator' = RegressionBenchmark(), manifold: 'BaseEstimator' = TSNE(), with_placebo: 'bool' = True, auto_document: 'ggdModelDocumentation' = &lt;class 'gingado.model_documentation.ModelCard'&gt;, random_state: 'int | None' = None)\n\nSynthetic controls with machine learning methods\n\nArgs:\n    cluster_alg (BaseEstimator | ClusterMixin | None): An instance of the clustering algorithm to use, or None to retain all entities.\n    estimator (BaseEstimator): Method to weight the control entities.\n    manifold (BaseEstimator): Algorithm for manifold learning.\n    with_placebo (bool): Include placebo estimations during prediction?\n    auto_document (ggdModelDocumentation): gingado Documenter template to facilitate model documentation.\n    random_state (int | None): The random seed to be used by the algorithm, if relevant.\n\nfit\n\nfit (self, X: 'pd.DataFrame', y: 'pd.DataFrame | pd.Series')\n\nFit the `MachineControl` model.\n\nArgs:\n    X (pd.DataFrame): A pandas DataFrame with pre-intervention data of shape (n_samples, n_control_entities).\n    y (pd.DataFrame | pd.Series): A pandas DataFrame or Series with pre-intervention data of shape (n_samples,).\n\n\npredict\n\npredict (self, X: 'pd.DataFrame', y: 'pd.DataFrame | pd.Series')\n\nCalculate the model predictions before and after the intervention.\n\nArgs:\n    X (pd.DataFrame): A pandas DataFrame with complete time series (pre- and post-intervention) of shape (n_samples, n_control_entities).\n    y (pd.DataFrame | pd.Series): A pandas DataFrame or Series with complete time series of shape (n_samples,).\n\n\nget_controls\n\nget_controls (self)\n\nGet the list of control entities\n\n\ndocument\n\ndocument (self, documenter: 'ggdModelDocumentation | None' = None)\n\nDocument the `MachineControl` model using the template in `documenter`.\n\nArgs:\n    documenter (ggdModelDocumentation | None): A gingado Documenter or the documenter set in `auto_document` if None.\n\n\nBrief econometric description\nThe goal of MachineControl is to estimate:\n\\[\n\\tau_t = Y_{1, t}^{I} - Y_{1, t}^{N}, t &gt; T0\n\\]\nwhere:\n\n\\(\\tau\\) is the effect on entity \\(i=1\\) of the intervention of interest\nwithout loss of generality, \\(i=1\\) is an entity that has undergone the intervention of interest, amongst \\(N\\) total entities\ntime period \\(T0\\) is a date in which the intervention occurred\nsuperscript \\(I\\) in an outcome variable denotes the occurence of the intervention, whereas superscript \\(N\\) is absence of intervention\nfor \\(t &gt; T0\\), \\(Y_{i, t}^{I}\\) is observed while \\(Y_{i, t}^{N}\\) must be estimated because it is a counterfacual.\n\n\\(Y_{i, t}^{N}\\) is calculated from the values of the other entities, \\(i \\neq 1\\). Collect this data in a vector \\(\\mathbb{Y}_{-1, t}^{N}\\). Then, following Doudchenko and Imbens (2016):\n\\[\n\\hat{Y}_{i, t}^{N} = f^*(\\mathbb{Y}_{-1, t}^{N}),\n\\]\nwith the star (\\(*\\)) superscript on the function \\(f(\\cdot)\\) representing that it was trained only with data up until the intervention date. The exact form of \\(f(\\cdot)\\) depends on the argument estimator. A general use estimator is the random forest (Breiman 2001).\nThe panel data itself might be the whole population in the data, or a subset when using the whole population might be too cumbersome to run analyses (eg, if the data contains too many entities). One way to select this subsample of control units without including subjective judgment in the data is quantitatilve. The control units are selected through a clustering algorithm (argument cluster_arg). One cluster algorithm that can be used is affinity propagation (Frey and Dueck 2007).\nTo finalise, the quality of the synthetic control can be assessed in many ways. One fully data-driven way to achieve this is by using manifold learning: lower-dimensional embeddings of a higher-dimensional data. A preferred manifold learning algorithm is t-SNE (Van der Maaten and Hinton 2008).\nThe relative distance between embeddings and the target centre, as well as the control and the target, represent the chance that a better feasible control (either from real or combined) will materialise. The intuition behind this test is:\n\nlet \\(d_{i,j}\\) be the Euclidean distance between the embeddings (2d points) of entities \\(i\\) and \\(j\\)\nif only a very small percentage of \\(d_{1, j \\in (2, ..., N)}\\) are lower than \\(d_{1, \\text{Synthetic control}}\\), than the synthetic control produced with \\(f(\\cdot)\\) is indeed a formula that provides one of the best alternative.\n\nMain references:\n\nAbadie and Gardeazabal (2003)\nAbadie, Diamond, and Hainmueller (2010)\nAbadie, Diamond, and Hainmueller (2015)\nDoudchenko and Imbens (2016)\nAbadie (2021)\n\n\n\nExample: impact of labour reform on productivity\nSee Machine controls: Synthetic controls with machine learning."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\n\n\n\nread_attr (obj)\n\nReads and yields the type and values of fitted attributes from an object.\n\nArgs:\n    obj: Object from which attributes will be read.\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=662456808),\n    DecisionTreeRegressor(max_features=1.0, random_state=2115260158),\n    DecisionTreeRegressor(max_features=1.0, random_state=1570210276)]},\n  {'estimators_samples_': [array([0, 1], dtype=int32),\n    array([1, 1], dtype=int32),\n    array([1, 1], dtype=int32)]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-model-documentation",
    "href": "utils.html#support-for-model-documentation",
    "title": "Utils",
    "section": "",
    "text": "get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\n\n\n\nread_attr (obj)\n\nReads and yields the type and values of fitted attributes from an object.\n\nArgs:\n    obj: Object from which attributes will be read.\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=662456808),\n    DecisionTreeRegressor(max_features=1.0, random_state=2115260158),\n    DecisionTreeRegressor(max_features=1.0, random_state=1570210276)]},\n  {'estimators_samples_': [array([0, 1], dtype=int32),\n    array([1, 1], dtype=int32),\n    array([1, 1], dtype=int32)]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-time-series",
    "href": "utils.html#support-for-time-series",
    "title": "Utils",
    "section": "Support for time series",
    "text": "Support for time series\nObjects of the class Lag are similar to scikit-learn’s transformers.\n\nLag\n\nLag (lags=1, jump=0, keep_contemporaneous_X=False)\n\nA transformer for lagging variables.\n\nArgs:\n    lags (int): The number of lags to apply.\n    jump (int): The number of initial observations to skip before applying the lag.\n    keep_contemporaneous_X (bool): Whether to keep the contemporaneous values of X in the output.\n\n\nfit\n\nfit (self, X: numpy.ndarray, y=None)\n\nFits the Lag transformer.\n\nArgs:\n    X (np.ndarray): Array-like data of shape (n_samples, n_features).\n    y: Array-like data of shape (n_samples,) or (n_samples, n_targets) or None.\n    \nReturns:\n    self: A fitted version of the `Lag` instance.\n\n\ntransform\n\ntransform (self, X: numpy.ndarray)\n\nApplies the lag transformation to the dataset `X`.\n\nArgs:\n    X (np.ndarray): Array-like data of shape (n_samples, n_features).\n    \nReturns:\n    A lagged version of `X`.\n\n\nfit_transform\n\nfit_transform (self, X, y=None, **fit_params)\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.\nThe code below demonstrates how Lag works in practice. Note in particular that, because Lag is a transformer, it can be used as part of a scikit-learn’s Pipeline.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nrandomX = np.random.rand(15, 2)\nrandomY = np.random.rand(15)\n\nlags = 3\njump = 2\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lagger', Lag(lags=lags, jump=jump, keep_contemporaneous_X=False))\n]).fit_transform(randomX, randomY)\n\nBelow we confirm that the lagger removes the correct number of rows corresponding to the lagged observations:\n\nassert randomX.shape[0] - lags - jump == pipe.shape[0]\n\nAnd because Lag is a transformer, its parameters (lags and jump) can be calibrated using hyperparameter tuning to achieve the best performance for a model."
  },
  {
    "objectID": "utils.html#support-for-data-augmentation-with-sdmx",
    "href": "utils.html#support-for-data-augmentation-with-sdmx",
    "title": "Utils",
    "section": "Support for data augmentation with SDMX",
    "text": "Support for data augmentation with SDMX\n\n\n\n\n\n\nNote\n\n\n\nplease note that working with SDMX may take some minutes depending on the amount of information you are downloading.\n\n\n\nlist_SDMX_sources\n\nlist_SDMX_sources ()\n\nFetches the list of SDMX sources.\n\nReturns:\n    The list of codes representing the SDMX sources available for data download.\n\nsources = list_SDMX_sources()\nprint(sources)\n\nassert len(sources) &gt; 0\n# all elements are of type 'str'\nassert sum([isinstance(src, str) for src in sources]) == len(sources)\n\n['ABS', 'ABS_JSON', 'BBK', 'BIS', 'COMP', 'ECB', 'EMPL', 'ESTAT', 'ESTAT3', 'ESTAT_COMEXT', 'GROW', 'ILO', 'IMF', 'INEGI', 'INSEE', 'ISTAT', 'LSD', 'NB', 'NBB', 'OECD', 'OECD_JSON', 'SGR', 'SPC', 'STAT_EE', 'UNESCO', 'UNICEF', 'UNSD', 'WB', 'WB_WDI']\n\n\n\n\nlist_all_dataflows\n\nlist_all_dataflows (codes_only: bool = False, return_pandas: bool = True)\n\nLists all SDMX dataflows. Note: When using as a parameter to an `AugmentSDMX` object\nor to the `load_SDMX_data` function, set `codes_only=True`\"\n\nArgs:\n    codes_only (bool): Whether to return only the dataflow codes.\n    return_pandas (bool): Whether to return the result in a pandas DataFrame format.\n    \nReturns:\n    All available dataflows for all SDMX sources.\n\ndflows = list_all_dataflows(return_pandas=False)\n\nassert isinstance(dflows, dict)\nall_sources = list_SDMX_sources()\nassert len([s for s in dflows.keys() if s in all_sources]) == len(dflows.keys())\n\nlist_all_dataflows returns by default a pandas Series, facilitating data discovery by users like so:\n\ndflows = list_all_dataflows(return_pandas=True)\nassert type(dflows) == pd.core.series.Series\n\ndflows\n\nABS   ABORIGINAL_POP_PROJ                 Projected population, Aboriginal and Torres St...\n      ABORIGINAL_POP_PROJ_REMOTE          Projected population, Aboriginal and Torres St...\n      ABS_ABORIGINAL_POPPROJ_INDREGION    Projected population, Aboriginal and Torres St...\n      ABS_ACLD_LFSTATUS                   Australian Census Longitudinal Dataset (ACLD):...\n      ABS_ACLD_TENURE                     Australian Census Longitudinal Dataset (ACLD):...\n                                                                ...                        \nUNSD  DF_UNData_UNFCC                                                       SDMX_GHG_UNDATA\nWB    DF_WITS_Tariff_TRAINS                                WITS - UNCTAD TRAINS Tariff Data\n      DF_WITS_TradeStats_Development                             WITS TradeStats Devlopment\n      DF_WITS_TradeStats_Tariff                                      WITS TradeStats Tariff\n      DF_WITS_TradeStats_Trade                                        WITS TradeStats Trade\nName: dataflow, Length: 31499, dtype: object\n\n\nThis format allows for more easily searching dflows by source:\n\nlist_all_dataflows(codes_only=True, return_pandas=True)\n\nABS   0                 ABORIGINAL_POP_PROJ\n      1          ABORIGINAL_POP_PROJ_REMOTE\n      2    ABS_ABORIGINAL_POPPROJ_INDREGION\n      3                   ABS_ACLD_LFSTATUS\n      4                     ABS_ACLD_TENURE\n                         ...               \nUNSD  3                     DF_UNData_UNFCC\nWB    0               DF_WITS_Tariff_TRAINS\n      1      DF_WITS_TradeStats_Development\n      2           DF_WITS_TradeStats_Tariff\n      3            DF_WITS_TradeStats_Trade\nName: dataflow, Length: 31499, dtype: object\n\n\n\ndflows['BIS']\n\nBIS_REL_CAL                                     BIS_RELEASE_CALENDAR\nWS_CBPOL                                                 Policy rate\nWS_CBS_PUB                                  BIS consolidated banking\nWS_CPMI_CASHLESS                      CPMI cashless payments (T5,T6)\nWS_CPMI_CT1                           CPMI comparative tables type 1\nWS_CPMI_CT2                           CPMI comparative tables type 2\nWS_CPMI_DEVICES                            CPMI payment devices (T4)\nWS_CPMI_INSTITUT                              CPMI institutions (T3)\nWS_CPMI_MACRO                                     CPMI macro (T1,T2)\nWS_CPMI_PARTICIP                  CPMI participants (T7,T10,T12,T15)\nWS_CPMI_SYSTEMS     CPMI systems (T8,T9,T11,T13,T14,T16,T17,T18,T19)\nWS_CPP                                    Commercial property prices\nWS_CREDIT_GAP                                 BIS credit-to-GDP gaps\nWS_DEBT_SEC2_PUB    BIS international debt securities (BIS-compiled)\nWS_DER_OTC_TOV                              OTC derivatives turnover\nWS_DPP                          Detailed residential property prices\nWS_DSR                                        BIS debt service ratio\nWS_EER                                  BIS effective exchange rates\nWS_GLI                                   Global liquidity indicators\nWS_LBS_D_PUB                                  BIS locational banking\nWS_LONG_CPI                                 BIS long consumer prices\nWS_NA_SEC_C3                          BIS debt securities statistics\nWS_NA_SEC_DSS                         BIS Debt securities statistics\nWS_OTC_DERIV2                            OTC derivatives outstanding\nWS_SPP                          Selected residential property prices\nWS_TC                                BIS long series on total credit\nWS_XRU                                      US dollar exchange rates\nWS_XTD_DERIV                             Exchange traded derivatives\nName: dataflow, dtype: object\n\n\nOr the user can search dataflows by their human-readable name instead of their code. For example, this is one way to see if any dataflow has information on interest rates:\n\ndflows[dflows.str.contains('Interest rate', case=False)]\n\nECB     IRS                                            Interest rate statistics\n        MIR                                        MFI Interest Rate Statistics\n        RIR                                               Retail Interest Rates\nESTAT   TEIMF040                                          3-month-interest rate\n        TEIMF100                         Day-to-day money market interest rates\n        IRT_ST_A                      Money market interest rates - annual data\n        IRT_ST_M                     Money market interest rates - monthly data\n        IRT_ST_Q                   Money market interest rates - quarterly data\n        EI_MFIR_M                                 Interest rates - monthly data\n        ENPE_IRT_LD                     Loan and deposit one year interest rate\n        ENPE_IRT_ST                                 Money market interest rates\nESTAT3  IRT_ST_A                      Money market interest rates - annual data\n        IRT_ST_M                     Money market interest rates - monthly data\n        IRT_ST_Q                   Money market interest rates - quarterly data\n        EI_MFIR_M                                 Interest rates - monthly data\n        ENPE_IRT_LD                     Loan and deposit one year interest rate\n        ENPE_IRT_ST                                 Money market interest rates\n        TEIMF040                                          3-month-interest rate\n        TEIMF100                         Day-to-day money market interest rates\nIMF     6SR                   M&B: Interest Rates and Share Prices (6SR) for...\n        INR                                                      Interest rates\n        INR_NSTD                                    Interest rates_Non-Standard\nNB      GOVT_GENERIC_RATES                               Generic interest rates\n        GOVT_IRS                                            Interest rate swaps\nName: dataflow, dtype: object\n\n\nThe function load_SDMX_data is a convenience function that downloads data from SDMX sources (and any specific dataflows passed as arguments) if they match the key and parameters set by the user.\n\n\nload_SDMX_data\n\nload_SDMX_data (sources: dict, keys: dict, params: dict, verbose: bool = True)\n\nLoads datasets from SDMX.\n\nArgs:\n    sources (dict): A dictionary with the sources and dataflows per source.\n    keys (dict): The keys to be used in the SDMX query.\n    params (dict): The parameters to be used in the SDMX query.\n    verbose (bool): Whether to communicate download steps to the user.\n    \nReturns:\n    A pandas DataFrame with data from SDMX or None if no data matches the sources, keys, and parameters.\n\ndf = load_SDMX_data(sources={'ECB': 'CISS', 'BIS': 'WS_CBPOL_D'}, keys={'FREQ': 'D'}, params={'startPeriod': 2003})\n\nassert type(df) == pd.DataFrame\nassert df.shape[0] &gt; 0\nassert df.shape[1] &gt; 0\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL' - Policy rate..."
  },
  {
    "objectID": "barrolee1994.html",
    "href": "barrolee1994.html",
    "title": "Using gingado to understand economic growth",
    "section": "",
    "text": "This notebook showcases one possible use of gingado by estimating economic growth across countries, using the dataset studied by Barro and Lee (1994). You can run this notebook interactively, by clicking on the appropriate link above.\nThis dataset has been widely studied in economics. Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021) are two studies of this dataset that are most related to machine learning.\nThis notebook will use gingado to compare quickly setup a well-performing machine learning model and use its results as evidence to support the conditional convergence hypothesis; compare different classes of models (and their combination in a single model), and use and document the best performing alternative.\nBecause the notebook is for pedagogical purposes only, please bear in mind some aspects of the machine learning workflow (such as carefully thinking about the cross-validation strategy) are glossed over in this notebook. Also, only the key academic references are cited; more references can be found in the papers mentioned in this example."
  },
  {
    "objectID": "barrolee1994.html#setting-the-stage",
    "href": "barrolee1994.html#setting-the-stage",
    "title": "Using gingado to understand economic growth",
    "section": "Setting the stage",
    "text": "Setting the stage\nWe will import packages as the work progresses. This will help highlight the specific steps in the workflow that gingado can be helpful with.\n\n\nCode\nimport pandas as pd\n\n\nThe data is available in the online annex to Giannone, Lenza, and Primiceri (2021). In that paper, this dataset corresponds to what the authors call “macro2”. The original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\n\nCode\nfrom gingado.datasets import load_BarroLee_1994\n\nX, y = load_BarroLee_1994()\n\n\nThe dataset contains explanatory variables representing per-capita growth between 1960 and 1985, for 90 countries.\n\n\nCode\nX.columns\n\n\nIndex(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65',\n       'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65',\n       'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n       'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65', 'highm65',\n       'highf65', 'highc65', 'highcm65', 'highcf65', 'human65', 'humanm65',\n       'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65', 'nom65', 'nof65',\n       'pinstab1', 'pop65', 'worker65', 'pop1565', 'pop6565', 'sec65',\n       'secm65', 'secf65', 'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65',\n       'syrf65', 'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'],\n      dtype='object')\n\n\n\n\nCode\nX.head().T\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nUnnamed: 0\n0.000000\n1.000000\n2.000000\n3.000000\n4.000000\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n\n\n...\n...\n...\n...\n...\n...\n\n\nteasec65\n17.300000\n18.000000\n20.700000\n22.700000\n17.600000\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n\n\n\n\n62 rows × 5 columns\n\n\n\n\nThe outcome variable is represented here:\n\n\nCode\ny.plot.hist(bins=90, title='GDP growth')"
  },
  {
    "objectID": "barrolee1994.html#establishing-a-benchmark-model",
    "href": "barrolee1994.html#establishing-a-benchmark-model",
    "title": "Using gingado to understand economic growth",
    "section": "Establishing a benchmark model",
    "text": "Establishing a benchmark model\nGenerally speaking, it is a good idea to establish a benchmark model at the first stages of development of the machine learning model. gingado offers a class of automatic benchmarks that can be used off-the-shelf depending on the task at hand: RegressionBenchmark and ClassificationBenchmark. It is also good to keep in mind that more advanced users can create their own benchmark on top of a base class provided by gingado: ggdBenchmark.\nFor this application, since we are interested in running a regression task, we will use RegressionBenchmark:\n\n\nCode\nfrom gingado.benchmark import RegressionBenchmark\n\n\nWhat this object does is the following:\n\nit creates a random forest\nthree different versions of the random forest are trained on the user data\nthe version that performs better is chosen as the benchmark\nright after it is trained, the benchmark is documented using gingado’s ModelCard documenter.\n\nThe user can easily change the parameters above. For example, instead of a random forest the user might prefer a neural network as the benchmark. Or, in lieu of the default parameters provided by gingado, users might have their own idea of what could be a reasonable parameter space to search.\nRandom forests are chosen as the go-to benchmark algorithm because of their reasonably good performance in a wide variety of settings, the fact that they don’t require much data transformation (ie, normalising the data to have zero mean and one standard deviation), and by virtue of their relatively transparency about the importance of each regressor.\nThe first step is to initialise the benchmark object. At this time, we pass some arguments about how we want it to behave. In this case, we set the verbosity level to produce output related to each alternative considered. Then we fit it to the data.\n\n\nCode\n#####\n#####\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X, y)\n\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor() \n\n\n\n\nCode\nbenchmark = RegressionBenchmark(verbose_grid=2)\nbenchmark.fit(X, y)\n\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n\n\nRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    verbose_grid=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RegressionBenchmarkiFittedRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    verbose_grid=2) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True) \n\n\nAs we can see above, with a few lines we have trained a random forest on the dataset. In this case, the benchmark was the better of six versions of the random forest, according to the default hyperparameters: 100 and 250 estimators were alternated with models for which the maximum number of regressors analysed by individual trees changesd fom the maximum, a square root and a log of the number of regressors. They were each trained using a 5-fold cross-validation.\nLet’s see which one was the best performing in this case, and hence our benchmark model:\n\n\nCode\npd.DataFrame(benchmark.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.279105\n0.619514\n0.261257\n0.639284\n0.551784\n1.295209\n\n\nstd_fit_time\n0.030683\n0.040625\n0.027281\n0.063855\n0.064243\n0.068889\n\n\nmean_score_time\n0.008451\n0.017304\n0.009542\n0.020414\n0.007748\n0.01687\n\n\nstd_score_time\n0.001713\n0.000956\n0.004093\n0.005933\n0.000585\n0.000892\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n0.022222\n0.158854\n0.112627\n0.093837\n0.085489\n0.042874\n\n\nsplit1_test_score\n-0.274611\n-0.427184\n-0.206619\n-0.360401\n-0.245164\n-0.258687\n\n\nsplit2_test_score\n0.039958\n0.196241\n0.198039\n0.183407\n0.388704\n0.397979\n\n\nsplit3_test_score\n0.178222\n0.318748\n0.271443\n0.214551\n0.448291\n0.413067\n\n\nsplit4_test_score\n0.210136\n0.007752\n0.061273\n0.08617\n0.050974\n-0.173193\n\n\nsplit5_test_score\n-1.262246\n-1.068622\n-0.973107\n-1.10762\n-1.538264\n-1.459526\n\n\nsplit6_test_score\n-0.974115\n-0.838309\n-0.89245\n-0.73637\n-0.492698\n-0.295093\n\n\nsplit7_test_score\n-0.400739\n-0.486603\n-0.443932\n-0.393923\n-0.454213\n-0.36956\n\n\nsplit8_test_score\n0.278045\n0.317476\n0.162235\n0.264284\n0.392476\n0.482193\n\n\nsplit9_test_score\n0.071673\n0.010885\n0.022654\n0.00408\n-0.031031\n0.021179\n\n\nmean_test_score\n-0.211145\n-0.181076\n-0.168784\n-0.175198\n-0.139543\n-0.119877\n\n\nstd_test_score\n0.499279\n0.469299\n0.430635\n0.437372\n0.565479\n0.535807\n\n\nrank_test_score\n6\n5\n3\n4\n2\n1\n\n\n\n\n\n\n\n\nThe values above are calculated with \\(R^2\\), the default scoring function for a random forest from the scikit-learn package. Suppose that instead we would like a benchmark model that is optimised on the maximum error, ie a benchmark that minimises the worst deviation from prediction to ground truth for all the sample. These are the steps that we would take. Note that a more complete list of ready-made scoring parameters and how to create your own function can be found here.\n\n\nCode\nbenchmark_lower_worsterror = RegressionBenchmark(scoring='max_error', verbose_grid=2)\nbenchmark_lower_worsterror.fit(X, y)\n\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n\n\nRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    scoring='max_error', verbose_grid=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RegressionBenchmarkiFittedRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    scoring='max_error', verbose_grid=2) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True) \n\n\n\n\nCode\npd.DataFrame(benchmark_lower_worsterror.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.245649\n0.600232\n0.235427\n0.571515\n0.479157\n1.181209\n\n\nstd_fit_time\n0.02014\n0.03286\n0.021821\n0.017653\n0.050392\n0.049456\n\n\nmean_score_time\n0.008057\n0.016804\n0.00694\n0.015608\n0.007507\n0.016092\n\n\nstd_score_time\n0.001876\n0.002585\n0.000218\n0.000306\n0.002461\n0.002455\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n-0.132316\n-0.141783\n-0.134236\n-0.136418\n-0.152165\n-0.153249\n\n\nsplit1_test_score\n-0.065404\n-0.067706\n-0.069448\n-0.064972\n-0.069087\n-0.066568\n\n\nsplit2_test_score\n-0.09812\n-0.099327\n-0.089429\n-0.096259\n-0.085318\n-0.084684\n\n\nsplit3_test_score\n-0.095069\n-0.085478\n-0.092528\n-0.096081\n-0.086887\n-0.079559\n\n\nsplit4_test_score\n-0.142936\n-0.145781\n-0.147287\n-0.139048\n-0.148897\n-0.149139\n\n\nsplit5_test_score\n-0.125054\n-0.123047\n-0.124663\n-0.123743\n-0.120451\n-0.121171\n\n\nsplit6_test_score\n-0.122386\n-0.124481\n-0.129092\n-0.121093\n-0.118594\n-0.125071\n\n\nsplit7_test_score\n-0.040615\n-0.047997\n-0.049907\n-0.04308\n-0.054585\n-0.048357\n\n\nsplit8_test_score\n-0.132765\n-0.133358\n-0.141014\n-0.130115\n-0.16107\n-0.163303\n\n\nsplit9_test_score\n-0.075396\n-0.080692\n-0.083752\n-0.081803\n-0.090028\n-0.095199\n\n\nmean_test_score\n-0.103006\n-0.104965\n-0.106136\n-0.103261\n-0.108708\n-0.10863\n\n\nstd_test_score\n0.032182\n0.03189\n0.03169\n0.030802\n0.035146\n0.037483\n\n\nrank_test_score\n1\n3\n4\n2\n6\n5\n\n\n\n\n\n\n\n\nNow we even have two benchmark models.\nWe could further tweak and adjust them, but one of the ideas behind having a benchmark is that it is simple and easy to set up.\nLet’s retain only the first benchmark, for simplicity, and now look at the predictions, comparing them to the original growth values.\n\n\nCode\ny_pred = benchmark.predict(X)\n\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(\n        x='y', y='y_pred',\n         grid=True, \n         title='Actual and predicted outcome',\n         xlabel='actual GDP growth',\n         ylabel='predicted GDP growth')\n\n\n\n\n\n\n\n\n\nAnd now a histogram of the benchmark’s errors:\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30, title='Residual')\n\n\n\n\n\n\n\n\n\nSince the benchmark is a random forest model, we can see what are the most important regressors, measured as the average reduction in impurity across the trees in the random forest that actually use that particular regressor. They are scaled so that the sum for all features is one. Higher importance amounts indicate that that particular regressor is a more important contributor to the final prediction.\n\n\nCode\nregressor_importance = pd.DataFrame(\n    benchmark.benchmark.best_estimator_.feature_importances_, \n    index=X.columns, \n    columns=[\"Importance\"]\n    )\n\nregressor_importance.sort_values(by=\"Importance\", ascending=False) \\\n    .plot.bar(figsize=(20, 8), title='Regressor importance')\n\n\n\n\n\n\n\n\n\nFrom the graph above, we can see that the regressor bmp1l (black-market premium on foreign exchange) predominates. Interestingly, Belloni, Chernozhukov, and Hansen (2011) using squared-root lasso also find this regressor to be important."
  },
  {
    "objectID": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "href": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "title": "Using gingado to understand economic growth",
    "section": "Testing the conditional converge hypothesis",
    "text": "Testing the conditional converge hypothesis\nNow we can leverage our automatic benchmark model to test the conditional converge hypothesis - ie, the preposition that countries with lower starting GDP tend to grow faster than other comparable countries. In other words, this hypothesis predicts that when GDP growth is regressed on the level of past GDP and on an adequate set of covariates \\(X\\), the coefficient on past GDP levels are negative.\nSince we have the results for the importance of each regressor in separating countries by their growth result, we can compare the estimated coefficient for GDP levels in regressions that include different regressors in the vector \\(X\\). To maintain this example a simple exercise, the following three models are estimated:\n\n\\(X\\) contains the five most important regressors, as estimated by the benchmark model (see the graph above)\n\\(X\\) contains the five least important regressors, from the same estimation as above\n\\(X\\) is the empty set - in other words, this is a simple equation on GDP growth on GDP levels\n\nA result that would be consistent with the conditionality of the conditional convergence hypothesis is the first equation resulting in a negative coefficient for starting GDP, while the following two equations may not necessarily be successful in identifying a negative coefficient. This is because the least important regressors are not likely to have sufficient predictive power to separate countries into comparable groups.\nThe five more and less important regressors are:\n\n\nCode\ntop_five = regressor_importance.sort_values(by=\"Importance\", ascending=False).head(5)\nbottom_five = regressor_importance.sort_values(by=\"Importance\", ascending=True).head(5)\n\ntop_five, bottom_five\n\n\n(            Importance\n bmp1l         0.170650\n pop6565       0.111373\n teasec65      0.047904\n Unnamed: 0    0.038037\n gpop1         0.036916,\n           Importance\n humanf65    0.001992\n pm65        0.002268\n s65         0.002466\n highc65     0.002601\n human65     0.002661)\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n\n\n\nCode\ngdp_level = 'gdpsh465'\n\n\n\n\nCode\nX_topfive = X[[gdp_level] + list(top_five.index)]\nX_topfive = sm.add_constant(X_topfive)\nX_topfive.head()\n\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nbmp1l\npop6565\nteasec65\nUnnamed: 0\ngpop1\n\n\n\n\n0\n1.0\n6.591674\n0.2837\n0.027591\n17.3\n0\n0.0203\n\n\n1\n1.0\n6.829794\n0.6141\n0.035637\n18.0\n1\n0.0185\n\n\n2\n1.0\n8.895082\n0.0000\n0.076685\n20.7\n2\n0.0188\n\n\n3\n1.0\n7.565275\n0.1997\n0.031039\n22.7\n3\n0.0345\n\n\n4\n1.0\n7.162397\n0.1740\n0.026281\n17.6\n4\n0.0310\n\n\n\n\n\n\n\n\n\n\nCode\nX_bottomfive = X[[gdp_level] + list(bottom_five.index)]\nX_bottomfive = sm.add_constant(X_bottomfive)\nX_bottomfive.head()\n\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nhumanf65\npm65\ns65\nhighc65\nhuman65\n\n\n\n\n0\n1.0\n6.591674\n0.043\n0.37\n0.04\n0.09\n0.301\n\n\n1\n1.0\n6.829794\n0.257\n1.00\n0.16\n0.63\n0.706\n\n\n2\n1.0\n8.895082\n8.384\n1.00\n0.56\n4.50\n8.317\n\n\n3\n1.0\n7.565275\n3.807\n1.00\n0.24\n2.11\n3.833\n\n\n4\n1.0\n7.162397\n1.720\n0.85\n0.17\n0.45\n1.900\n\n\n\n\n\n\n\n\n\n\nCode\nX_onlyGDPlevel = sm.add_constant(X[gdp_level])\nX_onlyGDPlevel.head()\n\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\n\n\n\n\n0\n1.0\n6.591674\n\n\n1\n1.0\n6.829794\n\n\n2\n1.0\n8.895082\n\n\n3\n1.0\n7.565275\n\n\n4\n1.0\n7.162397\n\n\n\n\n\n\n\n\n\n\nCode\nmodels = dict(\n    topfive = sm.OLS(y, X_topfive).fit(),\n    bottomfive = sm.OLS(y, X_bottomfive).fit(),\n    onlyGDPlevel = sm.OLS(y, X_onlyGDPlevel).fit()\n)\n\n\n\n\nCode\ncoefs = pd.DataFrame({name: model.conf_int().loc[gdp_level] for name, model in models.items()})\ncoefs.loc[0.5] = [model.params[gdp_level] for _, model in models.items()]\ncoefs = coefs.sort_index().reset_index(drop=True)\ncoefs.index = ['[0.025', 'coef on GDP levels', '0.975]']\ncoefs\n\n\n\n\n\n\n\n\n\n\ntopfive\nbottomfive\nonlyGDPlevel\n\n\n\n\n[0.025\n-0.033143\n-0.028588\n-0.010810\n\n\ncoef on GDP levels\n-0.016378\n-0.005465\n0.001317\n\n\n0.975]\n0.000387\n0.017659\n0.013444\n\n\n\n\n\n\n\n\nThe equation using the top five regressors in explanatory power yielded a coefficient that is statistically speaking negative under the usual confidence interval levels. In contrast, the regression using the bottom five regressors failed to maintain that level of statistical significance (although the coefficient point estimate was still negative). And finally the regression on GDP level solely resulted, as in the past literature, on a point estimate that is also statistically not different than zero.\nThese results above offer a different way to add evidence to the conditional convergence hypothesis. In particular, with the help of gingado’s RegressionBenchmark model, it is possible to identify which covariates can meaningfully serve as covariates in a growth equation from those that cannot. This is important because if the covariate selection for some reason included only variables with little explanatory power instead of the most relevant ones, an economist might erroneously reach a different conclusion."
  },
  {
    "objectID": "barrolee1994.html#model-documentation",
    "href": "barrolee1994.html#model-documentation",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nImportantly for model documentation, the benchmark already has some baseline documentation set up. If the user wishes, they can use that as a basis to document their model. Note that the output is in a raw format that is suitable for machine reading and writing. Intermediary and advanced users may wish to use that format to construct personalised forms, documents, etc.\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-09-03 01:21:14 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': RandomForestRegressor(max_features=None, n_estimators=250, oob_score=True),\n   'best_index_': 5,\n   'best_params_': {'max_features': None, 'n_estimators': 250},\n   'best_score_': -0.11987689668923698,\n   'cv_results_': {'mean_fit_time': array([0.27910519, 0.61951447, 0.26125662, 0.63928392, 0.5517837 ,\n           1.29520903]),\n    'std_fit_time': array([0.03068348, 0.04062525, 0.02728135, 0.0638546 , 0.06424348,\n           0.0688891 ]),\n    'mean_score_time': array([0.00845108, 0.0173043 , 0.0095423 , 0.02041352, 0.00774763,\n           0.01687021]),\n    'std_score_time': array([0.00171296, 0.00095639, 0.00409258, 0.00593271, 0.00058496,\n           0.00089159]),\n    'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'params': [{'max_features': 'sqrt', 'n_estimators': 100},\n     {'max_features': 'sqrt', 'n_estimators': 250},\n     {'max_features': 'log2', 'n_estimators': 100},\n     {'max_features': 'log2', 'n_estimators': 250},\n     {'max_features': None, 'n_estimators': 100},\n     {'max_features': None, 'n_estimators': 250}],\n    'split0_test_score': array([0.02222159, 0.15885397, 0.11262741, 0.0938368 , 0.0854889 ,\n           0.04287378]),\n    'split1_test_score': array([-0.27461128, -0.4271837 , -0.20661865, -0.36040113, -0.2451637 ,\n           -0.25868747]),\n    'split2_test_score': array([0.0399584 , 0.19624059, 0.19803907, 0.18340719, 0.38870431,\n           0.39797913]),\n    'split3_test_score': array([0.17822191, 0.31874757, 0.2714435 , 0.2145507 , 0.44829104,\n           0.41306683]),\n    'split4_test_score': array([ 0.21013592,  0.00775184,  0.06127309,  0.08617005,  0.05097398,\n           -0.17319335]),\n    'split5_test_score': array([-1.26224558, -1.06862191, -0.97310708, -1.10761958, -1.53826387,\n           -1.45952623]),\n    'split6_test_score': array([-0.97411463, -0.83830882, -0.89245004, -0.73637009, -0.49269767,\n           -0.2950928 ]),\n    'split7_test_score': array([-0.40073915, -0.48660256, -0.44393236, -0.39392259, -0.45421273,\n           -0.36956042]),\n    'split8_test_score': array([0.27804517, 0.31747558, 0.16223497, 0.26428427, 0.39247636,\n           0.48219266]),\n    'split9_test_score': array([ 0.07167319,  0.01088549,  0.02265445,  0.00408031, -0.0310312 ,\n            0.0211789 ]),\n    'mean_test_score': array([-0.21114545, -0.18107619, -0.16878356, -0.17519841, -0.13954346,\n           -0.1198769 ]),\n    'std_test_score': array([0.4992794 , 0.4692985 , 0.43063517, 0.43737225, 0.56547861,\n           0.53580687]),\n    'rank_test_score': array([6, 5, 3, 4, 2, 1], dtype=int32)},\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 10,\n   'refit_time_': 1.2930288314819336,\n   'scorer_': &lt;sklearn.metrics._scorer._PassthroughScorer at 0x1463a6d10&gt;},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nSince there is some information in the model documentation that was automatically added, we might want to concentrate on the fields in the model card that are yet to be answered. Actually, this is the purpose of gingado’s automatic documentation: to afford users more time so they can invest, if they want, on model documentation.\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nLet’s fill some information:\n\n\nCode\nbenchmark.model_documentation.fill_info({\n    'intended_use': {\n        'primary_uses': 'This model is trained for pedagogical uses only.',\n        'primary_users': 'Everyone is welcome to follow the description showing the development of this benchmark.'\n    }\n})\n\n\nNote the format, based on a Python dictionary. In particular, the open_questions method results include keys divided by double underscores. As seen above, these should be interpreted as different levels of the documentation template, leading to a nested dictionary.\nNow when we confirm that the questions answered above are no longer “open questions”:\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nIf we want, at any time we can save the documentation to a local JSON file, as well as read another document."
  },
  {
    "objectID": "barrolee1994.html#trying-out-model-alternatives",
    "href": "barrolee1994.html#trying-out-model-alternatives",
    "title": "Using gingado to understand economic growth",
    "section": "Trying out model alternatives",
    "text": "Trying out model alternatives\nThe benchmark model may be enough for some analyses, or maybe the user is interested in using the benchmark to explore the data and have an understanding of the importance of each regressor, to concentrate their work on data that can be meaningful for their purposes. But oftentimes a user will want to seek a machine learning model that performs as well as possible.\nFor users that want to manually create other models, gingado allows the possibility of comparing them with the benchmark. If the user model is better, it becomes the new benchmark!\nFor the following analyses, we will use K-fold as cross-validation, with 5 splits of the sample.\n\nFirst candidate: a gradient boosting tree\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n\n\nCode\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.25],\n    'max_depth': [3, 6, 9]\n}\n\nreg_gradbooster = GradientBoostingRegressor()\n\ngradboosterg_grid = GridSearchCV(\n    reg_gradbooster,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n\n\n\n\nCode\ny_pred = gradboosterg_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n\n\n\n\n\n\n\n\n\nSecond candidate: lasso\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nparam_grid = {\n    'alpha': [0.5, 1, 1.25],\n}\n\nreg_lasso = Lasso(fit_intercept=True)\n\nlasso_grid = GridSearchCV(\n    reg_lasso,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n\n\n\n\nCode\ny_pred = lasso_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)"
  },
  {
    "objectID": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "href": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "title": "Using gingado to understand economic growth",
    "section": "Comparing the models with the benchmark",
    "text": "Comparing the models with the benchmark\ngingado allows users to compare different candidate models with the existing benchmark in a very simple way: using the compare method.\n\n\nCode\ncandidates = [gradboosterg_grid, lasso_grid]\nbenchmark.compare(X, y, candidates)\n\n\nFitting 10 folds for each of 4 candidates, totalling 40 fits\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  36.3s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  37.1s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  34.8s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  34.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  35.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  37.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  37.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  36.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   1.7s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.9s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   1.0s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   1.7s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.9s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   1.6s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.9s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   1.6s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   1.7s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   1.5s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   1.7s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   2.0s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   1.6s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   1.0s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   1.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   1.0s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   1.5s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   1.8s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   1.0s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   1.6s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   1.7s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.5s\n[CV] END ....................learning_rate=0.01, max_depth=6; total time=   1.6s\n[CV] END .....................learning_rate=0.1, max_depth=9; total time=   1.7s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.8s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   1.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   1.0s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   1.4s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.7s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   1.8s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   0.9s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ....................learning_rate=0.01, max_depth=9; total time=   1.8s\n[CV] END ....................learning_rate=0.25, max_depth=3; total time=   1.0s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.7s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   1.1s\n[CV] END .....................learning_rate=0.1, max_depth=3; total time=   1.0s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   1.5s\n[CV] END ....................learning_rate=0.25, max_depth=9; total time=   0.7s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END ............................................alpha=1; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ....................learning_rate=0.01, max_depth=3; total time=   0.9s\n[CV] END .....................learning_rate=0.1, max_depth=6; total time=   1.5s\n[CV] END ....................learning_rate=0.25, max_depth=6; total time=   0.9s\n[CV] END ..........................................alpha=0.5; total time=   0.0s\n[CV] END .........................................alpha=1.25; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  35.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  35.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  18.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.3s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   2.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.7s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.1s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  41.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  37.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  34.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  34.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  37.8s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  38.7s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  37.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  37.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  36.8s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  37.3s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.6s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.5s\nBenchmark updated!\nNew benchmark:\nPipeline(steps=[('candidate_estimator',\n                 GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                              estimator=RandomForestRegressor(oob_score=True),\n                              param_grid={'max_features': ['sqrt', 'log2',\n                                                           None],\n                                          'n_estimators': [100, 250]},\n                              verbose=2))])\n\n\nThe output above clearly indicates that after evaluating the models - and their ensemble together with the existing benchmark - at least one of them was better than the current benchmark. Therefore, it will now be the new benchmark.\n\n\nCode\ny_pred = benchmark.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)"
  },
  {
    "objectID": "barrolee1994.html#model-documentation-1",
    "href": "barrolee1994.html#model-documentation-1",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nAfter this process, we can now see how the model documentation was updated automatically:\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-09-03 01:35:55 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': Pipeline(steps=[('candidate_estimator',\n                    GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                 estimator=RandomForestRegressor(oob_score=True),\n                                 param_grid={'max_features': ['sqrt', 'log2',\n                                                              None],\n                                             'n_estimators': [100, 250]},\n                                 verbose=2))]),\n   'best_index_': 0,\n   'best_params_': {'candidate_estimator': GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                 estimator=RandomForestRegressor(oob_score=True),\n                 param_grid={'max_features': ['sqrt', 'log2', None],\n                             'n_estimators': [100, 250]},\n                 verbose=2),\n    'candidate_estimator__cv': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n    'candidate_estimator__error_score': nan,\n    'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n    'candidate_estimator__estimator__bootstrap': True,\n    'candidate_estimator__estimator__ccp_alpha': 0.0,\n    'candidate_estimator__estimator__criterion': 'squared_error',\n    'candidate_estimator__estimator__max_depth': None,\n    'candidate_estimator__estimator__max_features': 1.0,\n    'candidate_estimator__estimator__max_leaf_nodes': None,\n    'candidate_estimator__estimator__max_samples': None,\n    'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n    'candidate_estimator__estimator__min_samples_leaf': 1,\n    'candidate_estimator__estimator__min_samples_split': 2,\n    'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n    'candidate_estimator__estimator__monotonic_cst': None,\n    'candidate_estimator__estimator__n_estimators': 100,\n    'candidate_estimator__estimator__n_jobs': None,\n    'candidate_estimator__estimator__oob_score': True,\n    'candidate_estimator__estimator__random_state': None,\n    'candidate_estimator__estimator__verbose': 0,\n    'candidate_estimator__estimator__warm_start': False,\n    'candidate_estimator__n_jobs': None,\n    'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n     'max_features': ['sqrt', 'log2', None]},\n    'candidate_estimator__pre_dispatch': '2*n_jobs',\n    'candidate_estimator__refit': True,\n    'candidate_estimator__return_train_score': False,\n    'candidate_estimator__scoring': None,\n    'candidate_estimator__verbose': 2},\n   'best_score_': -0.1162272794715987,\n   'cv_results_': {'mean_fit_time': array([36.04944594,  4.86748128,  0.05856576, 37.40339727]),\n    'std_fit_time': array([1.0429256 , 4.45386475, 0.00562121, 1.82237288]),\n    'mean_score_time': array([0.01474044, 0.00327621, 0.00293167, 0.0232136 ]),\n    'std_score_time': array([0.00461105, 0.00018317, 0.00024105, 0.01239425]),\n    'param_candidate_estimator': masked_array(data=[GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                    estimator=RandomForestRegressor(oob_score=True),\n                                    param_grid={'max_features': ['sqrt', 'log2', None],\n                                                'n_estimators': [100, 250]},\n                                    verbose=2)                                                                       ,\n                       GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                                    param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                                                'max_depth': [3, 6, 9]},\n                                    verbose=2)                                       ,\n                       GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                                    verbose=2)                                                         ,\n                       VotingRegressor(estimators=[('candidate_1',\n                                                    GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                                                 estimator=RandomForestRegressor(oob_score=True),\n                                                                 param_grid={'max_features': ['sqrt',\n                                                                                              'log2',\n                                                                                              None],\n                                                                             'n_estimators': [100,\n                                                                                              250]},\n                                                                 verbose=2)),\n                                                   ('candidate_2',\n                                                    GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                                 n_jobs=-1,\n                                                                 param_grid={'learning_rate': [0.01,\n                                                                                               0.1,\n                                                                                               0.25],\n                                                                             'max_depth': [3, 6, 9]},\n                                                                 verbose=2)),\n                                                   ('candidate_3',\n                                                    GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                                 param_grid={'alpha': [0.5, 1, 1.25]},\n                                                                 verbose=2))])                                                                    ],\n                 mask=[False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__cv': masked_array(data=[ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                       None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__error_score': masked_array(data=[nan, nan, nan, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator': masked_array(data=[RandomForestRegressor(oob_score=True),\n                       GradientBoostingRegressor(), Lasso(), --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__bootstrap': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__ccp_alpha': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__criterion': masked_array(data=['squared_error', 'friedman_mse', --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_depth': masked_array(data=[None, 3, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_features': masked_array(data=[1.0, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_leaf_nodes': masked_array(data=[None, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_samples': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_impurity_decrease': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_samples_leaf': masked_array(data=[1, 1, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_samples_split': masked_array(data=[2, 2, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_weight_fraction_leaf': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__monotonic_cst': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_estimators': masked_array(data=[100, 100, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_jobs': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__oob_score': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__random_state': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__verbose': masked_array(data=[0, 0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__warm_start': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__n_jobs': masked_array(data=[None, -1, -1, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__param_grid': masked_array(data=[{'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]},\n                       {'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]},\n                       {'alpha': [0.5, 1, 1.25]}, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__pre_dispatch': masked_array(data=['2*n_jobs', '2*n_jobs', '2*n_jobs', --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__refit': masked_array(data=[True, True, True, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__return_train_score': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__scoring': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__verbose': masked_array(data=[2, 2, 2, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__alpha': masked_array(data=[--, 0.9, 1.0, --],\n                 mask=[ True, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__init': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__learning_rate': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__loss': masked_array(data=[--, 'squared_error', --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_iter_no_change': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__subsample': masked_array(data=[--, 1.0, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__tol': masked_array(data=[--, 0.0001, 0.0001, --],\n                 mask=[ True, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__validation_fraction': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__copy_X': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__fit_intercept': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_iter': masked_array(data=[--, --, 1000, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__positive': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__precompute': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__selection': masked_array(data=[--, --, 'cyclic', --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'params': [{'candidate_estimator': GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                   estimator=RandomForestRegressor(oob_score=True),\n                   param_grid={'max_features': ['sqrt', 'log2', None],\n                               'n_estimators': [100, 250]},\n                   verbose=2),\n      'candidate_estimator__cv': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n      'candidate_estimator__estimator__bootstrap': True,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'squared_error',\n      'candidate_estimator__estimator__max_depth': None,\n      'candidate_estimator__estimator__max_features': 1.0,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__max_samples': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__monotonic_cst': None,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_jobs': None,\n      'candidate_estimator__estimator__oob_score': True,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': None,\n      'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n       'max_features': ['sqrt', 'log2', None]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                   param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                               'max_depth': [3, 6, 9]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': GradientBoostingRegressor(),\n      'candidate_estimator__estimator__alpha': 0.9,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'friedman_mse',\n      'candidate_estimator__estimator__init': None,\n      'candidate_estimator__estimator__learning_rate': 0.1,\n      'candidate_estimator__estimator__loss': 'squared_error',\n      'candidate_estimator__estimator__max_depth': 3,\n      'candidate_estimator__estimator__max_features': None,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_iter_no_change': None,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__subsample': 1.0,\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__validation_fraction': 0.1,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'learning_rate': [0.01, 0.1, 0.25],\n       'max_depth': [3, 6, 9]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': Lasso(),\n      'candidate_estimator__estimator__alpha': 1.0,\n      'candidate_estimator__estimator__copy_X': True,\n      'candidate_estimator__estimator__fit_intercept': True,\n      'candidate_estimator__estimator__max_iter': 1000,\n      'candidate_estimator__estimator__positive': False,\n      'candidate_estimator__estimator__precompute': False,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__selection': 'cyclic',\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'alpha': [0.5, 1, 1.25]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': VotingRegressor(estimators=[('candidate_1',\n                                   GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                                estimator=RandomForestRegressor(oob_score=True),\n                                                param_grid={'max_features': ['sqrt',\n                                                                             'log2',\n                                                                             None],\n                                                            'n_estimators': [100,\n                                                                             250]},\n                                                verbose=2)),\n                                  ('candidate_2',\n                                   GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                n_jobs=-1,\n                                                param_grid={'learning_rate': [0.01,\n                                                                              0.1,\n                                                                              0.25],\n                                                            'max_depth': [3, 6, 9]},\n                                                verbose=2)),\n                                  ('candidate_3',\n                                   GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                param_grid={'alpha': [0.5, 1, 1.25]},\n                                                verbose=2))])}],\n    'split0_test_score': array([0.35305747, 0.41085719, 0.00231961, 0.39988637]),\n    'split1_test_score': array([ 0.42414329,  0.26871286, -0.01843843,  0.27287641]),\n    'split2_test_score': array([-0.27298026, -1.07150624, -0.37769816, -0.4723756 ]),\n    'split3_test_score': array([ 0.4721695 ,  0.35770633, -0.01279034,  0.32865922]),\n    'split4_test_score': array([-0.03611144, -0.3393956 , -0.35910928, -0.28317954]),\n    'split5_test_score': array([ 0.02462377,  0.06527096, -0.00174132,  0.21117008]),\n    'split6_test_score': array([-1.5524308 , -1.72967676, -3.01413211, -1.42448069]),\n    'split7_test_score': array([-0.03760886, -0.40562145, -0.38665003,  0.00327047]),\n    'split8_test_score': array([-0.80089575, -1.12254076, -0.04366099, -0.41956903]),\n    'split9_test_score': array([0.26376029, 0.02164111, 0.07276125, 0.1718456 ]),\n    'mean_test_score': array([-0.11622728, -0.35445524, -0.41391398, -0.12118967]),\n    'std_test_score': array([0.59927383, 0.692606  , 0.88320839, 0.52708304]),\n    'rank_test_score': array([1, 3, 4, 2], dtype=int32)},\n   'feature_names_in_': array(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65',\n          'hm65', 'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65',\n          'fert65', 'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1',\n          'invsh41', 'geetot1', 'geerec1', 'gde1', 'govwb1', 'govsh41',\n          'gvxdxe41', 'high65', 'highm65', 'highf65', 'highc65', 'highcm65',\n          'highcf65', 'human65', 'humanm65', 'humanf65', 'hyr65', 'hyrm65',\n          'hyrf65', 'no65', 'nom65', 'nof65', 'pinstab1', 'pop65',\n          'worker65', 'pop1565', 'pop6565', 'sec65', 'secm65', 'secf65',\n          'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65', 'syrf65',\n          'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'], dtype=object),\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 10,\n   'refit_time_': 36.98274207115173,\n   'scorer_': &lt;sklearn.metrics._scorer._PassthroughScorer at 0x148530070&gt;},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nAnd as before, any remaining open questions can be viewed and answered using the same methods as above."
  },
  {
    "objectID": "barrolee1994.html#references",
    "href": "barrolee1994.html#references",
    "title": "Using gingado to understand economic growth",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37."
  },
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "Using gingado to forecast financial series",
    "section": "",
    "text": "This notebook illustrates the use of gingado to build models for forecasting, using foreign exchange (FX) rate movements as an example. Please note that the results or the model should not be taken as investment advice.\nForecasting exchange rates is notoriously difficult (Rossi (2013) and references therein).\nThis exercise will illustrate various functionalities provided by gingado:\nUnlike most scripts that concentrate the package imports at the beginning, this walkthrough will import as needed, to better highlight where each contribution of gingado is used in the workflow.\nFirst, we will use gingado to run a simple example with the following characteristics:"
  },
  {
    "objectID": "forecast.html#downloading-fx-rates",
    "href": "forecast.html#downloading-fx-rates",
    "title": "Using gingado to forecast financial series",
    "section": "Downloading FX rates",
    "text": "Downloading FX rates\nIn this exercise, we will concentrate on the bilateral FX rates between the 🇺🇸 US Dollar (USD) and the 🇧🇷 Brazilian Real (BRL), 🇨🇦 Canadian Dollar (CAD), 🇨🇭 Swiss Franc (CHF), 🇪🇺 Euro (EUR), 🇬🇧 British Pound (GBP), 🇯🇵 Japanese Yen (JPY) and 🇲🇽 Mexican Peso (MXN).\nThe rates are standardised to measure the units in foreign currency bought by one USD. Therefore, positive returns represent USD is more valued compared to the other currency, and vice-versa.\n\n\nCode\nfrom gingado.utils import load_SDMX_data\n\n\n\n\nCode\ndf = load_SDMX_data(\n    sources={'BIS': 'WS_XRU_D'},\n    keys={\n        'FREQ': 'D', \n        'CURRENCY': ['BRL', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'MXN'],\n        'REF_AREA': ['BR', 'CA', 'CH', 'XM', 'GB', 'JP', 'MX']\n        },\n    params={'startPeriod': 2003}\n)\n\n\nQuerying data from BIS's dataflow 'WS_XRU' - US dollar exchange rates, m,q,a...\nthis dataflow does not have data in the desired frequency and time period.\nQuerying data from BIS's dataflow 'WS_XRU_D' - US dollar exchange rates, daily...\n\n\nThe code below simplifies the column names by removing the identification of the SDMX sources, dataflows and keys and replacing it with the usual code for the bilateral exchange rates.\n\n\nCode\nprint(\"Original column names:\")\nprint(df.columns)\n\ndf.columns = ['USD' + col.split('_')[9] for col in df.columns]\n\nprint(\"New column names:\")\nprint(df.columns)\n\n\nOriginal column names:\nIndex(['BIS__WS_XRU_D_D__MX__MXN__A', 'BIS__WS_XRU_D_D__JP__JPY__A',\n       'BIS__WS_XRU_D_D__XM__EUR__A', 'BIS__WS_XRU_D_D__CH__CHF__A',\n       'BIS__WS_XRU_D_D__BR__BRL__A', 'BIS__WS_XRU_D_D__GB__GBP__A',\n       'BIS__WS_XRU_D_D__CA__CAD__A'],\n      dtype='object')\nNew column names:\nIndex(['USDMXN', 'USDJPY', 'USDEUR', 'USDCHF', 'USDBRL', 'USDGBP', 'USDCAD'], dtype='object')\n\n\nThe dataset looks like this so far (most recent 5 rows displayed only):\n\n\nCode\ndf.tail()\n\n\n\n\n\n\n\n\n\nUSDMXN\nUSDJPY\nUSDEUR\nUSDCHF\nUSDBRL\nUSDGBP\nUSDCAD\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n2024-02-13\n17.073288\n149.328268\n0.926526\n0.878440\n4.953674\n0.788455\n1.344483\n\n\n2024-02-14\n17.134229\n150.546066\n0.933445\n0.886120\n4.953701\n0.795837\n1.354336\n\n\n2024-02-15\n17.086661\n150.107046\n0.930839\n0.882807\n4.970772\n0.797124\n1.354091\n\n\n2024-02-16\n17.044577\n150.334324\n0.928678\n0.881408\n4.973161\n0.794994\n1.348254\n\n\n2024-02-19\n17.046956\n149.953601\n0.927988\n0.880846\n4.958519\n0.792947\n1.347624\n\n\n\n\n\n\n\nWe are interested in the percentage change from the previous day.\n\n\nCode\nFX_rate_changes = df.pct_change()\nFX_rate_changes.dropna(inplace=True)\n\n\n\n\nCode\nFX_rate_changes.plot(subplots=True, layout=(4, 2), figsize=(15, 15), sharex=True, title='Selected daily FX rate changes')\n\n\narray([[&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "forecast.html#augmenting-the-dataset",
    "href": "forecast.html#augmenting-the-dataset",
    "title": "Using gingado to forecast financial series",
    "section": "Augmenting the dataset",
    "text": "Augmenting the dataset\nWe will complement the FX rates data with two other datasets:\n\ndaily central bank policy rates from the Bank for International Settlements (BIS) (2017), and\nthe daily Composite Indicator of Systemic Stress (CISS), created by Hollo, Kremer, and Lo Duca (2012) and updated by the European Central Bank (ECB).\n\n\n\nCode\nfrom gingado.augmentation import AugmentSDMX\n\n\n\n\nCode\nX = AugmentSDMX(sources={'BIS': 'WS_CBPOL_D', 'ECB': 'CISS'}).fit_transform(FX_rate_changes)\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n\n\n\n\n\n\nNote\n\n\n\nit is acceptable in gingado to pass the variable of interest (the “y”, or in this case, FX_rate_changes) as the X argument in fit_transform. This is because this series will also be merged with the additional, augmented data and subsequently lagged along with it.\n\n\nYou can see below that the column names for the newly added columns reflect the source (BIS or ECB), the dataflow (separated from the source by a double underline), and then the specific keys to the series, which are specific to each dataflow.\n\n\nCode\nX.columns\n\n\nIndex(['USDMXN', 'USDJPY', 'USDEUR', 'USDCHF', 'USDBRL', 'USDGBP', 'USDCAD',\n       'BIS__WS_CBPOL_D_D__CH', 'BIS__WS_CBPOL_D_D__CL',\n       'BIS__WS_CBPOL_D_D__CN', 'BIS__WS_CBPOL_D_D__CO',\n       'BIS__WS_CBPOL_D_D__CZ', 'BIS__WS_CBPOL_D_D__DK',\n       'BIS__WS_CBPOL_D_D__GB', 'BIS__WS_CBPOL_D_D__HK',\n       'BIS__WS_CBPOL_D_D__HR', 'BIS__WS_CBPOL_D_D__HU',\n       'BIS__WS_CBPOL_D_D__ID', 'BIS__WS_CBPOL_D_D__IL',\n       'BIS__WS_CBPOL_D_D__IN', 'BIS__WS_CBPOL_D_D__IS',\n       'BIS__WS_CBPOL_D_D__JP', 'BIS__WS_CBPOL_D_D__AR',\n       'BIS__WS_CBPOL_D_D__KR', 'BIS__WS_CBPOL_D_D__MA',\n       'BIS__WS_CBPOL_D_D__MK', 'BIS__WS_CBPOL_D_D__MX',\n       'BIS__WS_CBPOL_D_D__BR', 'BIS__WS_CBPOL_D_D__MY',\n       'BIS__WS_CBPOL_D_D__NO', 'BIS__WS_CBPOL_D_D__NZ',\n       'BIS__WS_CBPOL_D_D__PE', 'BIS__WS_CBPOL_D_D__PH',\n       'BIS__WS_CBPOL_D_D__CA', 'BIS__WS_CBPOL_D_D__PL',\n       'BIS__WS_CBPOL_D_D__AU', 'BIS__WS_CBPOL_D_D__RO',\n       'BIS__WS_CBPOL_D_D__RS', 'BIS__WS_CBPOL_D_D__RU',\n       'BIS__WS_CBPOL_D_D__SA', 'BIS__WS_CBPOL_D_D__SE',\n       'BIS__WS_CBPOL_D_D__TH', 'BIS__WS_CBPOL_D_D__TR',\n       'BIS__WS_CBPOL_D_D__US', 'BIS__WS_CBPOL_D_D__XM',\n       'BIS__WS_CBPOL_D_D__ZA', 'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\nBefore proceeding, we also include a differentiated version of the central bank policy data. It will be sparse, since these changes occur infrequently for most central banks, but it can help the model uncover how FX rate changes respond to central bank policy changes.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nX_diff = X.loc[:, X.columns.str.contains(\"BIS__WS_CBPOL_D\", case=False)].diff()\nX_diff.columns = [col + \"_diff\" for col in X_diff.columns]\nX = pd.concat([X, X_diff], axis=1)\n\n\nThis is how the data looks like now. Note that the names of the added columns reflect the source, dataflow and keys, all separated by underlines (the source is separated from the dataflow by two underlines at all cases). For example, the last key is the jurisdiction of the central bank.\nWe will keep all the newly added variables - even those that are from countries not in the currency list. This is because the model may uncover any relationship of interest between central bank policies from other countries and each particular currency pair.\n\n\nCode\nX.describe().transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUSDMXN\n5461.0\n0.000124\n0.008089\n-0.070532\n-0.003982\n-0.000213\n0.003857\n0.118877\n\n\nUSDJPY\n5461.0\n0.000061\n0.006081\n-0.044831\n-0.003006\n0.000151\n0.003205\n0.032901\n\n\nUSDEUR\n5461.0\n0.000011\n0.005712\n-0.039574\n-0.003121\n0.000000\n0.003042\n0.048493\n\n\nUSDCHF\n5461.0\n-0.000063\n0.006389\n-0.139149\n-0.003251\n0.000011\n0.003225\n0.085326\n\n\nUSDBRL\n5461.0\n0.000117\n0.010476\n-0.080226\n-0.005706\n-0.000047\n0.005351\n0.120503\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nBIS__WS_CBPOL_D_D__TH_diff\n5460.0\n0.000137\n0.033666\n-1.000000\n0.000000\n0.000000\n0.000000\n0.500000\n\n\nBIS__WS_CBPOL_D_D__TR_diff\n5460.0\n0.000183\n0.309450\n-4.250000\n0.000000\n0.000000\n0.000000\n8.500000\n\n\nBIS__WS_CBPOL_D_D__US_diff\n5460.0\n0.000755\n0.041192\n-1.000000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D_D__XM_diff\n5460.0\n0.000321\n0.031443\n-0.750000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D_D__ZA_diff\n5460.0\n-0.000962\n0.062475\n-1.500000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\n\n\n107 rows × 8 columns\n\n\n\nThe policy rates for some central banks have less observations than the others, as seen above.\nBecause some data are missing, we will impute data for the missing dates, by simply propagating the last valid observation, and when that is not possible, replacing the missing information with a “0”.\n\n\nCode\nX.fillna(method='pad', inplace=True)\nX.fillna(value=0, inplace=True)\n\n\nNow is a good time to start the model documentation. For this, we can use the standard model card that already comes with gingado.\nThe goal is to facilitate economists who want to make model documentation a part of their normal workflow.\n\n\nCode\nfrom gingado.model_documentation import ModelCard\n\n\n\n\nCode\nmodel_doc = ModelCard()\nmodel_doc.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nAs an example, we can add the following information to the model:\n\n\nCode\nmodel_doc.fill_info({\n    'intended_use': {\n        'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n        'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'\n    },\n    'metrics': {\n        'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.'\n    },\n    'ethical_considerations': {\n        'sensitive_data': 'These models were not trained with sensitive data.',\n        'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.'\n    },\n    \n})"
  },
  {
    "objectID": "forecast.html#lagging-the-regressors",
    "href": "forecast.html#lagging-the-regressors",
    "title": "Using gingado to forecast financial series",
    "section": "Lagging the regressors",
    "text": "Lagging the regressors\nThis model will not include any contemporaneous variable. Therefore, all regresors must be lagged.\nFor illustration purposes, we use 5 lags in this exercise.\n\n\nCode\nfrom gingado.utils import Lag\n\n\n\n\nCode\nn_lags = 5\n\nX_lagged = Lag(lags=n_lags).fit_transform(X)\nX_lagged\n\ny = FX_rate_changes[n_lags:]\n\n\nNow is a good opportunity to check by how much we have increased our regressor space:\n\n\nCode\npd.Series({\n    \"FX rates only\": y.shape[1],\n    \"... with augmentation_\": X.shape[1],\n    \"... lagged\": X_lagged.shape[1]\n})\n\n\nFX rates only               7\n... with augmentation_    107\n... lagged                535\ndtype: int64"
  },
  {
    "objectID": "forecast.html#training-the-models",
    "href": "forecast.html#training-the-models",
    "title": "Using gingado to forecast financial series",
    "section": "Training the models",
    "text": "Training the models\nOur dataset is now complete. Before using it to train the models, we hold out the most recent data to serve as our testing dataset, so we can compare our models with real out-of-sample information.\nWe can choose, say, 1st January 2022.\n\n\nCode\ncutoff = '2020-01-01'\n\nX_train, X_test = X_lagged[:cutoff], X_lagged[cutoff:]\ny_train, y_test = y[:cutoff], y[cutoff:]\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'training_data': \n    {'training_data': \n        \"\"\"\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\n        * foreign exchange rates\n        * central bank policy rates\n        * an estimated indicator for systemic stress\n        The training and evaluation datasets are the same time series, only different windows in time.\"\"\"\n    }\n})\n\n\nThe current status of the documentation is:\n\n\nCode\npd.Series(model_doc.show_json())\n\n\nmodel_details              {'developer': 'Person or organisation developi...\nintended_use               {'primary_uses': 'These models are simplified ...\nfactors                    {'relevant': 'Relevant factors', 'evaluation':...\nmetrics                    {'performance_measures': 'Consistent with most...\nevaluation_data            {'datasets': 'Datasets', 'motivation': 'Motiva...\ntraining_data              {'training_data': '\n        The training data ...\nquant_analyses             {'unitary': 'Unitary results', 'intersectional...\nethical_considerations     {'sensitive_data': 'These models were not trai...\ncaveats_recommendations    {'caveats': 'For example, did the results sugg...\ndtype: object\n\n\n\nCreating a random walk benchmark\nRossi (2013) highlights that few predictors beat the random walk without drift model. This is a good opportunity to showcase how we can use gingado’s in-built base class ggdBenchmark to build our customised benchmark model, in this case a random walk.\nThe calculation of the random walk benchmark is very simple. Still, creating a gingado benchmark offers some advantages: it is easier to compare alternative models, and the model documentation is done more seamlessly.\nA custom benchmark model must implement the following steps:\n\nsub-class ggdBenchmark (or alternatively implement its methods)\ndefine an estimator that is compatible with scikit-learn’s API:\n\nat the very least, it has a fit method that returns self\n\n\nIf the user is relying on a custom estimator - like in this case, a random walk estimator to align with the literature - then this custom estimator also has some requirements:\n\nit should ideally subclass scikit-learn’s BaseEstimator (mostly for the get_params / set_params methods)\nthree methods are necessary:\n\nfit, which should at least create an attribute ending in an underline (“_“), so that gingado knows it is fitted\npredict\nscore\n\n\n\n\nCode\nimport numpy as np\nfrom gingado.benchmark import ggdBenchmark\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\n\nCode\nclass RandomWalkEstimator(BaseEstimator):\n    def __init__(self, scoring='neg_root_mean_squared_error'):\n        self.scoring = scoring\n    \n    def fit(self, X, y=None):\n        self.n_samples_ = X.shape[0]\n        return self\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n    def score(self, X, y, sample_weight=None):\n        from sklearn.metrics import mean_squared_error\n        y_pred = self.predict(X)\n        return mean_squared_error(y, y_pred, sample_weight=sample_weight, squared=False)\n\n    def forecast(self, forecast_horizon=1):\n        self.forecast_horizon = forecast_horizon\n        return np.zeros(self.forecast_horizon)\n\nclass RandomWalkBenchmark(ggdBenchmark):\n    def __init__(\n        self, \n        estimator=RandomWalkEstimator(), \n        auto_document=ModelCard,\n        cv=TimeSeriesSplit(n_splits=10, test_size=60), \n        ensemble_method=VotingRegressor, \n        verbose_grid=None):\n        self.estimator=estimator\n        self.auto_document=auto_document\n        self.cv=cv\n        self.ensemble_method=ensemble_method\n        self.verbose_grid=verbose_grid\n\n    def fit(self, X, y=None):\n        self.benchmark=self.estimator\n        self.benchmark.fit(X, y)\n        return self\n\n\n\n\nTraining the candidate models\nNow that we have a benchmark, we can create candidate models that will try to beat it.\nIn this simplified example, we will choose only two: a random forest, an AdaBoost regressor and a Lasso model. Their hyperparameters are not particularly important for the example, but of course they could be fine-tuned as well.\nIn the language of Rossi (2013), the models below are one “single-equation, lagged fundamental model” for each currency.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nforest = RandomForestRegressor(n_estimators=250, max_features='log2').fit(X_train, y_train['USDBRL'])\nadaboost = AdaBoostRegressor(n_estimators=150).fit(X_train, y_train['USDBRL'])\nlasso = Lasso(alpha=0.1).fit(X_train, y_train['USDBRL'])\n\nrw = RandomWalkBenchmark().fit(X_train, y_train['USDBRL'])\n\n\nWe can now compare the model results, using the test dataset we held out previously.\nNote that we must pass the criterion against which we are comparing the forecasts.\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nresults = rw.compare_fitted_candidates(\n    X_test, y_test['USDBRL'],\n    candidates=[forest, adaboost, lasso],\n    scoring_func=mean_squared_error)\n\npd.Series(results)\n\n\nRandomWalkEstimator()                                           0.000109\nRandomForestRegressor(max_features='log2', n_estimators=250)    0.000113\nAdaBoostRegressor(n_estimators=150)                             0.000112\nLasso(alpha=0.1)                                                0.000109\ndtype: float64\n\n\nAs mentioned above, benchmarks can facilitate the model documentation. In addition to the broader documentation that is already ongoing, each benchmark object create their own where they store model information. We can use that for the broader documentation.\nIn our case, the only parameter we created above during fit is the number of samples: not a particularly informative variable but it was included just for illustration purposes. In any case, the parameter appears in the “model_details” section, item “info”, of the benchmark’s rw documentation. Similarly, the parameters of more fully-fledged estimators also appear in that section.\n\n\nCode\nrw.document()\n\nrw.model_documentation.show_json()['model_details']['info']\n\n\n{'n_samples_': 4394}\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'model_details': {'info': rw.model_documentation.show_json()['model_details']['info']}\n})\n\n\n\n\nCode\nmodel_doc.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-27 09:09:35 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'n_samples_': 4394},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': '\\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\\n        * foreign exchange rates\\n        * central bank policy rates\\n        * an estimated indicator for systemic stress\\n        The training and evaluation datasets are the same time series, only different windows in time.'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'These models were not trained with sensitive data.',\n  'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nWe can save the documentation to disk in JSON format with model_doc.save_json(), or parse it to create other documents (eg, a PDF file) using third-party libraries."
  },
  {
    "objectID": "forecast.html#references",
    "href": "forecast.html#references",
    "title": "Using gingado to forecast financial series",
    "section": "References",
    "text": "References\n\n\nBank for International Settlements. 2017. “Recent Enhancements to the BIS Statistics.” BIS Quarterly Review. Vol. September. https://www.bis.org/publ/qtrpdf/r_qt1709c.htm.\n\n\nHollo, Daniel, Manfred Kremer, and Marco Lo Duca. 2012. “CISS-a Composite Indicator of Systemic Stress in the Financial System.”\n\n\nRossi, Barbara. 2013. “Exchange Rate Predictability.” Journal of Economic Literature 51 (4): 1063–1119."
  },
  {
    "objectID": "machine_controls.html",
    "href": "machine_controls.html",
    "title": "Machine controls",
    "section": "",
    "text": "This notebook illustrates the use of MachineControl, the gingado estimator that calculates a synthetic control model with machine learning techniques."
  },
  {
    "objectID": "machine_controls.html#setting",
    "href": "machine_controls.html#setting",
    "title": "Machine controls",
    "section": "Setting",
    "text": "Setting\nUse of the MachineControl estimator is illustrated with an admittedly simplistic estimation of the impact of softening labour regulation on output per worker, measured in constant 2017 international US dollars PPP.\nMore specifically, the example below focuses on Brazil’s 2017 labour reforms (Law No 13,467/2017). The reform markedly deregulated labour markets, with the purpose of increasing productivity and thereby unlocking growth. Some of its main points are:\n\nprominence of collective bargaining between firms and employees over statutory “blanket” provisions\nlower costs for employers of employment termination without just cause\ndiscouraging of labour litigation by employees, previously diagnosed as being excessive and contributing to clogging the judicial system\n\nThe reform was enacted in July 2017 and went into effect in November of the same year."
  },
  {
    "objectID": "machine_controls.html#using-machine-learning",
    "href": "machine_controls.html#using-machine-learning",
    "title": "Machine controls",
    "section": "Using machine learning",
    "text": "Using machine learning\nMachineControl does the following:\n\nautomatically select a group of countries from a global list to form a smaller set of control countries\nestimate a GDP value for “synthetic Brazil” using pre-enactment data on the outcome of interest\ncheck the statistical quality of the synthetic control\ncalculates the difference between post-reforms actual Brazilian GDP growth to synthetic Brazil’s to measure the effect of the labour reform.\n\n\nNote: There are many other variables that would be interesting for this study as well, such as various labour market indicators.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom gingado.utils import list_all_dataflows, load_SDMX_data\nfrom gingado.estimators import MachineControl\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.manifold import TSNE"
  },
  {
    "objectID": "machine_controls.html#downloading-the-data",
    "href": "machine_controls.html#downloading-the-data",
    "title": "Machine controls",
    "section": "Downloading the data",
    "text": "Downloading the data\nFirst, we list all dataflows obtainable with SDMX.\n\n\nCode\ndflows = list_all_dataflows(return_pandas=True)\n\n\n\n\nCode\ndflows[dflows.str.contains('per worker', case=False)]\n\n\nILO  DF_GDP_205U_NOC_NB    Output per worker (GDP constant 2015 US $) -- ...\n     DF_GDP_211P_NOC_NB    Output per worker (GDP constant 2017 internati...\n     DF_SDG_A821_NOC_RT    SDG indicator 8.2.1 - Annual growth rate of ou...\n     DF_SDG_B821_NOC_RT    SDG indicator 8.2.1 - Annual growth rate of ou...\nName: dataflow, dtype: object\n\n\nLet’s get the data on output per worker provided by the International Labour Organisation (ILO):\n\n\nCode\noutcome_var = load_SDMX_data(\n    sources={'ILO': 'DF_GDP_211P_NOC_NB'}, \n    keys={'FREQ': 'A'}, \n    params={'startPeriod': 2000, 'endPeriod': 2022}\n)\n\n\nQuerying data from ILO's dataflow 'DF_GDP_211P_NOC_NB' - Output per worker (GDP constant 2017 international $ at PPP) -- ILO modelled estimates, Nov. 2023...\n\n\nThis dataflow provides one series for each country, as seen below.\n\n\nCode\ncol_fields = pd.DataFrame([c.split(\"__\") for c in outcome_var.columns])\nfor col in col_fields.columns:\n    print(\"No of unique values for column No\", col, \": \", col_fields[col].nunique())\nprint(\"\\nFirst five rows:\")\nprint(col_fields.head())\n\n\nNo of unique values for column No 0 :  1\nNo of unique values for column No 1 :  274\nNo of unique values for column No 2 :  1\nNo of unique values for column No 3 :  1\n\nFirst five rows:\n     0                       1  2            3\n0  ILO  DF_GDP_211P_NOC_NB_AFG  A  GDP_211P_NB\n1  ILO  DF_GDP_211P_NOC_NB_AGO  A  GDP_211P_NB\n2  ILO  DF_GDP_211P_NOC_NB_ALB  A  GDP_211P_NB\n3  ILO  DF_GDP_211P_NOC_NB_ARE  A  GDP_211P_NB\n4  ILO  DF_GDP_211P_NOC_NB_ARG  A  GDP_211P_NB\n\n\nBecause 271 is much higher than the number of countries that usually report international statistics (close to 200), it is likely some 70 columns or more correspond to aggregations, typically used in statistics for convenience (eg, one code that encompasses all countries in the European Union, etc). We need to take them out, in case Brazil is a constituent of any of those aggregations.\nOne common way of spotting these aggregations in international statistics is finding the country codes that begin with “X”.\n\n\nCode\nlen([c for c in outcome_var.columns if \"DF_GDP_211P_NOC_NB_X\" in c])\n\n\n85\n\n\nAlso, we will take out other countries that have undergone labour reforms more or less around the same time. From Serra, Bottega, and Sanches (2022), I am aware of Argentina 🇦🇷, Costa Rica 🇨🇷, Paraguay 🇵🇾, and Uruguay 🇺🇾.\nIt is important to note, though, that these countries above were named by Serra, Bottega, and Sanches (2022) because their synthetic control donor pool consistent of geographically close countries. Other countries might have also enacted labour reforms in that period. For expositional purposes, we can assume no further country needs to be taken out of the same.\n\n\nCode\ncol_filter = [\n    c for c in outcome_var.columns\n    if \"DF_GDP_211P_NOC_NB_X\" not in c\n    and \"DF_GDP_211P_NOC_NB_ARG\" not in c\n    and \"DF_GDP_211P_NOC_NB_CRI\" not in c\n    and \"DF_GDP_211P_NOC_NB_PAR\" not in c\n    and \"DF_GDP_211P_NOC_NB_URY\" not in c\n]\n\nX = outcome_var[col_filter]\nX = X.dropna(axis=1)\n\n# cleaning out the name to remove the constant portions across all countries\nX.columns = [c.replace(\"ILO__DF_GDP_211P_NOC_NB_\", \"\") \\\n    .replace(\"__A__GDP_211P_NB\", \"\") for c in X.columns]\n\ncol_BRA = [c for c in X.columns if c == \"BRA\"]\ny = X.pop(col_BRA[0])\n\nassert X.shape[0] == y.shape[0]\n\n\nThis is what the series of annual output per worker in Brazil looks like. A vertical line marking November 2017, when the labour reforms entered into force.\n\n\nCode\nlaw_date = '2017-11-11'\nylabel = 'PPP 2017 US$ per worker'\n\nax = y.plot(legend=False)\nax.axvline(x=law_date, color='r', linestyle='--')\nplt.ylabel(ylabel)\nplt.xlabel('')\nplt.title('Labour productivity in Brazil')\n\n\nText(0.5, 1.0, 'Labour productivity in Brazil')\n\n\n\n\n\nBefore creating the MachineControl object, a final comment on the intervention date.\nSince the reforms were enacted and entered into force in the same year of 2017, we can be conservative and consider: - pre-intervention data up to end-2016 - post-intervention data from 2018 onwards\n\n\nCode\nX_pre, y_pre = X[:'2016-12-31'], y[:'2016-12-31']\n\nassert X_pre.shape[0] == y_pre.shape[0]\nX_pre.shape, y_pre.shape\n\n\n((17, 184), (17,))"
  },
  {
    "objectID": "machine_controls.html#using-the-machinecontrol-object",
    "href": "machine_controls.html#using-the-machinecontrol-object",
    "title": "Machine controls",
    "section": "Using the MachineControl object",
    "text": "Using the MachineControl object\nThe code chunk below shows how a MachineControl object can be created.\nAs illustrated below, users can not only choose the clustering, estimator and manifold learning algorithms that best suit their needs, but also pass specific arguments to each of these elements.\n\n\nCode\nsynth_BR = MachineControl(\n    cluster_alg=AffinityPropagation(max_iter=10_000),\n    estimator=RandomForestRegressor(),\n    manifold=TSNE(perplexity=5)\n)\n\n\nLet’s train and then inspect the MachineControl object:\n\n\nCode\nsynth_BR.fit(X_pre, y_pre)\n\n\nMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. MachineControliFittedMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5)) cluster_alg: AffinityPropagationAffinityPropagation(max_iter=10000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(max_iter=10000) estimator: RandomForestRegressorRandomForestRegressor()  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor() manifold: TSNETSNE(perplexity=5)  TSNE?Documentation for TSNETSNE(perplexity=5) \n\n\nAs shown above, the machine control estimator comprises:\n\nA clustering algorithm that selects the donor pool from the larger population (affinity propagation)\nA supervised learning algorithm that will use the donor pool to estimate contemporanous values for Brazil (random forest)\nA manifold learning algorithm that summarises the different time series in a 2-dimensional embedding space, enabling easier comparison of Brazil with each other country and with the synthetic control (t-SNE)\n\nAt this stage, we can extract the list of donor countries, ie, those that will be used in the control.\n\n\nCode\n\" \".join(synth_BR.donor_pool_)\n\n\n'BGR BIH BLR BLZ BRB COL CUB DOM EGY FJI GUY IRQ KAZ LCA MUS NAM PSE SRB SWZ TUN VCT YEM ZAF'\n\n\nThese countries are: Albania 🇦🇱, Bulgaria 🇧🇬, Bosnia and Herzegovina 🇧🇦, Belarus 🇧🇾, Barbados 🇧🇧, Botswana 🇧🇼, Colombia 🇨🇴, Cuba 🇨🇺, Dominican Republic 🇩🇴, Algeria 🇩🇿, Egypt 🇪🇬, Fiji 🇫🇯, Guyana 🇬🇾, Iraq 🇮🇶, Kazakhstan 🇰🇿, Saint Lucia 🇱🇨, North Macedonia 🇲🇰, Mauritius 🇲🇺, Namibia 🇳🇦, State of Palestine 🇵🇸, Serbia 🇷🇸, Eswatini 🇸🇿, Tunisia 🇹🇳, Saint Vincent and the Grenadines 🇻🇨, Yemen 🇾🇪 and South Africa 🇿🇦.\n\nNote: This list does not imply all countries contribute equally to estimating the synthetic version of Brazil. In fact, some might even end up not even contributing in the estimating equation. The selected list also does not imply a causal explanation from any of those countries into the Brazilian dynamics. They are merely closest to Brazil in this clustering exercise, and as such, likely to be a good predictor at the same time period.\n\nAnd here is how the outcome variable for these countries (grey) compares with Brazil’s (red).\n\n\nCode\nax = X_pre[synth_BR.donor_pool_].plot(legend=False, color=\"grey\", linewidth=0.5)\ny_pre.plot(ax=ax, color=\"red\", linewidth=2.5)\n\n\n&lt;Axes: xlabel='TIME_PERIOD'&gt;\n\n\n\n\n\nPlotting the result of the manifold learning underscores whether the synthetic control seems indeed to come from a similar space in the data distribution as the entity of interest.\n\n\nCode\ncolors = [(0.5, 0.5, 0.5, 0.35)] * X.shape[1] + ['red', 'blue']\n\nfig, ax = plt.subplots()\nax.scatter(\n    synth_BR.manifold_embed_[:, 0], synth_BR.manifold_embed_[:, 1],\n    color=colors,\n    )\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nsynth_BR.control_quality_test_ / 100\n\n\n0.016990172226110295\n\n\nAs can be seen in the graph above, the actual Brazil 🇧🇷 (blue) and the machine controls (red) are almost juxtaposed.\nTo confirm objectively that the control is good, the Euclidean distance between the embeddings for the control and the actual outcome are one of the lowest, more specifically at the 2% percentile.\nTogether, both point to a strong signal that the control managed to replicate the pre-intervention outcome.\nSo now it’s time to look at the results in the years following the event."
  },
  {
    "objectID": "machine_controls.html#results",
    "href": "machine_controls.html#results",
    "title": "Machine controls",
    "section": "Results",
    "text": "Results\n\n\nCode\nax = synth_BR.predict(X, y).plot(legend=False)\nax.axvline(x='2017-11-11', color='r', linestyle='--')\nplt.ylabel('PPP 2017 US$ per worker')\nplt.xlabel('')\nplt.title(\"Labour productivity in Brazil\")\nplt.show()\n\n\n\n\n\n\nTreatment effect\nThe main result is shown below, corresponding to the difference between actual and estimated value for the whole time series (from 2000). If the red line (Brazil) stays close to the other ones - placebo estimations.\n\n\nCode\nall_diffs = pd.concat([\n    synth_BR.placebo_diff_,\n    synth_BR.diff_\n], axis=1)\ncolors = ['grey'] * synth_BR.placebo_diff_.shape[1] + ['red']\ny_label = 'PPP 2017 US$ per worker'\neffect_title = \"Effect of deregulation on labour productivity in Brazil\"\n\nax = all_diffs.plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()\n\n\n\n\n\nIt seems there is some outlier effect with one of the control countries.\n\n\nCode\nsynth_BR.placebo_diff_.loc['2022-01-01'].sort_values(ascending=False).index[0]\n\n\n'GUY'\n\n\nAccording to the code above, it is Guyana 🇬🇾. Now plotting without this country:\n\n\n\nCode\ncolors = ['grey'] * (synth_BR.placebo_diff_.shape[1] - 1)+ ['red']\nax = all_diffs[[c for c in all_diffs.columns if c != \"GUY\"]].plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()"
  },
  {
    "objectID": "machine_controls.html#interpretation",
    "href": "machine_controls.html#interpretation",
    "title": "Machine controls",
    "section": "Interpretation",
    "text": "Interpretation\nThe above results hint at a null effect of the reforms on labour productivity after training cutoff date (black vertical line). If anything, the effect squarely close to zero throughout the years following the reform (red vertical line), even as the productivity for the majority of control countries actually went up in the years before the pandemic."
  },
  {
    "objectID": "dataset_transformation.html",
    "href": "dataset_transformation.html",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('gingado/data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/data/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "dataset_transformation.html#barro-and-lee-1994",
    "href": "dataset_transformation.html#barro-and-lee-1994",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('gingado/data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/data/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to gingado",
    "section": "",
    "text": "Welcome, and thank you for your interest in contributing to gingado! Whether it’s reporting issues, suggesting new features, or contributing to the code, documentation, or tests, your involvement is highly appreciated.\n\n\n\n\nTo get started with contributing to gingado, you need to set up your development environment. This includes installing necessary tools and configuring your system to work efficiently with our codebase.\nTo install the dependencies to work with gingado, you need to install the execution dependencies and the development dependencies of gingado:\npip install -r requirements.txt\npip install -r dev_requirements.txt\nTo work with the documentation an tests, we use quarto. You can either use RStudio or Visual Studio Code with the Quarto extension installed for editing .qmd files which are used for generating documentation. Here’s how you can set up your environment:\n\nInstall Quarto: If you haven’t already, install Quarto from Quarto’s official website. Follow the instructions for your operating system.\nConfigure Your Editor:\n\nFor RStudio: Quarto is integrated with RStudio. Ensure you have the latest version of RStudio to work with Quarto seamlessly.\nFor Visual Studio Code: Install the Quarto extension from the Visual Studio Code marketplace. This extension provides support for .qmd files, including syntax highlighting and preview capabilities.\n\n\n\n\n\nIf you encounter a bug, have suggestions, or want to propose new functionalities:\n\nCheck Existing Issues: Ensure the bug or suggestion hasn’t been reported/mentioned before by searching under Issues on GitHub.\nCreate a New Issue: If no existing issue addresses the problem or suggestion, please create a new issue, providing a descriptive title, a clear description, and as much relevant information as possible. For bugs, include a code sample or an executable test case demonstrating the expected behavior that is not occurring, along with complete error messages.\n\n\n\n\n\n\nTo contribute changes to the codebase, including documentation and tests, follow these guidelines:\n\nDocument New Features: Clearly document new functions or classes in the .qmd files. Write clear descriptions, specify expected inputs and outputs, and include any relevant information to understand the functionality.\nInclude Tests: Implement tests for new functionalities as part of the .qmd files to ensure the integrity and reliability of the code. Make sure your tests cover the expected behavior and edge cases.\n\n\n\n\n\nFocused PRs: Each PR should be focused on a single topic. Avoid combining unrelated changes.\nSeparate Style and Functional Changes: Do not mix style changes with functional changes in the same PR.\nPreserve File Style: Avoid adding or removing vertical whitespace unnecessarily. Keep the original style of the files you edit.\nDevelopment Process: Do not use a submitted PR as a development playground. If additional work is needed, consider closing the PR, completing the work, and then submitting a new PR.\nResponding to Feedback: If your PR requires changes based on feedback, continue committing to the same PR unless the changes are substantial. In that case, it might be better to start a new PR.\n\n\n\n\n\nWhen contributing to the documentation, ensure your contributions are made within .qmd files. This is essential for the changes to be correctly reflected in the generated documentation through Quarto.\n\n\n\n\nBy contributing to gingado, you are part of a community that values collaboration, innovation, and learning. We look forward to your contributions and are excited to see what we can build together."
  },
  {
    "objectID": "CONTRIBUTING.html#getting-started",
    "href": "CONTRIBUTING.html#getting-started",
    "title": "Contributing to gingado",
    "section": "",
    "text": "To get started with contributing to gingado, you need to set up your development environment. This includes installing necessary tools and configuring your system to work efficiently with our codebase.\nTo install the dependencies to work with gingado, you need to install the execution dependencies and the development dependencies of gingado:\npip install -r requirements.txt\npip install -r dev_requirements.txt\nTo work with the documentation an tests, we use quarto. You can either use RStudio or Visual Studio Code with the Quarto extension installed for editing .qmd files which are used for generating documentation. Here’s how you can set up your environment:\n\nInstall Quarto: If you haven’t already, install Quarto from Quarto’s official website. Follow the instructions for your operating system.\nConfigure Your Editor:\n\nFor RStudio: Quarto is integrated with RStudio. Ensure you have the latest version of RStudio to work with Quarto seamlessly.\nFor Visual Studio Code: Install the Quarto extension from the Visual Studio Code marketplace. This extension provides support for .qmd files, including syntax highlighting and preview capabilities.\n\n\n\n\n\nIf you encounter a bug, have suggestions, or want to propose new functionalities:\n\nCheck Existing Issues: Ensure the bug or suggestion hasn’t been reported/mentioned before by searching under Issues on GitHub.\nCreate a New Issue: If no existing issue addresses the problem or suggestion, please create a new issue, providing a descriptive title, a clear description, and as much relevant information as possible. For bugs, include a code sample or an executable test case demonstrating the expected behavior that is not occurring, along with complete error messages.\n\n\n\n\n\n\nTo contribute changes to the codebase, including documentation and tests, follow these guidelines:\n\nDocument New Features: Clearly document new functions or classes in the .qmd files. Write clear descriptions, specify expected inputs and outputs, and include any relevant information to understand the functionality.\nInclude Tests: Implement tests for new functionalities as part of the .qmd files to ensure the integrity and reliability of the code. Make sure your tests cover the expected behavior and edge cases.\n\n\n\n\n\nFocused PRs: Each PR should be focused on a single topic. Avoid combining unrelated changes.\nSeparate Style and Functional Changes: Do not mix style changes with functional changes in the same PR.\nPreserve File Style: Avoid adding or removing vertical whitespace unnecessarily. Keep the original style of the files you edit.\nDevelopment Process: Do not use a submitted PR as a development playground. If additional work is needed, consider closing the PR, completing the work, and then submitting a new PR.\nResponding to Feedback: If your PR requires changes based on feedback, continue committing to the same PR unless the changes are substantial. In that case, it might be better to start a new PR.\n\n\n\n\n\nWhen contributing to the documentation, ensure your contributions are made within .qmd files. This is essential for the changes to be correctly reflected in the generated documentation through Quarto."
  },
  {
    "objectID": "CONTRIBUTING.html#your-contributions-make-a-difference",
    "href": "CONTRIBUTING.html#your-contributions-make-a-difference",
    "title": "Contributing to gingado",
    "section": "",
    "text": "By contributing to gingado, you are part of a community that values collaboration, innovation, and learning. We look forward to your contributions and are excited to see what we can build together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to gingado!",
    "section": "",
    "text": "gingado seeks to facilitate the use of machine learning in economic and finance use cases, while promoting good practices. This package aims to be suitable for beginners and advanced users alike. Use cases may range from simple data retrievals to experimentation with machine learning algorithms to more complex model pipelines used in production."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to gingado!",
    "section": "Overview",
    "text": "Overview\ngingado is a free, open source library built different functionalities:\n\ndata augmentation, to add data from official sources, improving the machine models being trained by the user;\nrelevant datasets, both real and simulated, to allow for easier model development and comparison;\nautomatic benchmark model, to assess candidate models against a reasonably well-performant model;\nmachine learning-based estimators, to help answer questions of academic or practical importance;\nsupport for model documentation, to embed documentation and ethical considerations in the model development phase; and\nutilities, including tools to allow for lagging variables in a straightforward way.\n\nEach of these functionalities builds on top of the previous one. They can be used on a stand-alone basis, together, or even as part of a larger pipeline from data input to model training to documentation!\n\n\n\n\n\n\nTip\n\n\n\nNew functionalities are planned over time, so consider checking frequently on gingado for the latest toolsets."
  },
  {
    "objectID": "index.html#design-principles",
    "href": "index.html#design-principles",
    "title": "Welcome to gingado!",
    "section": "Design principles",
    "text": "Design principles\nThe choices made during development of gingado derive from the following principles, in no particular order:\n\nflexibility: users can use gingado out of the box or build custom processes on top of it;\ncompatibility: gingado works well with other widely used libraries in machine learning, such as scikit-learn and pandas; and\nresponsibility: gingado facilitates and promotes model documentation, including ethical considerations, as part of the machine learning development workflow.\n\nFor more information about gingado, please read the paper."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to gingado!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\ngingado’s API is inspired on the following libraries:\n\nscikit-learn (Buitinck et al. 2013)\nkeras (website here and also, this essay)\nfastai (Howard and Gugger 2020)\n\nIn addition, gingado is developed and maintained using quarto."
  },
  {
    "objectID": "index.html#presentations-talks-papers",
    "href": "index.html#presentations-talks-papers",
    "title": "Welcome to gingado!",
    "section": "Presentations, talks, papers",
    "text": "Presentations, talks, papers\nThe most current version of the paper describing gingado is here. The paper and other material about gingado (ie, slide decks, papers) in this dedicated repository. Interested users are welcome to visit the repository and comment on the drafts or slide decks, preferably by opening an issue. I also store in this repository suggestions I receive as issues, so users can see what others commented (anonymously unless requested) and comment along as well!"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Welcome to gingado!",
    "section": "Install",
    "text": "Install\n\n\n\n\n\n\nNote\n\n\n\nPlease make sure you have read and understood the license disclaimer in the NOTES.md file in our GitHub repository before using gingado.\n\n\nTo install gingado, simply run the following code on the terminal:\n$ pip install gingado"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Welcome to gingado!",
    "section": "References",
    "text": "References\n\n\nAraujo, Douglas KG. 2023. “Gingado: A Machine Learning Library Focused on Economics and Finance.” Working Paper 1122. BIS Working Paper. Bank for International Settlements.\n\n\nBuitinck, Lars, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, et al. 2013. “API Design for Machine Learning Software: Experiences from the Scikit-Learn Project.” CoRR abs/1309.0238. http://arxiv.org/abs/1309.0238.\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108."
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Welcome to gingado!",
    "section": "Attribution",
    "text": "Attribution\nIf you use this package in your work, please consider citing Araujo (2023).\nIn BibTeX format:\n@techreport{gingado,\n    author = {Araujo, Douglas KG},\n    title = {gingado: a machine learning library focused on economics and finance},\n    series = {BIS Working Paper},\n    type = {Working Paper},\n    institution = {Bank for International Settlements},\n    year = {2023},\n    number = {1122}\n}\nOver time, new tools that are described in specific papers might be added (eg, a machine learning-based econometric estimator). Please consider citing them as well if used in your work. Specific information, if any, can be found in the documentation."
  },
  {
    "objectID": "nowcast.html",
    "href": "nowcast.html",
    "title": "Nowcasting inflation with neural networks",
    "section": "",
    "text": "This notebook showcases how to set up neural networks to nowcast inflation using data measured in different frequencies. The goal here is to start with a very simple dataset containing only two variables, inflation (monthly) and oil prices (daily), to slowly build up a more complex neural network based nowcasting model, the TFT-MF available in gingado from its v0.3.0.\nNowcasting is essentially the use of the most current information possible to estimate in real time an economic series of interest such as inflation or GDP before it is actually released1. For example, if you could measure all prices every day, you could create on the last day of the month a very accurate nowcast for the headline inflation for that month - which would only be officialy published a few days later. In the case of GDP, this lag between the end of the reference period and actual publication tends to be significant, around 6-10 weeks. For policymakers, investors and other decisionmakers, a lot can happen in this period.\nA related use of nowcasting is to estimate what the current period’s reading will be as this period rolls out. In other words, estimating today what the inflation reading for this month (or GDP for this quarter) will likely be as new information is unveiled in real time.\nThe nowcasting model available in gingado from v0.3.0 onwards is an adjusted version of the Temporal Fusion Transformer (TFT) of Lim et al. (2021). This architecture combines flexibility to take on multiple datasets while learning which information to focus on and interpretability to provide insights on the important variables in each case."
  },
  {
    "objectID": "nowcast.html#roadmap",
    "href": "nowcast.html#roadmap",
    "title": "Nowcasting inflation with neural networks",
    "section": "Roadmap",
    "text": "Roadmap\nThe TFT model can be a bit complex to understand at first, so we will build it up, step by step. After loading the data in Section 2, the most basic neural network - a neuron layer - is presented in Section 3. This is followed by an architecture that is more suitable for time series in Section 4. Next, these elements are combined in Section 5 to show how the model knows what to focus on. The next individual element is the self-attention layer in Section 6. Finally, if you want to see the full picture directly, go to Section 7 to see how these elements are put together. Section 8 then trains the model and presents the results for this simple, illustrative nowcasting."
  },
  {
    "objectID": "nowcast.html#loading-the-data",
    "href": "nowcast.html#loading-the-data",
    "title": "Nowcasting inflation with neural networks",
    "section": "Loading the data",
    "text": "Loading the data\nLet’s use our SDMX connectors to find and download data from official sources in a reproducible way.\nTo abstract from currency issues, we will use US inflation and oil prices, which are denominated in US dollars.\n\n\nCode\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"\nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sdmx\nfrom gingado.utils import load_SDMX_data\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\nInflation\nSince this is a monthly nowcast of inflation, the best way to do this is to use a monthly change in the consumer price index, \\(\\pi_t^{(m)}=(\\text{CPI}_t - \\text{CPI}_{t-1})/\\text{CPI}_{t-1}\\), not the year-on-year rate, \\(\\pi_t^{(y)}=(\\text{CPI}_t - \\text{CPI}_{t-12})/\\text{CPI}_{t-12}\\), which is how people usually think of inflation. This is because we want to nowcast only the value at the margin; all other values are already known.\nThen, only at the end we combine rolling windows of 12 consecutive monthly inflation rates, of which only the last one or two are estimated, to correctly create an annual inflation rate.\nFormally, if we know all values except the current and last month’s, then: \\[\n\\hat{\\pi}_t^{(y)}=(\\prod_{l=0}^1 (1+\\hat{\\pi}_{t-l}^{(m)}) \\prod_{l=2}^{11} (1+\\pi_{t-l}^{(m)}) )-1,\n\\tag{1}\\]\nwhere the hat notation means that a particular value was estimated.\nFor inflation, we take a dataflow from the BIS, since we are looking for US data. Let’s explore it first and then choose the correct data specifications to download the time series. (See here for a practical walkthrough showing how to explore data with SDMX.)\n\n\nCode\nBIS = sdmx.Client(\"BIS\")\ncpi_msg = BIS.dataflow('WS_LONG_CPI')\ncpi_dsd = cpi_msg.structure\n\n\nThese are all possible keys:\n\n\nCode\ncpi_dsd['BIS_LONG_CPI'].dimensions.components\n\n\n[&lt;Dimension FREQ&gt;,\n &lt;Dimension REF_AREA&gt;,\n &lt;Dimension UNIT_MEASURE&gt;,\n &lt;TimeDimension TIME_PERIOD&gt;]\n\n\nFor example, “FREQ” (frequency) takes in these values:\n\n\nCode\ncl__FREQ = sdmx.to_pandas(cpi_dsd['BIS_LONG_CPI'].dimensions.get(\"FREQ\").local_representation.enumerated)\ncl__FREQ\n\n\nCL_FREQ\nA                                   Annual\nB    Daily - business week (not supported)\nD                                    Daily\nE                    Event (not supported)\nH                              Half-yearly\nM                                  Monthly\nQ                                Quarterly\nW                                   Weekly\nName: Code list for Frequency (FREQ), dtype: object\n\n\nAnd “REF_AREA” (reference area) can be set to:\n\n\nCode\ncl__REF_AREA = sdmx.to_pandas(cpi_dsd['BIS_LONG_CPI'].dimensions.get(\"REF_AREA\").local_representation.enumerated)\ncl__REF_AREA\n\n\nCL_AREA\n1X                                      ECB\n4T    Emerging market economies (aggregate)\n5A                  All reporting economies\n5R                       Advanced economies\nAE                     United Arab Emirates\n                      ...                  \nVN                                  Vietnam\nXM                                Euro area\nXW                                    World\nZA                             South Africa\n_Z                           Not applicable\nName: Reference area code list, Length: 101, dtype: object\n\n\nWe can check that the US is amongst the reference areas:\n::: {#cell-check US in REF_AREA codelist .cell execution_count=6}\n\nCode\ncl__REF_AREA['US']\n\n\n'United States'\n\n:::\nFinally, the “UNIT_MEASURE” values can be:\n::: {#cell-codelist for UNIT_MEASURE in dataflow BIS__WS_LONG_CPI .cell execution_count=7}\n\nCode\ncl__UNIT_MEASURE = sdmx.to_pandas(cpi_dsd['BIS_LONG_CPI'].dimensions.get(\"UNIT_MEASURE\").local_representation.enumerated)\ncl__UNIT_MEASURE\n\n\nCL_BIS_UNIT\n000                     Unknown\n001                 100 - yield\n002                   EUR / MWh\n003     Index, 1996 Jan 2 = 100\n004           Euro / troy ounce\n                 ...           \nZAR                        Rand\nZMK              Zambian Kwacha\nZMW              Zambian Kwacha\nZWD             Zimbabwe Dollar\nZWL    Zimbabwe Dollar (fourth)\nName: BIS_Unit, Length: 1051, dtype: object\n\n:::\nIn the BIS website for this data, we can see that the unit in levels is Index, 2010 = 100 (the other one is Year-on-year changes, in per cent, which as discussed above we don’t want for this case.)\n::: {#cell-finding code for index .cell execution_count=8}\n\nCode\ncl__UNIT_MEASURE[cl__UNIT_MEASURE.str.contains(\"Index, 2010 = 100\")]\n\n\nCL_BIS_UNIT\n628    Index, 2010 = 100\nName: BIS_Unit, dtype: object\n\n:::\nArmed with this knowledge, we can now download monthly consumer price index data for the US. Let’s start after 1985 so that we have a sufficiently long history but without too much influence of the tectonic shift of the US dollar devaluation in the early 1970s and ensuing high inflation:\n\n\nCode\ndf_infl = load_SDMX_data(\n    sources={\"BIS\": \"'WS_LONG_CPI'\"},\n    keys={\"FREQ\": \"M\", \"REF_AREA\": \"US\", \"UNIT_MEASURE\": \"628\"},\n    params={\"startPeriod\": 1985}\n)\n\ndf_infl.plot()\n\n\nQuerying data from BIS's dataflow 'WS_LONG_CPI' - BIS long consumer prices...\n\n\n\n\n\nFigure 1: US consumer price index, 2010 = 100\n\n\n\n\n\n\n\n\nAs you can see in ?@fig-CPI, we downloaded the series \\(\\{\\text{CPI}_t\\}\\). Transforming that into \\(\\{\\pi_t\\}\\), defined above, we have:\n\n\nCode\nfig, ax = plt.subplots()\nplt.axhline(y=0, linewidth=1.5, color=\"black\")\ndf_infl_m = df_infl.pct_change()\ndf_infl_m.index = df_infl_m.index + pd.offsets.MonthEnd(0) # move to month end\ndf_infl_m.plot(ax=ax)\nplt.show()\n\n\n\n\n\nFigure 2: US monthly inflation rate\n\n\n\n\n\n\n\n\n\n\nOil prices\nSince the focus is on US inflation, below we get WTI oil prices. This data is downloaded from the St Louis Fed’s FRED webpage.\n\n\nCode\ndf_oil = pd.read_csv(\"docs/DCOILWTICO.csv\")\ndf_oil['DCOILWTICO'] = pd.to_numeric(df_oil['DCOILWTICO'], errors='coerce')\ndf_oil['DATE'] = pd.to_datetime(df_oil['DATE'])\ndf_oil.set_index('DATE', inplace=True)\n\ndf_oil.plot()\n\n\n\n\n\nFigure 3: WTI oil prices\n\n\n\n\n\n\n\n\nFor the nowcasting, we are interested in the daily variation:\n\n\nCode\nfig, ax = plt.subplots()\nplt.axhline(y=0, linewidth=1.5, color=\"black\")\ndf_oil_d = df_oil.pct_change().dropna()\ndf_oil_d.plot(ax=ax)\nplt.show()\n\n\n\n\n\nFigure 4: Daily change in WTI oil prices\n\n\n\n\n\n\n\n\n\n\nTemporal features\nThere is a lot of information encoded in the temporal features of a time series: which day in the month it is, which month of the year, etc. For example, consider how consumers behave differently in response to oil prices over warmer months (when many decide or not to travel, and how far) compared to colder months (when energy prices factor in heating and is thus perhaps less elastic).\nIn this pedagogical exercise, we will not incorporate these temporal features to keep things simple. But in a more complete version later in this notebook we will incorporate them.\nIn any case, to simplify notation about time, instead of the usual subscript \\(t\\) as above to denote a time period, for precision we will follow this convention:\n\nsubscript \\(m\\) denotes a given month\nsubscript \\(d\\) denotes a given day\nsubscript \\(d(m)\\) denotes a given day in a given month; example: \\(d(m-1)\\) is a day in the previous month.\n\n\n\nSplitting the dataset\nWe will now split the dataset into training data up until end-2020 and validation data afterwards. And the training data will be further split into 5 temporally sequential folds (see here for more information).\nTo simplify, we will consider valid nowcasting input data for a given output in period \\(m\\) as:\n\nall monthly data up to, and including, \\(m-1\\); and\nall daily data up to, and including, \\(d(m)\\).\n\n\n\nCode\n# Lagging the nowcasted variable\ndf_infl_m_L = df_infl_m.shift(1).dropna()\ndf_infl_m = df_infl_m.loc[df_infl_m_L.index]\n\n# Training date cutoff\ncutoff = \"2020-12-31\"\n\ny_train, y_test = df_infl_m[:cutoff], df_infl_m[cutoff:]\nXm_train, Xm_test = df_infl_m_L[:cutoff], df_infl_m_L[cutoff:]\nXd_train, Xd_test = df_oil_d[:cutoff], df_oil_d[cutoff:][1:]\n\n# Setting up the temporal folds, using the dependent variable's date\ntscv = TimeSeriesSplit(n_splits=5)\nmonthly_cv = tscv.split(y_train)\nmonthly_cv_dates = [\n    (y_train.index[m], y_train.index[n]) \n    for m, n in monthly_cv\n]\n\n\nNow for every month \\(m\\) in the dependent variable, we can find all \\(m_{t-l}, l\\geq 1\\) and all \\(d(m_{t-s}), s\\geq 0\\).\n\n\nCode\ndef dates_Xy(X, y, date_list):\n    def get_filtered_data(frame, d):\n        filtered_data = frame.loc[frame.index &lt;= d]\n        return filtered_data if not filtered_data.empty else None\n\n    result = {\n        i: (\n            {\n                freq: get_filtered_data(Xdata, d)\n                for freq, Xdata in X.items()\n            }, \n            y.loc[d]\n        )\n        for i, d in enumerate(date_list)\n    }\n    return result\n\ndef create_Xy(X, y, splits):\n    folds = {}\n    for k, split in enumerate(splits):\n        folds[k] = {}\n        split_train, split_valid = split\n        folds[k]['Xy_train'] = dates_Xy(X, y, split_train)\n        folds[k]['Xy_valid'] = dates_Xy(X, y, split_valid)\n    return folds\n\n\nLet’s see how this time series fold will be structured:\n\n\nCode\ntrain_data = create_Xy(\n    X={\"m\": Xm_train, \"d\": Xd_train}, \n    y=y_train, \n    splits=monthly_cv_dates\n)\n\nfor ts_fold in range(len(train_data)):\n    print(f\"Fold No {ts_fold + 1}:\")\n    print(f\"  {len(train_data[ts_fold]['Xy_train'])} training X-y pairs\")\n    print(f\"  {len(train_data[ts_fold]['Xy_valid'])} validation X-y pairs\")\n\n\nFold No 1:\n  75 training X-y pairs\n  71 validation X-y pairs\nFold No 2:\n  146 training X-y pairs\n  71 validation X-y pairs\nFold No 3:\n  217 training X-y pairs\n  71 validation X-y pairs\nFold No 4:\n  288 training X-y pairs\n  71 validation X-y pairs\nFold No 5:\n  359 training X-y pairs\n  71 validation X-y pairs"
  },
  {
    "objectID": "nowcast.html#references",
    "href": "nowcast.html#references",
    "title": "Nowcasting inflation with neural networks",
    "section": "References",
    "text": "References\n\n\nBok, Brandyn, Daniele Caratelli, Domenico Giannone, Argia M. Sbordone, and Andrea Tambalotti. 2018. “Macroeconomic Nowcasting and Forecasting with Big Data.” Journal Article. Annual Review of Economics 10 (Volume 10, 2018): 615–43. https://doi.org/https://doi.org/10.1146/annurev-economics-080217-053214.\n\n\nGiannone, Domenico, Lucrezia Reichlin, and David Small. 2008. “Nowcasting: The Real-Time Informational Content of Macroeconomic Data.” Journal of Monetary Economics 55 (4): 665–76.\n\n\nHochreiter, S. 1997. “Long Short-Term Memory.” Neural Computation MIT-Press.\n\n\nLim, Bryan, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. “Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting.” International Journal of Forecasting 37 (4): 1748–64."
  },
  {
    "objectID": "nowcast.html#first-model-a-single-layer-neural-network",
    "href": "nowcast.html#first-model-a-single-layer-neural-network",
    "title": "Nowcasting inflation with neural networks",
    "section": "First model: a single layer neural network",
    "text": "First model: a single layer neural network\nThe goal of this model is to nowcast \\(\\pi_t\\) based on its past values \\(\\pi_{t-1}\\) and on current oil prices \\(o_{d(m-s)}, s \\geq 0\\). The first model we will train is a very simple neural network:\n\\[\n\\begin{align}\n\\xi &= \\phi(\\mathbf{W}_1 x_t + \\mathbf{b}_1) \\\\\ny_t &= \\mathbf{W}_0 \\xi + \\mathbf{b}_0,\n\\end{align}\n\\tag{2}\\]\nwhere \\(\\mathbf{W}_0 \\in \\mathbb{R}^{1 \\times d}\\), \\(b_0 \\in \\mathbb{R}\\), \\(\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times |x_t|}\\), \\(b_1 \\in \\mathbb{R}^{d}\\), \\(\\xi \\in \\mathbb{R}^d\\) and \\(\\phi\\) is an activation function. For simplicity, we will use the ReLU activation function, which is simply: \\(\\phi(z) = \\text{max}(z, 0)\\).\nFor this neural network, we need a fix dimensionality of the input data. In other words, the network needs to know how much data it will take in at any given time, and this should not change throughout training or inference time.\nFor simplicity, let’s fix it at a single lagged inflation (the past month, \\(\\pi_{m-1}\\)) and the most recent available oil price change, \\(o_d\\).\n\n\nCode\nd = 5 # sort of arbitrary\n\nnn = keras.Sequential([\n    keras.layers.Input(shape=(2,)),\n    keras.layers.Dense(units = d, activation=\"relu\"),\n    keras.layers.Dense(units=1)\n])\nnn.summary()\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 5)              │            15 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │             6 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 21 (84.00 B)\n\n\n\n Trainable params: 21 (84.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nChecking that it works:\n\n\nCode\n# taking the last day of the training set of the last training fold\nlast_day_key = len(train_data[ts_fold]['Xy_train']) - 1\ntmp_X, tmp_y = train_data[ts_fold]['Xy_train'][last_day_key]\ntmp_X = np.concatenate([v.iloc[-1].values for v in tmp_X.values()]).reshape(1, -1)\n\n# checking that the model runs\nnn_pred = nn(tmp_X)\nprint(f\"Prediction: {nn_pred}\")\nprint(f\"Actual value: {tmp_y.values}\")\nnn_loss = keras.losses.MeanSquaredError()(y_true=tmp_y.values, y_pred=nn_pred)\nprint(f\"Mean squared error: {nn_loss}\")\n\n\nPrediction: tensor([[0.0051]], device='mps:0', grad_fn=&lt;AddBackward0&gt;)\nActual value: [-0.00470589]\nMean squared error: 9.649517596699297e-05\n\n\nIt does, great!\n\nNote that this network always needs to take in input of a certain shape (in this case, a 2-columns matrix) to work.\n\nWe don’t need to bother training this neural network; the goal here is to use it as a building block for a mathematical/econometric intuition of the broader nowcasting model."
  },
  {
    "objectID": "nowcast.html#second-model-long-short-term-memory",
    "href": "nowcast.html#second-model-long-short-term-memory",
    "title": "Nowcasting inflation with neural networks",
    "section": "Second model: Long short-term memory",
    "text": "Second model: Long short-term memory"
  },
  {
    "objectID": "forecasters.html",
    "href": "forecasters.html",
    "title": "Forecasters",
    "section": "",
    "text": "Forecasting time series is one of the most traditional activities in economic practice. Machine learning-based methods are promising techniques across many domains due to their flexibility to fit complex data generation processes and to take up alternative data as input.\nIn the last years, the field of nowcasting has gained attention. Its goal is to take up ongoing information to anticipate key economic data (such as GDP or inflation) about current or immediate future periods before they are officially calculated and published.1"
  },
  {
    "objectID": "forecasters.html#sec-GLU",
    "href": "forecasters.html#sec-GLU",
    "title": "Forecasters",
    "section": "Gated Linear Unit (GLU)",
    "text": "Gated Linear Unit (GLU)\nDauphin et al. (2017)\nThe GLU is a key part of the Gated Residual Network, described in Section 1.2."
  },
  {
    "objectID": "forecasters.html#sec-GRN",
    "href": "forecasters.html#sec-GRN",
    "title": "Forecasters",
    "section": "Gated Residual Network (GRN)",
    "text": "Gated Residual Network (GRN)"
  },
  {
    "objectID": "forecasters.html#sec-VSN",
    "href": "forecasters.html#sec-VSN",
    "title": "Forecasters",
    "section": "Variable Selection Network (VSN)",
    "text": "Variable Selection Network (VSN)"
  },
  {
    "objectID": "forecasters.html#sec-TFTMF",
    "href": "forecasters.html#sec-TFTMF",
    "title": "Forecasters",
    "section": "TFT-MF",
    "text": "TFT-MF\nThe Temporal Fusion Transformer - Mixed Frequency (TFT-MF) is an adaptation of the original TFT."
  },
  {
    "objectID": "forecasters.html#footnotes",
    "href": "forecasters.html#footnotes",
    "title": "Forecasters",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKey references in nowcasting economic variables are Giannone, Reichlin, and Small (2008), … and … .↩︎"
  },
  {
    "objectID": "nowcast.html#sec-data",
    "href": "nowcast.html#sec-data",
    "title": "Nowcasting inflation with neural networks",
    "section": "Loading the data",
    "text": "Loading the data\nLet’s use our SDMX connectors to find and download data from official sources in a reproducible way.\nTo abstract from currency issues, we will use US inflation and oil prices, which are denominated in US dollars.\n\n\nCode\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sdmx\nfrom gingado.utils import load_SDMX_data\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\nInflation\nSince this is a monthly nowcast of inflation, the best way to do this is to use a monthly change in the consumer price index, \\(\\pi_t^{(m)}=(\\text{CPI}_t - \\text{CPI}_{t-1})/\\text{CPI}_{t-1}\\), not the year-on-year rate, \\(\\pi_t^{(y)}=(\\text{CPI}_t - \\text{CPI}_{t-12})/\\text{CPI}_{t-12}\\), which is how people usually think of inflation. This is because we want to nowcast only the value at the margin; 11 twelths of \\(\\pi_t^{(y)}\\) are already known, since \\(\\pi_t^{(y)} = -1 + \\prod_{l=0}^{11} (1+\\pi_{t-l})\\).\nThen, only at the end we combine rolling windows of 12 consecutive monthly inflation rates, of which only the last one or two are estimated, to correctly create an annual inflation rate.\nFormally, if we know all values except the current and last month’s, then: \\[\n\\hat{\\pi}_t^{(y)}=(\\prod_{l=0}^1 (1+\\hat{\\pi}_{t-l}^{(m)}) \\prod_{l=2}^{11} (1+\\pi_{t-l}^{(m)}) )-1,\n\\tag{1}\\]\nwhere the hat notation (\\(\\hat{ }\\)) means that a particular value was estimated.\nFor inflation, we take a dataflow from the BIS, since we are looking for US data. Let’s explore it first and then choose the correct data specifications to download the time series.2\n\n\nCode\nBIS = sdmx.Client(\"BIS\")\ncpi_msg = BIS.dataflow('WS_LONG_CPI')\ncpi_dsd = cpi_msg.structure\n\n\nThese are all possible keys:\n\n\nCode\ncpi_dsd['BIS_LONG_CPI'].dimensions.components\n\n\n[&lt;Dimension FREQ&gt;,\n &lt;Dimension REF_AREA&gt;,\n &lt;Dimension UNIT_MEASURE&gt;,\n &lt;TimeDimension TIME_PERIOD&gt;]\n\n\nFor example, “FREQ” (frequency) takes in these values:\n\n\nCode\ncl__FREQ = sdmx.to_pandas(cpi_dsd['BIS_LONG_CPI'].dimensions.get(\"FREQ\").local_representation.enumerated)\ncl__FREQ\n\n\nCL_FREQ\nA                                   Annual\nB    Daily - business week (not supported)\nD                                    Daily\nE                    Event (not supported)\nH                              Half-yearly\nM                                  Monthly\nQ                                Quarterly\nW                                   Weekly\nName: Code list for Frequency (FREQ), dtype: object\n\n\nAnd “REF_AREA” (reference area) can be set to:\n\n\nCode\ncl__REF_AREA = sdmx.to_pandas(cpi_dsd['BIS_LONG_CPI'].dimensions.get(\"REF_AREA\").local_representation.enumerated)\ncl__REF_AREA\n\n\nCL_AREA\n1X                                      ECB\n4T    Emerging market economies (aggregate)\n5A                  All reporting economies\n5R                       Advanced economies\nAE                     United Arab Emirates\n                      ...                  \nVN                                  Vietnam\nXM                                Euro area\nXW                                    World\nZA                             South Africa\n_Z                           Not applicable\nName: Reference area code list, Length: 101, dtype: object\n\n\nWe can check that the US is amongst the reference areas:\n::: {#cell-check US in REF_AREA codelist .cell execution_count=6}\n\nCode\ncl__REF_AREA['US']\n\n\n'United States'\n\n:::\nFinally, the “UNIT_MEASURE” values can be:\n::: {#cell-codelist for UNIT_MEASURE in dataflow BIS__WS_LONG_CPI .cell execution_count=7}\n\nCode\ncl__UNIT_MEASURE = sdmx.to_pandas(cpi_dsd['BIS_LONG_CPI'].dimensions.get(\"UNIT_MEASURE\").local_representation.enumerated)\ncl__UNIT_MEASURE\n\n\nCL_BIS_UNIT\n000                     Unknown\n001                 100 - yield\n002                   EUR / MWh\n003     Index, 1996 Jan 2 = 100\n004           Euro / troy ounce\n                 ...           \nZAR                        Rand\nZMK              Zambian Kwacha\nZMW              Zambian Kwacha\nZWD             Zimbabwe Dollar\nZWL    Zimbabwe Dollar (fourth)\nName: BIS_Unit, Length: 1051, dtype: object\n\n:::\nIn the BIS website for this data, we can see that the unit in levels is Index, 2010 = 100 (the other one is Year-on-year changes, in per cent, which as discussed above we don’t want for this case.)\n::: {#cell-finding code for index .cell execution_count=8}\n\nCode\ncl__UNIT_MEASURE[cl__UNIT_MEASURE.str.contains(\"Index, 2010 = 100\")]\n\n\nCL_BIS_UNIT\n628    Index, 2010 = 100\nName: BIS_Unit, dtype: object\n\n:::\nArmed with this knowledge, we can now download monthly consumer price index data for the US. Let’s start after 1985 so that we have a sufficiently long history but without too much influence of the tectonic shift of the US dollar devaluation in the early 1970s and ensuing high inflation:\n\n\nCode\ndf_infl = load_SDMX_data(\n    sources={\"BIS\": \"'WS_LONG_CPI'\"},\n    keys={\"FREQ\": \"M\", \"REF_AREA\": \"US\", \"UNIT_MEASURE\": \"628\"},\n    params={\"startPeriod\": 1985}\n)\n\ndf_infl.plot()\n\n\nQuerying data from BIS's dataflow 'WS_LONG_CPI' - BIS long consumer prices...\n\n\n\n\n\nFigure 1: US consumer price index, 2010 = 100\n\n\n\n\n\n\n\n\nAs you can see in ?@fig-CPI, we downloaded the series \\(\\{\\text{CPI}_t\\}\\). Transforming that into \\(\\{\\pi_t\\}\\), defined above, we have:\n\n\nCode\nfig, ax = plt.subplots()\nplt.axhline(y=0, linewidth=1.5, color=\"black\")\ndf_infl_m = df_infl.pct_change().dropna()\ndf_infl_m.index = df_infl_m.index + pd.offsets.MonthEnd(0) # move to month end\ndf_infl_m.plot(ax=ax)\nplt.show()\n\n\n\n\n\nFigure 2: US monthly inflation rate\n\n\n\n\n\n\n\n\n\n\nOil prices\nSince the focus is on US inflation, below we get WTI oil prices. This data is downloaded from the St Louis Fed’s FRED webpage.\n\n\nCode\ndf_oil = pd.read_csv(\"docs/DCOILWTICO.csv\")\ndf_oil['DCOILWTICO'] = pd.to_numeric(df_oil['DCOILWTICO'], errors='coerce')\ndf_oil['DATE'] = pd.to_datetime(df_oil['DATE'])\ndf_oil.set_index('DATE', inplace=True)\n\ndf_oil.plot()\n\n\n\n\n\nFigure 3: WTI oil prices\n\n\n\n\n\n\n\n\nFor the nowcasting, we are interested in the daily variation:\n\n\nCode\nfig, ax = plt.subplots()\nplt.axhline(y=0, linewidth=1.5, color=\"black\")\ndf_oil_d = df_oil.pct_change().dropna()\ndf_oil_d.plot(ax=ax)\nplt.show()\n\n\n\n\n\nFigure 4: Daily change in WTI oil prices\n\n\n\n\n\n\n\n\n\n\nTemporal features (not implemented for the time being)\nThere is a lot of information encoded in the temporal features of a time series: which day in the month it is, which month of the year, etc. For example, consider how consumers behave differently in response to oil prices over warmer months (when many decide or not to travel, and how far) compared to colder months (when energy prices factor in heating and is thus perhaps less elastic).\nTo simplify notation about time, instead of the usual subscript \\(t\\) as above to denote a time period, for precision about the frequency, we will follow this convention:\n\nsubscript \\(m\\) denotes a given month;\nsubscript \\(d\\) denotes a given day;\nsubscript \\(d(m)\\) denotes a given day in a given month; example: \\(d(m-1)\\) is a day in the previous month.\n\ngingado offers a practical way to set up the temporal features that requires only the dates of the dataset.\n\n\nCode\n# NOTE: to add documentation and tests, and later incorporate as a new function in gingado.utils\n# TO-DO: return also vocab_sizes dictionary, required to set up embedding layer.\n\ndef get_timefeat(df, freq=\"d\", features=None, add_to_df=False):\n    # For the future documentation: the add_to_df argument should be True if the data will be fed to an algorithm that takes in all data at once. If, like neural networks, the inputs are fed through different \"pipelines\", then use False and then take the result from this function an feed it separately to a neural network.\n    # the frequency is used to filter which features to add. For example, if monthly then no higher frequency features (day of ..., week of... ) are added because it doesn't make sense\n    # None or list. if futures is None, then add all temporal features that the frequency above allows. Otherwise adds only the names ones\n    all_freqs = [\"q\", \"m\", \"w\", \"d\"]\n    \n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        df.index = pd.to_datetime(df.index)\n\n\n    def i2s(index, df=df):\n        # mini-helper func that transforms an index into a pandas Series with the index\n        return pd.Series(index, index=df.index)\n    dict_timefeat = {}\n\n    if freq in all_freqs:\n        dict_timefeat['year_end'] = i2s(df.index.to_series().apply(lambda x: 1 if x.is_year_end else 0))\n        dict_timefeat['quarter_of_year'] = i2s(df.index.quarter)\n        dict_timefeat['quarter_end'] = i2s(df.index.to_series().apply(lambda x: 1 if x.is_quarter_end else 0))\n\n    if freq in [f for f in all_freqs if f not in [\"y\", \"q\"]]:\n        dict_timefeat['month_of_quarter'] = i2s(df.index.to_series().apply(lambda x: (x.month - 1) % 3 + 1))\n        dict_timefeat['month_of_year'] = i2s(df.index.month)\n\n    if freq in [f for f in all_freqs if f not in [\"y\", \"q\", \"m\"]]:\n        dict_timefeat['week_of_month'] = i2s(df.index.to_series().apply(lambda x: (x.day - 1) // 7 + 1))\n        dict_timefeat['week_of_quarter'] = i2s(df.index.to_series().apply(lambda x: ((x - pd.Timestamp(f'{x.year}-{(x.month - 1) // 3 * 3 + 1}-01')).days // 7) + 1))\n        dict_timefeat['week_of_year'] = i2s(df.index.isocalendar().week)\n\n    if freq == \"d\":\n        dict_timefeat['day_of_week'] = i2s(df.index.dayofweek)\n        dict_timefeat['day_of_month'] = i2s(df.index.day)\n        dict_timefeat['day_of_quarter'] = i2s(df.index.to_series().apply(lambda x: (x - pd.Timestamp(f'{x.year}-01-01')).days % 91 + 1))\n        dict_timefeat['day_of_year'] = i2s(df.index.dayofyear)\n\n    # Convert the dictionary of columns to a DataFrame\n    df_timefeat = pd.concat(dict_timefeat, axis=1)\n    var_thresh = VarianceThreshold(threshold=0)\n    df_timefeat = var_thresh.fit_transform(df_timefeat)\n    df_timefeat = pd.DataFrame(df_timefeat, columns=var_thresh.get_feature_names_out(), index=df.index).astype(int)\n\n    if features:\n        df_timefeat[features]\n    vocab_sizes = {col: df_timefeat[col].nunique() + 1 for col in df_timefeat}\n    if add_to_df:\n        return pd.concat([df, df_timefeat], axis=1)\n    else:\n        return df_timefeat\n\n\nSpecifically, temporal features are an excellent (and rare) type of known future input. Those are the data that we know will be like that during forecasting time, ie, at the time the observation \\(y_t\\) takes place. For example, it is trivial to know the day of the week, of the month etc, for any date we are forecasting in.\nFor this reason, we now calculate the temporal features of inflation.\n\n\nCode\ndf_timefeat = get_timefeat(df_infl_m, freq=\"m\")\n\n\n\n\nSplitting the dataset\nWe will now split the dataset into training data up until end-2020 and validation data afterwards. The training data will be further split into 5 temporally sequential folds.3\nTo simplify, we will consider valid nowcasting input data for a given output in period \\(m\\) as:\n\nall monthly data up to, and including, \\(m-1\\); and\nall daily data up to, and including, \\(d(m)\\).\n\n\n\nCode\n# Lagging the nowcasted variable\ndf_infl_m_L = df_infl_m.shift(1).dropna()\ndf_infl_m = df_infl_m.loc[df_infl_m_L.index]\n\n# Training date cutoff\ncutoff = \"2020-12-31\"\n\ny_train, y_test = df_infl_m[:cutoff], df_infl_m[cutoff:]\nXm_train, Xm_test = df_infl_m_L[:cutoff], df_infl_m_L[cutoff:]\nXd_train, Xd_test = df_oil_d[:cutoff], df_oil_d[cutoff:][1:]\n\n# Setting up the temporal folds, using the dependent variable's date\ntscv = TimeSeriesSplit(n_splits=5)\nmonthly_cv = tscv.split(y_train)\nmonthly_cv_dates = [\n    (y_train.index[m], y_train.index[n]) \n    for m, n in monthly_cv\n]\n\n\nNow for every month \\(m\\) in the dependent variable, we can find all \\(m_{t-l}, l\\geq 1\\) and all \\(d(m_{t-s}), s\\geq 0\\).\n\n\nCode\n# NOTE: Let's think/discuss if these functions are sufficiently generic to be included in gingado.utils\ndef dates_Xy(X, y, date_list, freq_y=\"m\"):\n    def get_filtered_data(frame, d):\n        filtered_data = frame.loc[frame.index &lt;= d]\n        return filtered_data if not filtered_data.empty else None\n\n    result = {\n        i: (\n            {\n                freq: get_filtered_data(Xdata, d)\n                for freq, Xdata in X.items()\n            }, \n            y.loc[d]\n        )\n        for i, d in enumerate(date_list)\n    }\n    ### The code below is throwing errors, need to solve in order to (a) calculate the temporal features of the y data, and then (b) add them to the first element of the tuple, along with the other X elements\n    # for i, d in enumerate(date_list):\n    #     result[i][0][\"future\"] = get_timefeat(y[d], freq=freq_y, add_to_df=False)\n    return result\n\ndef create_Xy(X, y, splits):\n    folds = {}\n    for k, split in enumerate(splits):\n        key = f\"fold_{k}\"\n        folds[key] = {}\n        split_train, split_valid = split\n        folds[key]['Xy_train'] = dates_Xy(X, y, split_train)\n        folds[key]['Xy_valid'] = dates_Xy(X, y, split_valid)\n    return folds\n\n\nLet’s see how this time series fold will be structured. Each fold is a sequentially longer window, so we get the following data points:\n\n\nCode\ntrain_data = create_Xy(\n    X={\"m\": Xm_train, \"d\": Xd_train}, \n    y=y_train, \n    splits=monthly_cv_dates\n)\n\nfor key in train_data.keys():\n    print(key)\n    print(f\"  {len(train_data[key]['Xy_train'])} training X-y pairs\")\n    print(f\"  {len(train_data[key]['Xy_valid'])} validation X-y pairs\")\n\n\nfold_0\n  75 training X-y pairs\n  71 validation X-y pairs\nfold_1\n  146 training X-y pairs\n  71 validation X-y pairs\nfold_2\n  217 training X-y pairs\n  71 validation X-y pairs\nfold_3\n  288 training X-y pairs\n  71 validation X-y pairs\nfold_4\n  359 training X-y pairs\n  71 validation X-y pairs"
  },
  {
    "objectID": "nowcast.html#sec-layer",
    "href": "nowcast.html#sec-layer",
    "title": "Nowcasting inflation with neural networks",
    "section": "First model: a fully connected neural network",
    "text": "First model: a fully connected neural network\nThe goal of this model is to nowcast \\(\\pi_t\\) based on its past values \\(\\pi_{t-1}\\) and on current oil prices \\(o_{d(m-s)}, s \\geq 0\\). The first model we will train is a very simple neural network:\n\\[\n\\begin{align}\n\\xi^{(m)} &= \\phi(\\mathbf{W}_1^{(m)} x_m + \\mathbf{b}_1^{(m)}) \\\\\n\\xi^{(d)} &= \\phi(\\mathbf{W}_1^{(d)} x_d + \\mathbf{b}_1^{(d)}) \\\\\ny_t &= \\mathbf{W}_2 [\\xi^{(m)}, \\xi^{(d)}] + \\mathbf{b}_2,\n\\end{align}\n\\tag{2}\\]\nwhere \\(x_t\\) is the input data, indexed in a frequency-related way as discussed above, the superscript in parenthesis indexes the frequency of the data to which it relates, and the subscript of parameters relates to the “depth” of the layer they belong to. \\(\\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times d}\\), \\(b_2 \\in \\mathbb{R}\\), \\(\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times |x_t|}\\), \\(b_1 \\in \\mathbb{R}^{d}\\), \\(\\xi \\in \\mathbb{R}^d\\) and \\(\\phi\\) is an activation function. For simplicity, we will use the ReLU activation function, which is simply: \\(\\phi(z) = \\text{max}(z, 0)\\).\nFor this neural network, we need a fix dimensionality of the input data. In other words, the network needs to know how much data it will take in at any given time, and this should not change throughout training or inference time.\nFor simplicity, let’s fix it at the first lag for inflation (the past month, \\(\\pi_{m-1}\\)) and the most recent available oil price change, \\(o_d\\).\n\n\nCode\ndim = 5 # arbitrary dimension\n\nfreqs = [\"m\", \"d\"] # using here the commonly-used frequency abbreviations\ninputs = {f: keras.layers.Input(shape=(None,1), name=f) for f in freqs}\nall_inputs = keras.layers.Concatenate(name=\"continuous\")([i for i in inputs.values()])\nout = keras.layers.Dense(units = dim, activation=\"relu\")(all_inputs)\nout = keras.layers.Dense(units=1)(out)\n\nnn_fc = keras.Model(\n    inputs=inputs, \n    outputs=out\n)\nnn_fc.compile(loss=keras.losses.MeanSquaredError())\nnn_fc.summary()\n\n\nModel: \"functional\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ m (InputLayer)      │ (None, None, 1)   │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ d (InputLayer)      │ (None, None, 1)   │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ continuous          │ (None, None, 2)   │          0 │ m[0][0], d[0][0]  │\n│ (Concatenate)       │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (Dense)       │ (None, None, 5)   │         15 │ continuous[0][0]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_1 (Dense)     │ (None, None, 1)   │          6 │ dense[0][0]       │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 21 (84.00 B)\n\n\n\n Trainable params: 21 (84.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n?@fig-FCarch presents the architecture of this model.\n\nCode\nkeras.utils.plot_model(nn_fc)\n\n\n\n\nFigure 5\n\n\n\nYou must install pydot (`pip install pydot`) for `plot_model` to work.\n\n\n\n\nChecking that it works. In the code below, we take the first fold as an example. We find the last day in the training data for that fold. Then we get the corresponding X and y data. Recall that the X is actually a dictionary of the monthly and the daily datasets. So we take the last value out of each one.\nThis simple neural network will then take one monthly and one daily data point, and try to estimate one inflation data point.\n::: {#cell-check nn model works .cell execution_count=20}\n\nCode\n# taking the last day of the training set of the last training fold\nlast_day_key = len(train_data[\"fold_0\"]['Xy_train']) - 1\nX, y = train_data[\"fold_0\"]['Xy_train'][last_day_key]\nlast_X = {}\nfor k, v in X.items():\n    idx = -2 if k == \"monthly\" else -1 # if monthly, take the first lag, if daily take the latest available\n    last_X[k] = v.iloc[idx]\n\nnn_fc.fit(last_X, y=y, epochs=4)\nnn_fc.predict(last_X)\n\n\nEpoch 1/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 715ms/step - loss: 2.3863e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 720ms/step - loss: 2.3863e-05\nEpoch 2/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 5.4180e-07\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 5.4180e-07\nEpoch 3/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 9.3277e-09\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 9.3277e-09\nEpoch 4/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 3.3482e-10\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 3.3482e-10\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 56ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 57ms/step\n\n\narray([[[0.00295398]]], dtype=float32)\n\n:::\nIt does, great!\n\nNote that this network always needs to take in input structured as a dictionary to work. The keys are the frequency codes: \"d\" for daily, \"m\" for monthly, etc.\n\nWe don’t need to bother training this very simple neural network; the goal here is to use it as a building block for a mathematical/econometric intuition of the broader nowcasting model."
  },
  {
    "objectID": "nowcast.html#sec-lstm",
    "href": "nowcast.html#sec-lstm",
    "title": "Nowcasting inflation with neural networks",
    "section": "Second model: Long short-term memory",
    "text": "Second model: Long short-term memory\nA marked improvement in how we can model time series data is the use of recurrent neural networks (RNNs). In essence, these are networks that learn to keep a stateful memory, which is updated as the network “visits” each sequential step in time, in turn using both the memory and the new data at that period to predict the output.\nOne particular type of RNN that has proven to be very successful in practice is the long short-term memory (LSTM) model, due to Hochreiter (1997). It is actually a combination of four different layers, similar to Equation 2, but built in a specific way. Here’s how:\n\\[\n\\begin{align}\nf_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\ni_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\no_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n\\tilde{c}_t &= \\omega(W_c x_t + U_c h_{t-1} + b_c) \\\\\nc_t &= \\underbrace{f_t \\odot c_{t-1}}_{\\text{Gated past data}} + \\underbrace{i_t \\odot \\tilde{c}_t}_{\\text{How much to learn}} \\\\\nh_t &= o_t \\odot \\omega(c_t),\n\\end{align}\n\\tag{3}\\]\nwhere \\(\\sigma\\) is the sigmoid function, and \\(\\omega\\) is the hyperbolic function. The hyperbolic is used here because…\nThe basic intuition of the LSTM layer is that some of the individual component layers essentially learn to look at the current data and the past memory and then decide how much new information to let through. Note that, because their activation is a sigmoid, the output of layers \\(f_t\\), \\(i_t\\) and \\(o_t\\) is a number between 0 and 1. This idea is important to bear in mind because it will be used at a much bigger scale by the whole TFT model - and will be one key feature of its interpretability.\n\n\nCode\ndim = 5 # arbitrary dimension\n\nfreqs = [\"m\", \"d\"] # using here the commonly-used frequency abbreviations\ninputs = {f: keras.layers.Input(shape=(None,1), name=f) for f in freqs}\nLSTMs = []\nfor k, v in inputs.items():\n    lstm = keras.layers.Masking(mask_value=0.0)(v)\n    lstm = keras.layers.LSTM(units=dim, return_sequences=False, name=f\"LSTM__freq_{k}\")(lstm)\n    LSTMs.append(lstm)\nencoded_series = keras.layers.Average(name=\"encoded_series\")(LSTMs)\nout = keras.layers.Dense(units = dim, activation=\"relu\")(encoded_series)\nout = keras.layers.Dense(units=1)(out)\n\nnn_lstm = keras.Model(\n    inputs=inputs, \n    outputs=out\n)\nnn_lstm.compile(loss=keras.losses.MeanSquaredError())\nnn_lstm.summary()\n\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ m (InputLayer)      │ (None, None, 1)   │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ d (InputLayer)      │ (None, None, 1)   │          0 │ -                 │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal           │ (None, None, 1)   │          0 │ m[0][0]           │\n│ (NotEqual)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ not_equal_1         │ (None, None, 1)   │          0 │ d[0][0]           │\n│ (NotEqual)          │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking (Masking)   │ (None, None, 1)   │          0 │ m[0][0]           │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ any (Any)           │ (None, None)      │          0 │ not_equal[0][0]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ masking_1 (Masking) │ (None, None, 1)   │          0 │ d[0][0]           │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ any_1 (Any)         │ (None, None)      │          0 │ not_equal_1[0][0] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LSTM__freq_m (LSTM) │ (None, 5)         │        140 │ masking[0][0],    │\n│                     │                   │            │ any[0][0]         │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LSTM__freq_d (LSTM) │ (None, 5)         │        140 │ masking_1[0][0],  │\n│                     │                   │            │ any_1[0][0]       │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ encoded_series      │ (None, 5)         │          0 │ LSTM__freq_m[0][… │\n│ (Average)           │                   │            │ LSTM__freq_d[0][… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (Dense)     │ (None, 5)         │         30 │ encoded_series[0… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (Dense)     │ (None, 1)         │          6 │ dense_2[0][0]     │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 316 (1.23 KB)\n\n\n\n Trainable params: 316 (1.23 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n?@fig-LSTMarch presents the architecture of this model.\n\nCode\nkeras.utils.plot_model(nn_lstm)\n\n\n\n\nFigure 6\n\n\n\nYou must install pydot (`pip install pydot`) for `plot_model` to work.\n\n\n\n\nNote that the input goes through a few steps before reaching the LSTM layer. This is due to a masking layer that effectively helps the model jumps time steps for which there is no data available.\nTo check that the LSTM-based neural network works, we need to feed this neural network a slightly different type of data. LSTM, as other recurrent neural networks, takes in time series data. So, unlike before, we now prepare a time series (or panel data) for each\nThis neural network will then take in the inputed time series data, and encode each frequency’s series separately through the different LSTM streams. The final result will no longer have a time dimension; it is then averaged, and this average embeddings of the different time series is used to forecast the variable of interest.\nNote the code includes an adjustment that creates a pandas Series with 0 in case data for a given frequency is not available in a given period.\n::: {#cell-check lstm model works .cell execution_count=23}\n\nCode\nlstm_x = [\n    {k: (v if v is not None else pd.Series(0.0)) for k, v in c[0].items()}  # Replace None values with pd.Series(0.0)\n    for c in train_data[\"fold_0\"]['Xy_train'].values()  # Iterate over values\n]\n\nlstm_y = [c[1] for c in train_data[\"fold_0\"]['Xy_train'].values()]\nnn_lstm.fit(lstm_x[0], y=lstm_y, epochs=4)\nnn_lstm.predict(lstm_x[0])\n\n\nEpoch 1/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 4s/step - loss: 1.3025e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 4s 4s/step - loss: 1.3025e-05\nEpoch 2/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 3.1185e-05\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 3.1185e-05\nEpoch 3/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 6.1054e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 6.1054e-06\nEpoch 4/4\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0456e-06\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0456e-06\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 654ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 655ms/step\n\n\narray([[0.00329422]], dtype=float32)\n\n:::"
  },
  {
    "objectID": "nowcast.html#sec-gates",
    "href": "nowcast.html#sec-gates",
    "title": "Nowcasting inflation with neural networks",
    "section": "Introducing… the gatekeepers",
    "text": "Introducing… the gatekeepers"
  },
  {
    "objectID": "nowcast.html#sec-transf",
    "href": "nowcast.html#sec-transf",
    "title": "Nowcasting inflation with neural networks",
    "section": "Now is a(nother) good time to pay attention",
    "text": "Now is a(nother) good time to pay attention"
  },
  {
    "objectID": "nowcast.html#sec-tftmf",
    "href": "nowcast.html#sec-tftmf",
    "title": "Nowcasting inflation with neural networks",
    "section": "Complete architecture",
    "text": "Complete architecture"
  },
  {
    "objectID": "nowcast.html#sec-nowcast",
    "href": "nowcast.html#sec-nowcast",
    "title": "Nowcasting inflation with neural networks",
    "section": "Nowcasting inflation with a simple model",
    "text": "Nowcasting inflation with a simple model"
  },
  {
    "objectID": "nowcast.html#footnotes",
    "href": "nowcast.html#footnotes",
    "title": "Nowcasting inflation with neural networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGiannone, Reichlin, and Small (2008) pioneered nowcasting in macroeconomics. See Bok et al. (2018) for a review.↩︎\nSee here for a practical walkthrough showing how to explore data with SDMX.↩︎\nSee here for more information on time series splitting.↩︎"
  }
]