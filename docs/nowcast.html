<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="A simple, mixed-frequency example">

<title>Nowcasting inflation with neural networks – gingado</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Nowcasting inflation with neural networks – gingado">
<meta property="og:description" content="A simple, mixed-frequency example">
<meta property="og:site_name" content="gingado">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">gingado</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-examples" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Examples</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-examples">    
        <li>
    <a class="dropdown-item" href="./barrolee1994.html">
 <span class="dropdown-text">Economic growth</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./forecast.html">
 <span class="dropdown-text">Forecasting FX rates</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./machine_controls.html">
 <span class="dropdown-text">Effects of labour reform</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-reference" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Reference</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-reference">    
        <li>
    <a class="dropdown-item" href="./augmentation.html">
 <span class="dropdown-text">gingado.augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./datasets.html">
 <span class="dropdown-text">gingado.datasets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./benchmark.html">
 <span class="dropdown-text">gingado.benchmark</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./estimators.html">
 <span class="dropdown-text">gingado.estimators</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./documentation.html">
 <span class="dropdown-text">gingado.model_documentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./utils.html">
 <span class="dropdown-text">gingado.utils</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/bis-med-it/gingado"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.sdmx.io"> <i class="bi bi-house-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#roadmap" id="toc-roadmap" class="nav-link active" data-scroll-target="#roadmap"><span class="header-section-number">1</span> Roadmap</a></li>
  <li><a href="#sec-data" id="toc-sec-data" class="nav-link" data-scroll-target="#sec-data"><span class="header-section-number">2</span> Loading the packages and data</a>
  <ul>
  <li><a href="#inflation" id="toc-inflation" class="nav-link" data-scroll-target="#inflation"><span class="header-section-number">2.1</span> Inflation</a></li>
  <li><a href="#oil-prices" id="toc-oil-prices" class="nav-link" data-scroll-target="#oil-prices"><span class="header-section-number">2.2</span> Oil prices</a></li>
  <li><a href="#temporal-features-not-implemented-for-the-time-being" id="toc-temporal-features-not-implemented-for-the-time-being" class="nav-link" data-scroll-target="#temporal-features-not-implemented-for-the-time-being"><span class="header-section-number">2.3</span> Temporal features (not implemented for the time being)</a></li>
  <li><a href="#splitting-the-dataset" id="toc-splitting-the-dataset" class="nav-link" data-scroll-target="#splitting-the-dataset"><span class="header-section-number">2.4</span> Splitting the dataset</a></li>
  </ul></li>
  <li><a href="#sec-fc" id="toc-sec-fc" class="nav-link" data-scroll-target="#sec-fc"><span class="header-section-number">3</span> A simple, fully connected neural network</a></li>
  <li><a href="#sec-glu" id="toc-sec-glu" class="nav-link" data-scroll-target="#sec-glu"><span class="header-section-number">4</span> A useful tool: a gate</a></li>
  <li><a href="#sec-lstm" id="toc-sec-lstm" class="nav-link" data-scroll-target="#sec-lstm"><span class="header-section-number">5</span> Long short-term memory</a></li>
  <li><a href="#sec-gates" id="toc-sec-gates" class="nav-link" data-scroll-target="#sec-gates"><span class="header-section-number">6</span> Introducing… the gatekeepers</a></li>
  <li><a href="#sec-timeembed" id="toc-sec-timeembed" class="nav-link" data-scroll-target="#sec-timeembed"><span class="header-section-number">7</span> Time to talk about time</a></li>
  <li><a href="#sec-encodcont" id="toc-sec-encodcont" class="nav-link" data-scroll-target="#sec-encodcont"><span class="header-section-number">8</span> Encoding continuous variables</a></li>
  <li><a href="#sec-varsel" id="toc-sec-varsel" class="nav-link" data-scroll-target="#sec-varsel"><span class="header-section-number">9</span> Variable selection networks</a></li>
  <li><a href="#sec-transf" id="toc-sec-transf" class="nav-link" data-scroll-target="#sec-transf"><span class="header-section-number">10</span> Now is a(nother) good time to pay attention</a></li>
  <li><a href="#sec-tftmf" id="toc-sec-tftmf" class="nav-link" data-scroll-target="#sec-tftmf"><span class="header-section-number">11</span> Complete architecture</a></li>
  <li><a href="#sec-nowcast" id="toc-sec-nowcast" class="nav-link" data-scroll-target="#sec-nowcast"><span class="header-section-number">12</span> Nowcasting inflation with a simple model</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">13</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/bis-med-it/gingado/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Nowcasting inflation with neural networks</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>

<div>
  <div class="description">
    A simple, mixed-frequency example
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Douglas K. G. Araujo </p>
             <p>Johannes Damp </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>This notebook showcases how to set up neural networks to nowcast inflation using data measured in different frequencies. The goal here is to start with a very simple dataset containing only two variables, inflation (monthly) and oil prices (daily), to slowly build up a more complex neural network based nowcasting model, the TFT-MF available in <code>gingado</code> from its v0.3.0.</p>
<p>Nowcasting is essentially the use of the most current information possible to estimate in real time an economic series of interest such as inflation or GDP before it is actually released<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. For example, if you could measure all prices every day, you could create on the last day of the month a very accurate nowcast for the headline inflation for that month - which would only be officially published a few days later. In the case of GDP, this lag between the end of the reference period and actual publication tends to be significant, around 6-10 weeks. For policymakers, investors and other decision-makers, a lot can happen in this period.</p>
<p>A related use of nowcasting is to estimate what the current period’s reading will be as this period rolls out. In other words, estimating today what the inflation reading for this month (or GDP for this quarter) will likely be as new information is unveiled in real time.</p>
<p>The nowcasting model available in <code>gingado</code> from v0.3.0 onwards is an adjusted version of the Temporal Fusion Transformer (TFT) of <span class="citation" data-cites="lim2021temporal">Lim et al. (<a href="#ref-lim2021temporal" role="doc-biblioref">2021</a>)</span>. This architecture combines <em>flexibility</em> to take on multiple datasets while learning which information to focus on and <em>interpretability</em> to provide insights on the important variables in each case. Empirical results with the TFT in finance and economics settings include stock prices (<span class="citation" data-cites="hu2021stock">Hu (<a href="#ref-hu2021stock" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="diaz2024causality">Dı́az Berenguer et al. (<a href="#ref-diaz2024causality" role="doc-biblioref">2024</a>)</span>) and GDP (<span class="citation" data-cites="laborda2023multi">Laborda, Ruano, and Zamanillo (<a href="#ref-laborda2023multi" role="doc-biblioref">2023</a>)</span>).</p>
<section id="roadmap" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="roadmap"><span class="header-section-number">1</span> Roadmap</h2>
<p>The TFT model can be a bit complex to understand at first, so we will build it up, step by step. After loading the data in <a href="#sec-data" class="quarto-xref">Section&nbsp;2</a>, the most basic neural network - a neuron layer - is presented in <a href="#sec-fc" class="quarto-xref">Section&nbsp;3</a>. Then a simple extension is shown where a neural network learns which data to let through or not in <a href="#sec-glu" class="quarto-xref">Section&nbsp;4</a>. Armed with these elements, <a href="#sec-lstm" class="quarto-xref">Section&nbsp;5</a> discusses the next architecture, more suitable for time series. Next, these elements are combined in <a href="#sec-gates" class="quarto-xref">Section&nbsp;6</a> to show how the model knows what to focus on. <a href="#sec-timeembed" class="quarto-xref">Section&nbsp;7</a> introduces the concept of embeddings of categorical variables, while <a href="#sec-encodcont" class="quarto-xref">Section&nbsp;8</a> explores this in the context of continuous variables. All this content is then put together in a way that dynamically selects useful inputs for each instance in <a href="#sec-varsel" class="quarto-xref">Section&nbsp;9</a>. The next component is the self-attention layer in <a href="#sec-transf" class="quarto-xref">Section&nbsp;10</a>. Finally, if you want to see the full picture directly, go to <a href="#sec-tftmf" class="quarto-xref">Section&nbsp;11</a> to see how these elements are put together. <a href="#sec-nowcast" class="quarto-xref">Section&nbsp;12</a> then trains the model and presents the results for this simple, illustrative nowcasting.</p>
</section>
<section id="sec-data" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-data"><span class="header-section-number">2</span> Loading the packages<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and data</h2>
<p>Let’s use our SDMX connectors to find and download data from official sources in a reproducible way.</p>
<p>To abstract from currency issues, we will use US inflation and oil prices, which are denominated in US dollars.</p>
<div id="load-packages" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span> <span class="co"># or "torch", "jax" according to user preference</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sdmx</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gingado.utils <span class="im">import</span> load_SDMX_data</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> TimeSeriesSplit</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="inflation" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="inflation"><span class="header-section-number">2.1</span> Inflation</h3>
<p>Since this is a monthly nowcast of inflation, the best way to do this is to use a <em>monthly change in the consumer price index</em>, <span class="math inline">\(\pi_t^{(m)}=(\text{CPI}_t - \text{CPI}_{t-1})/\text{CPI}_{t-1}\)</span>, not the year-on-year rate, <span class="math inline">\(\pi_t^{(y)}=(\text{CPI}_t - \text{CPI}_{t-12})/\text{CPI}_{t-12}\)</span>, which is how people usually think of inflation. This is because we want to nowcast only the value at the margin; 11 twelfths of <span class="math inline">\(\pi_t^{(y)}\)</span> are already known, since <span class="math inline">\(\pi_t^{(y)} = -1 + \prod_{l=0}^{11} (1+\pi_{t-l})\)</span>.</p>
<p>Then, only at the end we combine rolling windows of 12 consecutive monthly inflation rates, of which only the last one or two are estimated, to correctly create an annual inflation rate.</p>
<p>Formally, if we know all values except the current and last month’s, then: <span id="eq-finalnowcast"><span class="math display">\[
\hat{\pi}_t^{(y)}=(\prod_{l=0}^1 (1+\hat{\pi}_{t-l}^{(m)}) \prod_{l=2}^{11} (1+\pi_{t-l}^{(m)}) )-1,
\tag{1}\]</span></span></p>
<p>where the hat notation (<span class="math inline">\(\hat{\pi}\)</span>) means that a particular value was estimated.</p>
<p>For inflation, we take a dataflow from the <a href="https://data.bis.org/topics/CPI">BIS</a>, since we are looking for US data. Let’s explore it first and then choose the correct data specifications to download the time series.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div id="inflation-dataflow" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>BIS <span class="op">=</span> sdmx.Client(<span class="st">"BIS"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>cpi_msg <span class="op">=</span> BIS.dataflow(<span class="st">'WS_LONG_CPI'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>cpi_dsd <span class="op">=</span> cpi_msg.structure</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>These are all possible keys:</p>
<div id="cell-CPI_dimensions" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.components</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cpi_dimensions" class="cell-output cell-output-display" data-execution_count="3">
<pre><code>[&lt;Dimension FREQ&gt;,
 &lt;Dimension REF_AREA&gt;,
 &lt;Dimension UNIT_MEASURE&gt;,
 &lt;TimeDimension TIME_PERIOD&gt;]</code></pre>
</div>
</div>
<p>For example, “FREQ” (frequency) takes in these values:</p>
<div id="cell-FREQ_codelist" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cl__FREQ <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"FREQ"</span>).local_representation.enumerated)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>cl__FREQ</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="freq_codelist" class="cell-output cell-output-display" data-execution_count="4">
<pre><code>CL_FREQ
A                                   Annual
B    Daily - business week (not supported)
D                                    Daily
E                    Event (not supported)
H                              Half-yearly
M                                  Monthly
Q                                Quarterly
W                                   Weekly
Name: Code list for Frequency (FREQ), dtype: object</code></pre>
</div>
</div>
<p>And “REF_AREA” (reference area) can be set to:</p>
<div id="cell-REF_AREA_codelist" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"REF_AREA"</span>).local_representation.enumerated)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="ref_area_codelist" class="cell-output cell-output-display" data-execution_count="5">
<pre><code>CL_AREA
1X                                      ECB
4T    Emerging market economies (aggregate)
5A                  All reporting economies
5R                       Advanced economies
AE                     United Arab Emirates
                      ...                  
VN                                  Vietnam
XM                                Euro area
XW                                    World
ZA                             South Africa
_Z                           Not applicable
Name: Reference area code list, Length: 101, dtype: object</code></pre>
</div>
</div>
<p>We can check that the US is amongst the reference areas:</p>
<p>::: {#cell-check US in REF_AREA codelist .cell execution_count=6}</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA[<span class="st">'US'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="check-us-in-ref_area-codelist" class="cell-output cell-output-display" data-execution_count="6">
<pre><code>'United States'</code></pre>
</div>
<p>:::</p>
<p>Finally, the “UNIT_MEASURE” values can be:</p>
<p>::: {#cell-codelist for UNIT_MEASURE in dataflow BIS__WS_LONG_CPI .cell execution_count=7}</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"UNIT_MEASURE"</span>).local_representation.enumerated)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="codelist-for-unit_measure-in-dataflow-bis__ws_long_cpi" class="cell-output cell-output-display" data-execution_count="7">
<pre><code>CL_BIS_UNIT
000                     Unknown
001                 100 - yield
002                   EUR / MWh
003     Index, 1996 Jan 2 = 100
004           Euro / troy ounce
                 ...           
ZAR                        Rand
ZMK              Zambian Kwacha
ZMW              Zambian Kwacha
ZWD             Zimbabwe Dollar
ZWL    Zimbabwe Dollar (fourth)
Name: BIS_Unit, Length: 1051, dtype: object</code></pre>
</div>
<p>:::</p>
<p>In the <a href="https://data.bis.org/topics/CPI#faq">BIS website for this data</a>, we can see that the unit in levels is <code>Index, 2010 = 100</code> (the other one is <code>Year-on-year changes, in per cent</code>, which as discussed above we don’t want for this case.)</p>
<p>::: {#cell-finding code for index .cell execution_count=8}</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE[cl__UNIT_MEASURE.<span class="bu">str</span>.contains(<span class="st">"Index, 2010 = 100"</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="finding-code-for-index" class="cell-output cell-output-display" data-execution_count="8">
<pre><code>CL_BIS_UNIT
628    Index, 2010 = 100
Name: BIS_Unit, dtype: object</code></pre>
</div>
<p>:::</p>
<p>Armed with this knowledge, we can now download monthly consumer price index data for the US. Let’s start after 1985 so that we have a sufficiently long history but without too much influence of the tectonic shift of the US dollar devaluation in the early 1970s and ensuing high inflation:</p>
<div id="cell-fig-cpi" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df_infl <span class="op">=</span> load_SDMX_data(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    sources<span class="op">=</span>{<span class="st">"BIS"</span>: <span class="st">"'WS_LONG_CPI'"</span>},</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    keys<span class="op">=</span>{<span class="st">"FREQ"</span>: <span class="st">"M"</span>, <span class="st">"REF_AREA"</span>: <span class="st">"US"</span>, <span class="st">"UNIT_MEASURE"</span>: <span class="st">"628"</span>},</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>{<span class="st">"startPeriod"</span>: <span class="dv">1985</span>}</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>df_infl.plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Querying data from BIS's dataflow 'WS_LONG_CPI' - BIS long consumer prices...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-cpi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cpi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: US consumer price index, 2010 = 100
</figcaption>
<div aria-describedby="fig-cpi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-cpi-output-2.png" width="584" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>As you can see in <a href="#fig-cpi" class="quarto-xref">Figure&nbsp;1</a>, we downloaded the series <span class="math inline">\(\{\text{CPI}_t\}\)</span>. Transforming that into <span class="math inline">\(\{\pi_t\}\)</span>, defined above, we have:</p>
<div id="cell-fig-pi" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>df_infl_m <span class="op">=</span> df_infl.pct_change().dropna()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>df_infl_m.index <span class="op">=</span> df_infl_m.index <span class="op">+</span> pd.offsets.MonthEnd(<span class="dv">0</span>) <span class="co"># move to month end</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>df_infl_m.plot(ax<span class="op">=</span>ax)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-pi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: US monthly inflation rate
</figcaption>
<div aria-describedby="fig-pi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-pi-output-1.png" width="599" height="403" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="oil-prices" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="oil-prices"><span class="header-section-number">2.2</span> Oil prices</h3>
<p>Since the focus is on US inflation, below we get WTI oil prices. This data is downloaded from the <a href="https://fred.stlouisfed.org/series/DCOILWTICO">St Louis Fed’s FRED webpage</a>.</p>
<div id="cell-fig-oil" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>df_oil <span class="op">=</span> pd.read_csv(<span class="st">"docs/DCOILWTICO.csv"</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DCOILWTICO'</span>] <span class="op">=</span> pd.to_numeric(df_oil[<span class="st">'DCOILWTICO'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DATE'</span>] <span class="op">=</span> pd.to_datetime(df_oil[<span class="st">'DATE'</span>])</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>df_oil.set_index(<span class="st">'DATE'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>df_oil.plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-oil" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-oil-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: WTI oil prices
</figcaption>
<div aria-describedby="fig-oil-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-oil-output-1.png" width="577" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>For the nowcasting, we are interested in the daily variation, clipped because of the sharp movements during the onset of the Covid-19 pandemic:</p>
<div id="cell-fig-oilD" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>df_oil_d <span class="op">=</span> df_oil.pct_change().dropna()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>df_oil_d.plot(ax<span class="op">=</span>ax)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.25</span>, <span class="fl">0.25</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-oild" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-oild-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Daily change in WTI oil prices
</figcaption>
<div aria-describedby="fig-oild-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-oild-output-1.png" width="582" height="401" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="temporal-features-not-implemented-for-the-time-being" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="temporal-features-not-implemented-for-the-time-being"><span class="header-section-number">2.3</span> Temporal features (not implemented for the time being)</h3>
<p>There is a lot of information encoded in the temporal features of a time series: which day in the month it is, which month of the year, etc. For example, consider how consumers behave differently in response to oil prices over warmer months (when many decide or not to travel, and how far) compared to colder months (when energy prices factor in heating and is thus perhaps less elastic).</p>
<p>To simplify notation about time, instead of the usual subscript <span class="math inline">\(t\)</span> as above to denote a time period, for precision about the frequency, we will follow this convention:</p>
<ul>
<li><p>subscript <span class="math inline">\(m\)</span> denotes a given month;</p></li>
<li><p>subscript <span class="math inline">\(d\)</span> denotes a given day;</p></li>
<li><p>subscript <span class="math inline">\(d(m)\)</span> denotes a given day in a given month; example: <span class="math inline">\(d(m-1)\)</span> is a day in the previous month.</p></li>
</ul>
<p><code>gingado</code> offers a practical way to set up the temporal features that requires only the dates of the dataset.</p>
<div id="tempfeatures" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: to add documentation and tests, and later incorporate as a new function in gingado.utils</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_timefeat(df, freq<span class="op">=</span><span class="st">"d"</span>, features<span class="op">=</span><span class="va">None</span>, add_to_df<span class="op">=</span><span class="va">False</span>, remove_const<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the future documentation: the add_to_df argument should be True if the data will be fed to an algorithm that takes in all data at once. If, like neural networks, the inputs are fed through different "pipelines", then use False and then take the result from this function an feed it separately to a neural network.</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the frequency is used to filter which features to add. For example, if monthly then no higher frequency features (day of ..., week of... ) are added because it doesn't make sense</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># None or list. if futures is None, then add all temporal features that the frequency above allows. Otherwise adds only the names ones</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    all_freqs <span class="op">=</span> [<span class="st">"q"</span>, <span class="st">"m"</span>, <span class="st">"w"</span>, <span class="st">"d"</span>]</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> pd.api.types.is_datetime64_any_dtype(df.index):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        df.index <span class="op">=</span> pd.to_datetime(df.index)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> i2s(index, df<span class="op">=</span>df):</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mini-helper func that transforms an index into a pandas Series with the index</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.Series(index, index<span class="op">=</span>df.index)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    dict_timefeat <span class="op">=</span> {}</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> all_freqs:</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'year_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_year_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_of_year'</span>] <span class="op">=</span> i2s(df.index.quarter <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_quarter_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>]]:</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">3</span>))</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_year'</span>] <span class="op">=</span> i2s(df.index.month <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>, <span class="st">"m"</span>]]:</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_month'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.day <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">7</span>))</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: ((x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>(x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">-01'</span>)).days <span class="op">//</span> <span class="dv">7</span>)))</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_year'</span>] <span class="op">=</span> i2s(df.index.isocalendar().week)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="op">==</span> <span class="st">"d"</span>:</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_week'</span>] <span class="op">=</span> i2s(df.index.dayofweek)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_month'</span>] <span class="op">=</span> i2s(df.index.day)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-01-01'</span>)).days <span class="op">%</span> <span class="dv">91</span>))</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_year'</span>] <span class="op">=</span> i2s(df.index.dayofyear)</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the dictionary of columns to a DataFrame</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> pd.concat(dict_timefeat, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features:</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        df_timefeat <span class="op">=</span> df_timefeat[features]</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_const:</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        var_thresh <span class="op">=</span> VarianceThreshold(threshold<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        df_timefeat <span class="op">=</span> var_thresh.fit_transform(df_timefeat)</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>        df_timefeat <span class="op">=</span> pd.DataFrame(df_timefeat, columns<span class="op">=</span>var_thresh.get_feature_names_out(), index<span class="op">=</span>df.index).astype(<span class="bu">int</span>)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> add_to_df:</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.concat([df, df_timefeat], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> df_timefeat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Specifically, temporal features are an excellent (and rare) type of <em>known future</em> input. Those are the data that we know will be like that during forecasting time, ie, at the time the observation <span class="math inline">\(y_t\)</span> takes place. For example, it is trivial to know the day of the week, of the month etc, for any date we are forecasting in.</p>
<p>For this reason, we now calculate the temporal features of inflation.</p>
<div id="temporal-features-for-the-inflation-series" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df_timefeat <span class="op">=</span> get_timefeat(df_infl_m, freq<span class="op">=</span><span class="st">"m"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="splitting-the-dataset" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="splitting-the-dataset"><span class="header-section-number">2.4</span> Splitting the dataset</h3>
<p>We will now split the dataset into training data up until end-2020 and validation data afterwards. The training data will be further split into 5 temporally sequential folds.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>To simplify, we will consider valid nowcasting <em>input</em> data for a given output in period <span class="math inline">\(m\)</span> as:</p>
<ul>
<li><p>all monthly data up to, and including, <span class="math inline">\(m-1\)</span>; and</p></li>
<li><p>all daily data up to, and including, <span class="math inline">\(d(m)\)</span>.</p></li>
</ul>
<div id="time-series-splits" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training date cutoff</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="st">"2020-12-31"</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> df_infl_m[:cutoff][<span class="dv">1</span>:], df_infl_m[cutoff:][<span class="dv">1</span>:]</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>Xm_train, Xm_test <span class="op">=</span> df_infl_m[:cutoff][:<span class="op">-</span><span class="dv">1</span>], df_infl_m[cutoff:][:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>Xd_train, Xd_test <span class="op">=</span> df_oil_d[:cutoff], df_oil_d[cutoff:]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> {<span class="st">"m"</span>: Xm_train, <span class="st">"d"</span>: Xd_train}</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> {<span class="st">"m"</span>: Xm_test, <span class="st">"d"</span>: Xd_test}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now for every month <span class="math inline">\(m\)</span> in the dependent variable, we can find all <span class="math inline">\(m_{t-l}, l\geq 1\)</span> and all <span class="math inline">\(d(m_{t-s}), s\geq 0\)</span>.</p>
<p>Note that in the example below, for each data point that we want to forecast (<code>y</code>), we take 12 lags of the monthly covariates and 250 lags of the daily covariates (broadly corresponding to one year).</p>
<div id="create-data" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>maxlags <span class="op">=</span> {<span class="st">"m"</span>: <span class="dv">12</span>, <span class="st">"d"</span>: <span class="dv">250</span>}</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data(X, y, maxlags<span class="op">=</span>maxlags, tscv<span class="op">=</span>TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">5</span>), timedim<span class="op">=</span><span class="va">True</span>, return_dates<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If timeedim is true, then the dimensions of the tensors are n_samples/time dimension/features. If not then it is n_samples/time dimension (lag) * features, ie each lag is flattened as if it were a feature. Use True when passing to recurrent nets, use False for fully connected layers.</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    X_split <span class="op">=</span> {}</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    y_split <span class="op">=</span> {}</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> return_dates:</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        X_dates <span class="op">=</span> {}</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    n_feat <span class="op">=</span> {k: v.shape[<span class="dv">1</span>] <span class="cf">for</span> k, v <span class="kw">in</span> X.items()}</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tscv:</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        split_cv <span class="op">=</span> tscv.split(y)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>        cv_dates <span class="op">=</span> [</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>            (y.index[m], y.index[n]) <span class="co"># (train, valid) for each fold</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> m, n <span class="kw">in</span> split_cv</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n_fold, split <span class="kw">in</span> <span class="bu">enumerate</span>(cv_dates):</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>            fold <span class="op">=</span> <span class="ss">f"fold_</span><span class="sc">{</span>n_fold<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>            dates_split <span class="op">=</span> {</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>                <span class="st">"train"</span>: split[<span class="dv">0</span>],</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>                <span class="st">"valid"</span>: split[<span class="dv">1</span>]</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            X_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>            y_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> return_dates:</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>                X_dates[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> chunk, dates <span class="kw">in</span> dates_split.items():</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> ysample_date <span class="kw">in</span> tqdm(dates):</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>                    padded_x <span class="op">=</span> {}</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>                    dates_X <span class="op">=</span> {}</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> f <span class="kw">in</span> X.keys():</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">try</span>:</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">if</span> return_dates:</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>                                dates_X[f] <span class="op">=</span> X[f][:ysample_date][:<span class="op">-</span><span class="dv">1</span>].index</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> X[f][:ysample_date][:<span class="op">-</span><span class="dv">1</span>].values</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> keras.utils.pad_sequences([to_pad], maxlen<span class="op">=</span>maxlags[f], dtype<span class="op">=</span>np.float32)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>                        x_shape <span class="op">=</span> (<span class="dv">1</span>, maxlags[f], n_feat[f]) <span class="cf">if</span> timedim <span class="cf">else</span> (<span class="dv">1</span>, maxlags[f] <span class="op">*</span> n_feat[f])</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> padded_x[f].reshape(x_shape)</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>                    X_split[fold][chunk].append(padded_x)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>                    y_split[fold][chunk].append(y_train.loc[ysample_date])</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> return_dates:</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>                        X_dates[fold][chunk].append(dates_X)</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_dates:</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> X_split, y_split, X_dates</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X_split, y_split</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split, dates <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags, return_dates<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s see how this time series fold will be structured. Each fold is a sequentially longer window, so we get the following data points:<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<div id="create-batches-of-data" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> y_train_split.keys():</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'train'</span>])<span class="sc">}</span><span class="ss"> training X-y pairs"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'valid'</span>])<span class="sc">}</span><span class="ss"> validation X-y pairs"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>fold_0:
  75 training X-y pairs
  71 validation X-y pairs
fold_1:
  146 training X-y pairs
  71 validation X-y pairs
fold_2:
  217 training X-y pairs
  71 validation X-y pairs
fold_3:
  288 training X-y pairs
  71 validation X-y pairs
fold_4:
  359 training X-y pairs
  71 validation X-y pairs</code></pre>
</div>
</div>
<p>Another way to visualise this is in <a href="#fig-tssplit" class="quarto-xref">Figure&nbsp;5</a>, which shows a similar division of dataset along the time dimension, albeit for four folds.</p>
<div id="fig-tssplit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-tssplit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5
</figcaption>
<div aria-describedby="fig-tssplit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figcaption>Time series split illustration</figcaption>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_013.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-fc" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-fc"><span class="header-section-number">3</span> A simple, fully connected neural network</h2>
<p>The goal of this model is to nowcast <span class="math inline">\(\pi_t\)</span> based on its past values <span class="math inline">\(\pi_{t-1}\)</span> and on current oil prices <span class="math inline">\(o_{d(m-s)}, s \geq 0\)</span>. The first model we will train is a very simple neural network:</p>
<p><span id="eq-model0nnlayer"><span class="math display">\[
\begin{align}
\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) \\
y_t &amp;= \mathbf{W}_2 \xi + \mathbf{b}_2,
\end{align}
\tag{2}\]</span></span></p>
<p>where <span class="math inline">\(x_t\)</span> is the input data and the subscript of parameters relates to the “depth” of the layer they belong to. Using <span class="math inline">\(\lambda\)</span> as the dimensionality of the model, <span class="math inline">\(\mathbf{W}_2 \in \mathbb{R}^{1 \times \lambda}\)</span>, <span class="math inline">\(b_2 \in \mathbb{R}\)</span>, <span class="math inline">\(\mathbf{W}_1 \in \mathbb{R}^{\lambda \times |x_t|}\)</span>, <span class="math inline">\(b_1 \in \mathbb{R}^{d}\)</span>, <span class="math inline">\(\xi \in \mathbb{R}^\lambda\)</span> and <span class="math inline">\(\phi\)</span> is an activation function. For simplicity, we will use the ReLU activation function, which is simply: <span class="math inline">\(\phi(z) = \text{max}(z, 0)\)</span>.</p>
<p>For this neural network, we need a fix dimensionality of the input data. In other words, the network <em>needs</em> to know how much data it will take in at any given time, and this should not change throughout training or inference time.</p>
<p>For each data in our dependent variable, we simply stack the latest available monthly and daily data and their respective lags. Using the numbers above, this would be 12 lags for monthly data and 250 lags of daily oil data. Linking this to <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a> above, <span class="math inline">\(x_t = [\pi_{m-1}, \dots, \pi_{m-12}, o_d, \dots, o_{d-250}]\)</span>.</p>
<p>All of this data will be considered by the neural network at the same time. In a way, this is analogous to how a normal regression is run. However, the number of data points (12 + 250) used in this toy neural network is bigger than typical regressions.</p>
<div class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> createNN_fc(dim<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    nn_fc <span class="op">=</span> keras.Sequential([</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),)),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span>activation, name<span class="op">=</span><span class="st">"SummariseInput"</span>),</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    ], <span class="st">"FullyConnected"</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    nn_fc.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn_fc</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>nn_fc <span class="op">=</span> createNN_fc()</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>nn_fc.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-nn_fc_summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="18">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nn_fc_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Summary of fully connected model
</figcaption>
<div aria-describedby="tbl-nn_fc_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ SummariseInput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ CalculateOutput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,225</span> (16.50 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,225</span> (16.50 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<p><a href="#fig-arch_fc" class="quarto-xref">Figure&nbsp;6</a> presents the architecture of this model.</p>
<div id="cell-fig-arch_fc" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<div id="fig-arch_fc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_fc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Architecture of the model with fully connected layer
</figcaption>
<div aria-describedby="fig-arch_fc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_fc-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>Note in <a href="#fig-arch_fc" class="quarto-xref">Figure&nbsp;6</a> that a first dense layer (ie, as in <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a>) takes in 262 data points, uses the activation function ReLU, and then outputs 16 data points for the next layer.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>Checking that it works. In the code below, we take the last fold as an example.</p>
<div id="preparing-fully-connected-nn-data" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>X_train_split_fc, y_train_split_fc <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, timedim<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>fold <span class="op">=</span> <span class="st">"fold_4"</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>X_train_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"train"</span>]])</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>y_train_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"train"</span>])</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>X_valid_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"valid"</span>]])</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>y_valid_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"valid"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we train the network.</p>
<div id="fit-nn-model" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>history_fc <span class="op">=</span> nn_fc.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 29s 835ms/step - loss: 7.7084e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - loss: 5.9257e-04 - val_loss: 0.0092
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.4794e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.0408e-04 - val_loss: 0.0083
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 4.3850e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1152e-04 - val_loss: 0.0077
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.2073e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 7.1643e-05 - val_loss: 0.0072
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 4.1802e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 4.5998e-05 - val_loss: 0.0070
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 3.0665e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.3573e-05 - val_loss: 0.0068
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 2.4176e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.1779e-05 - val_loss: 0.0067
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 2.2917e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.1207e-05 - val_loss: 0.0066
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.5856e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.1206e-05 - val_loss: 0.0065
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 3.0572e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.9340e-05 - val_loss: 0.0064
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 9.7417e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.5347e-05 - val_loss: 0.0064
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.2201e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3976e-05 - val_loss: 0.0064
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 8.2848e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3562e-05 - val_loss: 0.0064
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 7.2619e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3433e-05 - val_loss: 0.0063
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 9.5079e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3710e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3650e-05 - val_loss: 0.0063
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.4738e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2243e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.2270e-05 - val_loss: 0.0063
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 2.2337e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1547e-05 - val_loss: 0.0062
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 5.8910e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1782e-05 - val_loss: 0.0062
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 7.7424e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1109e-05 - val_loss: 0.0062
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.1578e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1688e-05 - val_loss: 0.0062</code></pre>
</div>
</div>
<p>You can see in <a href="#fig-history_fc" class="quarto-xref">Figure&nbsp;7</a> that the loss decreases with training, but the validation loss (ie, calculated on held-out data) is higher than the in-sample loss. This suggests the model is learning <em>too much</em> how to fit the data. In other words, it is also fitting some level of noise, which is not reproducible out-of-sample.</p>
<div id="cell-fig-history_fc" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_fc.history).plot()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_fc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_fc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Losses calculated in a simple, fully-connected neural network.
</figcaption>
<div aria-describedby="fig-history_fc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_fc-output-1.png" width="606" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>We don’t need to bother training too much this very simple neural network; the goal here is to use it as a building block for a mathematical/econometric intuition of the broader nowcasting model.</p>
</section>
<section id="sec-glu" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-glu"><span class="header-section-number">4</span> A useful tool: a gate</h2>
<p>The network trained in the previous section can learn how to map the input data to the output data. But there are ways to take advantage of the incredible flexibility in architecture (ie, how neural layers are stacked). One such way is to have the data inform which layers’s outputs are actually used downstream or not. This section describes how.</p>
<p>First, we replicate the same simple fully connected layer of <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a> two times. One of the neural networks will behave as before: learning to map the input data to the output data. The second one will also look at the same data, but with a different goal: it will learn how much data to let through. Its output is a value between 0 and 1, which is then multiplied to the “original” network. When this part of the neural network yields values closer to 0, the mainstream values are effectively shut down. Conversely, when the values are close to 1, the data proceeds as normal.</p>
<p>Adjusting <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a> to include a gate could be as in <a href="#eq-gated" class="quarto-xref">Equation&nbsp;3</a>:</p>
<p><span id="eq-gated"><span class="math display">\[
\begin{align}
\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) \\
G &amp;= \sigma(\mathbf{W}_G x_t + \mathbf{b}_G) \\
y_t &amp;= \mathbf{W}_2 (\xi \odot G) + \mathbf{b}_2,
\end{align}
\tag{3}\]</span></span></p>
<p>with <span class="math inline">\(\sigma\)</span> representing the sigmoid function and <span class="math inline">\(\odot\)</span> the Hadamard multiplication.</p>
<p>This type of model where the gate is trained on the same data was introduced by <span class="citation" data-cites="dauphin2017language">Dauphin et al. (<a href="#ref-dauphin2017language" role="doc-biblioref">2017</a>)</span>. From now on, we refer to <a href="#eq-gated" class="quarto-xref">Equation&nbsp;3</a> as a Gated Linear Unit (GLU).</p>
<p>Note that the data that informs the gate does not necessarily need to be the same as the mainstream data.</p>
<div class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#mainstream = createNN_fc(activation="relu")</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co">#gate = createNN_fc(activation="sigmoid")</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),), name<span class="op">=</span><span class="st">"FlattenedLaggedData"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>mainstream <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(<span class="bu">input</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>gate <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"Gate"</span>)(<span class="bu">input</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>gated_data <span class="op">=</span> keras.layers.Multiply(name<span class="op">=</span><span class="st">"GatedData"</span>)([mainstream, gate])</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(gated_data)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>nn_fc_gated <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">input</span>, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GatedModel"</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-nn_fc_gated_summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="23">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nn_fc_gated_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Summary of gated model
</figcaption>
<div aria-describedby="tbl-nn_fc_gated_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "GatedModel"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ FlattenedLaggedData │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">262</span>)       │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ SummariseInput      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │ FlattenedLaggedD… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ Gate (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │ FlattenedLaggedD… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ GatedData           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ SummariseInput[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Multiply</span>)          │                   │            │ Gate[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ CalculateOutput     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ GatedData[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]   │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,433</span> (32.94 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,433</span> (32.94 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_gatedmodel" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc_gated, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<div id="fig-arch_gatedmodel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_gatedmodel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Architecture of gated neural network
</figcaption>
<div aria-describedby="fig-arch_gatedmodel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_gatedmodel-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>The model can now be fit with the same input data: it is then used by two different branches (referred to above in the text as “mainstream” and “gate”).</p>
<div id="fitting-gated-model" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>history_fc_gated <span class="op">=</span> nn_fc_gated.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 36s 1s/step - loss: 3.4065e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - loss: 1.5945e-04 - val_loss: 0.0048
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 5.9664e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 9.8038e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 9.7468e-05 - val_loss: 0.0045
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.9868e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 4.7776e-05 - val_loss: 0.0041
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 3.1224e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.5120e-05 - val_loss: 0.0039
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.8598e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.5249e-05 - val_loss: 0.0038
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.3345e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.4973e-05 - val_loss: 0.0036
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.2028e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.7700e-05 - val_loss: 0.0035
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.1317e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4936e-05 - val_loss: 0.0035
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.6257e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.7955e-05 - val_loss: 0.0034
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 6.1764e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4369e-05 - val_loss: 0.0033
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 8.9612e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4119e-05 - val_loss: 0.0032
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 5.3494e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2139e-05 - val_loss: 0.0031
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 4.0282e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6040e-05 - val_loss: 0.0032
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.1200e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2170e-05 - val_loss: 0.0031
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 7.2579e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4480e-05 - val_loss: 0.0031
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.3658e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2944e-05 - val_loss: 0.0031
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 8.4548e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1309e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.1389e-05 - val_loss: 0.0031
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.0804e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1245e-05 - val_loss: 0.0031
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.7261e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 1.2270e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.2262e-05 - val_loss: 0.0030
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.0356e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4989e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.4830e-05 - val_loss: 0.0031</code></pre>
</div>
</div>
<div id="cell-fig-history_fc_gated" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>gated_loss <span class="op">=</span> pd.DataFrame(history_fc_gated.history)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>gated_loss[<span class="st">"val_loss (no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> gated_loss.plot()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_fc_gated" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_fc_gated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Losses calculated in a simple, fully-connected neural network with gate.
</figcaption>
<div aria-describedby="fig-history_fc_gated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_fc_gated-output-1.png" width="606" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-lstm" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sec-lstm"><span class="header-section-number">5</span> Long short-term memory</h2>
<p>A marked improvement in how we can model time series data is the use of recurrent neural networks (RNNs). In essence, these are networks that learn to keep a stateful memory, which is updated as the network “visits” each sequential step in time, in turn using both the memory and the new data at that period to predict the output.</p>
<p>In contrast to the fully connected layer in <a href="#sec-fc" class="quarto-xref">Section&nbsp;3</a>, which need to look at different lags to pick up any history-dependent information, RNNs look at the observable variables at each period and learn a latent “state” (akin to Kalman filters, for example). The same network then slides up one step in time and uses that information and the previous state to update the state, and so on…</p>
<p>One particular type of RNN that has proven to be very successful in practice is the long short-term memory (LSTM) model, due to <span class="citation" data-cites="hochreiter1997long">Hochreiter (<a href="#ref-hochreiter1997long" role="doc-biblioref">1997</a>)</span>. It is actually a combination of four different layers, of which three are actually gates. These layers are built in a specific way. Here’s how the input vector <span class="math inline">\(x_t\)</span> and the learned LSTM state <span class="math inline">\(h_{t-1}\)</span> are used for the LSTM-forward pass at time step <span class="math inline">\(t\)</span>:</p>
<p><span id="eq-model1lstm"><span class="math display">\[
\begin{align}
f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
\tilde{c}_t &amp;= \omega(W_c x_t + U_c h_{t-1} + b_c) \\
c_t &amp;= \underbrace{f_t \odot c_{t-1}}_{\text{Gated past data}} + \underbrace{i_t \odot \tilde{c}_t}_{\text{How much to learn}} \\
h_t &amp;= o_t \odot \omega(c_t),
\end{align}
\tag{4}\]</span></span></p>
<p>where <span class="math inline">\(\omega\)</span> is the hyperbolic function.</p>
<p>The basic intuition of the LSTM is that some of the individual component layers essentially learn to look at the current data and the past memory and then decide how much new information to let through. Note that, because their activation is a sigmoid, the output of layers <span class="math inline">\(f_t\)</span>, <span class="math inline">\(i_t\)</span> and <span class="math inline">\(o_t\)</span> is a number between 0 and 1. This idea is important to bear in mind because it will be used at a much bigger scale by the whole TFT model - and will be one key feature of its interpretability.</p>
<p>With LSTM networks, it is easier to incorporate mixed-frequency data in a meaningful way. This is done below by passing data of each frequency through their own LSTM layers, and combining their last outputs.</p>
<div class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>] <span class="co"># using here the commonly-used frequency abbreviations</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> inputs.items():</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>nn_lstm <span class="op">=</span> keras.Model(</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"LSTMNetwork"</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>nn_lstm.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>nn_lstm.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-lstm_summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="27">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lstm_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Summary of LSTM model
</figcaption>
<div aria-describedby="tbl-lstm_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "LSTMNetwork"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ m (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ d (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal_1         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>)   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">1,152</span> │ masking[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],    │
│                     │                   │            │ any[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">1,152</span> │ masking_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],  │
│                     │                   │            │ any_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Average</span>)           │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">272</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,593</span> (10.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,593</span> (10.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_lstm" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_lstm, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="28">
<div id="fig-arch_lstm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_lstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Architecture of the network with LSTM layer
</figcaption>
<div aria-describedby="fig-arch_lstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_lstm-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>The reason why the dimensions in the input layer are now <code>(None, None, 1)</code>, with one addition <code>None</code> compared to before(for example, <a href="#fig-arch_fc" class="quarto-xref">Figure&nbsp;6</a>) is due to the <em>time dimension</em>. Whereas before the model didn’t know how many samples it would be fed, now also the length of the time window can change because each time step will pass through exactly the same parameters.</p>
<p>Note that the input goes through a few steps before reaching the LSTM layer. This is due to a masking layer that effectively helps the model jumps time steps for which there is no data available.</p>
<p>To check that the LSTM-based neural network works, we need to feed this neural network a slightly different type of data. LSTM, as other recurrent neural networks, takes in time series data. So, unlike before, we now prepare a time series (or panel data) for each</p>
<p>This neural network will then take in the inputted time series data, and encode each frequency’s series separately through the different LSTM streams. The final result will no longer have a time dimension; it is then averaged, and this average embeddings of the different time series is used to forecast the variable of interest.</p>
<div id="train-the-lstm" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>history_lstm <span class="op">=</span> nn_lstm.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3:24 6s/step - loss: 1.6215e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 1.6167e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.6017e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.5192e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 1.4480e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 72ms/step - loss: 1.4017e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 1.4704e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5181e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5496e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5682e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5771e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5784e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5743e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5649e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5531e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5453e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.5370e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.5308e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.5278e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5239e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.5177e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.5107e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.5027e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.4938e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4848e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4753e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4652e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4547e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4442e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4345e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4245e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4152e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4070e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.3992e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.3920e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.3856e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 9s 94ms/step - loss: 1.3795e-05 - val_loss: 6.0874e-06
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 95ms/step - loss: 7.3218e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 8.1143e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 8.0385e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 7.8478e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 7.7847e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.3449e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.5867e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 8.7567e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 9.0415e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.1856e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.2884e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.3640e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.4067e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.4238e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.4146e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.6381e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.8488e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 1.0025e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0156e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0309e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0425e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0523e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0633e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0723e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0799e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0859e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0910e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0945e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0972e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0993e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1007e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1015e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1023e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1036e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1050e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.1057e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - loss: 1.1063e-05 - val_loss: 6.8444e-06
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 110ms/step - loss: 6.3121e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 6.7788e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 6.5995e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 6.6104e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 7.0550e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 7.4157e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 7.7927e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.1901e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.4916e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.6887e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 8.8465e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.9628e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.0287e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.1167e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.1975e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.3517e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.4584e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.5275e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.5781e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.6136e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.6547e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.6847e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 9.7134e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 9.7314e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.7486e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.8185e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.8788e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.9257e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.9663e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.9988e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0025e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0063e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0094e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0125e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0148e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0173e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - loss: 1.0196e-05 - val_loss: 7.2171e-06
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 5.0996e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 6.2289e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 6.3416e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 7.9919e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 8.7527e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 9.0603e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.1739e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.1851e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.2198e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.2261e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.2545e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.6154e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.9251e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 1.0134e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.0289e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.0416e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.0525e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.0644e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.0735e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.0836e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.0909e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.0971e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.1012e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.1038e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.1052e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.1065e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1092e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1106e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1120e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1135e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1145e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1149e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1152e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1156e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1153e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1152e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - loss: 1.1150e-05 - val_loss: 6.0433e-06
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 7.9992e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 6.8666e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 6.5877e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 6.5303e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 6.9402e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 7.3870e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.8264e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.0976e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.3385e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.5359e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.6839e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.7701e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.8259e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.8788e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9038e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9369e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9510e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9558e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9444e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9588e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9686e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.9940e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.0153e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.0992e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.1847e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.2490e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3021e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3406e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3711e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.4220e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.4683e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.5171e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.5737e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6210e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6660e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7100e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 9.7515e-06 - val_loss: 5.8732e-06
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.2271e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.1818e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.0857e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.1667e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.1831e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.1946e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.1867e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.1741e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.1778e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.1880e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2024e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2115e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2185e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2206e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2204e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2180e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2135e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2093e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2077e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2065e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2066e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2056e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2042e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2100e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.2156e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.2203e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.2242e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.2269e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.2280e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.2280e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2270e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2250e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2233e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2207e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2181e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2149e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - loss: 1.2119e-05 - val_loss: 7.0616e-06
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 8.3612e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.8069e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.5514e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.7575e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 8.1777e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 8.3257e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.6232e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 8.7949e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 8.9814e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 9.1020e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.2517e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.6735e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0009e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0349e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0607e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0824e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0992e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1127e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1223e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1298e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1354e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1393e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1430e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1473e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1503e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1534e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1549e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1557e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1559e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1552e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.1538e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1523e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1506e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1487e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.1472e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.1466e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - loss: 1.1461e-05 - val_loss: 6.1168e-06
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 104ms/step - loss: 4.3154e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 5.7752e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 6.9605e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 7.6078e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 72ms/step - loss: 7.8117e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 7.9627e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 8.1012e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.2717e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.3933e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 8.5347e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 8.6480e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 8.7975e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.1548e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.4725e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.7121e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.9166e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.0137e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.0339e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.0496e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.0624e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.0720e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0802e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0863e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0906e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0941e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0963e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0976e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0994e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1016e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1028e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1034e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1033e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1027e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1032e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1032e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1033e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.1035e-05 - val_loss: 5.8459e-06
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 8.2131e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.9167e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.5093e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.6760e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 7.5738e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 7.4850e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.3686e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.2895e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.1916e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.2431e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.3203e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.4177e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.5299e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.6593e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.7531e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.8284e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.8882e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.9431e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.9946e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.0270e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.0534e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.1064e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.1713e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.2517e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.3258e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.4820e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.6412e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.7852e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.9114e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.0185e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.1154e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.2044e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.2875e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3628e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.4262e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.4799e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 9.5308e-06 - val_loss: 6.7750e-06
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 3.9393e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 4.5898e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 5.8740e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 6.7039e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 7.4390e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 8.0481e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.4714e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.6069e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0354e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0807e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1103e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1324e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1499e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1615e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1689e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1711e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1725e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1732e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1744e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1754e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1756e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1763e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1755e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1736e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1726e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1728e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1720e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1727e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1726e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1720e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.1709e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1715e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1717e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1712e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1702e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1691e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 73ms/step - loss: 1.1680e-05 - val_loss: 8.4834e-06
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 1.1901e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 1.0445e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0025e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.5653e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 9.2655e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 8.9502e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 8.6941e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.4813e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.5915e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.7202e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.8562e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 8.9665e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.0131e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.0732e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.1395e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.1849e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.2583e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.3183e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.3517e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.3729e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 9.3807e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.4787e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 9.5571e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 9.6345e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 9.7114e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 9.7804e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 9.8425e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 9.8899e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 9.9500e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.0006e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.0060e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.0108e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0153e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0194e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0226e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0259e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - loss: 1.0290e-05 - val_loss: 9.9534e-06
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 97ms/step - loss: 1.1185e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.0969e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.0742e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 1.0658e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.0543e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.0397e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.0323e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.0261e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.0166e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.0081e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.9845e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.8649e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.7417e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.6110e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 9.4955e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 9.3735e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 9.2781e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 9.1869e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.1277e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.0695e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.0234e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.9726e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.9275e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.8837e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.8549e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.8338e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 8.8378e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 8.8408e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.8960e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.9462e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 8.9879e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.0316e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.0799e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.1249e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.1748e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.2206e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 73ms/step - loss: 9.2639e-06 - val_loss: 5.8507e-06
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 4.6185e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 4.6536e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 5.8424e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 7.2582e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 7.8580e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 8.2902e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.5301e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.6695e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7784e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.9729e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.1376e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.2256e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.2848e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.3802e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.4646e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.5427e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.6231e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.6758e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.7306e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.7812e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.8395e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.9967e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0134e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0246e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0335e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0432e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0509e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0576e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0641e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0696e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0743e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0779e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0806e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0828e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0847e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0865e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 73ms/step - loss: 1.0882e-05 - val_loss: 5.9033e-06
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 2.4619e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 2.0696e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.8688e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.7167e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.6790e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.6395e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.5939e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.5447e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.5050e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4693e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.4391e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4084e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3787e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3538e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.3326e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3304e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3258e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.3191e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3121e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3056e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3001e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2967e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2949e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2922e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2884e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2836e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2791e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2754e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2715e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2678e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2641e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2603e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2565e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2529e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2495e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2460e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 73ms/step - loss: 1.2427e-05 - val_loss: 5.6872e-06
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 4.3173e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 4.3973e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 4.9365e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 6.9682e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 8.1287e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 8.6833e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.9409e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.0610e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.0753e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.0571e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.0616e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.0754e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.3371e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.5226e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.6606e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.7940e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.8954e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.9988e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0078e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0158e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0232e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0291e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0347e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0394e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0464e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0524e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0572e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0608e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0635e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0663e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0689e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0716e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0734e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0756e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0773e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0791e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.0808e-05 - val_loss: 7.1080e-06
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 7.5832e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 6.7430e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 9.1422e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.3274e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.5254e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.6100e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.6392e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.6428e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.6351e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.6244e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.6065e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.5914e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.5750e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.5608e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.5473e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.5328e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.5189e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.5043e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.4883e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.4724e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.4573e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.4448e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.4326e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.4208e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.4100e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3995e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3898e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3803e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3721e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3653e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3584e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3517e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3451e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3385e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3322e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.3259e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.3199e-05 - val_loss: 6.2509e-06
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 5.3777e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 5.1405e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 5.0892e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 5.2416e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 5.3497e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 5.4597e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 5.7172e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 6.8243e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.5962e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.1389e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.4975e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7894e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.0155e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.1726e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.2875e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.3939e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.5006e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.5917e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.6718e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.7290e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.7654e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8089e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8485e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8811e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9038e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9255e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9469e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9648e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9748e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9857e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0006e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0016e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0034e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0060e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0092e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0124e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.0155e-05 - val_loss: 6.7441e-06
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 1.3905e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1814e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.0981e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 1.0555e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.2162e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.3172e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.3804e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4143e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4396e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4550e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4644e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4697e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4695e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4638e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4578e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4573e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4560e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4514e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4480e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4426e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.4367e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.4311e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.4237e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.4150e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.4056e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.3957e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.3857e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.3777e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.3696e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.3618e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 1.3547e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 1.3483e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.3429e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.3375e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.3320e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.3267e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 81ms/step - loss: 1.3217e-05 - val_loss: 6.7333e-06
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 121ms/step - loss: 7.8030e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 7.8468e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 89ms/step - loss: 8.8571e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 9.3696e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 9.8200e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 1.0413e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.0712e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 1.0975e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 89ms/step - loss: 1.1073e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.1586e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1996e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.2279e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.2464e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.2596e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2744e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2853e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.2927e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.2973e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.3014e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.3043e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.3054e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.3053e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.3044e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.3021e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.3001e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2969e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2940e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2907e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2879e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2851e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2820e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2784e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2753e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2719e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2685e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2650e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - loss: 1.2616e-05 - val_loss: 6.6632e-06
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 107ms/step - loss: 4.3928e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 4s 126ms/step - loss: 3.4091e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 114ms/step - loss: 2.9058e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 107ms/step - loss: 2.5802e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 3s 102ms/step - loss: 2.3352e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 98ms/step - loss: 2.1627e-05  7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 2.0445e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 1.9513e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.8737e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.8062e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 91ms/step - loss: 1.7526e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 91ms/step - loss: 1.7027e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.6589e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.6220e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step - loss: 1.5909e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step - loss: 1.5626e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step - loss: 1.5363e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step - loss: 1.5126e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step - loss: 1.4922e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 1.4714e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 1.4519e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 1.4340e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.4168e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.4021e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.3888e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.3751e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.3619e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.3494e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3380e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3269e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3162e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.3103e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.3050e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.3001e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2954e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2905e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - loss: 1.2858e-05 - val_loss: 6.3402e-06</code></pre>
</div>
</div>
<div class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>lstm_loss <span class="op">=</span> pd.DataFrame(history_lstm.history)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC with gate)"</span>] <span class="op">=</span> history_fc_gated.history[<span class="st">"val_loss"</span>]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss.plot()</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss[[<span class="st">"loss"</span>, <span class="st">"val_loss"</span>]].plot()</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-history_lstm" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="30">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_lstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Losses calculated in an LSTM
</figcaption>
<div aria-describedby="fig-history_lstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-history_lstm-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-history_lstm-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) All models so far
</figcaption>
<div aria-describedby="fig-history_lstm-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_lstm-output-1.png" data-ref-parent="fig-history_lstm" width="606" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-history_lstm-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-history_lstm-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) LSTM only
</figcaption>
<div aria-describedby="fig-history_lstm-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_lstm-output-2.png" data-ref-parent="fig-history_lstm" width="589" height="443" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="sec-gates" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="sec-gates"><span class="header-section-number">6</span> Introducing… the gatekeepers</h2>
<p>Introducing gates in a model can bring important advantages. According to <span class="citation" data-cites="lim2021temporal">Lim et al. (<a href="#ref-lim2021temporal" role="doc-biblioref">2021</a>)</span>, GLUs:</p>
<ul>
<li>“… reduce the vanishing gradient problem for deep architectures by providing a linear path for gradients while retaining non-linear capabilities”, and</li>
<li>“… provide flexibility to suppress any parts of the architecture that are not required for a given dataset”.</li>
</ul>
<p>This section formalises the GLU model of <a href="#sec-glu" class="quarto-xref">Section&nbsp;4</a> as if it were a single layer. This serves as a building block for the Gated Residual Network (GRN), a group of layers that learns to dynamically adjust the complexity of a larger neural network.</p>
<p>First, the GLU as a layer is introduced below.</p>
<div id="class-glu" class="cell" data-execution_count="31">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: to add to codebase and replace here with an import</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedLinearUnit(keras.Layer):</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">|</span><span class="va">None</span><span class="op">=</span><span class="va">None</span>, <span class="co"># Dropout rate</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>        use_time_distributed:<span class="bu">bool</span><span class="op">=</span><span class="va">True</span>, <span class="co"># Apply the GLU across all time steps?</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        activation:<span class="bu">str</span><span class="op">|</span>Callable<span class="op">|</span><span class="va">None</span><span class="op">=</span><span class="va">None</span>, <span class="co"># Activation function</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Gated Linear Unit dynamically gates input data"</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_time_distributed <span class="op">=</span> use_time_distributed</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().build(input_shape)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> keras.layers.Dropout(<span class="va">self</span>.dropout_rate) <span class="cf">if</span> <span class="va">self</span>.dropout_rate <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation_layer <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, activation<span class="op">=</span><span class="va">self</span>.activation)</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gate_layer <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, activation<span class="op">=</span><span class="st">'sigmoid'</span>)</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.multiply <span class="op">=</span> keras.layers.Multiply()</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_time_distributed:</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activation_layer <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.activation_layer)</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gate_layer <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.gate_layer)</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>        inputs, </span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""List of outputs with shape: [</span></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a><span class="co">            (batch size, ..., d_model),</span></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a><span class="co">            (batch size, ..., d_model)</span></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a><span class="co">        ]"""</span></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.dropout <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> training:</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> <span class="va">self</span>.dropout(inputs)</span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>        activation_output <span class="op">=</span> <span class="va">self</span>.activation_layer(inputs)</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>        gate_output <span class="op">=</span> <span class="va">self</span>.gate_layer(inputs)</span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.multiply([activation_output, gate_output]), gate_output</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>        config.update({</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>            <span class="st">'d_model'</span>: <span class="va">self</span>.d_model,</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>            <span class="st">'dropout_rate'</span>: <span class="va">self</span>.dropout_rate,</span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>            <span class="st">'use_time_distributed'</span>: <span class="va">self</span>.use_time_distributed,</span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>            <span class="st">'activation'</span>: <span class="va">self</span>.activation</span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The GLU is simply:</p>
<p><span id="eq-glu"><span class="math display">\[
\text{GLU}(x) = \sigma(W_{G} x + b_G) \odot (W_1 x + b_1),
\tag{5}\]</span></span></p>
<p>with <span class="math inline">\(W_G, W_1 \in \mathbb{R}^{|x| \times \lambda}\)</span> and <span class="math inline">\(b_G, b_1 \in \mathbb{R}^{\lambda}\)</span>.</p>
<div class="cell" data-execution_count="32">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),),name<span class="op">=</span><span class="st">"FlattenedLaggedInput"</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>gated_features, gate <span class="op">=</span> GatedLinearUnit(d_model<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, use_time_distributed<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"Gate"</span>)(inputs)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(gated_features)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(output)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>nn_glu <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GLUModel"</span>)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>nn_glu.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>nn_glu.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_glu" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="32">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_glu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Summary of model with GLU
</figcaption>
<div aria-describedby="tbl-summary_glu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "GLUModel"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ FlattenedLaggedInput            │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">262</span>)            │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)                    │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Gate (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedLinearUnit</span>)          │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,    │         <span style="color: #00af00; text-decoration-color: #00af00">8,416</span> │
│                                 │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)]                   │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ SummariseInput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │           <span style="color: #00af00; text-decoration-color: #00af00">272</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ CalculateOutput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,705</span> (34.00 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,705</span> (34.00 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_glu" class="cell" data-execution_count="33">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_glu, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="33">
<div id="fig-arch_glu" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_glu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: GLU model
</figcaption>
<div aria-describedby="fig-arch_glu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_glu-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div id="fit-glu-model" class="cell" data-execution_count="34">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>history_glu <span class="op">=</span> nn_glu.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 43s 1s/step - loss: 1.0486e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 8.2140e-05 - val_loss: 0.0019
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.2122e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 4.5048e-05 - val_loss: 0.0017
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 9.1859e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.9112e-05 - val_loss: 0.0015
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 4.4426e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.1390e-05 - val_loss: 0.0014
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.2877e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6318e-05 - val_loss: 0.0014
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 5.3268e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2318e-05 - val_loss: 0.0013
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.6347e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.5493e-05 - val_loss: 0.0013
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.0470e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3861e-05 - val_loss: 0.0013
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.5617e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3595e-05 - val_loss: 0.0013
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 6.4008e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.9146e-06 - val_loss: 0.0013
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.1945e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.5273e-05 - val_loss: 0.0013
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.4145e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2694e-05 - val_loss: 0.0013
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 1.7502e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0783e-05 - val_loss: 0.0013
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.8218e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.5160e-05 - val_loss: 0.0013
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 1.0853e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1685e-05 - val_loss: 0.0013
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 5.5477e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6051e-05 - val_loss: 0.0013
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.6589e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1646e-05 - val_loss: 0.0013
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.0323e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0379e-05 - val_loss: 0.0013
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 1.6179e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0084e-05 - val_loss: 0.0013
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 9.0336e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0436e-05 - val_loss: 0.0013</code></pre>
</div>
</div>
<div id="cell-fig-history_glu" class="cell" data-execution_count="35">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_glu.history).plot()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_glu" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_glu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Losses calculated in a GLU neural network
</figcaption>
<div aria-describedby="fig-history_glu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_glu-output-1.png" width="623" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>As mentioned above, the GLU itself actually serves as an important component of a slightly larger group of layers, the GRNs. Now is the time to introduce them.</p>
<p>GRNs are formally:</p>
<p><span id="eq-grn"><span class="math display">\[
\text{GRN}(x) = \text{LayerNorm}(x + \text{GLU}(W_2 (\text{ELU}(W_1 x + b1 + W_c c )) + b_2)),
\tag{6}\]</span></span></p>
<p>where <span class="math inline">\(\text{LayerNorm}\)</span> (<span class="citation" data-cites="ba2016layer">Ba, Kiros, and Hinton (<a href="#ref-ba2016layer" role="doc-biblioref">2016</a>)</span>) normalises its inputs, ie subtracts its mean and divides by its standard deviation,<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span class="math inline">\(\text{ELU}\)</span> is the exponential linear unit function (<span class="citation" data-cites="clevert2015fast">Clevert, Unterthiner, and Hochreiter (<a href="#ref-clevert2015fast" role="doc-biblioref">2015</a>)</span>). Unlike ReLUs, ELUs allow for negative values, which pushes unit activations closer to zero at a lower computation complexity, and producing more accurate results.</p>
<p>The final component in <a href="#eq-grn" class="quarto-xref">Equation&nbsp;6</a> is <span class="math inline">\(c \in \mathbb{R}^{\lambda}\)</span>, or a context vector - more on that below in <a href="#sec-timeembed" class="quarto-xref">Section&nbsp;7</a>. For the moment, we can use <span class="math inline">\(c=\mathbf{0}\)</span> the zero vector.</p>
<p>Put simply, the GRN takes in a certain data <span class="math inline">\(x\)</span> and combines it with a non-linear transformation. This non-linear component goes through an GLU, which learns when to gate and when to let through the non-linear transformation of the data.</p>
<p>The GRN helps keep information only from relevant input variables and keeps the model as simple as possible by only applying non-linearities when relevant.</p>
<div id="class-grn" class="cell" data-execution_count="36">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedResidualNetwork(keras.layers.Layer):</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        output_size<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        dropout_rate<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        use_time_distributed<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Gated residual network"</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GatedResidualNetwork, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_size <span class="op">=</span> output_size <span class="cf">if</span> output_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> d_model</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_time_distributed <span class="op">=</span> use_time_distributed</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GatedResidualNetwork, <span class="va">self</span>).build(input_shape)</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.output_size)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dense <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model)</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dense_post <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_activation <span class="op">=</span> keras.layers.Activation(<span class="st">'elu'</span>)</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dense <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, use_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gating_layer <span class="op">=</span> GatedLinearUnit(</span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.output_size, </span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate, </span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">self</span>.use_time_distributed, </span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add <span class="op">=</span> keras.layers.Add()</span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l_norm <span class="op">=</span> keras.layers.LayerNormalization()</span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_time_distributed:</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dense <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.dense)</span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_dense <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.hidden_dense)</span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_dense <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.context_dense)</span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_dense_post <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.hidden_dense_post)</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, additional_context<span class="op">=</span><span class="va">None</span>, training<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Setup skip connection</span></span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a>        skip <span class="op">=</span> <span class="va">self</span>.dense(inputs) <span class="cf">if</span> <span class="va">self</span>.output_size <span class="cf">else</span> inputs</span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st step: eta2</span></span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.hidden_dense(inputs)</span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Context handling</span></span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> additional_context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>            hidden <span class="op">+=</span> <span class="va">self</span>.context_dense(additional_context)</span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.hidden_activation(hidden)</span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.hidden_dense_post(hidden)</span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2nd step: eta1 and 3rd step</span></span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a>        gating_layer, gate <span class="op">=</span> <span class="va">self</span>.gating_layer(hidden)</span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb48-55"><a href="#cb48-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final step</span></span>
<span id="cb48-56"><a href="#cb48-56" aria-hidden="true" tabindex="-1"></a>        GRN <span class="op">=</span> <span class="va">self</span>.add([skip, gating_layer])</span>
<span id="cb48-57"><a href="#cb48-57" aria-hidden="true" tabindex="-1"></a>        GRN <span class="op">=</span> <span class="va">self</span>.l_norm(GRN)</span>
<span id="cb48-58"><a href="#cb48-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-59"><a href="#cb48-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> GRN, gate</span>
<span id="cb48-60"><a href="#cb48-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-61"><a href="#cb48-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb48-62"><a href="#cb48-62" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>(GatedResidualNetwork, <span class="va">self</span>).get_config()</span>
<span id="cb48-63"><a href="#cb48-63" aria-hidden="true" tabindex="-1"></a>        config.update({</span>
<span id="cb48-64"><a href="#cb48-64" aria-hidden="true" tabindex="-1"></a>            <span class="st">'d_model'</span>: <span class="va">self</span>.d_model,</span>
<span id="cb48-65"><a href="#cb48-65" aria-hidden="true" tabindex="-1"></a>            <span class="st">'output_size'</span>: <span class="va">self</span>.output_size,</span>
<span id="cb48-66"><a href="#cb48-66" aria-hidden="true" tabindex="-1"></a>            <span class="st">'dropout_rate'</span>: <span class="va">self</span>.dropout_rate,</span>
<span id="cb48-67"><a href="#cb48-67" aria-hidden="true" tabindex="-1"></a>            <span class="st">'use_time_distributed'</span>: <span class="va">self</span>.use_time_distributed</span>
<span id="cb48-68"><a href="#cb48-68" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb48-69"><a href="#cb48-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="37">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),),name<span class="op">=</span><span class="st">"FlattenedLaggedInput"</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># inputs = keras.layers.Dense(units=dim, activation="relu", name="SummariseInput")(inputs)</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>gated_features, gate <span class="op">=</span> GatedResidualNetwork(d_model<span class="op">=</span><span class="dv">262</span>, use_time_distributed<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"GRN"</span>)(inputs)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(gated_features)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(output)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>nn_grn <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GRNModel"</span>)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>nn_grn.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>nn_grn.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_grn" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="37">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_grn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Summary of GRN
</figcaption>
<div aria-describedby="tbl-summary_grn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "GRNModel"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ FlattenedLaggedInput            │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">262</span>)            │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)                    │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ GRN (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedResidualNetwork</span>)      │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">262</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,   │       <span style="color: #00af00; text-decoration-color: #00af00">345,054</span> │
│                                 │ <span style="color: #00af00; text-decoration-color: #00af00">262</span>)]                  │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ SummariseInput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ CalculateOutput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">349,279</span> (1.33 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">349,279</span> (1.33 MB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_grn" class="cell" data-execution_count="38">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_grn, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="38">
<div id="fig-arch_grn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_grn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: GRN model architecture
</figcaption>
<div aria-describedby="fig-arch_grn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_grn-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div id="fitting-grn-model" class="cell" data-execution_count="39">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>history_grn <span class="op">=</span> nn_grn.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1:19 2s/step - loss: 0.176919/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.1113 36/36 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - loss: 0.0746 - val_loss: 0.1362
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 2.0870e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0208     36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0192 - val_loss: 0.0506
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.4108e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.7747e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.7243e-05 - val_loss: 0.0504
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.0944e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1807e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.1678e-05 - val_loss: 0.0499
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 4.7853e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.1667e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0218e-05 - val_loss: 0.0498
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.6164e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.5259e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4497e-05 - val_loss: 0.0496
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 9.6013e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.2271e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.2301e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.2276e-05 - val_loss: 0.0494
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 5.9587e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6199e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.4702e-05 - val_loss: 0.0496
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 6.0368e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6.8754e-06 32/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8.0655e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 8.4860e-06 - val_loss: 0.0496
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.4589e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.7556e-05 29/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.5166e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.4362e-05 - val_loss: 0.0495
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 9.2581e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.1530e-05 29/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0885e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 1.0990e-05 - val_loss: 0.0495
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 8.4020e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 9.9844e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0574e-05 - val_loss: 0.0495
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 8.7819e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0671e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0807e-05 - val_loss: 0.0498
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 6.6112e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0285e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0255e-05 - val_loss: 0.0496
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 4.0362e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8.5091e-06 34/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0056e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0128e-05 - val_loss: 0.0496
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.2086e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.2385e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.1777e-05 - val_loss: 0.0495
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 5.2377e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 7.8229e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8.9874e-06 - val_loss: 0.0496
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 8.1264e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0770e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 1.0807e-05 - val_loss: 0.0494
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.0260e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0626e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0382e-05 - val_loss: 0.0496
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 5.6796e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 8.2929e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 9.6198e-06 - val_loss: 0.0495</code></pre>
</div>
</div>
<div id="cell-fig-history_grn" class="cell" data-execution_count="40">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_grn.history).plot()</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_grn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_grn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Losses calculated in a GRN neural network
</figcaption>
<div aria-describedby="fig-history_grn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_grn-output-1.png" width="597" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-timeembed" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="sec-timeembed"><span class="header-section-number">7</span> Time to talk about time</h2>
<p>Nowcasting, or forecasting for that matter, involves trying to estimate the values of a variable of interest for a certain date. This trivial fact actually offers an important opportunity to improve estimates: it corresponds to something that is <em>known</em> about the future, ie about the variable we want to estimate.</p>
<p>But temporal features are not continuous variables as inflation and oil price changes. Because they are categorical, they need to be embedded in a vector space with real data to incorporate continuous variables. This is the goal of this section.</p>
<div id="getting-temporal-features-data" class="cell" data-execution_count="41">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>timefeat <span class="op">=</span> [<span class="st">"month_of_quarter"</span>, <span class="st">"month_of_year"</span>]</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>y_timefeat_train <span class="op">=</span> {timef:</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    np.array([</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        get_timefeat(pd.DataFrame(i.values, index<span class="op">=</span>[i.name], columns<span class="op">=</span>i.index), features<span class="op">=</span>timef)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> y_train_split_fc[fold][<span class="st">"train"</span>]</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> timef <span class="kw">in</span> timefeat</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>y_timefeat_valid <span class="op">=</span> {timef:</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    np.array([</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>        get_timefeat(pd.DataFrame(i.values, index<span class="op">=</span>[i.name], columns<span class="op">=</span>i.index), features<span class="op">=</span>timef)</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> y_train_split_fc[fold][<span class="st">"valid"</span>]</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> timef <span class="kw">in</span> timefeat</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>date_range <span class="op">=</span> pd.date_range(start<span class="op">=</span><span class="st">"2024-01-01"</span>, end<span class="op">=</span><span class="st">"2024-12-31"</span>, freq<span class="op">=</span><span class="st">"D"</span>)</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>time_feats <span class="op">=</span> get_timefeat(pd.DataFrame(index<span class="op">=</span>date_range), features<span class="op">=</span>timefeat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>When working with categorical data, it is necessary to be explicit about the number of possible different values (eg, there are 7 different days of the week).</p>
<div id="set-vocab-sizes" class="cell" data-execution_count="42">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>vocab_sizes <span class="op">=</span> {col: time_feats[col].nunique() <span class="cf">for</span> col <span class="kw">in</span> time_feats}</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_sizes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>{'month_of_quarter': 3, 'month_of_year': 12}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="43">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span>k, dtype<span class="op">=</span><span class="st">'int32'</span>)  <span class="co"># Input layer for each feature with shape (1,)</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>embedded_layers <span class="op">=</span> {</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Embedding(input_dim<span class="op">=</span>v, output_dim<span class="op">=</span>dim, name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>)(inputs[k])</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> vocab_sizes.items()</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>combined <span class="op">=</span> keras.layers.Average()(<span class="bu">list</span>(embedded_layers.values()))</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(dim, activation<span class="op">=</span><span class="st">"relu"</span>)(combined)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>nn_embed <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">list</span>(inputs.values()), outputs<span class="op">=</span>output)</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>nn_embed.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>nn_embed.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_embed" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="43">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: Summary of model with embedding
</figcaption>
<div aria-describedby="tbl-summary_embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ month_of_quarter    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ month_of_year       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ month_of_quarter_e… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │         <span style="color: #00af00; text-decoration-color: #00af00">48</span> │ month_of_quarter… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)         │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ month_of_year_embe… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">192</span> │ month_of_year[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)         │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ average (<span style="color: #0087ff; text-decoration-color: #0087ff">Average</span>)   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ month_of_quarter… │
│                     │                   │            │ month_of_year_em… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">272</span> │ average[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]     │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)      │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">529</span> (2.07 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">529</span> (2.07 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_embed" class="cell" data-fig-label="Architecture of model with embeddings" data-execution_count="44">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_embed, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="44">
<div id="fig-arch_embed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-arch_embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16
</figcaption>
<div aria-describedby="fig-arch_embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_embed-output-1.png" id="fig-arch_embed" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>history_embed <span class="op">=</span> nn_embed.fit(x<span class="op">=</span>y_timefeat_train, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(y_timefeat_valid, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-embed_loss" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="45">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-embed_loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17
</figcaption>
<div aria-describedby="fig-embed_loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1:08 2s/step - loss: 2.7112e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - loss: 6.2605e-05 - val_loss: 5.6296e-06
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 7.1494e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0214e-05 - val_loss: 1.2665e-05
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.4794e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1552e-05 - val_loss: 1.0500e-05
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.0853e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3028e-05 - val_loss: 8.4477e-06
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.2553e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.9488e-06 - val_loss: 7.2497e-06
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.8349e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1461e-05 - val_loss: 5.1284e-06
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 7.4379e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0753e-05 - val_loss: 5.5523e-06
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 3.3358e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2556e-05 - val_loss: 5.9000e-06
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 5.6578e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 8.8021e-06 - val_loss: 6.9396e-06
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 6.0728e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0122e-05 - val_loss: 5.0435e-06
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 1.1353e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0188e-05 - val_loss: 8.2152e-06
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: 1.6991e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.5082e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 9.6528e-06 - val_loss: 7.1386e-06
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.0802e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3313e-05 - val_loss: 7.1375e-06
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 8.1172e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1238e-05 - val_loss: 6.1384e-06
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 8.6668e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0842e-05 - val_loss: 1.6113e-05
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.0916e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.2596e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 9.4685e-06 - val_loss: 8.2630e-06
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 5.6534e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 8.7455e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 8.8723e-06 - val_loss: 1.1728e-05
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: 1.6771e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 8.9659e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 9.6327e-06 - val_loss: 6.4541e-06
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 34ms/step - loss: 1.0060e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0633e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0785e-05 - val_loss: 4.8855e-06
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.0497e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0476e-05 - val_loss: 6.4086e-06</code></pre>
</div>
</div>
</figure>
</div>
<p>As before, the losses are plotted below.</p>
<div id="cell-fig-history_embed" class="cell" data-execution_count="46">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_embed.history).plot()</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_embed" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Losses calculated in a simple, fully-connected neural network.
</figcaption>
<div aria-describedby="fig-history_embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_embed-output-1.png" width="589" height="443" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>The embeddings themselves can be inspected, as in <a href="#fig-embed" class="quarto-xref">Figure&nbsp;19</a>.</p>
<div class="cell" data-execution_count="47">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(nn_embed.layers[<span class="dv">2</span>].get_weights()[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>)  <span class="co"># Optional: Add colorbar to show scale</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Embedding vector elements'</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Month of the quarter'</span>)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>plt.imshow(nn_embed.layers[<span class="dv">3</span>].get_weights()[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>)  <span class="co"># Optional: Add colorbar to show scale</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Embedding vector elements'</span>)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Month of the year'</span>)</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-embed" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="47">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Embedding values
</figcaption>
<div aria-describedby="fig-embed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-embed-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-embed-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Month of the year
</figcaption>
<div aria-describedby="fig-embed-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-embed-output-1.png" data-ref-parent="fig-embed" width="676" height="508" class="figure-img">
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-embed-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-embed-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Quarter of the year
</figcaption>
<div aria-describedby="fig-embed-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-embed-output-2.png" data-ref-parent="fig-embed" width="660" height="503" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<p>To get an intuition how this type of data feeds into a nowcasting model, consider that each nowcasted date has a different combination of month of year and month of quarter. Then the inputs to the model change correspondingly. The time series is plotted below.</p>
<div id="ts_embed-calculations" class="cell" data-execution_count="48">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>y_time_feats <span class="op">=</span> get_timefeat(pd.DataFrame(index<span class="op">=</span>y_train.index), features<span class="op">=</span>timefeat)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>embed_layers <span class="op">=</span> [nn_embed.get_layer(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>) <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()]</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>embed_values <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[e.<span class="bu">input</span> <span class="cf">for</span> e <span class="kw">in</span> embed_layers], outputs<span class="op">=</span>[e.output <span class="cf">for</span> e <span class="kw">in</span> embed_layers])</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>embed_ts <span class="op">=</span> embed_values.predict(<span class="bu">dict</span>(y_time_feats))</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>embed_ts <span class="op">=</span> [</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    np.squeeze(i, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> embed_ts</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> 1/14 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step14/14 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step 14/14 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step</code></pre>
</div>
</div>
<div class="cell" data-execution_count="49">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>window_months <span class="op">=</span> <span class="dv">36</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(embed_ts[<span class="dv">0</span>][<span class="op">-</span>window_months:].T, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>) </span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Months"</span>)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding elements"</span>)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>plt.imshow(embed_ts[<span class="dv">1</span>][<span class="op">-</span>window_months:].T, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>) </span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Months"</span>)</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding elements"</span>)</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>avg_embed <span class="op">=</span> np.mean(np.stack(embed_ts, axis<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>plt.imshow(avg_embed[<span class="op">-</span>window_months:].T, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>) </span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Months"</span>)</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding elements"</span>)</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-embedts" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="49">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-embedts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Embedding values (last 24 months)
</figcaption>
<div aria-describedby="fig-embedts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-embedts-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-embedts-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Month of quarter
</figcaption>
<div aria-describedby="fig-embedts-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-embedts-output-1.png" data-ref-parent="fig-embedts" width="787" height="503" class="figure-img">
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-embedts-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-embedts-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Month of year
</figcaption>
<div aria-describedby="fig-embedts-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-embedts-output-2.png" data-ref-parent="fig-embedts" width="787" height="503" class="figure-img">
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-embedts-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-embedts-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Average of month of quarter and year embeddings
</figcaption>
<div aria-describedby="fig-embedts-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-embedts-output-3.png" data-ref-parent="fig-embedts" width="787" height="503" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
<p>One way these categorical variables can enrich models with continues variables that were used before is by using the embeddings for a particular instance as additional context, as the <span class="math inline">\(c\)</span> in <a href="#eq-grn" class="quarto-xref">Equation&nbsp;6</a>.</p>
<div class="cell" data-execution_count="50">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>inputs_cont <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),), name<span class="op">=</span><span class="st">"FlattenedLaggedInput"</span>)</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>continuous <span class="op">=</span> keras.layers.Dense(dim, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs_cont)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>inputs_time <span class="op">=</span> {</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span>k, dtype<span class="op">=</span><span class="st">'int32'</span>)  <span class="co"># Input layer for each feature with shape (1,)</span></span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Create embedding layers for each input</span></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>embedded_layers <span class="op">=</span> {</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Embedding(input_dim<span class="op">=</span>v, output_dim<span class="op">=</span>dim, name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>)(inputs_time[k])</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> vocab_sizes.items()</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> keras.layers.Average()(<span class="bu">list</span>(embedded_layers.values()))</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>gated_features, gate <span class="op">=</span> GatedResidualNetwork(d_model<span class="op">=</span>dim, use_time_distributed<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"GRN"</span>)(continuous, additional_context<span class="op">=</span>context)</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(gated_features)</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(output)</span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a>nn_grn_context <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[inputs_cont, inputs_time], outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GRNwithContextModel"</span>)</span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>nn_grn_context.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a>nn_grn_context.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_grn_context" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="50">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_grn_context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: GRN with Embeddings Summary
</figcaption>
<div aria-describedby="tbl-summary_grn_context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "GRNwithContextModel"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ month_of_quarter    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ month_of_year       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ FlattenedLaggedInp… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">262</span>)       │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ month_of_quarter_e… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │         <span style="color: #00af00; text-decoration-color: #00af00">48</span> │ month_of_quarter… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)         │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ month_of_year_embe… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">192</span> │ month_of_year[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)         │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │ FlattenedLaggedI… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ average (<span style="color: #0087ff; text-decoration-color: #0087ff">Average</span>)   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)     │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ month_of_quarter… │
│                     │                   │            │ month_of_year_em… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ GRN                 │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │      <span style="color: #00af00; text-decoration-color: #00af00">1,648</span> │ dense[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],      │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedResidualNetw…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │ average[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]     │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)]              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ SummariseInput      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │        <span style="color: #00af00; text-decoration-color: #00af00">272</span> │ GRN[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ CalculateOutput     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ SummariseInput[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">6,385</span> (24.94 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">6,385</span> (24.94 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_grn_context" class="cell" data-execution_count="51">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_grn_context, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="51">
<div id="fig-arch_grn_context" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_grn_context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Architecture of the GRN model with context
</figcaption>
<div aria-describedby="fig-arch_grn_context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_grn_context-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div id="fitting-grn-model-with-context" class="cell" data-execution_count="52">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>history_grn_context <span class="op">=</span> nn_grn_context.fit(x<span class="op">=</span>[X_train_fc, y_timefeat_train], y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>([X_valid_fc, y_timefeat_valid], y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 2:25 4s/step - loss: 0.138014/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0900 36/36 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - loss: 0.0588 - val_loss: 0.0592
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 0.003726/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0042 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0041 - val_loss: 0.0404
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.002329/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0020 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0019 - val_loss: 0.0375
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 0.002130/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0013 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0012 - val_loss: 0.0334
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 4.8342e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 7.0189e-04 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 6.9918e-04 - val_loss: 0.0267
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.7950e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 4.2645e-04 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 4.2492e-04 - val_loss: 0.0276
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 4.7666e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.4426e-04 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 3.4940e-04 - val_loss: 0.0239
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 35ms/step - loss: 2.3797e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.8963e-04 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 3.2866e-04 - val_loss: 0.0228
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: 1.9501e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.9654e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 3.0603e-05 - val_loss: 0.0214
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 7.8674e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6049e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.6758e-05 - val_loss: 0.0199
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 7.7074e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 5.8906e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 5.3962e-05 - val_loss: 0.0198
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 1.5481e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3190e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3062e-05 - val_loss: 0.0196
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: 1.3366e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0952e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.1227e-05 - val_loss: 0.0195
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.2961e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4034e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3563e-05 - val_loss: 0.0193
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.2262e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4918e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.4271e-05 - val_loss: 0.0193
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 6.0890e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3322e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3005e-05 - val_loss: 0.0192
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step - loss: 5.6528e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.8115e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.0428e-05 - val_loss: 0.0192
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 9.8520e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 9.4015e-06 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 9.5576e-06 - val_loss: 0.0192
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step - loss: 7.2337e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1338e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.1262e-05 - val_loss: 0.0191
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 32ms/step - loss: 1.4278e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4319e-05 36/36 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 1.3698e-05 - val_loss: 0.0191</code></pre>
</div>
</div>
<div id="cell-fig-history_grn_context" class="cell" data-execution_count="53">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_grn_context.history).plot()</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_grn_context" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_grn_context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Losses calculated in a GRN neural network with context
</figcaption>
<div aria-describedby="fig-history_grn_context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_grn_context-output-1.png" width="597" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-encodcont" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="sec-encodcont"><span class="header-section-number">8</span> Encoding continuous variables</h2>
<p>The intuition on embeddings built from <a href="#sec-timeembed" class="quarto-xref">Section&nbsp;7</a> can also serve now to think about embedding, or encoding, continuous variables.</p>
<p>In contrast to categorical variables like temporal features, or entity identity (ie, which country, or bank, or stock, etc), the possible values are not defined a priori. So there is no way to create a lookup table that will map a given category, say, the month of “September” to a specific vector.</p>
<p>However, continuous variables can still be mapped into a vector space defined in <span class="math inline">\(\mathbb{R}\)</span>, in a way that encodes useful information. The simplest way to achieve this is by training parameters of a linear transformation of each data point of variable <span class="math inline">\(j\)</span> in time <span class="math inline">\(t\)</span> into <span class="math inline">\(\xi_t^{(j)} \in \mathbb{R}^{\lambda}\)</span>.</p>
<div class="cell" data-execution_count="54">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>nn_encodcont <span class="op">=</span> keras.Model(</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"EncodedContinuousVarsNetwork"</span></span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>nn_encodcont.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>nn_encodcont.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_encodcont" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="54">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_encodcont-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: Summary of network embedding continuous variable_weights
</figcaption>
<div aria-describedby="tbl-summary_encodcont-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "EncodedContinuousVarsNetwork"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ m (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ d (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__m         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__d         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal_1         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>)   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ masking[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],    │
│                     │                   │            │ any[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ masking_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],  │
│                     │                   │            │ any_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Average</span>)           │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_9 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">272</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_10 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense_9[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]     │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,577</span> (17.88 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,577</span> (17.88 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_encodcont" class="cell" data-execution_count="55">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_encodcont, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="55">
<div id="fig-arch_encodcont" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_encodcont-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Architecture of the network with encoded continuous variables before the LSTM layers
</figcaption>
<div aria-describedby="fig-arch_encodcont-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_encodcont-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>Compare this figure with <a href="#fig-arch_lstm" class="quarto-xref">Figure&nbsp;10</a>. Note that while before, each LSTM received as input the original variable, now in <a href="#fig-arch_encodcont" class="quarto-xref">Figure&nbsp;23</a> the inputs to the LSTMs are actually <span class="math inline">\(\lambda=16\)</span> latent variables for each raw data variable we included.</p>
<p>The aim of this operation is to allow the model to learn richer nuance from the data in addition to the data point itself.</p>
<div id="train-network-with-encoded-continuous-variables" class="cell" data-execution_count="56">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>history_encodcont <span class="op">=</span> nn_encodcont.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 11:20 19s/step - loss: 2.9953e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.6722e-05   3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 2.5650e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.4488e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.3288e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 2.2252e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 2.1372e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 2.0545e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.9973e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.9423e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.8986e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.8636e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.8318e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.7989e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.7676e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.7397e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.7146e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6910e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6677e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6475e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6293e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6108e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.5938e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.5767e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.5600e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.5434e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.5283e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.5160e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.5044e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4987e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4934e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4878e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4821e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4760e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4700e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.4638e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 28s 233ms/step - loss: 1.4580e-05 - val_loss: 1.4199e-05
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 112ms/step - loss: 5.1336e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 8.2996e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 8.2302e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 8.7909e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 9.1887e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 9.5802e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 9.9012e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 1.0335e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.0567e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.0693e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 1.0751e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.0746e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.0707e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.0690e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step - loss: 1.0647e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step - loss: 1.0577e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.0496e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.0427e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.0372e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.0325e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.0277e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.0247e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 1.0229e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 1.0215e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 1.0193e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 95ms/step - loss: 1.0199e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 95ms/step - loss: 1.0205e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0204e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0250e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0299e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0338e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0375e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0407e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0434e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0460e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0484e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 4s 103ms/step - loss: 1.0506e-05 - val_loss: 1.3922e-05
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 122ms/step - loss: 7.0251e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 8.8615e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 9.3692e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 91ms/step - loss: 9.3439e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 9.6174e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 9.6052e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 3s 105ms/step - loss: 9.5335e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - loss: 9.4554e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - loss: 9.5549e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 102ms/step - loss: 9.6119e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 101ms/step - loss: 9.7001e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 2s 100ms/step - loss: 9.7216e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 2s 99ms/step - loss: 9.7064e-06 14/36 ━━━━━━━━━━━━━━━━━━━━ 2s 98ms/step - loss: 9.7021e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 2s 97ms/step - loss: 9.6787e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 2s 100ms/step - loss: 9.6373e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 100ms/step - loss: 9.5968e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 99ms/step - loss: 9.5741e-06 19/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 9.6754e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 9.7542e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 9.8249e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 9.8881e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 97ms/step - loss: 9.9362e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 1s 97ms/step - loss: 1.0003e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 1.0060e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 1s 102ms/step - loss: 1.0112e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - loss: 1.0147e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - loss: 1.0194e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - loss: 1.0234e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - loss: 1.0279e-05 31/36 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - loss: 1.0316e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 1.0349e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - loss: 1.0374e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - loss: 1.0398e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0416e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.0434e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 4s 101ms/step - loss: 1.0452e-05 - val_loss: 1.0781e-05
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 104ms/step - loss: 4.8836e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 4.0925e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 3.5941e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 3.2322e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 2.9714e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 2.7648e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.6019e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.4733e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.3666e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 2.2699e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 2.1817e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 2.1057e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 2.0360e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.9820e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.9323e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.8870e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.8464e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.8101e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.7757e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.7435e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.7156e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.6910e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.6671e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.6442e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.6226e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.6039e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5865e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5714e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5567e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5425e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5282e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5155e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.5028e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.4904e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.4786e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.4678e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - loss: 1.4576e-05 - val_loss: 1.3445e-05
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 6.0762e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 5.7113e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.4436e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 6.6377e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 6.6961e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 6.8250e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 7.1217e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 7.4106e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 7.6700e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 7.9758e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 8.2315e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 8.4914e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 8.6890e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 8.8279e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 8.9380e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.0200e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.0964e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.1389e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.1747e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.2055e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.2181e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.2188e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.2221e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.2194e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.2103e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.1970e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.1777e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.2208e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.2645e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.3000e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.3298e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.3537e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.3932e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 9.4331e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.4792e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.5197e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 9.5580e-06 - val_loss: 1.0935e-05
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 9.5474e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0270e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.0304e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0167e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0555e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0609e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0594e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.0571e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.0462e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0332e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0257e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0199e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0117e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0060e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.9995e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.9524e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.9025e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.8679e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.8622e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.8624e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 9.8764e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 9.9036e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 9.9379e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 9.9682e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 9.9866e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0003e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0021e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0036e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0044e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0043e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0047e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0103e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0156e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0201e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0243e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0278e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 81ms/step - loss: 1.0311e-05 - val_loss: 1.1473e-05
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 1.1131e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 9.8001e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 9.2675e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 8.9013e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7666e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7370e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7903e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7507e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7622e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.7285e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.6813e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.6233e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.6313e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.7430e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.8074e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.8492e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.8824e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.9177e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.9391e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.9792e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.0035e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.0321e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.0522e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.0782e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.1066e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.1366e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.1621e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.1926e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2158e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2317e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2523e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2723e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2880e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.3416e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.3931e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.4419e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 9.4881e-06 - val_loss: 1.2896e-05
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 101ms/step - loss: 1.0039e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.5021e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.5279e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.4936e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.4524e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.4411e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.5339e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.5850e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.6131e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.6215e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6210e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.6118e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.5994e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5847e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5684e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5551e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5433e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5305e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5184e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.5068e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.4946e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.4835e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.4724e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4618e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4522e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4427e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4327e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4232e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4136e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.4042e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.3959e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.3883e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.3803e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.3721e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.3639e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.3572e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - loss: 1.3508e-05 - val_loss: 1.3259e-05
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 100ms/step - loss: 2.9294e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 4.6767e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 7.0433e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.3177e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.0269e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.3086e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.5093e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0304e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0890e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1285e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1554e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1727e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1865e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1953e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2019e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2049e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2128e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2175e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2209e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2221e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2221e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.2247e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2266e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2296e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2322e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2333e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2328e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2321e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2320e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2322e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2325e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.2332e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.2331e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.2328e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.2321e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.2309e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - loss: 1.2298e-05 - val_loss: 1.1898e-05
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 106ms/step - loss: 6.0850e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 5.9074e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 6.7359e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 7.8102e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 8.1974e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 8.4813e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 8.8667e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 9.0393e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 9.2256e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 9.3746e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.4329e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.4353e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.4094e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.3693e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.3170e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.2664e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.2196e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.1879e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.1558e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.1323e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.1068e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.0992e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.0996e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.1051e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.1056e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.1699e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.2326e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.2920e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.3453e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.3890e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.4436e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.4950e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.5404e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.5964e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.6455e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.6887e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 83ms/step - loss: 9.7295e-06 - val_loss: 1.0891e-05
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 113ms/step - loss: 1.4608e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 4s 136ms/step - loss: 1.6077e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 115ms/step - loss: 1.6359e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 111ms/step - loss: 1.5831e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 3s 109ms/step - loss: 1.5381e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 3s 107ms/step - loss: 1.6114e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 3s 106ms/step - loss: 1.6425e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 105ms/step - loss: 1.6504e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - loss: 1.6577e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - loss: 1.6535e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 104ms/step - loss: 1.6424e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - loss: 1.6334e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 2s 103ms/step - loss: 1.6237e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 2s 108ms/step - loss: 1.6096e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 2s 107ms/step - loss: 1.5951e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 2s 107ms/step - loss: 1.5784e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 2s 106ms/step - loss: 1.5605e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 105ms/step - loss: 1.5453e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 105ms/step - loss: 1.5343e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 105ms/step - loss: 1.5227e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 105ms/step - loss: 1.5100e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 105ms/step - loss: 1.4988e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 104ms/step - loss: 1.4898e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 104ms/step - loss: 1.4801e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 1s 104ms/step - loss: 1.4707e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 1s 103ms/step - loss: 1.4637e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 103ms/step - loss: 1.4568e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 103ms/step - loss: 1.4504e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - loss: 1.4439e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - loss: 1.4371e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - loss: 1.4301e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 102ms/step - loss: 1.4228e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - loss: 1.4150e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - loss: 1.4069e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 101ms/step - loss: 1.3991e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 100ms/step - loss: 1.3914e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 4s 106ms/step - loss: 1.3840e-05 - val_loss: 1.3489e-05
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 110ms/step - loss: 8.9460e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 8.3079e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 8.3615e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 9.0198e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 9.4294e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 9.5998e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 9.6779e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 9.6834e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 9.6273e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 9.5485e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 9.5259e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 9.4776e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 9.3970e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 9.3987e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 9.5674e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 9.6771e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.7538e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.8007e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.8289e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 9.8344e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.8358e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.8603e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.8887e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 9.9148e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 9.9618e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0008e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0062e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0115e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0166e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0204e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0244e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0281e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0310e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0332e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0350e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0369e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 1.0388e-05 - val_loss: 1.1235e-05
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 105ms/step - loss: 4.3880e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 5.9182e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 6.6299e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 7.0854e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 7.3078e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 7.3842e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 7.6589e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 7.7736e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 7.8033e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 7.8091e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 7.8971e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.0684e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.2399e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.3761e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.5102e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.6098e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.6767e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.7259e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.8021e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.8565e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.9017e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 8.9879e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 9.0706e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 9.1534e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 9.2988e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.4302e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.5481e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.6475e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.7315e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 9.8104e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 9.8862e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 9.9486e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0004e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0050e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0086e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0117e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.0146e-05 - val_loss: 1.1297e-05
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 122ms/step - loss: 9.5057e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 95ms/step - loss: 1.3650e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 1.4059e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 89ms/step - loss: 1.3737e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 1.3300e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 1.3324e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.3497e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.3605e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - loss: 1.3636e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - loss: 1.3569e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - loss: 1.3476e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.3387e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.3553e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.3689e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.3766e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.3816e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.3844e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3857e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3841e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3809e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3761e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3704e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.3636e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3570e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3500e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3432e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3365e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3302e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3245e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3190e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3136e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3084e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.3036e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.2985e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.2932e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.2885e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - loss: 1.2841e-05 - val_loss: 1.2398e-05
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 99ms/step - loss: 3.5678e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 4.6677e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 7.2234e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.2281e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7421e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.8749e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.2645e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.6374e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.9551e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0188e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0390e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0542e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0669e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0786e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0862e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0936e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0982e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1032e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1069e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1092e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1103e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1101e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.1091e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.1072e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.1062e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.1046e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.1023e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0993e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0960e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0929e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0898e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0871e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0856e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0840e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0856e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0873e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 1.0889e-05 - val_loss: 1.1661e-05
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 101ms/step - loss: 1.5662e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.2511e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.1748e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0970e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0505e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0127e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0130e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0195e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0233e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0201e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0185e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0130e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0085e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0014e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9483e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9505e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9649e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9724e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9695e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9562e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9372e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.9336e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.9527e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.9968e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0031e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0054e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0076e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0097e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0108e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0173e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0232e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0281e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0321e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0351e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0376e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0398e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 1.0419e-05 - val_loss: 1.1948e-05
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 97ms/step - loss: 7.9748e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.8674e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.2301e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 9.1938e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 9.1980e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 9.0494e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.9101e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.7208e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.6183e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.5502e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.5221e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.5287e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.5302e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.6358e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.7267e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.8070e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.8502e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.0052e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.1761e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.3116e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.4297e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 9.5496e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.6550e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.7427e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.8190e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.8891e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.9694e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0045e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0108e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0160e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0213e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0261e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0301e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0330e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0355e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0373e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 1.0391e-05 - val_loss: 1.1711e-05
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 100ms/step - loss: 6.8725e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 9.6600e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 1.1524e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.2202e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.2329e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.2136e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.1832e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.1676e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.1719e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1695e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1587e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.1460e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1358e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1281e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1222e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1163e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1093e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.1029e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0988e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0947e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0905e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0870e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0827e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0782e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0741e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0722e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0730e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0728e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0717e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0756e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0785e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0808e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0827e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0846e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0858e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0870e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 81ms/step - loss: 1.0880e-05 - val_loss: 1.5027e-05
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 97ms/step - loss: 4.2639e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 5.8408e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 5.9026e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 5.9141e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.4398e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.7281e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.9624e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 7.0848e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 7.1119e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 7.2425e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 7.4996e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 7.7209e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 7.8685e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 8.0127e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 8.1311e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.2200e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.2962e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.3513e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.4190e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.4726e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 8.5264e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 8.5757e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 8.6359e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 8.6939e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 8.7733e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 8.8356e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 8.9529e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.0547e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.1398e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2143e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.2820e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.3492e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.4069e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.4559e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.5078e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.5594e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 9.6082e-06 - val_loss: 1.1609e-05
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 101ms/step - loss: 1.1049e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.2146e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.1239e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.0493e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.0065e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 9.8247e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 9.5918e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 9.4030e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 9.3133e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 9.2974e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.2971e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.3193e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.3900e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.4459e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.5148e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.5772e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.6284e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.6637e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.7215e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.8169e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.8902e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.9454e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 9.9936e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0025e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0058e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0075e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0150e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0212e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0261e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0305e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0357e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0399e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0432e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0460e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0488e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0510e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 82ms/step - loss: 1.0531e-05 - val_loss: 1.0878e-05</code></pre>
</div>
</div>
<p>Now that the network is trained, we can inspect how these embeddings of continuous variables work. Compare <a href="#fig-encodcontts" class="quarto-xref">Figure&nbsp;24</a> below with <a href="#fig-embedts-3" class="quarto-xref">Figure&nbsp;20 (c)</a>.</p>
<div class="cell" data-execution_count="57">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>encodcont_ts_m <span class="op">=</span> nn_encodcont.get_layer(<span class="st">"encoding__m"</span>)(np.expand_dims(lstm_X_train[<span class="st">"m"</span>][:,<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>], axis<span class="op">=</span>(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>plt.plot(np.squeeze(encodcont_ts_m))</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>encodcont_ts_m <span class="op">=</span> nn_encodcont.get_layer(<span class="st">"encoding__d"</span>)(np.expand_dims(lstm_X_train[<span class="st">"d"</span>][:,<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>], axis<span class="op">=</span>(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>plt.plot(np.squeeze(encodcont_ts_m))</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-encodcontts" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="57">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-encodcontts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Time series of continuous embedding
</figcaption>
<div aria-describedby="fig-encodcontts-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-encodcontts-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-encodcontts-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Monthly data
</figcaption>
<div aria-describedby="fig-encodcontts-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-encodcontts-output-1.png" data-ref-parent="fig-encodcontts" width="599" height="411" class="figure-img">
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-encodcontts-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-encodcontts-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Daily data
</figcaption>
<div aria-describedby="fig-encodcontts-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-encodcontts-output-2.png" data-ref-parent="fig-encodcontts" width="590" height="411" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="sec-varsel" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="sec-varsel"><span class="header-section-number">9</span> Variable selection networks</h2>
<p>Now is a good time to put these elements together in a more directly useful way. In particular, we can consider how it can help select the most informative variables amongst a set of input series (eg, lagged inflation and oil prices in the current toy example.)</p>
<p>The Variable Selection Networks (VSN) are components of the TFT that are built with GRNs, taking in each covariate and outputting a weighted average of the covariates, where the weights are learned by the model.</p>
<p>VSNs can be used to select amongst continuous, categorical or even static variables. The code below focuses on the continuous variables of the current toy model, <span class="math inline">\(\pi_m\)</span> and <span class="math inline">\(o_d\)</span>.</p>
<div id="class-svs" class="cell" data-execution_count="58">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> StaticVariableSelection(keras.Layer):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>, </span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Static variable selection network"</span></span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(StaticVariableSelection, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define GRNs for the transformed embeddings</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grns_transformed_embeddings <span class="op">=</span> []  <span class="co"># This will be a list of GRN layers</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flat <span class="op">=</span> keras.layers.Flatten()</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.softmax <span class="op">=</span> keras.layers.Activation(<span class="st">'softmax'</span>)</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mult <span class="op">=</span> keras.layers.Multiply()</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(StaticVariableSelection, <span class="va">self</span>).build(input_shape)</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>        num_static <span class="op">=</span> input_shape[<span class="dv">2</span>]</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the GRN for the sparse weights</span></span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grn_sparse_weights <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>num_static,</span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_static):</span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a GRN for each static variable</span></span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grns_transformed_embeddings.append(</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>                GatedResidualNetwork(</span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.d_model, </span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>                    use_time_distributed<span class="op">=</span><span class="va">False</span></span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a>        _, _, num_static, _ <span class="op">=</span> inputs.shape <span class="co"># batch size / one time step (since it's static) / num static variables / d_model</span></span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a>        flattened <span class="op">=</span> <span class="va">self</span>.flat(inputs)</span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute sparse weights</span></span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>        grn_outputs, _ <span class="op">=</span> <span class="va">self</span>.grn_sparse_weights(flattened, training<span class="op">=</span>training)</span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>        sparse_weights <span class="op">=</span> <span class="va">self</span>.softmax(grn_outputs)</span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a>        sparse_weights <span class="op">=</span> keras.ops.expand_dims(sparse_weights, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-50"><a href="#cb76-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-51"><a href="#cb76-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute transformed embeddings</span></span>
<span id="cb76-52"><a href="#cb76-52" aria-hidden="true" tabindex="-1"></a>        transformed_embeddings <span class="op">=</span> []</span>
<span id="cb76-53"><a href="#cb76-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_static):</span>
<span id="cb76-54"><a href="#cb76-54" aria-hidden="true" tabindex="-1"></a>            embed, _ <span class="op">=</span> <span class="va">self</span>.grns_transformed_embeddings[i](inputs[:, <span class="dv">0</span>, i:i<span class="op">+</span><span class="dv">1</span>, :], training<span class="op">=</span>training)</span>
<span id="cb76-55"><a href="#cb76-55" aria-hidden="true" tabindex="-1"></a>            transformed_embeddings.append(embed)</span>
<span id="cb76-56"><a href="#cb76-56" aria-hidden="true" tabindex="-1"></a>        transformed_embedding <span class="op">=</span> keras.ops.concatenate(transformed_embeddings, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb76-57"><a href="#cb76-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-58"><a href="#cb76-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine with sparse weights</span></span>
<span id="cb76-59"><a href="#cb76-59" aria-hidden="true" tabindex="-1"></a>        combined <span class="op">=</span> <span class="va">self</span>.mult([sparse_weights, transformed_embedding])</span>
<span id="cb76-60"><a href="#cb76-60" aria-hidden="true" tabindex="-1"></a>        static_vec <span class="op">=</span> keras.ops.<span class="bu">sum</span>(combined, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb76-61"><a href="#cb76-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-62"><a href="#cb76-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> static_vec, sparse_weights</span>
<span id="cb76-63"><a href="#cb76-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-64"><a href="#cb76-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb76-65"><a href="#cb76-65" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>(StaticVariableSelectionLayer, <span class="va">self</span>).get_config()</span>
<span id="cb76-66"><a href="#cb76-66" aria-hidden="true" tabindex="-1"></a>        config.update({</span>
<span id="cb76-67"><a href="#cb76-67" aria-hidden="true" tabindex="-1"></a>            <span class="st">'d_model'</span>: <span class="va">self</span>.d_model,</span>
<span id="cb76-68"><a href="#cb76-68" aria-hidden="true" tabindex="-1"></a>            <span class="st">'dropout_rate'</span>: <span class="va">self</span>.dropout_rate</span>
<span id="cb76-69"><a href="#cb76-69" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb76-70"><a href="#cb76-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Temporal variable selection networks work by considering all variables (of a given frequency) at the same time, together with an (optional) static context embedding, to calculate weights that are multiplied by a processed version of each variable. Breaking this up in steps:</p>
<ul>
<li><p>consider <span class="math inline">\(\xi_t^{(j)} \in \mathbb{R}^{\lambda}\)</span> to be the encoding (as in <a href="#tbl-summary_embed" class="quarto-xref">Table&nbsp;6</a> or <a href="#tbl-summary_encodcont" class="quarto-xref">Table&nbsp;8</a>) of the <span class="math inline">\(j\)</span>th variable in time <span class="math inline">\(t\)</span>, and collect the values at time <span class="math inline">\(t\)</span> for <em>all</em> <span class="math inline">\(J\)</span> input variables for that frequency in the flattened vector <span class="math inline">\(\Xi_t\)</span></p></li>
<li><p>collect the static variable encoders for use as context, <span class="math inline">\(c_s \in \mathbb{R}^{\lambda}\)</span>,</p></li>
<li><p>then, the selection weights for that frequency are the result of <span class="math inline">\(\nu_t = \text{Softmax}(\text{GRN}(\Xi_t, c_s))  \in \mathbb{R}^{|J|}\)</span>$,</p></li>
<li><p>these weights, which sum to one due to the softmax operation, are used on a version of the encoded variables that has itself been further encoded by a variable-specific GRN: <span class="math inline">\(\tilde{\xi}_t = \sum_{j=1}^{J}\nu_t^{(j)} \text{GRN}_j (\xi_t^{(j)}) \in \mathbb{R}^{\lambda}\)</span>.</p></li>
</ul>
<p>In short, the temporal variable selection network takes in data with the shape (batch size / number of time steps / number of variables / embedding dimension <span class="math inline">\(\lambda\)</span>) and output a tensor with the shape (batch size / number of time steps / embedding dimension <span class="math inline">\(\lambda\)</span>).</p>
<div id="class-tvs" class="cell" data-execution_count="59">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TemporalVariableSelection(keras.Layer):</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>, </span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Temporal variable selection"</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TemporalVariableSelection, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mult <span class="op">=</span> keras.layers.Multiply()</span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TemporalVariableSelection, <span class="va">self</span>).build(input_shape)</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.time_steps, <span class="va">self</span>.num_input_vars, <span class="va">self</span>.d_model <span class="op">=</span> input_shape[<span class="dv">1</span>:]</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.var_sel_weights <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span><span class="va">self</span>.num_input_vars,</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.softmax <span class="op">=</span> keras.layers.Activation(<span class="st">'softmax'</span>)</span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a GRN for each temporal variable</span></span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grns_transformed_embeddings <span class="op">=</span> [</span>
<span id="cb77-29"><a href="#cb77-29" aria-hidden="true" tabindex="-1"></a>            GatedResidualNetwork(</span>
<span id="cb77-30"><a href="#cb77-30" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb77-31"><a href="#cb77-31" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb77-32"><a href="#cb77-32" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span></span>
<span id="cb77-33"><a href="#cb77-33" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb77-34"><a href="#cb77-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_input_vars)</span>
<span id="cb77-35"><a href="#cb77-35" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb77-36"><a href="#cb77-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-37"><a href="#cb77-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb77-38"><a href="#cb77-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb77-39"><a href="#cb77-39" aria-hidden="true" tabindex="-1"></a>        inputs, <span class="co"># List of temporal embeddings, static context</span></span>
<span id="cb77-40"><a href="#cb77-40" aria-hidden="true" tabindex="-1"></a>        context<span class="op">=</span><span class="va">None</span>, <span class="co"># Used for static_context in the TFT</span></span>
<span id="cb77-41"><a href="#cb77-41" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb77-42"><a href="#cb77-42" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb77-43"><a href="#cb77-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shape of inputs (temporal_embeddings): (batch size / num time steps / num variables / embedding dimension, ie dim_model)</span></span>
<span id="cb77-44"><a href="#cb77-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-45"><a href="#cb77-45" aria-hidden="true" tabindex="-1"></a>        flattened_embed <span class="op">=</span> keras.ops.reshape( <span class="co"># \Xi_t</span></span>
<span id="cb77-46"><a href="#cb77-46" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb77-47"><a href="#cb77-47" aria-hidden="true" tabindex="-1"></a>            (<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.time_steps, <span class="va">self</span>.num_input_vars <span class="op">*</span> <span class="va">self</span>.d_model)</span>
<span id="cb77-48"><a href="#cb77-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb77-49"><a href="#cb77-49" aria-hidden="true" tabindex="-1"></a>        parallel_variables <span class="op">=</span> keras.ops.split(</span>
<span id="cb77-50"><a href="#cb77-50" aria-hidden="true" tabindex="-1"></a>            inputs, </span>
<span id="cb77-51"><a href="#cb77-51" aria-hidden="true" tabindex="-1"></a>            indices_or_sections<span class="op">=</span><span class="va">self</span>.num_input_vars,</span>
<span id="cb77-52"><a href="#cb77-52" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb77-53"><a href="#cb77-53" aria-hidden="true" tabindex="-1"></a>        ) <span class="co"># tensor is shaped this way so that a GRN can be applied to each variable of each batch</span></span>
<span id="cb77-54"><a href="#cb77-54" aria-hidden="true" tabindex="-1"></a>        c_s <span class="op">=</span> keras.ops.expand_dims(context, axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">if</span> context <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb77-55"><a href="#cb77-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-56"><a href="#cb77-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># variable weights</span></span>
<span id="cb77-57"><a href="#cb77-57" aria-hidden="true" tabindex="-1"></a>        grn_outputs, _ <span class="op">=</span> <span class="va">self</span>.var_sel_weights(flattened_embed, c_s, training<span class="op">=</span>training)</span>
<span id="cb77-58"><a href="#cb77-58" aria-hidden="true" tabindex="-1"></a>        variable_weights <span class="op">=</span> <span class="va">self</span>.softmax(grn_outputs)</span>
<span id="cb77-59"><a href="#cb77-59" aria-hidden="true" tabindex="-1"></a>        variable_weights <span class="op">=</span> keras.ops.expand_dims(variable_weights, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb77-60"><a href="#cb77-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-61"><a href="#cb77-61" aria-hidden="true" tabindex="-1"></a>        transformed_embeddings, _ <span class="op">=</span> [</span>
<span id="cb77-62"><a href="#cb77-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grns_transformed_embeddings[i](parallel_variables[i], training<span class="op">=</span>training)</span>
<span id="cb77-63"><a href="#cb77-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_input_vars)</span>
<span id="cb77-64"><a href="#cb77-64" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb77-65"><a href="#cb77-65" aria-hidden="true" tabindex="-1"></a>        transformed_embeddings <span class="op">=</span> keras.ops.stack(transformed_embeddings, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb77-66"><a href="#cb77-66" aria-hidden="true" tabindex="-1"></a>        combined <span class="op">=</span> keras.layers.Multiply()([variable_weights, transformed_embeddings])</span>
<span id="cb77-67"><a href="#cb77-67" aria-hidden="true" tabindex="-1"></a>        temporal_vec <span class="op">=</span> keras.<span class="bu">sum</span>(combined, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb77-68"><a href="#cb77-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb77-69"><a href="#cb77-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.variable_weights <span class="op">=</span> variable_weights</span>
<span id="cb77-70"><a href="#cb77-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> temporal_vec</span>
<span id="cb77-71"><a href="#cb77-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-72"><a href="#cb77-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_selection_weights(<span class="va">self</span>):</span>
<span id="cb77-73"><a href="#cb77-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.variable_weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="60">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;continuous variables&gt;</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>cont_inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span><span class="ss">f"input__freq_</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">"</span>) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> cont_inputs.items()</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a><span class="co"># expand dims to add a dimension for the number of variables (in this case, one)</span></span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {k: keras.ops.expand_dims(v, axis<span class="op">=</span><span class="dv">2</span>) <span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items()}</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;/continuous variables&gt;</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;categorical variables&gt;</span></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span>k, dtype<span class="op">=</span><span class="st">'int32'</span>)  <span class="co"># Input layer for each feature with shape (1,)</span></span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-25"><a href="#cb78-25" aria-hidden="true" tabindex="-1"></a>embedded_layers <span class="op">=</span> {</span>
<span id="cb78-26"><a href="#cb78-26" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Embedding(input_dim<span class="op">=</span>v, output_dim<span class="op">=</span>dim, name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>)(inputs[k])</span>
<span id="cb78-27"><a href="#cb78-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> vocab_sizes.items()</span>
<span id="cb78-28"><a href="#cb78-28" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-29"><a href="#cb78-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-30"><a href="#cb78-30" aria-hidden="true" tabindex="-1"></a>combined <span class="op">=</span> keras.layers.Average()(<span class="bu">list</span>(embedded_layers.values()))</span>
<span id="cb78-31"><a href="#cb78-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-32"><a href="#cb78-32" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(dim, activation<span class="op">=</span><span class="st">"relu"</span>)(combined)</span>
<span id="cb78-33"><a href="#cb78-33" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb78-34"><a href="#cb78-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-35"><a href="#cb78-35" aria-hidden="true" tabindex="-1"></a>nn_embed <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">list</span>(inputs.values()), outputs<span class="op">=</span>output)</span>
<span id="cb78-36"><a href="#cb78-36" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;/categorical variables&gt;</span></span>
<span id="cb78-37"><a href="#cb78-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-38"><a href="#cb78-38" aria-hidden="true" tabindex="-1"></a><span class="co">### &lt;VSN&gt;</span></span>
<span id="cb78-39"><a href="#cb78-39" aria-hidden="true" tabindex="-1"></a>flattened <span class="op">=</span> {</span>
<span id="cb78-40"><a href="#cb78-40" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Reshape(target_shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, v.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">*</span> v.shape[<span class="op">-</span><span class="dv">2</span>]), name<span class="op">=</span><span class="ss">f"reshape__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(v) <span class="co"># \Xi_t</span></span>
<span id="cb78-41"><a href="#cb78-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items()</span>
<span id="cb78-42"><a href="#cb78-42" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-43"><a href="#cb78-43" aria-hidden="true" tabindex="-1"></a>parallel_variables <span class="op">=</span> {</span>
<span id="cb78-44"><a href="#cb78-44" aria-hidden="true" tabindex="-1"></a>    k: keras.ops.split(</span>
<span id="cb78-45"><a href="#cb78-45" aria-hidden="true" tabindex="-1"></a>            v, </span>
<span id="cb78-46"><a href="#cb78-46" aria-hidden="true" tabindex="-1"></a>            indices_or_sections<span class="op">=</span>v.shape[<span class="dv">2</span>],</span>
<span id="cb78-47"><a href="#cb78-47" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb78-48"><a href="#cb78-48" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb78-49"><a href="#cb78-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items()</span>
<span id="cb78-50"><a href="#cb78-50" aria-hidden="true" tabindex="-1"></a>} <span class="co"># each value is a list, with each item of the list corresponding to one variable</span></span>
<span id="cb78-51"><a href="#cb78-51" aria-hidden="true" tabindex="-1"></a>var_weights <span class="op">=</span> {</span>
<span id="cb78-52"><a href="#cb78-52" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Activation(<span class="st">'softmax'</span>)(</span>
<span id="cb78-53"><a href="#cb78-53" aria-hidden="true" tabindex="-1"></a>        GatedResidualNetwork(</span>
<span id="cb78-54"><a href="#cb78-54" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span>dim,</span>
<span id="cb78-55"><a href="#cb78-55" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span><span class="dv">1</span>, <span class="co"># number of input variables in each frequency</span></span>
<span id="cb78-56"><a href="#cb78-56" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb78-57"><a href="#cb78-57" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="ss">f"variable_selection__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb78-58"><a href="#cb78-58" aria-hidden="true" tabindex="-1"></a>        )(v)[<span class="dv">0</span>]</span>
<span id="cb78-59"><a href="#cb78-59" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-60"><a href="#cb78-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> flattened.items()</span>
<span id="cb78-61"><a href="#cb78-61" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-62"><a href="#cb78-62" aria-hidden="true" tabindex="-1"></a>varspecific_grn <span class="op">=</span> {</span>
<span id="cb78-63"><a href="#cb78-63" aria-hidden="true" tabindex="-1"></a>    k: keras.ops.concatenate([</span>
<span id="cb78-64"><a href="#cb78-64" aria-hidden="true" tabindex="-1"></a>        GatedResidualNetwork(</span>
<span id="cb78-65"><a href="#cb78-65" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span>dim,</span>
<span id="cb78-66"><a href="#cb78-66" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb78-67"><a href="#cb78-67" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="ss">f"variable_specific__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb78-68"><a href="#cb78-68" aria-hidden="true" tabindex="-1"></a>        )(v[i])[<span class="dv">0</span>]</span>
<span id="cb78-69"><a href="#cb78-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(v)) <span class="co"># number of input variables in each frequency</span></span>
<span id="cb78-70"><a href="#cb78-70" aria-hidden="true" tabindex="-1"></a>    ], axis<span class="op">=</span><span class="dv">2</span>) <span class="co"># along the num_input_vars dimension, resulting in batch size / num time steps / num input vars / embedding dim (lambda)</span></span>
<span id="cb78-71"><a href="#cb78-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> parallel_variables.items()</span>
<span id="cb78-72"><a href="#cb78-72" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-73"><a href="#cb78-73" aria-hidden="true" tabindex="-1"></a>combined_features <span class="op">=</span> {}</span>
<span id="cb78-74"><a href="#cb78-74" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> freqs:</span>
<span id="cb78-75"><a href="#cb78-75" aria-hidden="true" tabindex="-1"></a>    broadcasted_weights <span class="op">=</span> keras.ops.tile(keras.ops.expand_dims(var_weights[f], axis<span class="op">=-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, dim))</span>
<span id="cb78-76"><a href="#cb78-76" aria-hidden="true" tabindex="-1"></a>    combined_features[f] <span class="op">=</span> keras.ops.<span class="bu">sum</span>(</span>
<span id="cb78-77"><a href="#cb78-77" aria-hidden="true" tabindex="-1"></a>        keras.layers.Multiply(name<span class="op">=</span><span class="ss">f"weighting__freq_</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">"</span>)([broadcasted_weights, varspecific_grn[f]]),</span>
<span id="cb78-78"><a href="#cb78-78" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb78-79"><a href="#cb78-79" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-80"><a href="#cb78-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-81"><a href="#cb78-81" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;/VSN&gt;</span></span>
<span id="cb78-82"><a href="#cb78-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-83"><a href="#cb78-83" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb78-84"><a href="#cb78-84" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> combined_features.items():</span>
<span id="cb78-85"><a href="#cb78-85" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb78-86"><a href="#cb78-86" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb78-87"><a href="#cb78-87" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb78-88"><a href="#cb78-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-89"><a href="#cb78-89" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb78-90"><a href="#cb78-90" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"FinalFeatureTransformation"</span>)(encoded_series)</span>
<span id="cb78-91"><a href="#cb78-91" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb78-92"><a href="#cb78-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-93"><a href="#cb78-93" aria-hidden="true" tabindex="-1"></a>nn_vsn <span class="op">=</span> keras.Model(</span>
<span id="cb78-94"><a href="#cb78-94" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>cont_inputs, </span>
<span id="cb78-95"><a href="#cb78-95" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb78-96"><a href="#cb78-96" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"VSN"</span></span>
<span id="cb78-97"><a href="#cb78-97" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-98"><a href="#cb78-98" aria-hidden="true" tabindex="-1"></a>nn_vsn.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb78-99"><a href="#cb78-99" aria-hidden="true" tabindex="-1"></a>nn_vsn.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_vsn" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="60">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_vsn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: Summary of VSN (work in progress)
</figcaption>
<div aria-describedby="tbl-summary_vsn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "VSN"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input__freq_m       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ input__freq_d       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__freq_m    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ input__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__freq_d    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ input__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ expand_dims         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__freq_m… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ExpandDims</span>)        │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ expand_dims_1       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__freq_d… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ExpandDims</span>)        │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ reshape__freq_m     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ expand_dims[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Reshape</span>)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ reshape__freq_d     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ expand_dims_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Reshape</span>)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ variable_selection… │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>), │        <span style="color: #00af00; text-decoration-color: #00af00">597</span> │ reshape__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedResidualNetw…</span> │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)]  │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ variable_selection… │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>), │        <span style="color: #00af00; text-decoration-color: #00af00">597</span> │ reshape__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedResidualNetw…</span> │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)]  │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ variable_selecti… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ split (<span style="color: #0087ff; text-decoration-color: #0087ff">Split</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ expand_dims[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ activation_2        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ variable_selecti… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ split_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Split</span>)     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ expand_dims_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ expand_dims_2       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ activation[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]  │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ExpandDims</span>)        │ <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ variable_specific_… │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,  │      <span style="color: #00af00; text-decoration-color: #00af00">1,392</span> │ split[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedResidualNetw…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │                   │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)]           │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ expand_dims_3       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ activation_2[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ExpandDims</span>)        │ <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ variable_specific_… │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,  │      <span style="color: #00af00; text-decoration-color: #00af00">1,392</span> │ split_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]     │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">GatedResidualNetw…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │                   │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">1</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)]           │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ tile (<span style="color: #0087ff; text-decoration-color: #0087ff">Tile</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ expand_dims_2[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ variable_specifi… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>)       │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ tile_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Tile</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ expand_dims_3[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
│                     │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_1       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ variable_specifi… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>)       │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ weighting__freq_m   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ tile[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],       │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Multiply</span>)          │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │ concatenate[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ weighting__freq_d   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>,   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ tile_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],     │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Multiply</span>)          │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>)               │            │ concatenate_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sum (<span style="color: #0087ff; text-decoration-color: #0087ff">Sum</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ weighting__freq_… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sum_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Sum</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ weighting__freq_… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ sum[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal_1         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ sum_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>)   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ sum[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ sum_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ masking[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],    │
│                     │                   │            │ any[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ masking_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],  │
│                     │                   │            │ any_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Average</span>)           │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ FinalFeatureTransf… │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">272</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_28 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ FinalFeatureTran… │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,555</span> (33.42 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,555</span> (33.42 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_vsn" class="cell" data-execution_count="61">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_vsn, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="61">
<div id="fig-arch_vsn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_vsn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Architecture of model with variable selection network
</figcaption>
<div aria-describedby="fig-arch_vsn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_vsn-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div id="train-the-model-with-temporal-variable-selection" class="cell" data-execution_count="62">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split, X_dates <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags, return_dates<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_vsn(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    X_vsn <span class="op">=</span> {}</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_vsn:</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>                X_vsn[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>            X_vsn[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>    vsn_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_vsn.items()}</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vsn_X</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>vsn_X_train <span class="op">=</span> adjust_data_vsn(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>vsn_X_valid <span class="op">=</span> adjust_data_vsn(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>y_freq <span class="op">=</span> <span class="st">"m"</span> <span class="co">#&nbsp;only do the date features for the same frequency as the dependent variable</span></span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a>date_feats <span class="op">=</span> {fold: {chunk: [] <span class="cf">for</span> chunk <span class="kw">in</span> X_dates[fold].keys()} <span class="cf">for</span> fold <span class="kw">in</span> X_dates.keys()}</span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> X_dates.keys():</span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> chunk <span class="kw">in</span> X_dates[fold].keys():</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a>        dates_fold <span class="op">=</span> [i[y_freq] <span class="cf">for</span> i <span class="kw">in</span> X_dates[fold][chunk]]</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> dates_fold:</span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>            to_pad <span class="op">=</span> get_timefeat(pd.DataFrame(index<span class="op">=</span>i), freq<span class="op">=</span>y_freq, features<span class="op">=</span>timefeat).values</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a>            padded <span class="op">=</span> keras.utils.pad_sequences([to_pad], maxlen<span class="op">=</span>maxlags[y_freq], dtype<span class="op">=</span>np.int32)</span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>            date_feats[fold][chunk].append(padded)</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&lt;VSN date padding&gt; -- only to be done in the same frequency as the dependent variable</span></span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a><span class="co"># try:</span></span>
<span id="cb80-28"><a href="#cb80-28" aria-hidden="true" tabindex="-1"></a><span class="co">#     if return_dates:</span></span>
<span id="cb80-29"><a href="#cb80-29" aria-hidden="true" tabindex="-1"></a><span class="co">#         dates_X[f] = X[f][:ysample_date][:-1].index</span></span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a><span class="co">#     to_pad = X[f][:ysample_date][:-1].values</span></span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a><span class="co"># except KeyError:</span></span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a><span class="co">#     to_pad = np.zeros((1,1))</span></span>
<span id="cb80-33"><a href="#cb80-33" aria-hidden="true" tabindex="-1"></a><span class="co"># padded_x[f] = keras.utils.pad_sequences([to_pad], maxlen=maxlags[f], dtype=np.float32)</span></span>
<span id="cb80-34"><a href="#cb80-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-35"><a href="#cb80-35" aria-hidden="true" tabindex="-1"></a><span class="co"># x_shape = (1, maxlags[f], n_feat[f]) if timedim else (1, maxlags[f] * n_feat[f])</span></span>
<span id="cb80-36"><a href="#cb80-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-37"><a href="#cb80-37" aria-hidden="true" tabindex="-1"></a><span class="co"># padded_x[f] = padded_x[f].reshape(x_shape)</span></span>
<span id="cb80-38"><a href="#cb80-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&lt;/VSN date padding&gt;</span></span>
<span id="cb80-39"><a href="#cb80-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-40"><a href="#cb80-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-41"><a href="#cb80-41" aria-hidden="true" tabindex="-1"></a><span class="co"># y_timefeat_train = {timef:</span></span>
<span id="cb80-42"><a href="#cb80-42" aria-hidden="true" tabindex="-1"></a><span class="co">#     np.array([</span></span>
<span id="cb80-43"><a href="#cb80-43" aria-hidden="true" tabindex="-1"></a><span class="co">#         get_timefeat(pd.DataFrame(i.values, index=[i.name], columns=i.index), features=timef)</span></span>
<span id="cb80-44"><a href="#cb80-44" aria-hidden="true" tabindex="-1"></a><span class="co">#         for i in y_train_split_fc[fold]["train"]</span></span>
<span id="cb80-45"><a href="#cb80-45" aria-hidden="true" tabindex="-1"></a><span class="co">#     ])</span></span>
<span id="cb80-46"><a href="#cb80-46" aria-hidden="true" tabindex="-1"></a><span class="co">#     for timef in timefeat</span></span>
<span id="cb80-47"><a href="#cb80-47" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span>
<span id="cb80-48"><a href="#cb80-48" aria-hidden="true" tabindex="-1"></a><span class="co"># y_timefeat_valid = {timef:</span></span>
<span id="cb80-49"><a href="#cb80-49" aria-hidden="true" tabindex="-1"></a><span class="co">#     np.array([</span></span>
<span id="cb80-50"><a href="#cb80-50" aria-hidden="true" tabindex="-1"></a><span class="co">#         get_timefeat(pd.DataFrame(i.values, index=[i.name], columns=i.index), features=timef)</span></span>
<span id="cb80-51"><a href="#cb80-51" aria-hidden="true" tabindex="-1"></a><span class="co">#         for i in y_train_split_fc[fold]["valid"]</span></span>
<span id="cb80-52"><a href="#cb80-52" aria-hidden="true" tabindex="-1"></a><span class="co">#     ])</span></span>
<span id="cb80-53"><a href="#cb80-53" aria-hidden="true" tabindex="-1"></a><span class="co">#     for timef in timefeat</span></span>
<span id="cb80-54"><a href="#cb80-54" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span>
<span id="cb80-55"><a href="#cb80-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-56"><a href="#cb80-56" aria-hidden="true" tabindex="-1"></a>history_vsn <span class="op">=</span> nn_vsn.fit(x<span class="op">=</span>vsn_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(vsn_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/36 ━━━━━━━━━━━━━━━━━━━━ 1:57:47 202s/step - loss: 6.0832e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 206ms/step - loss: 0.0127         3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 204ms/step - loss: 0.0152 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 203ms/step - loss: 0.0155 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 204ms/step - loss: 0.0151 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 0.0144 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 0.0137 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 205ms/step - loss: 0.0130 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 205ms/step - loss: 0.012410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 0.011811/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 0.011312/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 0.010813/36 ━━━━━━━━━━━━━━━━━━━━ 4s 203ms/step - loss: 0.010414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 0.010015/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 0.009616/36 ━━━━━━━━━━━━━━━━━━━━ 4s 207ms/step - loss: 0.009317/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 0.009018/36 ━━━━━━━━━━━━━━━━━━━━ 3s 205ms/step - loss: 0.008719/36 ━━━━━━━━━━━━━━━━━━━━ 3s 208ms/step - loss: 0.008420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 207ms/step - loss: 0.008121/36 ━━━━━━━━━━━━━━━━━━━━ 3s 207ms/step - loss: 0.007922/36 ━━━━━━━━━━━━━━━━━━━━ 2s 208ms/step - loss: 0.007723/36 ━━━━━━━━━━━━━━━━━━━━ 2s 207ms/step - loss: 0.007524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 207ms/step - loss: 0.007325/36 ━━━━━━━━━━━━━━━━━━━━ 2s 207ms/step - loss: 0.007126/36 ━━━━━━━━━━━━━━━━━━━━ 2s 207ms/step - loss: 0.006927/36 ━━━━━━━━━━━━━━━━━━━━ 1s 207ms/step - loss: 0.006828/36 ━━━━━━━━━━━━━━━━━━━━ 1s 206ms/step - loss: 0.006629/36 ━━━━━━━━━━━━━━━━━━━━ 1s 207ms/step - loss: 0.006530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 207ms/step - loss: 0.006431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 207ms/step - loss: 0.006232/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 0.006133/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 0.006034/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 0.005935/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 0.005836/36 ━━━━━━━━━━━━━━━━━━━━ 0s 209ms/step - loss: 0.005736/36 ━━━━━━━━━━━━━━━━━━━━ 239s 1s/step - loss: 0.0056 - val_loss: 7.3811e-04
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 239ms/step - loss: 8.2265e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 228ms/step - loss: 8.5041e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 238ms/step - loss: 8.6041e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 7s 227ms/step - loss: 8.6499e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 7s 227ms/step - loss: 8.5378e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 228ms/step - loss: 8.3280e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 228ms/step - loss: 8.0667e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 6s 227ms/step - loss: 7.7863e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 6s 225ms/step - loss: 7.5062e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 223ms/step - loss: 7.2344e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 5s 222ms/step - loss: 6.9740e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 5s 221ms/step - loss: 6.7284e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 5s 220ms/step - loss: 6.4993e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 219ms/step - loss: 6.2854e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 218ms/step - loss: 6.0899e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 4s 218ms/step - loss: 5.9167e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 4s 217ms/step - loss: 5.7668e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 5.6510e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 217ms/step - loss: 5.5780e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 217ms/step - loss: 5.5318e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 3s 216ms/step - loss: 5.4934e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 3s 217ms/step - loss: 5.4532e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 217ms/step - loss: 5.4107e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 216ms/step - loss: 5.3655e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 216ms/step - loss: 5.3185e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 215ms/step - loss: 5.2715e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 5.2262e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 5.1841e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 5.1450e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 5.1090e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 5.0759e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 5.0432e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 5.0097e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 4.9751e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 4.9399e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 4.9049e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 230ms/step - loss: 4.8719e-04 - val_loss: 2.3384e-04
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 221ms/step - loss: 1.2871e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 206ms/step - loss: 1.4854e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 206ms/step - loss: 1.7987e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 206ms/step - loss: 2.1485e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 2.6184e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 3.0189e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 207ms/step - loss: 3.7348e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 206ms/step - loss: 4.3068e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 206ms/step - loss: 4.6906e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 207ms/step - loss: 4.9264e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 5s 206ms/step - loss: 5.0618e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 4s 206ms/step - loss: 5.1307e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 4s 206ms/step - loss: 5.1558e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 206ms/step - loss: 5.1511e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 206ms/step - loss: 5.1251e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 4s 206ms/step - loss: 5.0842e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 5.0334e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 207ms/step - loss: 4.9760e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 4.9149e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 4.8512e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 3s 207ms/step - loss: 4.7868e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 2s 206ms/step - loss: 4.7226e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 206ms/step - loss: 4.6591e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 206ms/step - loss: 4.5968e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 205ms/step - loss: 4.5390e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 205ms/step - loss: 4.4901e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 205ms/step - loss: 4.4526e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 205ms/step - loss: 4.4231e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 205ms/step - loss: 4.3961e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 4.3678e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 4.3382e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 4.3073e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 4.2755e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 4.2434e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 4.2111e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 4.1787e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 219ms/step - loss: 4.1481e-04 - val_loss: 5.5075e-05
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 219ms/step - loss: 4.0349e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 199ms/step - loss: 4.0687e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 201ms/step - loss: 4.9124e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 203ms/step - loss: 5.5619e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 207ms/step - loss: 6.4493e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 204ms/step - loss: 8.0119e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 202ms/step - loss: 1.0097e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 1.2779e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 208ms/step - loss: 1.4981e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 208ms/step - loss: 1.6745e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 5s 208ms/step - loss: 1.8199e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 4s 207ms/step - loss: 1.9318e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 4s 210ms/step - loss: 2.0157e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 209ms/step - loss: 2.0781e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 209ms/step - loss: 2.1260e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 4s 209ms/step - loss: 2.1627e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 3s 210ms/step - loss: 2.1902e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 210ms/step - loss: 2.2095e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 210ms/step - loss: 2.2227e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 209ms/step - loss: 2.2315e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 3s 211ms/step - loss: 2.2385e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 2s 211ms/step - loss: 2.2448e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 211ms/step - loss: 2.2507e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 211ms/step - loss: 2.2562e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 2.2613e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 2.2652e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step - loss: 2.2676e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 209ms/step - loss: 2.2688e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 209ms/step - loss: 2.2689e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 209ms/step - loss: 2.2684e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 209ms/step - loss: 2.2670e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 209ms/step - loss: 2.2640e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 2.2594e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 2.2535e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 2.2463e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 2.2384e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 223ms/step - loss: 2.2309e-04 - val_loss: 7.4804e-05
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 226ms/step - loss: 4.4381e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 196ms/step - loss: 4.2069e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 197ms/step - loss: 4.0751e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 199ms/step - loss: 4.3483e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 200ms/step - loss: 4.9923e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 6.2383e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 201ms/step - loss: 8.2588e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 199ms/step - loss: 1.0294e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 198ms/step - loss: 1.2039e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 196ms/step - loss: 1.3317e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 4s 196ms/step - loss: 1.4205e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 4s 197ms/step - loss: 1.4812e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 4s 198ms/step - loss: 1.5207e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 199ms/step - loss: 1.5449e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 199ms/step - loss: 1.5582e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 3s 199ms/step - loss: 1.5641e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 3s 199ms/step - loss: 1.5657e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 199ms/step - loss: 1.5662e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 199ms/step - loss: 1.5669e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 199ms/step - loss: 1.5664e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 2s 199ms/step - loss: 1.5634e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 2s 199ms/step - loss: 1.5579e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 201ms/step - loss: 1.5503e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 201ms/step - loss: 1.5409e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 201ms/step - loss: 1.5304e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 201ms/step - loss: 1.5190e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 202ms/step - loss: 1.5069e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 202ms/step - loss: 1.4943e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 202ms/step - loss: 1.4814e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 203ms/step - loss: 1.4686e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 203ms/step - loss: 1.4562e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 1.4467e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 1.4438e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 1.4459e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 1.4492e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 1.4524e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 218ms/step - loss: 1.4554e-04 - val_loss: 1.3182e-04
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 235ms/step - loss: 1.4619e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 196ms/step - loss: 1.3702e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 199ms/step - loss: 1.3051e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 1.2455e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 1.1860e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 204ms/step - loss: 1.1227e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 1.0625e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 1.0073e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 202ms/step - loss: 9.5974e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 9.1762e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 8.8083e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 8.4859e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 8.1970e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 7.9538e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 7.7973e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 203ms/step - loss: 7.7597e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 7.8130e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 7.9267e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 8.0716e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 8.2096e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 8.3317e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 8.4343e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - loss: 8.5160e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - loss: 8.5814e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - loss: 8.6342e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - loss: 8.6803e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 201ms/step - loss: 8.7230e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 201ms/step - loss: 8.7590e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 201ms/step - loss: 8.7859e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 201ms/step - loss: 8.8057e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 201ms/step - loss: 8.8158e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 201ms/step - loss: 8.8181e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 202ms/step - loss: 8.8162e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 202ms/step - loss: 8.8211e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 203ms/step - loss: 8.8400e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 8.8720e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 220ms/step - loss: 8.9022e-05 - val_loss: 2.6948e-04
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 226ms/step - loss: 2.8497e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 2.5629e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 206ms/step - loss: 2.3841e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 207ms/step - loss: 2.2425e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 212ms/step - loss: 2.1084e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 210ms/step - loss: 1.9801e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 209ms/step - loss: 1.8652e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 210ms/step - loss: 1.7626e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 209ms/step - loss: 1.6713e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 209ms/step - loss: 1.5928e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 5s 209ms/step - loss: 1.5235e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 5s 209ms/step - loss: 1.4643e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 4s 210ms/step - loss: 1.4173e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 209ms/step - loss: 1.3803e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 208ms/step - loss: 1.3500e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 4s 207ms/step - loss: 1.3243e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 1.3022e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 1.2827e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 206ms/step - loss: 1.2641e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 208ms/step - loss: 1.2474e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 3s 208ms/step - loss: 1.2328e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 2s 208ms/step - loss: 1.2216e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 208ms/step - loss: 1.2139e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 207ms/step - loss: 1.2072e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 208ms/step - loss: 1.2017e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 208ms/step - loss: 1.1976e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 208ms/step - loss: 1.1929e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 208ms/step - loss: 1.1874e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 208ms/step - loss: 1.1814e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 208ms/step - loss: 1.1748e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 208ms/step - loss: 1.1676e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 210ms/step - loss: 1.1601e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 211ms/step - loss: 1.1522e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 212ms/step - loss: 1.1441e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 1.1361e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 215ms/step - loss: 1.1284e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 233ms/step - loss: 1.1212e-04 - val_loss: 1.4989e-04
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 241ms/step - loss: 1.3540e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 217ms/step - loss: 1.5502e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 219ms/step - loss: 1.7258e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 218ms/step - loss: 1.8401e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 218ms/step - loss: 1.8989e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 216ms/step - loss: 1.9044e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 216ms/step - loss: 1.8816e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 213ms/step - loss: 1.8497e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 211ms/step - loss: 1.8156e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 213ms/step - loss: 1.7783e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 5s 216ms/step - loss: 1.7383e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 5s 215ms/step - loss: 1.6973e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 4s 216ms/step - loss: 1.6563e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 217ms/step - loss: 1.6155e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 216ms/step - loss: 1.5760e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 4s 217ms/step - loss: 1.5389e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 4s 216ms/step - loss: 1.5042e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 216ms/step - loss: 1.4727e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 216ms/step - loss: 1.4450e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 216ms/step - loss: 1.4218e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 3s 215ms/step - loss: 1.4037e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 3s 216ms/step - loss: 1.3900e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 216ms/step - loss: 1.3795e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 217ms/step - loss: 1.3696e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 217ms/step - loss: 1.3593e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 218ms/step - loss: 1.3483e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 219ms/step - loss: 1.3367e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 218ms/step - loss: 1.3247e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 218ms/step - loss: 1.3129e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 217ms/step - loss: 1.3010e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 217ms/step - loss: 1.2891e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 217ms/step - loss: 1.2775e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 217ms/step - loss: 1.2659e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 217ms/step - loss: 1.2545e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 217ms/step - loss: 1.2433e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 216ms/step - loss: 1.2329e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 232ms/step - loss: 1.2231e-04 - val_loss: 2.0142e-04
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 220ms/step - loss: 2.0247e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 201ms/step - loss: 2.2080e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 208ms/step - loss: 2.2634e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 207ms/step - loss: 2.1963e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 207ms/step - loss: 2.0941e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 1.9855e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 1.8819e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 202ms/step - loss: 1.7852e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 1.6971e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 5s 205ms/step - loss: 1.6188e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 1.5496e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 1.4882e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 1.4345e-0414/36 ━━━━━━━━━━━━━━━━━━━━ 4s 202ms/step - loss: 1.3898e-0415/36 ━━━━━━━━━━━━━━━━━━━━ 4s 202ms/step - loss: 1.3568e-0416/36 ━━━━━━━━━━━━━━━━━━━━ 4s 202ms/step - loss: 1.3296e-0417/36 ━━━━━━━━━━━━━━━━━━━━ 3s 201ms/step - loss: 1.3042e-0418/36 ━━━━━━━━━━━━━━━━━━━━ 3s 201ms/step - loss: 1.2797e-0419/36 ━━━━━━━━━━━━━━━━━━━━ 3s 201ms/step - loss: 1.2573e-0420/36 ━━━━━━━━━━━━━━━━━━━━ 3s 202ms/step - loss: 1.2372e-0421/36 ━━━━━━━━━━━━━━━━━━━━ 3s 202ms/step - loss: 1.2186e-0422/36 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - loss: 1.2013e-0423/36 ━━━━━━━━━━━━━━━━━━━━ 2s 202ms/step - loss: 1.1862e-0424/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 1.1735e-0425/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 1.1626e-0426/36 ━━━━━━━━━━━━━━━━━━━━ 2s 204ms/step - loss: 1.1530e-0427/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 1.1441e-0428/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 1.1355e-0429/36 ━━━━━━━━━━━━━━━━━━━━ 1s 206ms/step - loss: 1.1273e-0430/36 ━━━━━━━━━━━━━━━━━━━━ 1s 206ms/step - loss: 1.1193e-0431/36 ━━━━━━━━━━━━━━━━━━━━ 1s 207ms/step - loss: 1.1116e-0432/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 1.1043e-0433/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 1.0972e-0434/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 1.0900e-0435/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 1.0827e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 1.0754e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 8s 223ms/step - loss: 1.0685e-04 - val_loss: 1.5732e-05
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 230ms/step - loss: 1.8815e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 2.4455e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 203ms/step - loss: 3.1623e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 203ms/step - loss: 4.0405e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 5.2402e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 206ms/step - loss: 6.3506e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 205ms/step - loss: 7.0306e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 7.4027e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 7.5946e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 7.6874e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 203ms/step - loss: 7.7145e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 4s 203ms/step - loss: 7.6897e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 7.6328e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 7.5510e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 7.4542e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 204ms/step - loss: 7.3540e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 3s 204ms/step - loss: 7.2636e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 7.1703e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 7.0759e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 6.9859e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 203ms/step - loss: 6.9064e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 6.8453e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 6.8239e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 6.8372e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 6.8676e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 204ms/step - loss: 6.8997e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 203ms/step - loss: 6.9261e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 6.9458e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 205ms/step - loss: 6.9574e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 206ms/step - loss: 6.9632e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 207ms/step - loss: 6.9658e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.9653e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.9618e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.9567e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.9503e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 209ms/step - loss: 6.9440e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 225ms/step - loss: 6.9380e-05 - val_loss: 7.0577e-05
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 224ms/step - loss: 1.0034e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 203ms/step - loss: 9.7790e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 242ms/step - loss: 9.6644e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 7s 230ms/step - loss: 9.5567e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 225ms/step - loss: 9.3460e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 220ms/step - loss: 9.1664e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 217ms/step - loss: 9.0299e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 6s 216ms/step - loss: 8.8522e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 218ms/step - loss: 8.6395e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 218ms/step - loss: 8.4207e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 222ms/step - loss: 8.2225e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 5s 223ms/step - loss: 8.0567e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 5s 222ms/step - loss: 7.9212e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 221ms/step - loss: 7.8017e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 222ms/step - loss: 7.6942e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 220ms/step - loss: 7.6046e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 219ms/step - loss: 7.5264e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 219ms/step - loss: 7.4650e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 220ms/step - loss: 7.4180e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 219ms/step - loss: 7.3760e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 7.3369e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 7.2954e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 217ms/step - loss: 7.2487e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 217ms/step - loss: 7.1960e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 216ms/step - loss: 7.1390e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 216ms/step - loss: 7.0791e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 7.0173e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 214ms/step - loss: 6.9542e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 6.8902e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step - loss: 6.8265e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 216ms/step - loss: 6.7639e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 216ms/step - loss: 6.7040e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 216ms/step - loss: 6.6509e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 215ms/step - loss: 6.6147e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 216ms/step - loss: 6.5929e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 217ms/step - loss: 6.5795e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 234ms/step - loss: 6.5669e-05 - val_loss: 1.1487e-04
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 233ms/step - loss: 1.2666e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 228ms/step - loss: 1.1181e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 237ms/step - loss: 1.0028e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 7s 229ms/step - loss: 9.1050e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 7s 226ms/step - loss: 8.3726e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 223ms/step - loss: 7.8174e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 239ms/step - loss: 7.4471e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 6s 239ms/step - loss: 7.1791e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 6s 235ms/step - loss: 6.9647e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 6s 232ms/step - loss: 6.7852e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 230ms/step - loss: 6.6310e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 5s 227ms/step - loss: 6.4968e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 5s 225ms/step - loss: 6.3794e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 224ms/step - loss: 6.2954e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 224ms/step - loss: 6.2436e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 224ms/step - loss: 6.2138e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 224ms/step - loss: 6.1877e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 4s 224ms/step - loss: 6.1637e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 226ms/step - loss: 6.1480e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 228ms/step - loss: 6.1413e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 227ms/step - loss: 6.1415e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 227ms/step - loss: 6.1396e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 226ms/step - loss: 6.1312e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 226ms/step - loss: 6.1159e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 226ms/step - loss: 6.0944e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 225ms/step - loss: 6.0694e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 2s 225ms/step - loss: 6.0394e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 225ms/step - loss: 6.0057e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 226ms/step - loss: 5.9692e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 226ms/step - loss: 5.9315e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 225ms/step - loss: 5.8933e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 225ms/step - loss: 5.8547e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 224ms/step - loss: 5.8217e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - loss: 5.7948e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - loss: 5.7801e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - loss: 5.7752e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 9s 239ms/step - loss: 5.7705e-05 - val_loss: 1.0977e-04
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 239ms/step - loss: 7.9929e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 219ms/step - loss: 7.0804e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 214ms/step - loss: 6.2898e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 212ms/step - loss: 5.8040e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 214ms/step - loss: 5.5655e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 211ms/step - loss: 5.5080e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 212ms/step - loss: 5.5171e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 211ms/step - loss: 5.5087e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 211ms/step - loss: 5.4627e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 215ms/step - loss: 5.3911e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 215ms/step - loss: 5.3011e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 5s 217ms/step - loss: 5.1985e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 5s 222ms/step - loss: 5.0958e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 222ms/step - loss: 5.0021e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 220ms/step - loss: 4.9151e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 221ms/step - loss: 4.8337e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 220ms/step - loss: 4.7650e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 219ms/step - loss: 4.7156e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 4.6956e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 4.7039e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 4.7303e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 218ms/step - loss: 4.7686e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 218ms/step - loss: 4.8127e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 217ms/step - loss: 4.8539e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 219ms/step - loss: 4.8838e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 219ms/step - loss: 4.9073e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 219ms/step - loss: 4.9237e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 218ms/step - loss: 4.9347e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 219ms/step - loss: 4.9434e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 219ms/step - loss: 4.9501e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 219ms/step - loss: 4.9543e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 219ms/step - loss: 4.9557e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 219ms/step - loss: 4.9533e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 218ms/step - loss: 4.9474e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 218ms/step - loss: 4.9383e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 218ms/step - loss: 4.9268e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 232ms/step - loss: 4.9158e-05 - val_loss: 8.7813e-06
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 246ms/step - loss: 1.0984e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 205ms/step - loss: 1.2390e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 204ms/step - loss: 1.4521e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 203ms/step - loss: 1.9760e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 208ms/step - loss: 3.0050e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 207ms/step - loss: 4.0817e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 208ms/step - loss: 4.9094e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 208ms/step - loss: 5.4489e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 209ms/step - loss: 5.8141e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 209ms/step - loss: 6.0289e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 212ms/step - loss: 6.1413e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 5s 214ms/step - loss: 6.2002e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 4s 213ms/step - loss: 6.2694e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 213ms/step - loss: 6.3648e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 213ms/step - loss: 6.4492e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 212ms/step - loss: 6.5096e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 212ms/step - loss: 6.5526e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 212ms/step - loss: 6.5807e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 211ms/step - loss: 6.5980e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 210ms/step - loss: 6.6009e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 211ms/step - loss: 6.5929e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 2s 211ms/step - loss: 6.5795e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 6.5615e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 6.5421e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 6.5290e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 6.5206e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step - loss: 6.5119e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step - loss: 6.5017e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step - loss: 6.4916e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step - loss: 6.4826e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 209ms/step - loss: 6.4754e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 209ms/step - loss: 6.4679e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.4577e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.4449e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 208ms/step - loss: 6.4315e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 207ms/step - loss: 6.4170e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 221ms/step - loss: 6.4033e-05 - val_loss: 2.2889e-05
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 227ms/step - loss: 2.2326e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 188ms/step - loss: 2.0058e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 199ms/step - loss: 1.9181e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 199ms/step - loss: 1.8736e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 199ms/step - loss: 1.7895e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 5s 199ms/step - loss: 1.7273e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 199ms/step - loss: 1.6887e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 200ms/step - loss: 1.7221e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 200ms/step - loss: 1.8399e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 200ms/step - loss: 2.0437e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 200ms/step - loss: 2.2423e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 4s 199ms/step - loss: 2.4133e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 4s 200ms/step - loss: 2.5759e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 200ms/step - loss: 2.7240e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 200ms/step - loss: 2.8433e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 200ms/step - loss: 2.9401e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 3s 201ms/step - loss: 3.0155e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 202ms/step - loss: 3.0774e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 202ms/step - loss: 3.1270e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 202ms/step - loss: 3.1655e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 202ms/step - loss: 3.1933e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 3.2123e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 203ms/step - loss: 3.2278e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 204ms/step - loss: 3.2407e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 204ms/step - loss: 3.2517e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 204ms/step - loss: 3.2617e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 3.2779e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 203ms/step - loss: 3.3054e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 203ms/step - loss: 3.3410e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 203ms/step - loss: 3.3803e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 204ms/step - loss: 3.4174e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 3.4523e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 3.4826e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 3.5093e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 3.5327e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 204ms/step - loss: 3.5539e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 219ms/step - loss: 3.5740e-05 - val_loss: 2.8401e-05
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 234ms/step - loss: 3.4145e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 6s 201ms/step - loss: 3.4148e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 3.4565e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 3.7079e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 6s 201ms/step - loss: 4.1589e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 202ms/step - loss: 4.5766e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 4.9320e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 5s 205ms/step - loss: 5.2973e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 5s 204ms/step - loss: 5.5544e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 207ms/step - loss: 5.7162e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 208ms/step - loss: 5.8004e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 4s 208ms/step - loss: 5.8331e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 4s 210ms/step - loss: 5.8300e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 4s 211ms/step - loss: 5.8025e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 212ms/step - loss: 5.7590e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 213ms/step - loss: 5.7058e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 212ms/step - loss: 5.6479e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 3s 211ms/step - loss: 5.5993e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 211ms/step - loss: 5.5789e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 210ms/step - loss: 5.5740e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 209ms/step - loss: 5.5803e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 2s 209ms/step - loss: 5.5919e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 209ms/step - loss: 5.6026e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 209ms/step - loss: 5.6085e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 209ms/step - loss: 5.6077e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 210ms/step - loss: 5.6032e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 1s 211ms/step - loss: 5.5972e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 212ms/step - loss: 5.5879e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 212ms/step - loss: 5.5757e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 213ms/step - loss: 5.5606e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 213ms/step - loss: 5.5437e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 213ms/step - loss: 5.5257e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 5.5066e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 214ms/step - loss: 5.4897e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 213ms/step - loss: 5.4759e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 213ms/step - loss: 5.4621e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 8s 228ms/step - loss: 5.4491e-05 - val_loss: 4.4027e-05
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 7s 227ms/step - loss: 1.0725e-04 2/36 ━━━━━━━━━━━━━━━━━━━━ 8s 262ms/step - loss: 1.0807e-04 3/36 ━━━━━━━━━━━━━━━━━━━━ 8s 250ms/step - loss: 1.0031e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 8s 259ms/step - loss: 9.2244e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 8s 266ms/step - loss: 8.4965e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 7s 263ms/step - loss: 7.9926e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 7s 255ms/step - loss: 7.6613e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 6s 250ms/step - loss: 7.4277e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 6s 247ms/step - loss: 7.2433e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 6s 247ms/step - loss: 7.0829e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 6s 245ms/step - loss: 7.0281e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 5s 246ms/step - loss: 7.0272e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 5s 244ms/step - loss: 7.0188e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 5s 243ms/step - loss: 6.9845e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 5s 242ms/step - loss: 6.9319e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 240ms/step - loss: 6.8698e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 238ms/step - loss: 6.8136e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 4s 238ms/step - loss: 6.7706e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 4s 239ms/step - loss: 6.7430e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 240ms/step - loss: 6.7221e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 239ms/step - loss: 6.7008e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 239ms/step - loss: 6.6815e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 3s 239ms/step - loss: 6.6590e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 238ms/step - loss: 6.6347e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 237ms/step - loss: 6.6095e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 236ms/step - loss: 6.5858e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 2s 235ms/step - loss: 6.5622e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 234ms/step - loss: 6.5365e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 233ms/step - loss: 6.5078e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 233ms/step - loss: 6.4759e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 232ms/step - loss: 6.4409e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 231ms/step - loss: 6.4035e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 230ms/step - loss: 6.3641e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 229ms/step - loss: 6.3237e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 228ms/step - loss: 6.2819e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 227ms/step - loss: 6.2401e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 9s 241ms/step - loss: 6.2006e-05 - val_loss: 1.3707e-05
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 251ms/step - loss: 1.6961e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 234ms/step - loss: 1.4031e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 8s 244ms/step - loss: 1.3077e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 7s 243ms/step - loss: 1.2593e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 7s 257ms/step - loss: 1.4899e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 7s 257ms/step - loss: 1.8054e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 7s 258ms/step - loss: 2.3497e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 7s 262ms/step - loss: 3.0258e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 7s 262ms/step - loss: 3.5765e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 6s 263ms/step - loss: 3.9725e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 6s 263ms/step - loss: 4.2695e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 6s 263ms/step - loss: 4.4943e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 6s 263ms/step - loss: 4.6564e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 5s 262ms/step - loss: 4.7723e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 5s 260ms/step - loss: 4.8494e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 5s 257ms/step - loss: 4.8968e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 254ms/step - loss: 4.9241e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 4s 253ms/step - loss: 4.9359e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 4s 251ms/step - loss: 4.9334e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 250ms/step - loss: 4.9217e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 250ms/step - loss: 4.9103e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 250ms/step - loss: 4.9036e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 3s 248ms/step - loss: 4.9007e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 247ms/step - loss: 4.9029e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 246ms/step - loss: 4.9117e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 246ms/step - loss: 4.9266e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 2s 245ms/step - loss: 4.9449e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 244ms/step - loss: 4.9668e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 244ms/step - loss: 4.9869e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 245ms/step - loss: 5.0023e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 246ms/step - loss: 5.0128e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 247ms/step - loss: 5.0187e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 248ms/step - loss: 5.0211e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 247ms/step - loss: 5.0200e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 248ms/step - loss: 5.0158e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 252ms/step - loss: 5.0095e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 10s 268ms/step - loss: 5.0035e-05 - val_loss: 1.4933e-05
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 10s 300ms/step - loss: 2.0780e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 8s 245ms/step - loss: 2.0193e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 237ms/step - loss: 2.1324e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 7s 234ms/step - loss: 2.2925e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 7s 236ms/step - loss: 2.5446e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 7s 253ms/step - loss: 2.7816e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 7s 256ms/step - loss: 2.9282e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 7s 254ms/step - loss: 3.0023e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 6s 256ms/step - loss: 3.0651e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 6s 257ms/step - loss: 3.1030e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 6s 257ms/step - loss: 3.1655e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 6s 255ms/step - loss: 3.2813e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 5s 257ms/step - loss: 3.4507e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 5s 256ms/step - loss: 3.6084e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 5s 255ms/step - loss: 3.7419e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 5s 255ms/step - loss: 3.8642e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 253ms/step - loss: 3.9644e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 4s 250ms/step - loss: 4.0448e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 4s 248ms/step - loss: 4.1111e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 246ms/step - loss: 4.1654e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 244ms/step - loss: 4.2071e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 244ms/step - loss: 4.2378e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 3s 243ms/step - loss: 4.2590e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 241ms/step - loss: 4.2716e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 240ms/step - loss: 4.2771e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 238ms/step - loss: 4.2770e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 2s 237ms/step - loss: 4.2722e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 237ms/step - loss: 4.2635e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 236ms/step - loss: 4.2513e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 236ms/step - loss: 4.2371e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 237ms/step - loss: 4.2235e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 237ms/step - loss: 4.2158e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 238ms/step - loss: 4.2208e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 237ms/step - loss: 4.2335e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 238ms/step - loss: 4.2479e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 238ms/step - loss: 4.2619e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 9s 254ms/step - loss: 4.2752e-05 - val_loss: 2.0702e-05
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 8s 237ms/step - loss: 2.2694e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 7s 224ms/step - loss: 2.4844e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 7s 224ms/step - loss: 2.5245e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 7s 235ms/step - loss: 2.4886e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 7s 233ms/step - loss: 2.4235e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 6s 233ms/step - loss: 2.3582e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 6s 229ms/step - loss: 2.2878e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 6s 229ms/step - loss: 2.2290e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 6s 227ms/step - loss: 2.1855e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 5s 231ms/step - loss: 2.1712e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 5s 234ms/step - loss: 2.1835e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 5s 238ms/step - loss: 2.2451e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 5s 235ms/step - loss: 2.3416e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 5s 233ms/step - loss: 2.4478e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 4s 231ms/step - loss: 2.5381e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 4s 230ms/step - loss: 2.6079e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 4s 228ms/step - loss: 2.6576e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 4s 229ms/step - loss: 2.6947e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 3s 228ms/step - loss: 2.7201e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 3s 227ms/step - loss: 2.7431e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 3s 227ms/step - loss: 2.7748e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 3s 226ms/step - loss: 2.8154e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 2s 226ms/step - loss: 2.8728e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 2s 225ms/step - loss: 2.9359e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 2s 225ms/step - loss: 2.9978e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 2s 224ms/step - loss: 3.0521e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 2s 224ms/step - loss: 3.1006e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 1s 223ms/step - loss: 3.1429e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 1s 223ms/step - loss: 3.1784e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 1s 223ms/step - loss: 3.2082e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 1s 223ms/step - loss: 3.2336e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 222ms/step - loss: 3.2551e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 222ms/step - loss: 3.2735e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 222ms/step - loss: 3.2891e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 222ms/step - loss: 3.3012e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 223ms/step - loss: 3.3110e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 9s 240ms/step - loss: 3.3203e-05 - val_loss: 6.2806e-06</code></pre>
</div>
</div>
<div id="cell-fig-history_vsn" class="cell" data-execution_count="63">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_vsn.history).plot()</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_vsn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_vsn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Losses of the model with variable selection
</figcaption>
<div aria-describedby="fig-history_vsn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_vsn-output-1.png" width="614" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-transf" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="sec-transf"><span class="header-section-number">10</span> Now is a(nother) good time to pay attention</h2>
<p>The transformer architecture (<span class="citation" data-cites="vaswani2023attentionneed">Vaswani et al. (<a href="#ref-vaswani2023attentionneed" role="doc-biblioref">2017</a>)</span>) is based on an altogether different way to look at time series compared to recurrent neural networks as the LSTMs above. Its basic idea is to look at the whole sequence at the same time instead of sequentially. This allows the network to learn relevant connections between data that might be far off in time.</p>
<p>Starting with just one frequency for clarity, consider we input time windows of size <span class="math inline">\(N = k + \tau_{\text{max}}\)</span>, for <span class="math inline">\(k\)</span> the maximum number of lags and <span class="math inline">\(\tau_{\text{max}}\)</span> the number of forecasted steps. Each of these time steps is associated with a value vector, <span class="math inline">\(v \in \mathbb{R}^{\lambda_{\text{value}}}\)</span>; the time series of these values is <span class="math inline">\(\mathbf{V} \in \mathbb{R}^{N \times \lambda_{\text{value}}}\)</span>. The self-attention mechanism will scale the <span class="math inline">\(\mathbf{V}\)</span> to reflect the similarity between two other vectors also associated with each time steps.</p>
<p>These vectors are their keys and queries, respectively <span class="math inline">\(\mathbf{K}, \mathbf{Q} \in \mathbb{R}^{\lambda_{\text{attention}}}\)</span>. “Attention” refers to how their similarity is calculated. Essentially, key vectors that are more similar to query vectors will generate larger numbers of “attention”; similarly, if they are orthogonal, this will result in a low or null attention between a pair of time steps. We follow common practice and calculate the attention weights using the dot-product between the key and query vectors, normalised by softmax to sum to one and span only the positive space:</p>
<p><span class="math inline">\(A(\mathbf{Q}, \mathbf{K}) = \text{Softmax}(\mathbf{Q}\mathbf{K}'/\sqrt{\lambda_{\text{attention}}})\)</span>.</p>
<div id="class-scaleddotproductattention" class="cell" data-execution_count="64">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScaledDotProductAttention(keras.Layer):</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.0</span>, <span class="co"># Will be ignored if `training=False`</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ScaledDotProductAttention, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ScaledDotProductAttention, <span class="va">self</span>).build(input_shape)</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> keras.layers.Dropout(rate<span class="op">=</span><span class="va">self</span>.dropout_rate)</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> keras.layers.Activation(<span class="st">'softmax'</span>)</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dot_22 <span class="op">=</span> keras.layers.Dot(axes<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="co"># both inputs need to have the same size in their axis=2 dimensions, in this case, d_model for both</span></span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dot_21 <span class="op">=</span> keras.layers.Dot(axes<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>)) <span class="co"># the size of the first input's axis=2 dimension needs to match the size of the second input's axis=1 dimension</span></span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lambda_layer <span class="op">=</span> keras.layers.Lambda(<span class="kw">lambda</span> x: (<span class="op">-</span><span class="fl">1e9</span>) <span class="op">*</span> (<span class="fl">1.</span> <span class="op">-</span> keras.ops.cast(x, <span class="st">'float32'</span>)))</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add <span class="op">=</span> keras.layers.Add()</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a>        q, <span class="co"># Queries, tensor of shape (?, time, D_model)</span></span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a>        k, <span class="co"># Keys, tensor of shape (?, time, D_model)</span></span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>        v, <span class="co"># Values, tensor of shape (?, time, D_model)</span></span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>        mask, <span class="co"># Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)</span></span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span>, <span class="co"># Whether the layer is being trained or used in inference</span></span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># returns Tuple (layer outputs, attention weights)</span></span>
<span id="cb84-28"><a href="#cb84-28" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> keras.ops.sqrt(keras.ops.cast(keras.ops.shape(k)[<span class="op">-</span><span class="dv">1</span>], dtype<span class="op">=</span><span class="st">'float32'</span>))</span>
<span id="cb84-29"><a href="#cb84-29" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> <span class="va">self</span>.dot_22([q, k]) <span class="op">/</span> scale</span>
<span id="cb84-30"><a href="#cb84-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">#attention = keras.ops.einsum("bij,bjk-&gt;bik", q, keras.ops.transpose(k, axes=(0, 2, 1))) / scale</span></span>
<span id="cb84-31"><a href="#cb84-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb84-32"><a href="#cb84-32" aria-hidden="true" tabindex="-1"></a>            mmask <span class="op">=</span> <span class="va">self</span>.lambda_layer(mask)</span>
<span id="cb84-33"><a href="#cb84-33" aria-hidden="true" tabindex="-1"></a>            attention <span class="op">=</span> <span class="va">self</span>.add([attention, mmask])</span>
<span id="cb84-34"><a href="#cb84-34" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> <span class="va">self</span>.activation(attention)</span>
<span id="cb84-35"><a href="#cb84-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb84-36"><a href="#cb84-36" aria-hidden="true" tabindex="-1"></a>            attention <span class="op">=</span> <span class="va">self</span>.dropout(attention)</span>
<span id="cb84-37"><a href="#cb84-37" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.dot_21([attention, v])</span>
<span id="cb84-38"><a href="#cb84-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = keras.ops.einsum("btt,btd-&gt;bt", attention, v)</span></span>
<span id="cb84-39"><a href="#cb84-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="65">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb85-12"><a href="#cb85-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb85-13"><a href="#cb85-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb85-14"><a href="#cb85-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-15"><a href="#cb85-15" aria-hidden="true" tabindex="-1"></a>Attns <span class="op">=</span> []</span>
<span id="cb85-16"><a href="#cb85-16" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb85-17"><a href="#cb85-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb85-18"><a href="#cb85-18" aria-hidden="true" tabindex="-1"></a>    attn, attn_weights <span class="op">=</span> ScaledDotProductAttention(name<span class="op">=</span><span class="ss">f"Attention__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(q<span class="op">=</span>v, k<span class="op">=</span>v, v<span class="op">=</span>v, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb85-19"><a href="#cb85-19" aria-hidden="true" tabindex="-1"></a>    Attns.append(attn)</span>
<span id="cb85-20"><a href="#cb85-20" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(attn)</span>
<span id="cb85-21"><a href="#cb85-21" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb85-22"><a href="#cb85-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-23"><a href="#cb85-23" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Concatenate(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb85-24"><a href="#cb85-24" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb85-25"><a href="#cb85-25" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb85-26"><a href="#cb85-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-27"><a href="#cb85-27" aria-hidden="true" tabindex="-1"></a>nn_sdpa <span class="op">=</span> keras.Model(</span>
<span id="cb85-28"><a href="#cb85-28" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb85-29"><a href="#cb85-29" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb85-30"><a href="#cb85-30" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"Attention"</span></span>
<span id="cb85-31"><a href="#cb85-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb85-32"><a href="#cb85-32" aria-hidden="true" tabindex="-1"></a>nn_sdpa.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb85-33"><a href="#cb85-33" aria-hidden="true" tabindex="-1"></a>nn_sdpa.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_sdpa" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="65">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_sdpa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10: Summary of network with attention
</figcaption>
<div aria-describedby="tbl-summary_sdpa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "Attention"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ m (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ d (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__m         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__d         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ Attention__freq_m   │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ScaledDotProductA…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│                     │ <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)]            │            │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ Attention__freq_d   │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ScaledDotProductA…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│                     │ <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)]            │            │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ Attention__freq_… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ Attention__freq_… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>)       │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_31 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">528</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_32 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense_31[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]    │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,833</span> (18.88 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,833</span> (18.88 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_sdpa" class="cell" data-execution_count="66">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_sdpa, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="66">
<div id="fig-arch_sdpa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_sdpa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Architecture of the multi-frequency attention Model
</figcaption>
<div aria-describedby="fig-arch_sdpa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_sdpa-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>Note that in this simple model, the way the different frequencies are combined in <a href="#fig-arch_sdpa" class="quarto-xref">Figure&nbsp;27</a> is by passing the result of the respective self-attention layers (each with their own size along the time dimension due to different frequencies) through their own LSTM layer returning one value. This value is then weighted by a dense network, producing the output.</p>
<div id="train-the-lstm-with-attention" class="cell" data-execution_count="67">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>history_sdpa <span class="op">=</span> nn_sdpa.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 11:28 20s/step - loss: 8.8683e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 61ms/step - loss: 8.1319e-06   3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 62ms/step - loss: 8.8584e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 1.5140e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.8020e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.9266e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 1.9890e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.0404e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.0734e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.1052e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.1205e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.1215e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.1176e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.1086e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.1003e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.0899e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.0771e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.0614e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 2.0433e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 0s 62ms/step - loss: 2.0243e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 2.0054e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.9889e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.9716e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.9535e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.9361e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.9185e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.9026e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8884e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8742e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8601e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8457e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8333e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8227e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8128e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.8027e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.7930e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 27s 198ms/step - loss: 1.7838e-05 - val_loss: 1.5373e-05
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 95ms/step - loss: 1.6187e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.3860e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.3382e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.3843e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.6984e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.8451e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.9602e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 2.0094e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 2.0258e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 2.0299e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 2.0298e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 2.0187e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 2.0020e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.9802e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.9577e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.9332e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.9112e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.8934e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.8756e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.8576e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.8393e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.8210e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.8037e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.7866e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.7708e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.7555e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.7409e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.7259e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.7108e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6955e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6808e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6670e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6544e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6447e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6367e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.6286e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 69ms/step - loss: 1.6211e-05 - val_loss: 7.3595e-06
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - loss: 1.5662e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 63ms/step - loss: 1.5507e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - loss: 1.5505e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 63ms/step - loss: 1.5078e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.4440e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.3840e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.3446e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.3185e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2947e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2776e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2567e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2437e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2346e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2283e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2216e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2145e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2070e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2000e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1951e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2009e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2043e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2077e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2102e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2112e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2117e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2108e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2102e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2101e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2111e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2112e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2109e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2102e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2093e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2079e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2061e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2042e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 73ms/step - loss: 1.2024e-05 - val_loss: 6.9014e-06
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 1.2369e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.0464e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.0640e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.1991e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.2279e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2420e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2308e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2172e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1982e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1956e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1904e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1856e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1774e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1672e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1561e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1436e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1319e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1204e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1096e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.0994e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0999e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1027e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1054e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1071e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1088e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1104e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1115e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1124e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1128e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1129e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1125e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1122e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1125e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1132e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1138e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1139e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 69ms/step - loss: 1.1140e-05 - val_loss: 6.8049e-06
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 2.0013e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 2.0383e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 1.8612e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.7016e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.6072e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.5137e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4393e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3735e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3700e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3616e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.3506e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3404e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3272e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3133e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3043e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2945e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.2897e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.2849e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2802e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2748e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2702e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2650e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2608e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2566e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2517e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2467e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2419e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2384e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2352e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2319e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2285e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2254e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2216e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2186e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2169e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2152e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.2136e-05 - val_loss: 8.2752e-06
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 1.0332e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.2046e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.2427e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.1913e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 1.2271e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.2336e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2471e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2435e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2398e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2344e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2293e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2208e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2199e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2156e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2109e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2055e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2005e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.1946e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1913e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.1902e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1890e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1863e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1916e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1959e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1988e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.2000e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1996e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1982e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1961e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1942e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1932e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1924e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1911e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1893e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1882e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1869e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 69ms/step - loss: 1.1856e-05 - val_loss: 9.0406e-06
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 3.6613e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 5.7438e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 6.6088e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 7.1889e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 7.3093e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 7.4132e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 7.7972e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 8.2144e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 8.7247e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 9.0764e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.3181e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 9.4913e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 9.6331e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.7194e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.8106e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.9135e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.9968e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.0046e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.0095e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.0146e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0290e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0415e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0524e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0640e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0732e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0807e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0874e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0924e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0961e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.0992e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1021e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1042e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1058e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 64ms/step - loss: 1.1073e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.1084e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.1090e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.1096e-05 - val_loss: 6.7683e-06
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 116ms/step - loss: 5.0142e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 6.9891e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 7.3984e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - loss: 8.6151e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 9.5780e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.0133e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.0413e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.0607e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.0680e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 1.0676e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0664e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0628e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0640e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0642e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.0603e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0569e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0551e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0533e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0513e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0502e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0509e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0524e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0544e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0578e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0613e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0655e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0683e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0702e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0713e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0719e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0767e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0807e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0836e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0858e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0875e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.0888e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - loss: 1.0901e-05 - val_loss: 6.7855e-06
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 106ms/step - loss: 9.7834e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 9.3988e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 9.7446e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.0225e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 1.0508e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0711e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0718e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0669e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0558e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.0619e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.0637e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.0722e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0805e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0847e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0864e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 1.0844e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 1.0812e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0774e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.0725e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0670e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0613e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0655e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.0699e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 77ms/step - loss: 1.0726e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0753e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step - loss: 1.0768e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0775e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 1.0772e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.0769e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.0761e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.0756e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 1.0750e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 1.0742e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 1.0751e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 1.0756e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.0766e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 78ms/step - loss: 1.0775e-05 - val_loss: 7.7236e-06
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 3.1246e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 62ms/step - loss: 3.2281e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 61ms/step - loss: 3.2931e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - loss: 4.1586e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 4.8706e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 5.2806e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 5.5237e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 5.7558e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 5.9102e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 6.0347e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 6.1989e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 6.6656e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 7.0587e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 7.4082e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 7.6995e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 7.9538e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.1628e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.3516e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.5091e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.6461e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 8.7837e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 8.8934e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 8.9887e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 9.0696e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 9.1630e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 9.2440e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 9.3161e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 9.3957e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.4682e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.5245e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.5992e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.6709e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.7344e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.7861e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.8370e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 75ms/step - loss: 9.8817e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 81ms/step - loss: 9.9239e-06 - val_loss: 7.2865e-06
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 119ms/step - loss: 1.4622e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.1907e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.1922e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1603e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1431e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.1112e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.0783e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.0487e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.0356e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.0265e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.0258e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.0245e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0202e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0203e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0207e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0213e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0192e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0184e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.0158e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0126e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0122e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0150e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0165e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0182e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.0186e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.0209e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0220e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0226e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0231e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0281e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0331e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0373e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0414e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0448e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0475e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0498e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - loss: 1.0519e-05 - val_loss: 9.0383e-06
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 88ms/step - loss: 4.1975e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.6581e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.8682e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.8857e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.8364e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.7745e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.7165e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 72ms/step - loss: 1.6624e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 1.6156e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 1.5704e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 1.5322e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 1.4943e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 1.4610e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.4291e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.4055e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.3834e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.3660e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.3502e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.3350e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.3194e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.3049e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.2916e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2808e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2711e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2637e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2558e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2477e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2400e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step - loss: 1.2323e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2254e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2200e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2164e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2125e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2085e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2059e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.2034e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 77ms/step - loss: 1.2011e-05 - val_loss: 9.7328e-06
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 112ms/step - loss: 9.2490e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1228e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.2749e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.2957e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 1.3073e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.2923e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.2715e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.2555e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.2341e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.2093e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1880e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1689e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1594e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1486e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.1373e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.1265e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.1165e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.1094e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.1036e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.0984e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0943e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0915e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0886e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0875e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0857e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0839e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0826e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0806e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0782e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0760e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0736e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0712e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0695e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0679e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0707e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0730e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 85ms/step - loss: 1.0753e-05 - val_loss: 8.2707e-06
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 101ms/step - loss: 3.3720e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 5.5723e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 6.9758e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 7.4280e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 7.6420e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 7.7193e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.8445e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.0850e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.3104e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.4855e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.5827e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.6789e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.7664e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9290e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.0612e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.1446e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.2315e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.2905e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.3564e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.3911e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.4143e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.4363e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.5190e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.6026e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.6807e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.7748e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.8611e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 9.9326e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.9921e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0044e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0086e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0134e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0172e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0203e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0232e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0256e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.0278e-05 - val_loss: 1.3493e-05
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 114ms/step - loss: 6.3026e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 88ms/step - loss: 5.3083e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 5.2091e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 5.2735e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 6.1415e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 6.9849e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.4512e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 8.1213e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 8.6089e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 8.9999e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.2586e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.4674e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.6171e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.7346e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.8159e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.8839e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.9164e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.9403e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.9539e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 9.9646e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.9564e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.9597e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.9540e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.9389e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.9219e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.9020e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.8841e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.8693e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.8565e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.8439e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.8304e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.8710e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.9098e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.9424e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.9743e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.0013e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - loss: 1.0050e-05 - val_loss: 8.2233e-06
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 105ms/step - loss: 9.0954e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 8.4795e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 7.5801e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 7.6542e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 7.8495e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 7.8849e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 7.8877e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.0589e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.1066e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 8.1127e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 8.1327e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.1917e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 8.2430e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 8.2922e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.3555e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.4051e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 8.4438e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 8.5418e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 8.6112e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 8.7783e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 73ms/step - loss: 8.9199e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 74ms/step - loss: 9.0468e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.1503e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 9.2498e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 9.3394e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.4210e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.4901e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.5561e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.6255e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.6921e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.7575e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.8244e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.8856e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.9373e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.9806e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 1.0015e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 79ms/step - loss: 1.0048e-05 - val_loss: 6.8343e-06
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 1.8927e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 1.5990e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.4352e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.3729e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.3145e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2610e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2434e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2294e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2209e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2131e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2047e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1963e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2152e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2281e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2348e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2406e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2433e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2428e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2413e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2404e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.2381e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2348e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2318e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2287e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2252e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2227e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2230e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2219e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2199e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2181e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2158e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.2129e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2105e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2084e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2074e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.2063e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - loss: 1.2053e-05 - val_loss: 7.4114e-06
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 87ms/step - loss: 3.2245e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 3.5200e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - loss: 3.6599e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - loss: 4.1747e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 4.9256e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 5.5817e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 6.1973e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 6.5949e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 6.9637e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 7.2339e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 7.4162e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 7.5843e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 7.7521e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.9726e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.1507e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.4998e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 8.7865e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.0264e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.2365e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.4194e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.6001e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.7562e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.8855e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 9.9964e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0083e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0162e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0238e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0301e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0354e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0398e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0435e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0461e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.0486e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0502e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0522e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.0536e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - loss: 1.0549e-05 - val_loss: 7.0583e-06
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 7.3435e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.2155e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - loss: 1.3108e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 63ms/step - loss: 1.2807e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 1.2344e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.1986e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 1.1747e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 1.1517e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.1369e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.1209e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.1017e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0854e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0762e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0665e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0551e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0472e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0444e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0432e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0410e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 1.0381e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0350e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0398e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0442e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0491e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0540e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0574e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0599e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0616e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0621e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0638e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0650e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0671e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0687e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0699e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0713e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step - loss: 1.0726e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.0738e-05 - val_loss: 6.7758e-06
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 6.5666e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 6.3294e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 62ms/step - loss: 6.4732e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 1s 61ms/step - loss: 6.7027e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 62ms/step - loss: 6.9898e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 61ms/step - loss: 7.1127e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 63ms/step - loss: 7.1873e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 7.2339e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.2696e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.2515e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.2403e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.3352e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.4227e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.5076e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.5749e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.6405e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.6928e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.7457e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.7873e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.8232e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 7.8689e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.0008e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.1415e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.2554e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 8.3532e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 8.4428e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 8.5268e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 8.6195e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 8.6960e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 8.7783e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 8.8511e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 8.9132e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 8.9871e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 73ms/step - loss: 9.0494e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.1076e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 74ms/step - loss: 9.1675e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 80ms/step - loss: 9.2241e-06 - val_loss: 8.4181e-06</code></pre>
</div>
</div>
<div id="cell-fig-history_sdpa" class="cell" data-execution_count="68">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_sdpa.history).plot()</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_sdpa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_sdpa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Losses of the model with attention
</figcaption>
<div aria-describedby="fig-history_sdpa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_sdpa-output-1.png" width="589" height="443" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>Taking this concept of an attention layer further is the interpretable multi-head attention layer. This is just a combination of different scaled dot-product attention layers with the following actions:</p>
<ul>
<li><p>linear transformation of the query, keys and values before being passed to each attention layer,</p></li>
<li><p>concatenation and a linear combination of the results of the layer.</p></li>
</ul>
<p>In effect, this allows the attention layers to attend to different aspects of the time series. For example, one attention head might focus on seasonality issues, while another on non-linearities and jumps in volatility.</p>
<p>Note that there is no way to know in advance what aspects each head will be focusing on. However, this can be inspected (and subjectively interpreted) ex post.</p>
<div id="class-interpretablemultiheadattention" class="cell" data-execution_count="69">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InterpretableMultiHeadAttention(keras.Layer):</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>        n_head:<span class="bu">int</span><span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.0</span>, <span class="co"># Will be ignored if `training=False`</span></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(InterpretableMultiHeadAttention, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> n_head</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_qk <span class="op">=</span> <span class="va">self</span>.d_v <span class="op">=</span> d_model <span class="op">//</span> n_head <span class="co"># the original model divides by number of heads</span></span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb90-16"><a href="#cb90-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(InterpretableMultiHeadAttention, <span class="va">self</span>).build(input_shape)</span>
<span id="cb90-17"><a href="#cb90-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb90-18"><a href="#cb90-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># using the same value layer facilitates interpretability</span></span>
<span id="cb90-19"><a href="#cb90-19" aria-hidden="true" tabindex="-1"></a>        vs_layer <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_v, use_bias<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"shared_attn_value"</span>)</span>
<span id="cb90-20"><a href="#cb90-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-21"><a href="#cb90-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># creates list of queries, keys and values across heads</span></span>
<span id="cb90-22"><a href="#cb90-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qs_layers <span class="op">=</span> [keras.layers.Dense(<span class="va">self</span>.d_qk) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head)]</span>
<span id="cb90-23"><a href="#cb90-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ks_layers <span class="op">=</span> [keras.layers.Dense(<span class="va">self</span>.d_qk) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head)]</span>
<span id="cb90-24"><a href="#cb90-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vs_layers <span class="op">=</span> [vs_layer <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head)]</span>
<span id="cb90-25"><a href="#cb90-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-26"><a href="#cb90-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> ScaledDotProductAttention(dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate)</span>
<span id="cb90-27"><a href="#cb90-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_o <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, use_bias<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"W_v"</span>) <span class="co"># W_v in Eqs. (14)-(16), output weight matrix to project internal state to the original TFT</span></span>
<span id="cb90-28"><a href="#cb90-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> keras.layers.Dropout(<span class="va">self</span>.dropout_rate)</span>
<span id="cb90-29"><a href="#cb90-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-30"><a href="#cb90-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb90-31"><a href="#cb90-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb90-32"><a href="#cb90-32" aria-hidden="true" tabindex="-1"></a>        q, <span class="co"># Queries, tensor of shape (?, time, d_model)</span></span>
<span id="cb90-33"><a href="#cb90-33" aria-hidden="true" tabindex="-1"></a>        k, <span class="co"># Keys, tensor of shape (?, time, d_model)</span></span>
<span id="cb90-34"><a href="#cb90-34" aria-hidden="true" tabindex="-1"></a>        v, <span class="co"># Values, tensor of shape (?, time, d_model)</span></span>
<span id="cb90-35"><a href="#cb90-35" aria-hidden="true" tabindex="-1"></a>        mask<span class="op">=</span><span class="va">None</span>, <span class="co"># Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)</span></span>
<span id="cb90-36"><a href="#cb90-36" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb90-37"><a href="#cb90-37" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb90-38"><a href="#cb90-38" aria-hidden="true" tabindex="-1"></a>        heads <span class="op">=</span> []</span>
<span id="cb90-39"><a href="#cb90-39" aria-hidden="true" tabindex="-1"></a>        attns <span class="op">=</span> []</span>
<span id="cb90-40"><a href="#cb90-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head):</span>
<span id="cb90-41"><a href="#cb90-41" aria-hidden="true" tabindex="-1"></a>            qs <span class="op">=</span> <span class="va">self</span>.qs_layers[i](q)</span>
<span id="cb90-42"><a href="#cb90-42" aria-hidden="true" tabindex="-1"></a>            ks <span class="op">=</span> <span class="va">self</span>.ks_layers[i](q)</span>
<span id="cb90-43"><a href="#cb90-43" aria-hidden="true" tabindex="-1"></a>            vs <span class="op">=</span> <span class="va">self</span>.vs_layers[i](v)</span>
<span id="cb90-44"><a href="#cb90-44" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb90-45"><a href="#cb90-45" aria-hidden="true" tabindex="-1"></a>            head, attn <span class="op">=</span> <span class="va">self</span>.attention(qs, ks, vs, mask, training<span class="op">=</span>training)</span>
<span id="cb90-46"><a href="#cb90-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> training:</span>
<span id="cb90-47"><a href="#cb90-47" aria-hidden="true" tabindex="-1"></a>                head <span class="op">=</span> <span class="va">self</span>.dropout(head)</span>
<span id="cb90-48"><a href="#cb90-48" aria-hidden="true" tabindex="-1"></a>            heads.append(head)</span>
<span id="cb90-49"><a href="#cb90-49" aria-hidden="true" tabindex="-1"></a>            attns.append(attn)</span>
<span id="cb90-50"><a href="#cb90-50" aria-hidden="true" tabindex="-1"></a>        head <span class="op">=</span> keras.ops.stack(heads) <span class="cf">if</span> <span class="va">self</span>.n_head <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> heads[<span class="dv">0</span>]</span>
<span id="cb90-51"><a href="#cb90-51" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> keras.ops.stack(attns)</span>
<span id="cb90-52"><a href="#cb90-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-53"><a href="#cb90-53" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> keras.ops.mean(head, axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> <span class="va">self</span>.n_head <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> head <span class="co"># H_tilde</span></span>
<span id="cb90-54"><a href="#cb90-54" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.w_o(outputs)</span>
<span id="cb90-55"><a href="#cb90-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb90-56"><a href="#cb90-56" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> <span class="va">self</span>.dropout(outputs)</span>
<span id="cb90-57"><a href="#cb90-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-58"><a href="#cb90-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, attn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="70">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>Attns <span class="op">=</span> []</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>    attn, attn_weights <span class="op">=</span> InterpretableMultiHeadAttention(name<span class="op">=</span><span class="ss">f"IMHA__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(q<span class="op">=</span>v, k<span class="op">=</span>v, v<span class="op">=</span>v, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>    Attns.append(attn)</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(attn)</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Concatenate(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>nn_imha <span class="op">=</span> keras.Model(</span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"Attention"</span></span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a>nn_imha.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>nn_imha.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-imha" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="70">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-imha-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11: Summary of network with interpretable multi-head attention
</figcaption>
<div aria-describedby="tbl-imha-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "Attention"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ m (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ d (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__m         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__d         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ IMHA__freq_m        │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │        <span style="color: #00af00; text-decoration-color: #00af00">672</span> │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InterpretableMult…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00af00; text-decoration-color: #00af00">4</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,    │            │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│                     │ <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)]      │            │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ IMHA__freq_d        │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │        <span style="color: #00af00; text-decoration-color: #00af00">672</span> │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InterpretableMult…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00af00; text-decoration-color: #00af00">4</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,    │            │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│                     │ <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)]      │            │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ IMHA__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ IMHA__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>)       │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_51 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">528</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_52 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense_51[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]    │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">6,177</span> (24.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">6,177</span> (24.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_imha" class="cell" data-execution_count="71">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_imha, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="71">
<div id="fig-arch_imha" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_imha-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: Architecture of the model with multi-frequency interpretable multi-head attention
</figcaption>
<div aria-describedby="fig-arch_imha-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_imha-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<div id="train-the-lstm-with-multi-head-attention" class="cell" data-execution_count="72">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a>history_imha <span class="op">=</span> nn_imha.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 16:31 28s/step - loss: 1.6899e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 8.9567e-05   3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 99ms/step - loss: 1.2813e-04 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 1.3547e-04 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 1.3410e-04 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 1.2981e-04 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 97ms/step - loss: 1.2468e-04 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 1.1955e-04 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 1.1465e-0410/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 1.1010e-0411/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 1.0588e-0412/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 1.0199e-0413/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 9.8376e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 9.5054e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 9.1997e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 8.9168e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 8.6562e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 93ms/step - loss: 8.4315e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 8.2197e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 8.0187e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 7.8297e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 7.6505e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 7.4810e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 7.3211e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 7.1696e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 7.0251e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.8873e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.7587e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.6360e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.5190e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.4105e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.3142e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.2210e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.1314e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 98ms/step - loss: 6.0449e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 99ms/step - loss: 5.9620e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 37s 257ms/step - loss: 5.8836e-05 - val_loss: 7.3108e-06
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 102ms/step - loss: 8.5129e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 7.9552e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 8.0406e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 8.2438e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 8.2837e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 8.2383e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 8.4004e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 8.4597e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 8.5383e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.6954e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 9.1754e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 9.5206e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 9.8046e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 9.9893e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0129e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0255e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0334e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0397e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.0432e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0490e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0529e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0570e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.0603e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.0629e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0648e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0677e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.0692e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0701e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0700e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0695e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.0691e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0687e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0705e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0714e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0725e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.0734e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 1.0742e-05 - val_loss: 7.2133e-06
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 137ms/step - loss: 5.6108e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 6.2809e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 99ms/step - loss: 7.0985e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 7.3673e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 3s 99ms/step - loss: 7.6197e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 3s 101ms/step - loss: 7.8470e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 100ms/step - loss: 8.1285e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 99ms/step - loss: 8.2197e-06  9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 98ms/step - loss: 8.2881e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 98ms/step - loss: 8.3104e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 97ms/step - loss: 8.3699e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 2s 97ms/step - loss: 8.4339e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 8.4951e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 8.5240e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 8.5410e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 8.5606e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 8.5895e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 8.6515e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 8.7055e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 8.7590e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 96ms/step - loss: 8.7900e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 8.8189e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 8.8558e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 8.8847e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 8.9058e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 94ms/step - loss: 8.9442e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 93ms/step - loss: 8.9831e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 93ms/step - loss: 9.0144e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 92ms/step - loss: 9.0416e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 92ms/step - loss: 9.0650e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 91ms/step - loss: 9.0903e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 91ms/step - loss: 9.1093e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 91ms/step - loss: 9.1300e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 91ms/step - loss: 9.1476e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 91ms/step - loss: 9.1623e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 91ms/step - loss: 9.2155e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 4s 97ms/step - loss: 9.2660e-06 - val_loss: 1.0030e-05
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 112ms/step - loss: 8.4165e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.0715e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1730e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.2266e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.2216e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.2247e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.2331e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.3046e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.3445e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.3758e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.3931e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.4008e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.4005e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 1.4001e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.3988e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 95ms/step - loss: 1.3957e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 1.3895e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - loss: 1.3856e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 99ms/step - loss: 1.3808e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 99ms/step - loss: 1.3750e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 1.3722e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 1.3686e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 1.3635e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 98ms/step - loss: 1.3579e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 1s 97ms/step - loss: 1.3523e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - loss: 1.3465e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - loss: 1.3399e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 97ms/step - loss: 1.3327e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.3252e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.3190e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.3130e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.3070e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.3017e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.2965e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.2919e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 96ms/step - loss: 1.2877e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 4s 103ms/step - loss: 1.2837e-05 - val_loss: 7.2400e-06
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 130ms/step - loss: 1.7718e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 1.4429e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 1.3286e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 1.2365e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.1812e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 89ms/step - loss: 1.1304e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.0880e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.0595e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.0422e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.0219e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - loss: 1.0034e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 9.8619e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 9.7167e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 9.5784e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step - loss: 9.6812e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 9.7799e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 9.8561e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 9.9290e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 9.9980e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 1.0070e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - loss: 1.0144e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.0202e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.0249e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.0289e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0323e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0367e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0401e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.0445e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.0478e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.0503e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.0519e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.0527e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0534e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0541e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0545e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0559e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.0572e-05 - val_loss: 6.9307e-06
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 100ms/step - loss: 1.6506e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.2723e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.1676e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1117e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1135e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.1147e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.1105e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.1031e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0935e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0899e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 1.0903e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0882e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0835e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1111e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1318e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1533e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1689e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1815e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1905e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.1982e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.2027e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.2087e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.2151e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2197e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2242e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.2274e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2293e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2307e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2319e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2324e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2320e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2311e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2295e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2272e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2246e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.2220e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 85ms/step - loss: 1.2195e-05 - val_loss: 6.9198e-06
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 112ms/step - loss: 4.4508e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 4.9508e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 6.5364e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 6.9753e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.9790e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 8.8227e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 9.3186e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 9.5224e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 9.7264e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 9.8331e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.9269e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 9.9939e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0073e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0347e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0562e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0741e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.0938e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 1.1107e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.1249e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.1363e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.1444e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.1505e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.1548e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.1584e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1624e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1656e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1687e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1709e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1721e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.1726e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1726e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1731e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1728e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1721e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1712e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.1704e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 1.1696e-05 - val_loss: 7.2859e-06
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 112ms/step - loss: 9.5618e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 1.2444e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.2461e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1998e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.1476e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1163e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.0931e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.0988e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.0943e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 1.0844e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0729e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0611e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0492e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0401e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0342e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0478e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0577e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0650e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0713e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.0749e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0765e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0769e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.0773e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0771e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0773e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0778e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0780e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0794e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0820e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0841e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0853e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0862e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0865e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0870e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0870e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0878e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.0886e-05 - val_loss: 6.8781e-06
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 104ms/step - loss: 2.4636e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 2.0133e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.8161e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.7726e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.7193e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.6782e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.6248e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.5728e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.5455e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.5204e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.4965e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.4797e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.4616e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4477e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4544e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4575e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4565e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4555e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.4540e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.4517e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.4479e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.4437e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.4407e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.4363e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.4307e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.4245e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 1.4179e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.4112e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 1.4050e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.3983e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.3919e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.3854e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.3791e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.3727e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.3660e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.3592e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 1.3528e-05 - val_loss: 9.3335e-06
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 120ms/step - loss: 1.2647e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 3s 97ms/step - loss: 1.1258e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 100ms/step - loss: 1.0919e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 1.0444e-05  5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 1.0161e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - loss: 9.8485e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 9.6870e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 9.5633e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 95ms/step - loss: 9.4219e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 96ms/step - loss: 9.7493e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.0014e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 93ms/step - loss: 1.0202e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.0444e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 2s 92ms/step - loss: 1.0647e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step - loss: 1.0813e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step - loss: 1.0946e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step - loss: 1.1034e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step - loss: 1.1106e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step - loss: 1.1155e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 90ms/step - loss: 1.1214e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step - loss: 1.1254e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step - loss: 1.1283e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step - loss: 1.1314e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - loss: 1.1341e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step - loss: 1.1353e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1371e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1382e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1381e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1382e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1395e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1408e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1416e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1420e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1418e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1413e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 87ms/step - loss: 1.1409e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 1.1406e-05 - val_loss: 9.4251e-06
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 120ms/step - loss: 7.2239e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 6.4067e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 7.7263e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 9.3813e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 89ms/step - loss: 1.0153e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 1.0371e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 91ms/step - loss: 1.0420e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 1.0391e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 90ms/step - loss: 1.0421e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 89ms/step - loss: 1.0379e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 1.0360e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 2s 87ms/step - loss: 1.0392e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.0420e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.0478e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.0520e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.0539e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.0572e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.0600e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.0616e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0613e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0599e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0595e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.0591e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0585e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0613e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0628e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0635e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0636e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0632e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0618e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0601e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 1.0583e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0562e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0538e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0555e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 1.0573e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 86ms/step - loss: 1.0589e-05 - val_loss: 7.1569e-06
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 99ms/step - loss: 3.1599e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 2.4952e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 2.2299e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 2.0316e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.9006e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.8099e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.7314e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.6575e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.5952e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.5405e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 1.4963e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4567e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4292e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.4022e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3786e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.3565e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.3375e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.3182e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.2999e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.2836e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2722e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2629e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2555e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2473e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2394e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2318e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2300e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2298e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2291e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2274e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2258e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2242e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2230e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2217e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2198e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2180e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.2163e-05 - val_loss: 6.8176e-06
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 114ms/step - loss: 9.7011e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 8.4604e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.9550e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 7.9641e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 8.3521e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 8.5548e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 8.6227e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 8.5912e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 8.5454e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 8.4896e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.4796e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 8.6165e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 8.7043e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 8.7589e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.8035e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.8250e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.8279e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.8466e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.8860e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.9075e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 8.9179e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 9.0146e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 9.0979e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.1705e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.2265e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.2729e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.3403e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.3959e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.4382e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 9.4794e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.5347e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.5895e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.6397e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.6809e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.7278e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.7698e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 85ms/step - loss: 9.8096e-06 - val_loss: 7.0814e-06
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 104ms/step - loss: 1.0731e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.0797e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.1822e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 1.1850e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.1842e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1805e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1713e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1587e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 1.1498e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.1823e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2082e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2227e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.2413e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2600e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2706e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2771e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2818e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2838e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2850e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2842e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2844e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.2834e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.2820e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2834e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2840e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2841e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2835e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2825e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2811e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2793e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2768e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2739e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2706e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2667e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2626e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2587e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 84ms/step - loss: 1.2549e-05 - val_loss: 7.7353e-06
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 104ms/step - loss: 6.4777e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 6.2505e-06  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 71ms/step - loss: 5.8908e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 5.5970e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 5.3648e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 5.3803e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 5.4378e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 77ms/step - loss: 5.5016e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 5.4986e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 5.5412e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 5.6378e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 5.8000e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 5.9395e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 6.0298e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 6.1015e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 6.1809e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 6.2584e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 6.5137e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 6.7640e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 6.9811e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 7.1622e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 7.3220e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 7.4859e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 7.6316e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 7.7803e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 7.9096e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 8.0294e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 8.1308e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 8.2176e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 86ms/step - loss: 8.3078e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 8.3976e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 8.4776e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 8.5608e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 8.6487e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 8.7233e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 85ms/step - loss: 8.7939e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 8.8607e-06 - val_loss: 6.9145e-06
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 98ms/step - loss: 8.1247e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 8.0682e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 8.6350e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 88ms/step - loss: 8.8394e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 8.7108e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.4937e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.2876e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.1268e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 8.0468e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 7.9959e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 7.9241e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 7.8967e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 7.8802e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 7.9044e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 7.9265e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 7.9521e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 7.9812e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 8.0400e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 8.0777e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 8.1037e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.1561e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 8.2053e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 8.2502e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.2989e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.3397e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.3731e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.3989e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.4197e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 8.4399e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 8.4618e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 8.4881e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.5164e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.5475e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.6180e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.6908e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 81ms/step - loss: 8.7564e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 88ms/step - loss: 8.8184e-06 - val_loss: 6.9384e-06
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 96ms/step - loss: 8.5333e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 7.7570e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.9907e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 6.5673e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.4096e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 6.4659e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 74ms/step - loss: 6.6333e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.7092e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 6.8027e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 75ms/step - loss: 6.8854e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.0102e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 7.1968e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 7.4037e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 81ms/step - loss: 7.6203e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 7.7935e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 7.9751e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.1298e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.2461e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.3280e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.4377e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.5315e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.6112e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 8.7088e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 8.7957e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 8.8924e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 80ms/step - loss: 8.9892e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.0675e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.1308e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.1803e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.2211e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.2615e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.2994e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.3289e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.3563e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.4222e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 9.4805e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 85ms/step - loss: 9.5357e-06 - val_loss: 6.9884e-06
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 99ms/step - loss: 4.5014e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 6.0493e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 6.8155e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 6.7600e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.2911e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.5710e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 79ms/step - loss: 7.7475e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 7.8959e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 8.0392e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 2s 82ms/step - loss: 8.1620e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 8.2861e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 8.3674e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 8.4562e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 8.5498e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 8.6292e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 8.7125e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 8.7691e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 8.8774e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 8.9976e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 9.0962e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 9.2721e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 9.4242e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 9.5545e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.6638e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.7552e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.8336e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.9090e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 9.9737e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0036e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0088e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0130e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0176e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0218e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0252e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.0282e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 82ms/step - loss: 1.0309e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.0334e-05 - val_loss: 6.9524e-06
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 100ms/step - loss: 2.2596e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 80ms/step - loss: 2.0136e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 78ms/step - loss: 1.8114e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.7311e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.6434e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.5865e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 76ms/step - loss: 1.5467e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.5070e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - loss: 1.4650e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.4320e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 76ms/step - loss: 1.4023e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.3934e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - loss: 1.3833e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 78ms/step - loss: 1.3702e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.3794e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.3850e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.3858e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.3850e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.3834e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.3801e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - loss: 1.3762e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.3713e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 79ms/step - loss: 1.3661e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.3609e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.3558e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.3501e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 79ms/step - loss: 1.3443e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3389e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3334e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3280e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3225e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3172e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3116e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3074e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.3029e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 78ms/step - loss: 1.2982e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 85ms/step - loss: 1.2937e-05 - val_loss: 7.0127e-06
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 106ms/step - loss: 1.3216e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 81ms/step - loss: 1.1394e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.0888e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.0474e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.0636e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1183e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 86ms/step - loss: 1.1401e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.1538e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 2s 85ms/step - loss: 1.1585e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 2s 84ms/step - loss: 1.1969e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 2s 83ms/step - loss: 1.2250e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2397e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 83ms/step - loss: 1.2531e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 82ms/step - loss: 1.2625e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.2661e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.2682e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.2678e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 86ms/step - loss: 1.2687e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.2674e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.2692e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.2691e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.2686e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - loss: 1.2669e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - loss: 1.2649e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2624e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2589e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2548e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2497e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2447e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2411e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 84ms/step - loss: 1.2375e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2341e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2304e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2266e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2238e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 83ms/step - loss: 1.2212e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.2187e-05 - val_loss: 8.4957e-06</code></pre>
</div>
</div>
<div id="cell-fig-history_imha" class="cell" data-execution_count="73">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>imha_loss <span class="op">=</span> history_imha.history</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>imha_loss[<span class="st">"val_loss_SDPA"</span>] <span class="op">=</span> history_sdpa.history[<span class="st">"val_loss"</span>]</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(imha_loss).plot()</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_imha" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_imha-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: Losses of the model with interpretable multi-head attention
</figcaption>
<div aria-describedby="fig-history_imha-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_imha-output-1.png" width="589" height="443" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-tftmf" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="sec-tftmf"><span class="header-section-number">11</span> Complete architecture</h2>
<p>This section uses the following notation, using as much as possible the original TFT paper notation.</p>
<ul>
<li>Panel data
<ul>
<li>individuals (countries, banks, households, etc) are indexed by <span class="math inline">\(i \in (1, \dots, I)\)</span>.</li>
<li>time steps are indexed by <span class="math inline">\(t \in (t0, t1, \dots, T)\)</span></li>
</ul></li>
<li>Input variables
<ul>
<li>static variables: <span class="math inline">\(s_i \in \mathbb{R}^{m_s}\)</span>, for <span class="math inline">\(m_s\)</span> the number of static variables for each individual <span class="math inline">\(i\)</span></li>
<li>time-dependent variables: <span class="math inline">\(\chi_{i,t} \in \mathbb{R}^{m_{\chi}}\)</span>, for <span class="math inline">\(m_{\chi}\)</span> the number of time-dependent variable</li>
</ul></li>
<li>Model dimensionality
<ul>
<li><span class="math inline">\(k(f)\)</span> is the number of lags for each frequency</li>
<li><span class="math inline">\(\tau_{\text{max}}\)</span> is the number of steps ahead that is forecasted for the dependent variable <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(N\)</span> is the number of time steps that feed into the attention layer</li>
</ul></li>
</ul>
<div id="class-tft" class="cell" data-execution_count="74">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TFT(keras.Model):</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>        quantiles<span class="op">=</span>[<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>],</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>        output_size:<span class="bu">int</span><span class="op">=</span><span class="dv">1</span>, <span class="co"># How many periods to nowcast/forecast?</span></span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>        n_head:<span class="bu">int</span><span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>        skip_attention:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>, <span class="co"># Build a partial TFT without attention</span></span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TFT, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quantiles <span class="op">=</span> quantiles</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_size <span class="op">=</span> output_size</span>
<span id="cb96-16"><a href="#cb96-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> n_head</span>
<span id="cb96-17"><a href="#cb96-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb96-18"><a href="#cb96-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.skip_attention <span class="op">=</span> skip_attention</span>
<span id="cb96-19"><a href="#cb96-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-20"><a href="#cb96-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb96-21"><a href="#cb96-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TFT, <span class="va">self</span>).build(input_shape)</span>
<span id="cb96-22"><a href="#cb96-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-23"><a href="#cb96-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_layer <span class="op">=</span> InputTFT(</span>
<span id="cb96-24"><a href="#cb96-24" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-25"><a href="#cb96-25" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"input"</span></span>
<span id="cb96-26"><a href="#cb96-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-27"><a href="#cb96-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.svars <span class="op">=</span> StaticVariableSelection(</span>
<span id="cb96-28"><a href="#cb96-28" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-29"><a href="#cb96-29" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-30"><a href="#cb96-30" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_variable_selection"</span></span>
<span id="cb96-31"><a href="#cb96-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-32"><a href="#cb96-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tvars_hist <span class="op">=</span> TemporalVariableSelection(</span>
<span id="cb96-33"><a href="#cb96-33" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-34"><a href="#cb96-34" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-35"><a href="#cb96-35" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"historical_variable_selection"</span></span>
<span id="cb96-36"><a href="#cb96-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-37"><a href="#cb96-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tvars_fut <span class="op">=</span> TemporalVariableSelection(</span>
<span id="cb96-38"><a href="#cb96-38" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-39"><a href="#cb96-39" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-40"><a href="#cb96-40" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"future_variable_selection"</span></span>
<span id="cb96-41"><a href="#cb96-41" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-42"><a href="#cb96-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_s_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb96-43"><a href="#cb96-43" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-44"><a href="#cb96-44" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-45"><a href="#cb96-45" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb96-46"><a href="#cb96-46" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_variable_selection"</span></span>
<span id="cb96-47"><a href="#cb96-47" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-48"><a href="#cb96-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_h_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb96-49"><a href="#cb96-49" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-50"><a href="#cb96-50" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-51"><a href="#cb96-51" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb96-52"><a href="#cb96-52" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_LSTM_state_h"</span></span>
<span id="cb96-53"><a href="#cb96-53" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-54"><a href="#cb96-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_c_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb96-55"><a href="#cb96-55" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-56"><a href="#cb96-56" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-57"><a href="#cb96-57" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb96-58"><a href="#cb96-58" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_LSTM_state_c"</span></span>
<span id="cb96-59"><a href="#cb96-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-60"><a href="#cb96-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_e_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb96-61"><a href="#cb96-61" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-62"><a href="#cb96-62" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-63"><a href="#cb96-63" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb96-64"><a href="#cb96-64" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_enrichment_of"</span></span>
<span id="cb96-65"><a href="#cb96-65" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-66"><a href="#cb96-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.temporal_features <span class="op">=</span> TemporalFeatures(</span>
<span id="cb96-67"><a href="#cb96-67" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-68"><a href="#cb96-68" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-69"><a href="#cb96-69" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"LSTM_encoder"</span></span>
<span id="cb96-70"><a href="#cb96-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-71"><a href="#cb96-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_enrichment <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb96-72"><a href="#cb96-72" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-73"><a href="#cb96-73" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-74"><a href="#cb96-74" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb96-75"><a href="#cb96-75" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_enrichment"</span></span>
<span id="cb96-76"><a href="#cb96-76" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-77"><a href="#cb96-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.skip_attention:</span>
<span id="cb96-78"><a href="#cb96-78" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention <span class="op">=</span> InterpretableMultiHeadAttention(</span>
<span id="cb96-79"><a href="#cb96-79" aria-hidden="true" tabindex="-1"></a>                n_head<span class="op">=</span><span class="va">self</span>.n_head,</span>
<span id="cb96-80"><a href="#cb96-80" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-81"><a href="#cb96-81" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-82"><a href="#cb96-82" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"attention_heads"</span></span>
<span id="cb96-83"><a href="#cb96-83" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb96-84"><a href="#cb96-84" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention_gating <span class="op">=</span> GatedLinearUnit(</span>
<span id="cb96-85"><a href="#cb96-85" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-86"><a href="#cb96-86" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-87"><a href="#cb96-87" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb96-88"><a href="#cb96-88" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb96-89"><a href="#cb96-89" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"attention_gating"</span></span>
<span id="cb96-90"><a href="#cb96-90" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb96-91"><a href="#cb96-91" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attn_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb96-92"><a href="#cb96-92" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-93"><a href="#cb96-93" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-94"><a href="#cb96-94" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb96-95"><a href="#cb96-95" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"output_nonlinear_processing"</span></span>
<span id="cb96-96"><a href="#cb96-96" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb96-97"><a href="#cb96-97" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.final_skip <span class="op">=</span> GatedLinearUnit(</span>
<span id="cb96-98"><a href="#cb96-98" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb96-99"><a href="#cb96-99" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb96-100"><a href="#cb96-100" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb96-101"><a href="#cb96-101" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb96-102"><a href="#cb96-102" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"final_skip_connection"</span></span>
<span id="cb96-103"><a href="#cb96-103" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb96-104"><a href="#cb96-104" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.add <span class="op">=</span> keras.layers.Add()</span>
<span id="cb96-105"><a href="#cb96-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l_norm <span class="op">=</span> keras.layers.LayerNormalization()</span>
<span id="cb96-106"><a href="#cb96-106" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-107"><a href="#cb96-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flat <span class="op">=</span> keras.layers.Flatten(name<span class="op">=</span><span class="st">"flatten"</span>)</span>
<span id="cb96-108"><a href="#cb96-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-109"><a href="#cb96-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-110"><a href="#cb96-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layers:</span></span>
<span id="cb96-111"><a href="#cb96-111" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In order to enforce monotonicity of the quantiles, forecast only the lowest quantile</span></span>
<span id="cb96-112"><a href="#cb96-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># from a base forecast layer, and use output_len - 1 additional layers with ReLU activation</span></span>
<span id="cb96-113"><a href="#cb96-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to produce the difference between the current quantile and the previous one</span></span>
<span id="cb96-114"><a href="#cb96-114" aria-hidden="true" tabindex="-1"></a>        output_len <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.quantiles)</span>
<span id="cb96-115"><a href="#cb96-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-116"><a href="#cb96-116" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_output_layer <span class="op">=</span> keras.layers.TimeDistributed(</span>
<span id="cb96-117"><a href="#cb96-117" aria-hidden="true" tabindex="-1"></a>            keras.layers.Dense(<span class="dv">1</span>),</span>
<span id="cb96-118"><a href="#cb96-118" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"output"</span></span>
<span id="cb96-119"><a href="#cb96-119" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-120"><a href="#cb96-120" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> elu_plus(x):</span>
<span id="cb96-121"><a href="#cb96-121" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> keras.activations.elu(x) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb96-122"><a href="#cb96-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-123"><a href="#cb96-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quantile_diff_layers <span class="op">=</span> [</span>
<span id="cb96-124"><a href="#cb96-124" aria-hidden="true" tabindex="-1"></a>            keras.layers.TimeDistributed(</span>
<span id="cb96-125"><a href="#cb96-125" aria-hidden="true" tabindex="-1"></a>                keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span>elu_plus),</span>
<span id="cb96-126"><a href="#cb96-126" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="ss">f"quantile_diff_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb96-127"><a href="#cb96-127" aria-hidden="true" tabindex="-1"></a>            ) </span>
<span id="cb96-128"><a href="#cb96-128" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(output_len <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb96-129"><a href="#cb96-129" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb96-130"><a href="#cb96-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-131"><a href="#cb96-131" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb96-132"><a href="#cb96-132" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb96-133"><a href="#cb96-133" aria-hidden="true" tabindex="-1"></a>        config.update(</span>
<span id="cb96-134"><a href="#cb96-134" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb96-135"><a href="#cb96-135" aria-hidden="true" tabindex="-1"></a>                <span class="st">"quantiles"</span>: <span class="va">self</span>.quantiles,</span>
<span id="cb96-136"><a href="#cb96-136" aria-hidden="true" tabindex="-1"></a>                <span class="st">"d_model"</span>: <span class="va">self</span>.d_model,</span>
<span id="cb96-137"><a href="#cb96-137" aria-hidden="true" tabindex="-1"></a>                <span class="st">"output_size"</span>: <span class="va">self</span>.output_size,</span>
<span id="cb96-138"><a href="#cb96-138" aria-hidden="true" tabindex="-1"></a>                <span class="st">"n_head"</span>: <span class="va">self</span>.n_head,</span>
<span id="cb96-139"><a href="#cb96-139" aria-hidden="true" tabindex="-1"></a>                <span class="st">"dropout_rate"</span> :<span class="va">self</span>.dropout_rate,</span>
<span id="cb96-140"><a href="#cb96-140" aria-hidden="true" tabindex="-1"></a>                <span class="st">"skip_attention"</span>:  <span class="va">self</span>.skip_attention,</span>
<span id="cb96-141"><a href="#cb96-141" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb96-142"><a href="#cb96-142" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-143"><a href="#cb96-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb96-144"><a href="#cb96-144" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb96-145"><a href="#cb96-145" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb96-146"><a href="#cb96-146" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb96-147"><a href="#cb96-147" aria-hidden="true" tabindex="-1"></a>        inputs,</span>
<span id="cb96-148"><a href="#cb96-148" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb96-149"><a href="#cb96-149" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb96-150"><a href="#cb96-150" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Creates the model architecture"</span></span>
<span id="cb96-151"><a href="#cb96-151" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-152"><a href="#cb96-152" aria-hidden="true" tabindex="-1"></a>        <span class="co"># embedding the inputs</span></span>
<span id="cb96-153"><a href="#cb96-153" aria-hidden="true" tabindex="-1"></a>        cont_hist, cat_hist, cat_fut, cat_stat <span class="op">=</span> inputs</span>
<span id="cb96-154"><a href="#cb96-154" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(cat_stat.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb96-155"><a href="#cb96-155" aria-hidden="true" tabindex="-1"></a>            cat_stat <span class="op">=</span> keras.ops.expand_dims(cat_stat, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb96-156"><a href="#cb96-156" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb96-157"><a href="#cb96-157" aria-hidden="true" tabindex="-1"></a>        xi_hist, xi_fut, xi_stat <span class="op">=</span> <span class="va">self</span>.input_layer([cont_hist, cat_hist, cat_fut, cat_stat])</span>
<span id="cb96-158"><a href="#cb96-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-159"><a href="#cb96-159" aria-hidden="true" tabindex="-1"></a>        <span class="co"># selecting the static covariates</span></span>
<span id="cb96-160"><a href="#cb96-160" aria-hidden="true" tabindex="-1"></a>        static_selected_vars, static_selection_weights <span class="op">=</span> <span class="va">self</span>.svars(xi_stat, training<span class="op">=</span>training)</span>
<span id="cb96-161"><a href="#cb96-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-162"><a href="#cb96-162" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create context vectors from static data</span></span>
<span id="cb96-163"><a href="#cb96-163" aria-hidden="true" tabindex="-1"></a>        c_s, _ <span class="op">=</span> <span class="va">self</span>.static_context_s_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for variable selection</span></span>
<span id="cb96-164"><a href="#cb96-164" aria-hidden="true" tabindex="-1"></a>        c_h, _ <span class="op">=</span> <span class="va">self</span>.static_context_h_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for LSTM state h</span></span>
<span id="cb96-165"><a href="#cb96-165" aria-hidden="true" tabindex="-1"></a>        c_c, _ <span class="op">=</span> <span class="va">self</span>.static_context_c_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for LSTM state c</span></span>
<span id="cb96-166"><a href="#cb96-166" aria-hidden="true" tabindex="-1"></a>        c_e, _ <span class="op">=</span> <span class="va">self</span>.static_context_e_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for context enrichment of post-LSTM features</span></span>
<span id="cb96-167"><a href="#cb96-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-168"><a href="#cb96-168" aria-hidden="true" tabindex="-1"></a>        <span class="co"># temporal variable selection</span></span>
<span id="cb96-169"><a href="#cb96-169" aria-hidden="true" tabindex="-1"></a>        hist_selected_vars, hist_selection_weights <span class="op">=</span> <span class="va">self</span>.tvars_hist(</span>
<span id="cb96-170"><a href="#cb96-170" aria-hidden="true" tabindex="-1"></a>            [xi_hist, c_s],</span>
<span id="cb96-171"><a href="#cb96-171" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb96-172"><a href="#cb96-172" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-173"><a href="#cb96-173" aria-hidden="true" tabindex="-1"></a>        fut_selected_vars, fut_selection_weights <span class="op">=</span> <span class="va">self</span>.tvars_fut(</span>
<span id="cb96-174"><a href="#cb96-174" aria-hidden="true" tabindex="-1"></a>            [xi_fut, c_s],</span>
<span id="cb96-175"><a href="#cb96-175" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb96-176"><a href="#cb96-176" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-177"><a href="#cb96-177" aria-hidden="true" tabindex="-1"></a>        input_embeddings <span class="op">=</span> keras.ops.concatenate(</span>
<span id="cb96-178"><a href="#cb96-178" aria-hidden="true" tabindex="-1"></a>            [hist_selected_vars, fut_selected_vars],</span>
<span id="cb96-179"><a href="#cb96-179" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb96-180"><a href="#cb96-180" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-181"><a href="#cb96-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-182"><a href="#cb96-182" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.temporal_features(</span>
<span id="cb96-183"><a href="#cb96-183" aria-hidden="true" tabindex="-1"></a>            [hist_selected_vars, fut_selected_vars, c_h, c_c],</span>
<span id="cb96-184"><a href="#cb96-184" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb96-185"><a href="#cb96-185" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-186"><a href="#cb96-186" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-187"><a href="#cb96-187" aria-hidden="true" tabindex="-1"></a>        <span class="co"># static context enrichment</span></span>
<span id="cb96-188"><a href="#cb96-188" aria-hidden="true" tabindex="-1"></a>        enriched, _ <span class="op">=</span> <span class="va">self</span>.static_context_enrichment(</span>
<span id="cb96-189"><a href="#cb96-189" aria-hidden="true" tabindex="-1"></a>            features, </span>
<span id="cb96-190"><a href="#cb96-190" aria-hidden="true" tabindex="-1"></a>            additional_context<span class="op">=</span>keras.ops.expand_dims(c_e, axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb96-191"><a href="#cb96-191" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb96-192"><a href="#cb96-192" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb96-193"><a href="#cb96-193" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.skip_attention:</span>
<span id="cb96-194"><a href="#cb96-194" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> get_decoder_mask(enriched)</span>
<span id="cb96-195"><a href="#cb96-195" aria-hidden="true" tabindex="-1"></a>            attn_output, self_attn <span class="op">=</span> <span class="va">self</span>.attention(</span>
<span id="cb96-196"><a href="#cb96-196" aria-hidden="true" tabindex="-1"></a>                q<span class="op">=</span>enriched,</span>
<span id="cb96-197"><a href="#cb96-197" aria-hidden="true" tabindex="-1"></a>                k<span class="op">=</span>enriched,</span>
<span id="cb96-198"><a href="#cb96-198" aria-hidden="true" tabindex="-1"></a>                v<span class="op">=</span>enriched,</span>
<span id="cb96-199"><a href="#cb96-199" aria-hidden="true" tabindex="-1"></a>                mask<span class="op">=</span>mask,</span>
<span id="cb96-200"><a href="#cb96-200" aria-hidden="true" tabindex="-1"></a>                training<span class="op">=</span>training</span>
<span id="cb96-201"><a href="#cb96-201" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb96-202"><a href="#cb96-202" aria-hidden="true" tabindex="-1"></a>            attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention_gating(attn_output)</span>
<span id="cb96-203"><a href="#cb96-203" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.add([enriched, attn_output])</span>
<span id="cb96-204"><a href="#cb96-204" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.l_norm(output)</span>
<span id="cb96-205"><a href="#cb96-205" aria-hidden="true" tabindex="-1"></a>            output, _ <span class="op">=</span> <span class="va">self</span>.attn_grn(output)</span>
<span id="cb96-206"><a href="#cb96-206" aria-hidden="true" tabindex="-1"></a>            output, _ <span class="op">=</span> <span class="va">self</span>.final_skip(output)</span>
<span id="cb96-207"><a href="#cb96-207" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.add([features, output])</span>
<span id="cb96-208"><a href="#cb96-208" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb96-209"><a href="#cb96-209" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> enriched</span>
<span id="cb96-210"><a href="#cb96-210" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.l_norm(output)</span>
<span id="cb96-211"><a href="#cb96-211" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-212"><a href="#cb96-212" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Base quantile output</span></span>
<span id="cb96-213"><a href="#cb96-213" aria-hidden="true" tabindex="-1"></a>        base_output <span class="op">=</span> output[<span class="va">Ellipsis</span>,hist_selected_vars.shape[<span class="dv">1</span>]:,:]</span>
<span id="cb96-214"><a href="#cb96-214" aria-hidden="true" tabindex="-1"></a>        base_quantile <span class="op">=</span> <span class="va">self</span>.base_output_layer(base_output)</span>
<span id="cb96-215"><a href="#cb96-215" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb96-216"><a href="#cb96-216" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Additional layers for remaining quantiles</span></span>
<span id="cb96-217"><a href="#cb96-217" aria-hidden="true" tabindex="-1"></a>        quantile_outputs <span class="op">=</span> [base_quantile]</span>
<span id="cb96-218"><a href="#cb96-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.quantiles) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb96-219"><a href="#cb96-219" aria-hidden="true" tabindex="-1"></a>            quantile_diff <span class="op">=</span> <span class="va">self</span>.quantile_diff_layers[i](base_output)</span>
<span id="cb96-220"><a href="#cb96-220" aria-hidden="true" tabindex="-1"></a>            quantile_output <span class="op">=</span> quantile_outputs[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> quantile_diff</span>
<span id="cb96-221"><a href="#cb96-221" aria-hidden="true" tabindex="-1"></a>            quantile_outputs.append(quantile_output)</span>
<span id="cb96-222"><a href="#cb96-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-223"><a href="#cb96-223" aria-hidden="true" tabindex="-1"></a>        final_output <span class="op">=</span> keras.ops.concatenate(quantile_outputs, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb96-224"><a href="#cb96-224" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb96-225"><a href="#cb96-225" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> final_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="75">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>Attns <span class="op">=</span> []</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a>    attn, attn_weights <span class="op">=</span> ScaledDotProductAttention(name<span class="op">=</span><span class="ss">f"Attention__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(q<span class="op">=</span>v, k<span class="op">=</span>v, v<span class="op">=</span>v, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>    Attns.append(attn)</span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(attn)</span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Concatenate(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb97-25"><a href="#cb97-25" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb97-26"><a href="#cb97-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-27"><a href="#cb97-27" aria-hidden="true" tabindex="-1"></a>nn_sdpa <span class="op">=</span> keras.Model(</span>
<span id="cb97-28"><a href="#cb97-28" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb97-29"><a href="#cb97-29" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb97-30"><a href="#cb97-30" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"Attention"</span></span>
<span id="cb97-31"><a href="#cb97-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb97-32"><a href="#cb97-32" aria-hidden="true" tabindex="-1"></a>nn_sdpa.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb97-33"><a href="#cb97-33" aria-hidden="true" tabindex="-1"></a>nn_sdpa.summary()</span>
<span id="cb97-34"><a href="#cb97-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-35"><a href="#cb97-35" aria-hidden="true" tabindex="-1"></a>tft <span class="op">=</span> TFT()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-summary_tft" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="75">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-summary_tft-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12: Summary of TFT Model
</figcaption>
<div aria-describedby="tbl-summary_tft-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "Attention"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ m (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ d (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__m         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoding__d         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)  │         <span style="color: #00af00; text-decoration-color: #00af00">32</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">TimeDistributed</span>)   │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ Attention__freq_m   │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ScaledDotProductA…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│                     │ <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)]            │            │ encoding__m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ Attention__freq_d   │ [(<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>,     │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">ScaledDotProductA…</span> │ <span style="color: #00af00; text-decoration-color: #00af00">16</span>), (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, │            │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│                     │ <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)]            │            │ encoding__d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ Attention__freq_… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">2,112</span> │ Attention__freq_… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Concatenate</span>)       │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_55 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">528</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_56 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense_55[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]    │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,833</span> (18.88 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,833</span> (18.88 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="sec-nowcast" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="sec-nowcast"><span class="header-section-number">12</span> Nowcasting inflation with a simple model</h2>
<p>It all boils down to this: using past data, coupled with higher frequency current data, to nowcast current values. Continuing with the simplistic example in this page, now it is time to nowcast monthly inflation from its lags and from daily changes in oil prices.</p>
</section>
<section id="references" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="references"><span class="header-section-number">13</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ba2016layer" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. <span>“Layer Normalization.”</span> <em>arXiv Preprint arXiv:1607.06450</em>.
</div>
<div id="ref-bok2018nowcasting" class="csl-entry" role="listitem">
Bok, Brandyn, Daniele Caratelli, Domenico Giannone, Argia M. Sbordone, and Andrea Tambalotti. 2018. <span>“Macroeconomic Nowcasting and Forecasting with Big Data.”</span> Journal Article. <em>Annual Review of Economics</em> 10 (Volume 10, 2018): 615–43. https://doi.org/<a href="https://doi.org/10.1146/annurev-economics-080217-053214">https://doi.org/10.1146/annurev-economics-080217-053214</a>.
</div>
<div id="ref-clevert2015fast" class="csl-entry" role="listitem">
Clevert, Djork-Arné, Thomas Unterthiner, and Sepp Hochreiter. 2015. <span>“Fast and Accurate Deep Network Learning by Exponential Linear Units (Elus).”</span> <em>arXiv Preprint arXiv:1511.07289</em>.
</div>
<div id="ref-dauphin2017language" class="csl-entry" role="listitem">
Dauphin, Yann N, Angela Fan, Michael Auli, and David Grangier. 2017. <span>“Language Modeling with Gated Convolutional Networks.”</span> In <em>International Conference on Machine Learning</em>, 933–41. PMLR.
</div>
<div id="ref-diaz2024causality" class="csl-entry" role="listitem">
Dı́az Berenguer, Abel, Yifei Da, Matı́as Nicolás Bossa, Meshia Cédric Oveneke, and Hichem Sahli. 2024. <span>“Causality-Driven Multivariate Stock Movement Forecasting.”</span> <em>Plos One</em> 19 (4): e0302197.
</div>
<div id="ref-giannone2008nowcasting" class="csl-entry" role="listitem">
Giannone, Domenico, Lucrezia Reichlin, and David Small. 2008. <span>“Nowcasting: The Real-Time Informational Content of Macroeconomic Data.”</span> <em>Journal of Monetary Economics</em> 55 (4): 665–76.
</div>
<div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, S. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation MIT-Press</em>.
</div>
<div id="ref-hu2021stock" class="csl-entry" role="listitem">
Hu, Xiaokang. 2021. <span>“Stock Price Prediction Based on Temporal Fusion Transformer.”</span> In <em>2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)</em>, 60–66. IEEE.
</div>
<div id="ref-laborda2023multi" class="csl-entry" role="listitem">
Laborda, Juan, Sonia Ruano, and Ignacio Zamanillo. 2023. <span>“Multi-Country and Multi-Horizon GDP Forecasting Using Temporal Fusion Transformers.”</span> <em>Mathematics</em> 11 (12): 2625.
</div>
<div id="ref-lim2021temporal" class="csl-entry" role="listitem">
Lim, Bryan, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. <span>“Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting.”</span> <em>International Journal of Forecasting</em> 37 (4): 1748–64.
</div>
<div id="ref-vaswani2023attentionneed" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.
</div>
</div>


<!-- -->

</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="giannone2008nowcasting">Giannone, Reichlin, and Small (<a href="#ref-giannone2008nowcasting" role="doc-biblioref">2008</a>)</span> pioneered nowcasting in macroeconomics. See <span class="citation" data-cites="bok2018nowcasting">Bok et al. (<a href="#ref-bok2018nowcasting" role="doc-biblioref">2018</a>)</span> for a review.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Mac users might need to include the following line after importing the package <code>os</code> and before importing <code>keras</code>: <code>os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"</code>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See <a href="https://sdmx1.readthedocs.io/en/latest/walkthrough.html">here</a> for a practical walkthrough showing how to explore data with SDMX.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">here</a> for more information on time series splitting.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The division of the data between folds might produce slightly unequal-sized folds, as can be seen in the current example.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Ignore the <code>None</code> in the shapes; this means the dimensionality along that axis depends on each case. Specifically, the first dimension of the shapes in keras are always the number of samples going in the model. Because this varies with every training, application, etc, it is not fixed as the other dimensions are. Another way of thinking about this is as follows: in a regression, you know exactly how many variables you need to have, but the number of data points can vary.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Normalising a layer helps to avoid the numbers from becoming too large, which is detrimental to gradient transmission and therefore to learning. This is helpful for example when summing up the outputs from intermediary layers.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb98" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Nowcasting inflation with neural networks</span></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> A simple, mixed-frequency example</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a><span class="an">output-file:</span><span class="co"> nowcast.html</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a><span class="an">authors:</span><span class="co"> </span></span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - Douglas K. G. Araujo</span></span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Johannes Damp</span></span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> show</span></span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb98-12"><a href="#cb98-12" aria-hidden="true" tabindex="-1"></a><span class="an">warning:</span><span class="co"> false</span></span>
<span id="cb98-13"><a href="#cb98-13" aria-hidden="true" tabindex="-1"></a><span class="an">fig-cap-location:</span><span class="co"> top</span></span>
<span id="cb98-14"><a href="#cb98-14" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb98-15"><a href="#cb98-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-16"><a href="#cb98-16" aria-hidden="true" tabindex="-1"></a>This notebook showcases how to set up neural networks to nowcast inflation using data measured in different frequencies. The goal here is to start with a very simple dataset containing only two variables, inflation (monthly) and oil prices (daily), to slowly build up a more complex neural network based nowcasting model, the TFT-MF available in <span class="in">`gingado`</span> from its v0.3.0.</span>
<span id="cb98-17"><a href="#cb98-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-18"><a href="#cb98-18" aria-hidden="true" tabindex="-1"></a>Nowcasting is essentially the use of the most current information possible to estimate in real time an economic series of interest such as inflation or GDP before it is actually released<span class="ot">[^review]</span>. For example, if you could measure all prices every day, you could create on the last day of the month a very accurate nowcast for the headline inflation for that month - which would only be officially published a few days later. In the case of GDP, this lag between the end of the reference period and actual publication tends to be significant, around 6-10 weeks. For policymakers, investors and other decision-makers, a lot can happen in this period.</span>
<span id="cb98-19"><a href="#cb98-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-20"><a href="#cb98-20" aria-hidden="true" tabindex="-1"></a><span class="ot">[^review]: </span>@giannone2008nowcasting pioneered nowcasting in macroeconomics. See @bok2018nowcasting for a review.</span>
<span id="cb98-21"><a href="#cb98-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-22"><a href="#cb98-22" aria-hidden="true" tabindex="-1"></a>A related use of nowcasting is to estimate what the current period's reading will be as this period rolls out. In other words, estimating today what the inflation reading for this month (or GDP for this quarter) will likely be as new information is unveiled in real time.</span>
<span id="cb98-23"><a href="#cb98-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-24"><a href="#cb98-24" aria-hidden="true" tabindex="-1"></a>The nowcasting model available in <span class="in">`gingado`</span> from v0.3.0 onwards is an adjusted version of the Temporal Fusion Transformer (TFT) of @lim2021temporal. This architecture combines *flexibility* to take on multiple datasets while learning which information to focus on and *interpretability* to provide insights on the important variables in each case. Empirical results with the TFT in finance and economics settings include stock prices (@hu2021stock, @diaz2024causality) and GDP (@laborda2023multi).</span>
<span id="cb98-25"><a href="#cb98-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-26"><a href="#cb98-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Roadmap</span></span>
<span id="cb98-27"><a href="#cb98-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-28"><a href="#cb98-28" aria-hidden="true" tabindex="-1"></a>The TFT model can be a bit complex to understand at first, so we will build it up, step by step. After loading the data in @sec-data, the most basic neural network - a neuron layer - is presented in @sec-fc. Then a simple extension is shown where a neural network learns which data to let through or not in @sec-glu. Armed with these elements, @sec-lstm discusses the next architecture, more suitable for time series. Next, these elements are combined in @sec-gates to show how the model knows what to focus on. @sec-timeembed introduces the concept of embeddings of categorical variables, while @sec-encodcont explores this in the context of continuous variables. All this content is then put together in a way that dynamically selects useful inputs for each instance in @sec-varsel. The next component is the self-attention layer in @sec-transf. Finally, if you want to see the full picture directly, go to @sec-tftmf to see how these elements are put together. @sec-nowcast then trains the model and presents the results for this simple, illustrative nowcasting.</span>
<span id="cb98-29"><a href="#cb98-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-30"><a href="#cb98-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loading the packages[^mac] and data {#sec-data}</span></span>
<span id="cb98-31"><a href="#cb98-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-32"><a href="#cb98-32" aria-hidden="true" tabindex="-1"></a><span class="ot">[^mac]: </span>Mac users might need to include the following line after importing the package <span class="in">`os`</span> and before importing <span class="in">`keras`</span>: <span class="in">`os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"`</span>.</span>
<span id="cb98-33"><a href="#cb98-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-34"><a href="#cb98-34" aria-hidden="true" tabindex="-1"></a>Let's use our SDMX connectors to find and download data from official sources in a reproducible way.</span>
<span id="cb98-35"><a href="#cb98-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-36"><a href="#cb98-36" aria-hidden="true" tabindex="-1"></a>To abstract from currency issues, we will use US inflation and oil prices, which are denominated in US dollars.</span>
<span id="cb98-37"><a href="#cb98-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-40"><a href="#cb98-40" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-41"><a href="#cb98-41" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load packages</span></span>
<span id="cb98-42"><a href="#cb98-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb98-43"><a href="#cb98-43" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span> <span class="co"># or "torch", "jax" according to user preference</span></span>
<span id="cb98-44"><a href="#cb98-44" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb98-45"><a href="#cb98-45" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb98-46"><a href="#cb98-46" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb98-47"><a href="#cb98-47" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb98-48"><a href="#cb98-48" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sdmx</span>
<span id="cb98-49"><a href="#cb98-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gingado.utils <span class="im">import</span> load_SDMX_data</span>
<span id="cb98-50"><a href="#cb98-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb98-51"><a href="#cb98-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> TimeSeriesSplit</span>
<span id="cb98-52"><a href="#cb98-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb98-53"><a href="#cb98-53" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb98-54"><a href="#cb98-54" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-55"><a href="#cb98-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-56"><a href="#cb98-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-57"><a href="#cb98-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inflation</span></span>
<span id="cb98-58"><a href="#cb98-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-59"><a href="#cb98-59" aria-hidden="true" tabindex="-1"></a>Since this is a monthly nowcast of inflation, the best way to do this is to use a *monthly change in the consumer price index*, $\pi_t^{(m)}=(\text{CPI}_t - \text{CPI}_{t-1})/\text{CPI}_{t-1}$, not the year-on-year rate, $\pi_t^{(y)}=(\text{CPI}_t - \text{CPI}_{t-12})/\text{CPI}_{t-12}$, which is how people usually think of inflation. This is because we want to nowcast only the value at the margin; 11 twelfths of $\pi_t^{(y)}$ are already known, since $\pi_t^{(y)} = -1 + \prod_{l=0}^{11} (1+\pi_{t-l})$.</span>
<span id="cb98-60"><a href="#cb98-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-61"><a href="#cb98-61" aria-hidden="true" tabindex="-1"></a>Then, only at the end we combine rolling windows of 12 consecutive monthly inflation rates, of which only the last one or two are estimated, to correctly create an annual inflation rate. </span>
<span id="cb98-62"><a href="#cb98-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-63"><a href="#cb98-63" aria-hidden="true" tabindex="-1"></a>Formally, if we know all values except the current and last month's, then: </span>
<span id="cb98-64"><a href="#cb98-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb98-65"><a href="#cb98-65" aria-hidden="true" tabindex="-1"></a>\hat{\pi}_t^{(y)}=(\prod_{l=0}^1 (1+\hat{\pi}_{t-l}^{(m)}) \prod_{l=2}^{11} (1+\pi_{t-l}^{(m)}) )-1,</span>
<span id="cb98-66"><a href="#cb98-66" aria-hidden="true" tabindex="-1"></a>$$ {#eq-finalnowcast}</span>
<span id="cb98-67"><a href="#cb98-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-68"><a href="#cb98-68" aria-hidden="true" tabindex="-1"></a>where the hat notation ($\hat{\pi}$) means that a particular value was estimated.</span>
<span id="cb98-69"><a href="#cb98-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-70"><a href="#cb98-70" aria-hidden="true" tabindex="-1"></a>For inflation, we take a dataflow from the <span class="co">[</span><span class="ot">BIS</span><span class="co">](https://data.bis.org/topics/CPI)</span>, since we are looking for US data. Let's explore it first and then choose the correct data specifications to download the time series.<span class="ot">[^sdmx]</span></span>
<span id="cb98-71"><a href="#cb98-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-72"><a href="#cb98-72" aria-hidden="true" tabindex="-1"></a><span class="ot">[^sdmx]: </span>See <span class="co">[</span><span class="ot">here</span><span class="co">](https://sdmx1.readthedocs.io/en/latest/walkthrough.html)</span> for a practical walkthrough showing how to explore data with SDMX.</span>
<span id="cb98-73"><a href="#cb98-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-76"><a href="#cb98-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-77"><a href="#cb98-77" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "inflation dataflow"</span></span>
<span id="cb98-78"><a href="#cb98-78" aria-hidden="true" tabindex="-1"></a>BIS <span class="op">=</span> sdmx.Client(<span class="st">"BIS"</span>)</span>
<span id="cb98-79"><a href="#cb98-79" aria-hidden="true" tabindex="-1"></a>cpi_msg <span class="op">=</span> BIS.dataflow(<span class="st">'WS_LONG_CPI'</span>)</span>
<span id="cb98-80"><a href="#cb98-80" aria-hidden="true" tabindex="-1"></a>cpi_dsd <span class="op">=</span> cpi_msg.structure</span>
<span id="cb98-81"><a href="#cb98-81" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-82"><a href="#cb98-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-83"><a href="#cb98-83" aria-hidden="true" tabindex="-1"></a>These are all possible keys:</span>
<span id="cb98-84"><a href="#cb98-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-87"><a href="#cb98-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-88"><a href="#cb98-88" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "CPI_dimensions"</span></span>
<span id="cb98-89"><a href="#cb98-89" aria-hidden="true" tabindex="-1"></a>cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.components</span>
<span id="cb98-90"><a href="#cb98-90" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-91"><a href="#cb98-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-92"><a href="#cb98-92" aria-hidden="true" tabindex="-1"></a>For example, "FREQ" (frequency) takes in these values:</span>
<span id="cb98-93"><a href="#cb98-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-96"><a href="#cb98-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-97"><a href="#cb98-97" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "FREQ_codelist"</span></span>
<span id="cb98-98"><a href="#cb98-98" aria-hidden="true" tabindex="-1"></a>cl__FREQ <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"FREQ"</span>).local_representation.enumerated)</span>
<span id="cb98-99"><a href="#cb98-99" aria-hidden="true" tabindex="-1"></a>cl__FREQ</span>
<span id="cb98-100"><a href="#cb98-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-101"><a href="#cb98-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-102"><a href="#cb98-102" aria-hidden="true" tabindex="-1"></a>And "REF_AREA" (reference area) can be set to:</span>
<span id="cb98-103"><a href="#cb98-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-106"><a href="#cb98-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-107"><a href="#cb98-107" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "REF_AREA_codelist"</span></span>
<span id="cb98-108"><a href="#cb98-108" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"REF_AREA"</span>).local_representation.enumerated)</span>
<span id="cb98-109"><a href="#cb98-109" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA</span>
<span id="cb98-110"><a href="#cb98-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-111"><a href="#cb98-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-112"><a href="#cb98-112" aria-hidden="true" tabindex="-1"></a>We can check that the US is amongst the reference areas:</span>
<span id="cb98-113"><a href="#cb98-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-116"><a href="#cb98-116" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-117"><a href="#cb98-117" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "check US in REF_AREA codelist"</span></span>
<span id="cb98-118"><a href="#cb98-118" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA[<span class="st">'US'</span>]</span>
<span id="cb98-119"><a href="#cb98-119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-120"><a href="#cb98-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-121"><a href="#cb98-121" aria-hidden="true" tabindex="-1"></a>Finally, the "UNIT_MEASURE" values can be:</span>
<span id="cb98-122"><a href="#cb98-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-125"><a href="#cb98-125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-126"><a href="#cb98-126" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "codelist for UNIT_MEASURE in dataflow BIS__WS_LONG_CPI"</span></span>
<span id="cb98-127"><a href="#cb98-127" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"UNIT_MEASURE"</span>).local_representation.enumerated)</span>
<span id="cb98-128"><a href="#cb98-128" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE</span>
<span id="cb98-129"><a href="#cb98-129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-130"><a href="#cb98-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-131"><a href="#cb98-131" aria-hidden="true" tabindex="-1"></a>In the <span class="co">[</span><span class="ot">BIS website for this data</span><span class="co">](https://data.bis.org/topics/CPI#faq)</span>, we can see that the unit in levels is <span class="in">`Index, 2010 = 100`</span> (the other one is <span class="in">`Year-on-year changes, in per cent`</span>, which as discussed above we don't want for this case.)</span>
<span id="cb98-132"><a href="#cb98-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-135"><a href="#cb98-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-136"><a href="#cb98-136" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "finding code for index"</span></span>
<span id="cb98-137"><a href="#cb98-137" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE[cl__UNIT_MEASURE.<span class="bu">str</span>.contains(<span class="st">"Index, 2010 = 100"</span>)]</span>
<span id="cb98-138"><a href="#cb98-138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-139"><a href="#cb98-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-140"><a href="#cb98-140" aria-hidden="true" tabindex="-1"></a>Armed with this knowledge, we can now download monthly consumer price index data for the US. Let's start after 1985 so that we have a sufficiently long history but without too much influence of the tectonic shift of the US dollar devaluation in the early 1970s and ensuing high inflation:</span>
<span id="cb98-141"><a href="#cb98-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-144"><a href="#cb98-144" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-145"><a href="#cb98-145" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cpi</span></span>
<span id="cb98-146"><a href="#cb98-146" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "US consumer price index, 2010 = 100"</span></span>
<span id="cb98-147"><a href="#cb98-147" aria-hidden="true" tabindex="-1"></a>df_infl <span class="op">=</span> load_SDMX_data(</span>
<span id="cb98-148"><a href="#cb98-148" aria-hidden="true" tabindex="-1"></a>    sources<span class="op">=</span>{<span class="st">"BIS"</span>: <span class="st">"'WS_LONG_CPI'"</span>},</span>
<span id="cb98-149"><a href="#cb98-149" aria-hidden="true" tabindex="-1"></a>    keys<span class="op">=</span>{<span class="st">"FREQ"</span>: <span class="st">"M"</span>, <span class="st">"REF_AREA"</span>: <span class="st">"US"</span>, <span class="st">"UNIT_MEASURE"</span>: <span class="st">"628"</span>},</span>
<span id="cb98-150"><a href="#cb98-150" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>{<span class="st">"startPeriod"</span>: <span class="dv">1985</span>}</span>
<span id="cb98-151"><a href="#cb98-151" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-152"><a href="#cb98-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-153"><a href="#cb98-153" aria-hidden="true" tabindex="-1"></a>df_infl.plot()</span>
<span id="cb98-154"><a href="#cb98-154" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-155"><a href="#cb98-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-156"><a href="#cb98-156" aria-hidden="true" tabindex="-1"></a>As you can see in @fig-cpi, we downloaded the series $<span class="sc">\{</span>\text{CPI}_t<span class="sc">\}</span>$. Transforming that into $<span class="sc">\{</span>\pi_t<span class="sc">\}</span>$, defined above, we have:</span>
<span id="cb98-157"><a href="#cb98-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-160"><a href="#cb98-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-161"><a href="#cb98-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pi</span></span>
<span id="cb98-162"><a href="#cb98-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: US monthly inflation rate</span></span>
<span id="cb98-163"><a href="#cb98-163" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb98-164"><a href="#cb98-164" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb98-165"><a href="#cb98-165" aria-hidden="true" tabindex="-1"></a>df_infl_m <span class="op">=</span> df_infl.pct_change().dropna()</span>
<span id="cb98-166"><a href="#cb98-166" aria-hidden="true" tabindex="-1"></a>df_infl_m.index <span class="op">=</span> df_infl_m.index <span class="op">+</span> pd.offsets.MonthEnd(<span class="dv">0</span>) <span class="co"># move to month end</span></span>
<span id="cb98-167"><a href="#cb98-167" aria-hidden="true" tabindex="-1"></a>df_infl_m.plot(ax<span class="op">=</span>ax)</span>
<span id="cb98-168"><a href="#cb98-168" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-169"><a href="#cb98-169" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-170"><a href="#cb98-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-171"><a href="#cb98-171" aria-hidden="true" tabindex="-1"></a><span class="fu">### Oil prices</span></span>
<span id="cb98-172"><a href="#cb98-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-173"><a href="#cb98-173" aria-hidden="true" tabindex="-1"></a>Since the focus is on US inflation, below we get WTI oil prices. This data is downloaded from the <span class="co">[</span><span class="ot">St Louis Fed's FRED webpage</span><span class="co">](https://fred.stlouisfed.org/series/DCOILWTICO)</span>.</span>
<span id="cb98-174"><a href="#cb98-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-177"><a href="#cb98-177" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-178"><a href="#cb98-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-oil</span></span>
<span id="cb98-179"><a href="#cb98-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: WTI oil prices</span></span>
<span id="cb98-180"><a href="#cb98-180" aria-hidden="true" tabindex="-1"></a>df_oil <span class="op">=</span> pd.read_csv(<span class="st">"docs/DCOILWTICO.csv"</span>)</span>
<span id="cb98-181"><a href="#cb98-181" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DCOILWTICO'</span>] <span class="op">=</span> pd.to_numeric(df_oil[<span class="st">'DCOILWTICO'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb98-182"><a href="#cb98-182" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DATE'</span>] <span class="op">=</span> pd.to_datetime(df_oil[<span class="st">'DATE'</span>])</span>
<span id="cb98-183"><a href="#cb98-183" aria-hidden="true" tabindex="-1"></a>df_oil.set_index(<span class="st">'DATE'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-184"><a href="#cb98-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-185"><a href="#cb98-185" aria-hidden="true" tabindex="-1"></a>df_oil.plot()</span>
<span id="cb98-186"><a href="#cb98-186" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-187"><a href="#cb98-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-188"><a href="#cb98-188" aria-hidden="true" tabindex="-1"></a>For the nowcasting, we are interested in the daily variation, clipped because of the sharp movements during the onset of the Covid-19 pandemic:</span>
<span id="cb98-189"><a href="#cb98-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-192"><a href="#cb98-192" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-193"><a href="#cb98-193" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-oilD</span></span>
<span id="cb98-194"><a href="#cb98-194" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Daily change in WTI oil prices</span></span>
<span id="cb98-195"><a href="#cb98-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-196"><a href="#cb98-196" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb98-197"><a href="#cb98-197" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb98-198"><a href="#cb98-198" aria-hidden="true" tabindex="-1"></a>df_oil_d <span class="op">=</span> df_oil.pct_change().dropna()</span>
<span id="cb98-199"><a href="#cb98-199" aria-hidden="true" tabindex="-1"></a>df_oil_d.plot(ax<span class="op">=</span>ax)</span>
<span id="cb98-200"><a href="#cb98-200" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.25</span>, <span class="fl">0.25</span>)</span>
<span id="cb98-201"><a href="#cb98-201" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-202"><a href="#cb98-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-203"><a href="#cb98-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-204"><a href="#cb98-204" aria-hidden="true" tabindex="-1"></a><span class="fu">### Temporal features (not implemented for the time being)</span></span>
<span id="cb98-205"><a href="#cb98-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-206"><a href="#cb98-206" aria-hidden="true" tabindex="-1"></a>There is a lot of information encoded in the temporal features of a time series: which day in the month it is, which month of the year, etc. For example, consider how consumers behave differently in response to oil prices over warmer months (when many decide or not to travel, and how far) compared to colder months (when energy prices factor in heating and is thus perhaps less elastic).</span>
<span id="cb98-207"><a href="#cb98-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-208"><a href="#cb98-208" aria-hidden="true" tabindex="-1"></a>To simplify notation about time, instead of the usual subscript $t$ as above to denote a time period, for precision about the frequency, we will follow this convention:</span>
<span id="cb98-209"><a href="#cb98-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-210"><a href="#cb98-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>subscript $m$ denotes a given month;</span>
<span id="cb98-211"><a href="#cb98-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-212"><a href="#cb98-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>subscript $d$ denotes a given day;</span>
<span id="cb98-213"><a href="#cb98-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-214"><a href="#cb98-214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>subscript $d(m)$ denotes a given day in a given month; example: $d(m-1)$ is a day in the previous month.</span>
<span id="cb98-215"><a href="#cb98-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-216"><a href="#cb98-216" aria-hidden="true" tabindex="-1"></a><span class="in">`gingado`</span> offers a practical way to set up the temporal features that requires only the dates of the dataset.</span>
<span id="cb98-217"><a href="#cb98-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-220"><a href="#cb98-220" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-221"><a href="#cb98-221" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tempfeatures</span></span>
<span id="cb98-222"><a href="#cb98-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-223"><a href="#cb98-223" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: to add documentation and tests, and later incorporate as a new function in gingado.utils</span></span>
<span id="cb98-224"><a href="#cb98-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-225"><a href="#cb98-225" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_timefeat(df, freq<span class="op">=</span><span class="st">"d"</span>, features<span class="op">=</span><span class="va">None</span>, add_to_df<span class="op">=</span><span class="va">False</span>, remove_const<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb98-226"><a href="#cb98-226" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the future documentation: the add_to_df argument should be True if the data will be fed to an algorithm that takes in all data at once. If, like neural networks, the inputs are fed through different "pipelines", then use False and then take the result from this function an feed it separately to a neural network.</span></span>
<span id="cb98-227"><a href="#cb98-227" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the frequency is used to filter which features to add. For example, if monthly then no higher frequency features (day of ..., week of... ) are added because it doesn't make sense</span></span>
<span id="cb98-228"><a href="#cb98-228" aria-hidden="true" tabindex="-1"></a>    <span class="co"># None or list. if futures is None, then add all temporal features that the frequency above allows. Otherwise adds only the names ones</span></span>
<span id="cb98-229"><a href="#cb98-229" aria-hidden="true" tabindex="-1"></a>    all_freqs <span class="op">=</span> [<span class="st">"q"</span>, <span class="st">"m"</span>, <span class="st">"w"</span>, <span class="st">"d"</span>]</span>
<span id="cb98-230"><a href="#cb98-230" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb98-231"><a href="#cb98-231" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> pd.api.types.is_datetime64_any_dtype(df.index):</span>
<span id="cb98-232"><a href="#cb98-232" aria-hidden="true" tabindex="-1"></a>        df.index <span class="op">=</span> pd.to_datetime(df.index)</span>
<span id="cb98-233"><a href="#cb98-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-234"><a href="#cb98-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-235"><a href="#cb98-235" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> i2s(index, df<span class="op">=</span>df):</span>
<span id="cb98-236"><a href="#cb98-236" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mini-helper func that transforms an index into a pandas Series with the index</span></span>
<span id="cb98-237"><a href="#cb98-237" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.Series(index, index<span class="op">=</span>df.index)</span>
<span id="cb98-238"><a href="#cb98-238" aria-hidden="true" tabindex="-1"></a>    dict_timefeat <span class="op">=</span> {}</span>
<span id="cb98-239"><a href="#cb98-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-240"><a href="#cb98-240" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> all_freqs:</span>
<span id="cb98-241"><a href="#cb98-241" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'year_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_year_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb98-242"><a href="#cb98-242" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_of_year'</span>] <span class="op">=</span> i2s(df.index.quarter <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb98-243"><a href="#cb98-243" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_quarter_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb98-244"><a href="#cb98-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-245"><a href="#cb98-245" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>]]:</span>
<span id="cb98-246"><a href="#cb98-246" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">3</span>))</span>
<span id="cb98-247"><a href="#cb98-247" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_year'</span>] <span class="op">=</span> i2s(df.index.month <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb98-248"><a href="#cb98-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-249"><a href="#cb98-249" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>, <span class="st">"m"</span>]]:</span>
<span id="cb98-250"><a href="#cb98-250" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_month'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.day <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">7</span>))</span>
<span id="cb98-251"><a href="#cb98-251" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: ((x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>(x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">-01'</span>)).days <span class="op">//</span> <span class="dv">7</span>)))</span>
<span id="cb98-252"><a href="#cb98-252" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_year'</span>] <span class="op">=</span> i2s(df.index.isocalendar().week)</span>
<span id="cb98-253"><a href="#cb98-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-254"><a href="#cb98-254" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="op">==</span> <span class="st">"d"</span>:</span>
<span id="cb98-255"><a href="#cb98-255" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_week'</span>] <span class="op">=</span> i2s(df.index.dayofweek)</span>
<span id="cb98-256"><a href="#cb98-256" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_month'</span>] <span class="op">=</span> i2s(df.index.day)</span>
<span id="cb98-257"><a href="#cb98-257" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-01-01'</span>)).days <span class="op">%</span> <span class="dv">91</span>))</span>
<span id="cb98-258"><a href="#cb98-258" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_year'</span>] <span class="op">=</span> i2s(df.index.dayofyear)</span>
<span id="cb98-259"><a href="#cb98-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-260"><a href="#cb98-260" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the dictionary of columns to a DataFrame</span></span>
<span id="cb98-261"><a href="#cb98-261" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> pd.concat(dict_timefeat, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb98-262"><a href="#cb98-262" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features:</span>
<span id="cb98-263"><a href="#cb98-263" aria-hidden="true" tabindex="-1"></a>        df_timefeat <span class="op">=</span> df_timefeat[features]</span>
<span id="cb98-264"><a href="#cb98-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-265"><a href="#cb98-265" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_const:</span>
<span id="cb98-266"><a href="#cb98-266" aria-hidden="true" tabindex="-1"></a>        var_thresh <span class="op">=</span> VarianceThreshold(threshold<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb98-267"><a href="#cb98-267" aria-hidden="true" tabindex="-1"></a>        df_timefeat <span class="op">=</span> var_thresh.fit_transform(df_timefeat)</span>
<span id="cb98-268"><a href="#cb98-268" aria-hidden="true" tabindex="-1"></a>        df_timefeat <span class="op">=</span> pd.DataFrame(df_timefeat, columns<span class="op">=</span>var_thresh.get_feature_names_out(), index<span class="op">=</span>df.index).astype(<span class="bu">int</span>)</span>
<span id="cb98-269"><a href="#cb98-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-270"><a href="#cb98-270" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> add_to_df:</span>
<span id="cb98-271"><a href="#cb98-271" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.concat([df, df_timefeat], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb98-272"><a href="#cb98-272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb98-273"><a href="#cb98-273" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> df_timefeat</span>
<span id="cb98-274"><a href="#cb98-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-275"><a href="#cb98-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-276"><a href="#cb98-276" aria-hidden="true" tabindex="-1"></a>Specifically, temporal features are an excellent (and rare) type of *known future* input. Those are the data that we know will be like that during forecasting time, ie, at the time the observation $y_t$ takes place. For example, it is trivial to know the day of the week, of the month etc, for any date we are forecasting in.</span>
<span id="cb98-277"><a href="#cb98-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-278"><a href="#cb98-278" aria-hidden="true" tabindex="-1"></a>For this reason, we now calculate the temporal features of inflation.</span>
<span id="cb98-279"><a href="#cb98-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-282"><a href="#cb98-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-283"><a href="#cb98-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: temporal features for the inflation series</span></span>
<span id="cb98-284"><a href="#cb98-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-285"><a href="#cb98-285" aria-hidden="true" tabindex="-1"></a>df_timefeat <span class="op">=</span> get_timefeat(df_infl_m, freq<span class="op">=</span><span class="st">"m"</span>)</span>
<span id="cb98-286"><a href="#cb98-286" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-287"><a href="#cb98-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-288"><a href="#cb98-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-289"><a href="#cb98-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-290"><a href="#cb98-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Splitting the dataset</span></span>
<span id="cb98-291"><a href="#cb98-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-292"><a href="#cb98-292" aria-hidden="true" tabindex="-1"></a>We will now split the dataset into training data up until end-2020 and validation data afterwards. The training data will be further split into 5 temporally sequential folds.<span class="ot">[^tssplit]</span></span>
<span id="cb98-293"><a href="#cb98-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-294"><a href="#cb98-294" aria-hidden="true" tabindex="-1"></a><span class="ot">[^tssplit]: </span>See <span class="co">[</span><span class="ot">here</span><span class="co">](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)</span> for more information on time series splitting.</span>
<span id="cb98-295"><a href="#cb98-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-296"><a href="#cb98-296" aria-hidden="true" tabindex="-1"></a>To simplify, we will consider valid nowcasting *input* data for a given output in period $m$ as:</span>
<span id="cb98-297"><a href="#cb98-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-298"><a href="#cb98-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>all monthly data up to, and including, $m-1$; and</span>
<span id="cb98-299"><a href="#cb98-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-300"><a href="#cb98-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>all daily data up to, and including, $d(m)$.</span>
<span id="cb98-301"><a href="#cb98-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-304"><a href="#cb98-304" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-305"><a href="#cb98-305" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "time series splits"</span></span>
<span id="cb98-306"><a href="#cb98-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-307"><a href="#cb98-307" aria-hidden="true" tabindex="-1"></a><span class="co"># Training date cutoff</span></span>
<span id="cb98-308"><a href="#cb98-308" aria-hidden="true" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="st">"2020-12-31"</span></span>
<span id="cb98-309"><a href="#cb98-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-310"><a href="#cb98-310" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> df_infl_m[:cutoff][<span class="dv">1</span>:], df_infl_m[cutoff:][<span class="dv">1</span>:]</span>
<span id="cb98-311"><a href="#cb98-311" aria-hidden="true" tabindex="-1"></a>Xm_train, Xm_test <span class="op">=</span> df_infl_m[:cutoff][:<span class="op">-</span><span class="dv">1</span>], df_infl_m[cutoff:][:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb98-312"><a href="#cb98-312" aria-hidden="true" tabindex="-1"></a>Xd_train, Xd_test <span class="op">=</span> df_oil_d[:cutoff], df_oil_d[cutoff:]</span>
<span id="cb98-313"><a href="#cb98-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-314"><a href="#cb98-314" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> {<span class="st">"m"</span>: Xm_train, <span class="st">"d"</span>: Xd_train}</span>
<span id="cb98-315"><a href="#cb98-315" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> {<span class="st">"m"</span>: Xm_test, <span class="st">"d"</span>: Xd_test}</span>
<span id="cb98-316"><a href="#cb98-316" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-317"><a href="#cb98-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-318"><a href="#cb98-318" aria-hidden="true" tabindex="-1"></a>Now for every month $m$ in the dependent variable, we can find all $m_{t-l}, l\geq 1$ and all $d(m_{t-s}), s\geq 0$.</span>
<span id="cb98-319"><a href="#cb98-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-320"><a href="#cb98-320" aria-hidden="true" tabindex="-1"></a>Note that in the example below, for each data point that we want to forecast (<span class="in">`y`</span>), we take 12 lags of the monthly covariates and 250 lags of the daily covariates (broadly corresponding to one year).</span>
<span id="cb98-321"><a href="#cb98-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-324"><a href="#cb98-324" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-325"><a href="#cb98-325" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: create data</span></span>
<span id="cb98-326"><a href="#cb98-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-327"><a href="#cb98-327" aria-hidden="true" tabindex="-1"></a>maxlags <span class="op">=</span> {<span class="st">"m"</span>: <span class="dv">12</span>, <span class="st">"d"</span>: <span class="dv">250</span>}</span>
<span id="cb98-328"><a href="#cb98-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-329"><a href="#cb98-329" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data(X, y, maxlags<span class="op">=</span>maxlags, tscv<span class="op">=</span>TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">5</span>), timedim<span class="op">=</span><span class="va">True</span>, return_dates<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb98-330"><a href="#cb98-330" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If timeedim is true, then the dimensions of the tensors are n_samples/time dimension/features. If not then it is n_samples/time dimension (lag) * features, ie each lag is flattened as if it were a feature. Use True when passing to recurrent nets, use False for fully connected layers.</span></span>
<span id="cb98-331"><a href="#cb98-331" aria-hidden="true" tabindex="-1"></a>    X_split <span class="op">=</span> {}</span>
<span id="cb98-332"><a href="#cb98-332" aria-hidden="true" tabindex="-1"></a>    y_split <span class="op">=</span> {}</span>
<span id="cb98-333"><a href="#cb98-333" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> return_dates:</span>
<span id="cb98-334"><a href="#cb98-334" aria-hidden="true" tabindex="-1"></a>        X_dates <span class="op">=</span> {}</span>
<span id="cb98-335"><a href="#cb98-335" aria-hidden="true" tabindex="-1"></a>    n_feat <span class="op">=</span> {k: v.shape[<span class="dv">1</span>] <span class="cf">for</span> k, v <span class="kw">in</span> X.items()}</span>
<span id="cb98-336"><a href="#cb98-336" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tscv:</span>
<span id="cb98-337"><a href="#cb98-337" aria-hidden="true" tabindex="-1"></a>        split_cv <span class="op">=</span> tscv.split(y)</span>
<span id="cb98-338"><a href="#cb98-338" aria-hidden="true" tabindex="-1"></a>        cv_dates <span class="op">=</span> [</span>
<span id="cb98-339"><a href="#cb98-339" aria-hidden="true" tabindex="-1"></a>            (y.index[m], y.index[n]) <span class="co"># (train, valid) for each fold</span></span>
<span id="cb98-340"><a href="#cb98-340" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> m, n <span class="kw">in</span> split_cv</span>
<span id="cb98-341"><a href="#cb98-341" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb98-342"><a href="#cb98-342" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n_fold, split <span class="kw">in</span> <span class="bu">enumerate</span>(cv_dates):</span>
<span id="cb98-343"><a href="#cb98-343" aria-hidden="true" tabindex="-1"></a>            fold <span class="op">=</span> <span class="ss">f"fold_</span><span class="sc">{</span>n_fold<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-344"><a href="#cb98-344" aria-hidden="true" tabindex="-1"></a>            dates_split <span class="op">=</span> {</span>
<span id="cb98-345"><a href="#cb98-345" aria-hidden="true" tabindex="-1"></a>                <span class="st">"train"</span>: split[<span class="dv">0</span>],</span>
<span id="cb98-346"><a href="#cb98-346" aria-hidden="true" tabindex="-1"></a>                <span class="st">"valid"</span>: split[<span class="dv">1</span>]</span>
<span id="cb98-347"><a href="#cb98-347" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb98-348"><a href="#cb98-348" aria-hidden="true" tabindex="-1"></a>            X_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb98-349"><a href="#cb98-349" aria-hidden="true" tabindex="-1"></a>            y_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb98-350"><a href="#cb98-350" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> return_dates:</span>
<span id="cb98-351"><a href="#cb98-351" aria-hidden="true" tabindex="-1"></a>                X_dates[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb98-352"><a href="#cb98-352" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> chunk, dates <span class="kw">in</span> dates_split.items():</span>
<span id="cb98-353"><a href="#cb98-353" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> ysample_date <span class="kw">in</span> tqdm(dates):</span>
<span id="cb98-354"><a href="#cb98-354" aria-hidden="true" tabindex="-1"></a>                    padded_x <span class="op">=</span> {}</span>
<span id="cb98-355"><a href="#cb98-355" aria-hidden="true" tabindex="-1"></a>                    dates_X <span class="op">=</span> {}</span>
<span id="cb98-356"><a href="#cb98-356" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> f <span class="kw">in</span> X.keys():</span>
<span id="cb98-357"><a href="#cb98-357" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">try</span>:</span>
<span id="cb98-358"><a href="#cb98-358" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">if</span> return_dates:</span>
<span id="cb98-359"><a href="#cb98-359" aria-hidden="true" tabindex="-1"></a>                                dates_X[f] <span class="op">=</span> X[f][:ysample_date][:<span class="op">-</span><span class="dv">1</span>].index</span>
<span id="cb98-360"><a href="#cb98-360" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> X[f][:ysample_date][:<span class="op">-</span><span class="dv">1</span>].values</span>
<span id="cb98-361"><a href="#cb98-361" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb98-362"><a href="#cb98-362" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb98-363"><a href="#cb98-363" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> keras.utils.pad_sequences([to_pad], maxlen<span class="op">=</span>maxlags[f], dtype<span class="op">=</span>np.float32)</span>
<span id="cb98-364"><a href="#cb98-364" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb98-365"><a href="#cb98-365" aria-hidden="true" tabindex="-1"></a>                        x_shape <span class="op">=</span> (<span class="dv">1</span>, maxlags[f], n_feat[f]) <span class="cf">if</span> timedim <span class="cf">else</span> (<span class="dv">1</span>, maxlags[f] <span class="op">*</span> n_feat[f])</span>
<span id="cb98-366"><a href="#cb98-366" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb98-367"><a href="#cb98-367" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> padded_x[f].reshape(x_shape)</span>
<span id="cb98-368"><a href="#cb98-368" aria-hidden="true" tabindex="-1"></a>                    X_split[fold][chunk].append(padded_x)</span>
<span id="cb98-369"><a href="#cb98-369" aria-hidden="true" tabindex="-1"></a>                    y_split[fold][chunk].append(y_train.loc[ysample_date])</span>
<span id="cb98-370"><a href="#cb98-370" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> return_dates:</span>
<span id="cb98-371"><a href="#cb98-371" aria-hidden="true" tabindex="-1"></a>                        X_dates[fold][chunk].append(dates_X)</span>
<span id="cb98-372"><a href="#cb98-372" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_dates:</span>
<span id="cb98-373"><a href="#cb98-373" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> X_split, y_split, X_dates</span>
<span id="cb98-374"><a href="#cb98-374" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X_split, y_split</span>
<span id="cb98-375"><a href="#cb98-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-376"><a href="#cb98-376" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split, dates <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags, return_dates<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-377"><a href="#cb98-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-378"><a href="#cb98-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-379"><a href="#cb98-379" aria-hidden="true" tabindex="-1"></a>Let's see how this time series fold will be structured. Each fold is a sequentially longer window, so we get the following data points:<span class="ot">[^difflengths]</span></span>
<span id="cb98-380"><a href="#cb98-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-381"><a href="#cb98-381" aria-hidden="true" tabindex="-1"></a><span class="ot">[^difflengths]: </span>The division of the data between folds might produce slightly unequal-sized folds, as can be seen in the current example. </span>
<span id="cb98-382"><a href="#cb98-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-385"><a href="#cb98-385" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-386"><a href="#cb98-386" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "Create batches of data"</span></span>
<span id="cb98-387"><a href="#cb98-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-388"><a href="#cb98-388" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> y_train_split.keys():</span>
<span id="cb98-389"><a href="#cb98-389" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb98-390"><a href="#cb98-390" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'train'</span>])<span class="sc">}</span><span class="ss"> training X-y pairs"</span>)</span>
<span id="cb98-391"><a href="#cb98-391" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'valid'</span>])<span class="sc">}</span><span class="ss"> validation X-y pairs"</span>)</span>
<span id="cb98-392"><a href="#cb98-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-393"><a href="#cb98-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-394"><a href="#cb98-394" aria-hidden="true" tabindex="-1"></a>Another way to visualise this is in @fig-tssplit, which shows a similar division of dataset along the time dimension, albeit for four folds.</span>
<span id="cb98-395"><a href="#cb98-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-396"><a href="#cb98-396" aria-hidden="true" tabindex="-1"></a>::: {#fig-tssplit}</span>
<span id="cb98-397"><a href="#cb98-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-398"><a href="#cb98-398" aria-hidden="true" tabindex="-1"></a><span class="al">![Time series split illustration](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_013.png)</span>{fig-scap='Source: scikit-learn.org'}</span>
<span id="cb98-399"><a href="#cb98-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-400"><a href="#cb98-400" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb98-401"><a href="#cb98-401" aria-hidden="true" tabindex="-1"></a><span class="fu">## A simple, fully connected neural network {#sec-fc}</span></span>
<span id="cb98-402"><a href="#cb98-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-403"><a href="#cb98-403" aria-hidden="true" tabindex="-1"></a>The goal of this model is to nowcast $\pi_t$ based on its past values $\pi_{t-1}$ and on current oil prices $o_{d(m-s)}, s \geq 0$. The first model we will train is a very simple neural network:</span>
<span id="cb98-404"><a href="#cb98-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-405"><a href="#cb98-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb98-406"><a href="#cb98-406" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb98-407"><a href="#cb98-407" aria-hidden="true" tabindex="-1"></a>\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) <span class="sc">\\</span></span>
<span id="cb98-408"><a href="#cb98-408" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mathbf{W}_2 \xi + \mathbf{b}_2,</span>
<span id="cb98-409"><a href="#cb98-409" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb98-410"><a href="#cb98-410" aria-hidden="true" tabindex="-1"></a>$$ {#eq-model0nnlayer}</span>
<span id="cb98-411"><a href="#cb98-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-412"><a href="#cb98-412" aria-hidden="true" tabindex="-1"></a>where $x_t$ is the input data and the subscript of parameters relates to the "depth" of the layer they belong to. Using $\lambda$ as the dimensionality of the model, $\mathbf{W}_2 \in \mathbb{R}^{1 \times \lambda}$, $b_2 \in \mathbb{R}$, $\mathbf{W}_1 \in \mathbb{R}^{\lambda \times |x_t|}$, $b_1 \in \mathbb{R}^{d}$, $\xi \in \mathbb{R}^\lambda$ and $\phi$ is an activation function. For simplicity, we will use the ReLU activation function, which is simply: $\phi(z) = \text{max}(z, 0)$.</span>
<span id="cb98-413"><a href="#cb98-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-414"><a href="#cb98-414" aria-hidden="true" tabindex="-1"></a>For this neural network, we need a fix dimensionality of the input data. In other words, the network *needs* to know how much data it will take in at any given time, and this should not change throughout training or inference time.</span>
<span id="cb98-415"><a href="#cb98-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-416"><a href="#cb98-416" aria-hidden="true" tabindex="-1"></a>For each data in our dependent variable, we simply stack the latest available monthly and daily data and their respective lags. Using the numbers above, this would be 12 lags for monthly data  and 250 lags of daily oil data. Linking this to @eq-model0nnlayer above, $x_t = <span class="co">[</span><span class="ot">\pi_{m-1}, \dots, \pi_{m-12}, o_d, \dots, o_{d-250}</span><span class="co">]</span>$. </span>
<span id="cb98-417"><a href="#cb98-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-418"><a href="#cb98-418" aria-hidden="true" tabindex="-1"></a>All of this data will be considered by the neural network at the same time. In a way, this is analogous to how a normal regression is run. However, the number of data points (12 + 250) used in this toy neural network is bigger than typical regressions.</span>
<span id="cb98-419"><a href="#cb98-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-422"><a href="#cb98-422" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-423"><a href="#cb98-423" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-nn_fc_summary</span></span>
<span id="cb98-424"><a href="#cb98-424" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of fully connected model</span></span>
<span id="cb98-425"><a href="#cb98-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-426"><a href="#cb98-426" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> createNN_fc(dim<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>):</span>
<span id="cb98-427"><a href="#cb98-427" aria-hidden="true" tabindex="-1"></a>    nn_fc <span class="op">=</span> keras.Sequential([</span>
<span id="cb98-428"><a href="#cb98-428" aria-hidden="true" tabindex="-1"></a>        keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),)),</span>
<span id="cb98-429"><a href="#cb98-429" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span>activation, name<span class="op">=</span><span class="st">"SummariseInput"</span>),</span>
<span id="cb98-430"><a href="#cb98-430" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)</span>
<span id="cb98-431"><a href="#cb98-431" aria-hidden="true" tabindex="-1"></a>    ], <span class="st">"FullyConnected"</span>)</span>
<span id="cb98-432"><a href="#cb98-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-433"><a href="#cb98-433" aria-hidden="true" tabindex="-1"></a>    nn_fc.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-434"><a href="#cb98-434" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn_fc</span>
<span id="cb98-435"><a href="#cb98-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-436"><a href="#cb98-436" aria-hidden="true" tabindex="-1"></a>nn_fc <span class="op">=</span> createNN_fc()</span>
<span id="cb98-437"><a href="#cb98-437" aria-hidden="true" tabindex="-1"></a>nn_fc.summary()</span>
<span id="cb98-438"><a href="#cb98-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-439"><a href="#cb98-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-440"><a href="#cb98-440" aria-hidden="true" tabindex="-1"></a>@fig-arch_fc presents the architecture of this model.</span>
<span id="cb98-441"><a href="#cb98-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-444"><a href="#cb98-444" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-445"><a href="#cb98-445" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_fc</span></span>
<span id="cb98-446"><a href="#cb98-446" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the model with fully connected layer</span></span>
<span id="cb98-447"><a href="#cb98-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-448"><a href="#cb98-448" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-449"><a href="#cb98-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-450"><a href="#cb98-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-451"><a href="#cb98-451" aria-hidden="true" tabindex="-1"></a>Note in @fig-arch_fc that a first dense layer (ie, as in @eq-model0nnlayer) takes in 262 data points, uses the activation function ReLU, and then outputs 16 data points for the next layer.<span class="ot">[^NoneDim]</span></span>
<span id="cb98-452"><a href="#cb98-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-453"><a href="#cb98-453" aria-hidden="true" tabindex="-1"></a><span class="ot">[^NoneDim]: </span>Ignore the <span class="in">`None`</span> in the shapes; this means the dimensionality along that axis depends on each case. Specifically, the first dimension of the shapes in keras are always the number of samples going in the model. Because this varies with every training, application, etc, it is not fixed as the other dimensions are. Another way of thinking about this is as follows: in a regression, you know exactly how many variables you need to have, but the number of data points can vary.</span>
<span id="cb98-454"><a href="#cb98-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-455"><a href="#cb98-455" aria-hidden="true" tabindex="-1"></a>Checking that it works. In the code below, we take the last fold as an example. </span>
<span id="cb98-456"><a href="#cb98-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-457"><a href="#cb98-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-460"><a href="#cb98-460" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-461"><a href="#cb98-461" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: preparing fully connected NN data</span></span>
<span id="cb98-462"><a href="#cb98-462" aria-hidden="true" tabindex="-1"></a>X_train_split_fc, y_train_split_fc <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, timedim<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb98-463"><a href="#cb98-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-464"><a href="#cb98-464" aria-hidden="true" tabindex="-1"></a>fold <span class="op">=</span> <span class="st">"fold_4"</span></span>
<span id="cb98-465"><a href="#cb98-465" aria-hidden="true" tabindex="-1"></a>X_train_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"train"</span>]])</span>
<span id="cb98-466"><a href="#cb98-466" aria-hidden="true" tabindex="-1"></a>y_train_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"train"</span>])</span>
<span id="cb98-467"><a href="#cb98-467" aria-hidden="true" tabindex="-1"></a>X_valid_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"valid"</span>]])</span>
<span id="cb98-468"><a href="#cb98-468" aria-hidden="true" tabindex="-1"></a>y_valid_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"valid"</span>])</span>
<span id="cb98-469"><a href="#cb98-469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-470"><a href="#cb98-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-471"><a href="#cb98-471" aria-hidden="true" tabindex="-1"></a>Now we train the network.</span>
<span id="cb98-472"><a href="#cb98-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-475"><a href="#cb98-475" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-476"><a href="#cb98-476" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fit nn model</span></span>
<span id="cb98-477"><a href="#cb98-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-478"><a href="#cb98-478" aria-hidden="true" tabindex="-1"></a>history_fc <span class="op">=</span> nn_fc.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-479"><a href="#cb98-479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-480"><a href="#cb98-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-481"><a href="#cb98-481" aria-hidden="true" tabindex="-1"></a>You can see in @fig-history_fc that the loss decreases with training, but the validation loss (ie, calculated on held-out data) is higher than the in-sample loss. This suggests the model is learning *too much* how to fit the data. In other words, it is also fitting some level of noise, which is not reproducible out-of-sample.</span>
<span id="cb98-482"><a href="#cb98-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-485"><a href="#cb98-485" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-486"><a href="#cb98-486" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_fc</span></span>
<span id="cb98-487"><a href="#cb98-487" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a simple, fully-connected neural network.</span></span>
<span id="cb98-488"><a href="#cb98-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-489"><a href="#cb98-489" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_fc.history).plot()</span>
<span id="cb98-490"><a href="#cb98-490" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-491"><a href="#cb98-491" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-492"><a href="#cb98-492" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-493"><a href="#cb98-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-494"><a href="#cb98-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-495"><a href="#cb98-495" aria-hidden="true" tabindex="-1"></a>We don't need to bother training too much this very simple neural network; the goal here is to use it as a building block for a mathematical/econometric intuition of the broader nowcasting model.</span>
<span id="cb98-496"><a href="#cb98-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-497"><a href="#cb98-497" aria-hidden="true" tabindex="-1"></a><span class="fu">## A useful tool: a gate {#sec-glu}</span></span>
<span id="cb98-498"><a href="#cb98-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-499"><a href="#cb98-499" aria-hidden="true" tabindex="-1"></a>The network trained in the previous section can learn how to map the input data to the output data. But there are ways to take advantage of the incredible flexibility in architecture (ie, how neural layers are stacked). One such way is to have the data inform which layers's outputs are actually used downstream or not. This section describes how.</span>
<span id="cb98-500"><a href="#cb98-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-501"><a href="#cb98-501" aria-hidden="true" tabindex="-1"></a>First, we replicate the same simple fully connected layer of @eq-model0nnlayer two times. One of the neural networks will behave as before: learning to map the input data to the output data. The second one will also look at the same data, but with a different goal: it will learn how much data to let through. Its output is a value between 0 and 1, which is then multiplied to the "original" network. When this part of the neural network yields values closer to 0, the mainstream values are effectively shut down. Conversely, when the values are close to 1, the data proceeds as normal.</span>
<span id="cb98-502"><a href="#cb98-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-503"><a href="#cb98-503" aria-hidden="true" tabindex="-1"></a>Adjusting @eq-model0nnlayer to include a gate could be as in @eq-gated:</span>
<span id="cb98-504"><a href="#cb98-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-505"><a href="#cb98-505" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb98-506"><a href="#cb98-506" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb98-507"><a href="#cb98-507" aria-hidden="true" tabindex="-1"></a>\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) <span class="sc">\\</span></span>
<span id="cb98-508"><a href="#cb98-508" aria-hidden="true" tabindex="-1"></a>G &amp;= \sigma(\mathbf{W}_G x_t + \mathbf{b}_G) <span class="sc">\\</span></span>
<span id="cb98-509"><a href="#cb98-509" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mathbf{W}_2 (\xi \odot G) + \mathbf{b}_2,</span>
<span id="cb98-510"><a href="#cb98-510" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb98-511"><a href="#cb98-511" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gated}</span>
<span id="cb98-512"><a href="#cb98-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-513"><a href="#cb98-513" aria-hidden="true" tabindex="-1"></a>with $\sigma$ representing the sigmoid function and $\odot$ the Hadamard multiplication. </span>
<span id="cb98-514"><a href="#cb98-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-515"><a href="#cb98-515" aria-hidden="true" tabindex="-1"></a>This type of model where the gate is trained on the same data was introduced by @dauphin2017language. From now on, we refer to @eq-gated as a Gated Linear Unit (GLU).</span>
<span id="cb98-516"><a href="#cb98-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-517"><a href="#cb98-517" aria-hidden="true" tabindex="-1"></a>Note that the data that informs the gate does not necessarily need to be the same as the mainstream data.</span>
<span id="cb98-518"><a href="#cb98-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-521"><a href="#cb98-521" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-522"><a href="#cb98-522" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-nn_fc_gated_summary</span></span>
<span id="cb98-523"><a href="#cb98-523" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of gated model</span></span>
<span id="cb98-524"><a href="#cb98-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-525"><a href="#cb98-525" aria-hidden="true" tabindex="-1"></a><span class="co">#mainstream = createNN_fc(activation="relu")</span></span>
<span id="cb98-526"><a href="#cb98-526" aria-hidden="true" tabindex="-1"></a><span class="co">#gate = createNN_fc(activation="sigmoid")</span></span>
<span id="cb98-527"><a href="#cb98-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-528"><a href="#cb98-528" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb98-529"><a href="#cb98-529" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),), name<span class="op">=</span><span class="st">"FlattenedLaggedData"</span>)</span>
<span id="cb98-530"><a href="#cb98-530" aria-hidden="true" tabindex="-1"></a>mainstream <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(<span class="bu">input</span>)</span>
<span id="cb98-531"><a href="#cb98-531" aria-hidden="true" tabindex="-1"></a>gate <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"Gate"</span>)(<span class="bu">input</span>)</span>
<span id="cb98-532"><a href="#cb98-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-533"><a href="#cb98-533" aria-hidden="true" tabindex="-1"></a>gated_data <span class="op">=</span> keras.layers.Multiply(name<span class="op">=</span><span class="st">"GatedData"</span>)([mainstream, gate])</span>
<span id="cb98-534"><a href="#cb98-534" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(gated_data)</span>
<span id="cb98-535"><a href="#cb98-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-536"><a href="#cb98-536" aria-hidden="true" tabindex="-1"></a>nn_fc_gated <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">input</span>, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GatedModel"</span>)</span>
<span id="cb98-537"><a href="#cb98-537" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-538"><a href="#cb98-538" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.summary()</span>
<span id="cb98-539"><a href="#cb98-539" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-540"><a href="#cb98-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-543"><a href="#cb98-543" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-544"><a href="#cb98-544" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_gatedmodel</span></span>
<span id="cb98-545"><a href="#cb98-545" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of gated neural network</span></span>
<span id="cb98-546"><a href="#cb98-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-547"><a href="#cb98-547" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc_gated, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-548"><a href="#cb98-548" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-549"><a href="#cb98-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-550"><a href="#cb98-550" aria-hidden="true" tabindex="-1"></a>The model can now be fit with the same input data: it is then used by two different branches (referred to above in the text as "mainstream" and "gate").</span>
<span id="cb98-551"><a href="#cb98-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-554"><a href="#cb98-554" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-555"><a href="#cb98-555" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fitting gated model</span></span>
<span id="cb98-556"><a href="#cb98-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-557"><a href="#cb98-557" aria-hidden="true" tabindex="-1"></a>history_fc_gated <span class="op">=</span> nn_fc_gated.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-558"><a href="#cb98-558" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-559"><a href="#cb98-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-562"><a href="#cb98-562" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-563"><a href="#cb98-563" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_fc_gated</span></span>
<span id="cb98-564"><a href="#cb98-564" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a simple, fully-connected neural network with gate.</span></span>
<span id="cb98-565"><a href="#cb98-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-566"><a href="#cb98-566" aria-hidden="true" tabindex="-1"></a>gated_loss <span class="op">=</span> pd.DataFrame(history_fc_gated.history)</span>
<span id="cb98-567"><a href="#cb98-567" aria-hidden="true" tabindex="-1"></a>gated_loss[<span class="st">"val_loss (no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb98-568"><a href="#cb98-568" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> gated_loss.plot()</span>
<span id="cb98-569"><a href="#cb98-569" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-570"><a href="#cb98-570" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-571"><a href="#cb98-571" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-572"><a href="#cb98-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-573"><a href="#cb98-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-574"><a href="#cb98-574" aria-hidden="true" tabindex="-1"></a><span class="fu">## Long short-term memory {#sec-lstm}</span></span>
<span id="cb98-575"><a href="#cb98-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-576"><a href="#cb98-576" aria-hidden="true" tabindex="-1"></a>A marked improvement in how we can model time series data is the use of recurrent neural networks (RNNs). In essence, these are networks that learn to keep a stateful memory, which is updated as the network "visits" each sequential step in time, in turn using both the memory and the new data at that period to predict the output.</span>
<span id="cb98-577"><a href="#cb98-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-578"><a href="#cb98-578" aria-hidden="true" tabindex="-1"></a>In contrast to the fully connected layer in @sec-fc, which need to look at different lags to pick up any history-dependent information, RNNs look at the observable variables at each period and learn a latent "state" (akin to Kalman filters, for example). The same network then slides up one step in time and uses that information and the previous state to update the state, and so on...</span>
<span id="cb98-579"><a href="#cb98-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-580"><a href="#cb98-580" aria-hidden="true" tabindex="-1"></a>One particular type of RNN that has proven to be very successful in practice is the long short-term memory (LSTM) model, due to @hochreiter1997long. It is actually a combination of four different layers, of which three are actually gates. These layers are built in a specific way.</span>
<span id="cb98-581"><a href="#cb98-581" aria-hidden="true" tabindex="-1"></a> Here's how the input vector $x_t$ and the learned LSTM state $h_{t-1}$ are used for the LSTM-forward pass at time step $t$:</span>
<span id="cb98-582"><a href="#cb98-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-583"><a href="#cb98-583" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb98-584"><a href="#cb98-584" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb98-585"><a href="#cb98-585" aria-hidden="true" tabindex="-1"></a>f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f) <span class="sc">\\</span></span>
<span id="cb98-586"><a href="#cb98-586" aria-hidden="true" tabindex="-1"></a>i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i) <span class="sc">\\</span></span>
<span id="cb98-587"><a href="#cb98-587" aria-hidden="true" tabindex="-1"></a>o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + b_o) <span class="sc">\\</span></span>
<span id="cb98-588"><a href="#cb98-588" aria-hidden="true" tabindex="-1"></a>\tilde{c}_t &amp;= \omega(W_c x_t + U_c h_{t-1} + b_c) <span class="sc">\\</span></span>
<span id="cb98-589"><a href="#cb98-589" aria-hidden="true" tabindex="-1"></a>c_t &amp;= \underbrace{f_t \odot c_{t-1}}_{\text{Gated past data}} + \underbrace{i_t \odot \tilde{c}_t}_{\text{How much to learn}} <span class="sc">\\</span></span>
<span id="cb98-590"><a href="#cb98-590" aria-hidden="true" tabindex="-1"></a>h_t &amp;= o_t \odot \omega(c_t),</span>
<span id="cb98-591"><a href="#cb98-591" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb98-592"><a href="#cb98-592" aria-hidden="true" tabindex="-1"></a>$$ {#eq-model1lstm}</span>
<span id="cb98-593"><a href="#cb98-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-594"><a href="#cb98-594" aria-hidden="true" tabindex="-1"></a>where $\omega$ is the hyperbolic function.</span>
<span id="cb98-595"><a href="#cb98-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-596"><a href="#cb98-596" aria-hidden="true" tabindex="-1"></a>The basic intuition of the LSTM is that some of the individual component layers essentially learn to look at the current data and the past memory and then decide how much new information to let through. Note that, because their activation is a sigmoid, the output of layers $f_t$, $i_t$ and $o_t$ is a number between 0 and 1. This idea is important to bear in mind because it will be used at a much bigger scale by the whole TFT model - and will be one key feature of its interpretability.</span>
<span id="cb98-597"><a href="#cb98-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-598"><a href="#cb98-598" aria-hidden="true" tabindex="-1"></a>With LSTM networks, it is easier to incorporate mixed-frequency data in a meaningful way. This is done below by passing data of each frequency through their own LSTM layers, and combining their last outputs.</span>
<span id="cb98-599"><a href="#cb98-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-602"><a href="#cb98-602" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-603"><a href="#cb98-603" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-lstm_summary</span></span>
<span id="cb98-604"><a href="#cb98-604" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of LSTM model</span></span>
<span id="cb98-605"><a href="#cb98-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-606"><a href="#cb98-606" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb98-607"><a href="#cb98-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-608"><a href="#cb98-608" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>] <span class="co"># using here the commonly-used frequency abbreviations</span></span>
<span id="cb98-609"><a href="#cb98-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-610"><a href="#cb98-610" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb98-611"><a href="#cb98-611" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb98-612"><a href="#cb98-612" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> inputs.items():</span>
<span id="cb98-613"><a href="#cb98-613" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb98-614"><a href="#cb98-614" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb98-615"><a href="#cb98-615" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb98-616"><a href="#cb98-616" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb98-617"><a href="#cb98-617" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb98-618"><a href="#cb98-618" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb98-619"><a href="#cb98-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-620"><a href="#cb98-620" aria-hidden="true" tabindex="-1"></a>nn_lstm <span class="op">=</span> keras.Model(</span>
<span id="cb98-621"><a href="#cb98-621" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb98-622"><a href="#cb98-622" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb98-623"><a href="#cb98-623" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"LSTMNetwork"</span></span>
<span id="cb98-624"><a href="#cb98-624" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-625"><a href="#cb98-625" aria-hidden="true" tabindex="-1"></a>nn_lstm.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-626"><a href="#cb98-626" aria-hidden="true" tabindex="-1"></a>nn_lstm.summary()</span>
<span id="cb98-627"><a href="#cb98-627" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-628"><a href="#cb98-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-631"><a href="#cb98-631" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-632"><a href="#cb98-632" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_lstm</span></span>
<span id="cb98-633"><a href="#cb98-633" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the network with LSTM layer</span></span>
<span id="cb98-634"><a href="#cb98-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-635"><a href="#cb98-635" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_lstm, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-636"><a href="#cb98-636" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-637"><a href="#cb98-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-638"><a href="#cb98-638" aria-hidden="true" tabindex="-1"></a>The reason why the dimensions in the input layer are now <span class="in">`(None, None, 1)`</span>, with one addition <span class="in">`None`</span> compared to before(for example, @fig-arch_fc) is due to the *time dimension*. Whereas before the model didn't know how many samples it would be fed, now also the length of the time window can change because each time step will pass through exactly the same parameters. </span>
<span id="cb98-639"><a href="#cb98-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-640"><a href="#cb98-640" aria-hidden="true" tabindex="-1"></a>Note that the input goes through a few steps before reaching the LSTM layer. This is due to a masking layer that effectively helps the model jumps time steps for which there is no data available.</span>
<span id="cb98-641"><a href="#cb98-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-642"><a href="#cb98-642" aria-hidden="true" tabindex="-1"></a>To check that the LSTM-based neural network works, we need to feed this neural network a slightly different type of data. LSTM, as other recurrent neural networks, takes in time series data. So, unlike before, we now prepare a time series (or panel data) for each </span>
<span id="cb98-643"><a href="#cb98-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-644"><a href="#cb98-644" aria-hidden="true" tabindex="-1"></a>This neural network will then take in the inputted time series data, and encode each frequency's series separately through the different LSTM streams. The final result will no longer have a time dimension; it is then averaged, and this average embeddings of the different time series is used to forecast the variable of interest.</span>
<span id="cb98-645"><a href="#cb98-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-648"><a href="#cb98-648" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-649"><a href="#cb98-649" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train the lstm</span></span>
<span id="cb98-650"><a href="#cb98-650" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb98-651"><a href="#cb98-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-652"><a href="#cb98-652" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb98-653"><a href="#cb98-653" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb98-654"><a href="#cb98-654" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb98-655"><a href="#cb98-655" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb98-656"><a href="#cb98-656" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb98-657"><a href="#cb98-657" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb98-658"><a href="#cb98-658" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb98-659"><a href="#cb98-659" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb98-660"><a href="#cb98-660" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb98-661"><a href="#cb98-661" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb98-662"><a href="#cb98-662" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb98-663"><a href="#cb98-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-664"><a href="#cb98-664" aria-hidden="true" tabindex="-1"></a>history_lstm <span class="op">=</span> nn_lstm.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-665"><a href="#cb98-665" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-666"><a href="#cb98-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-669"><a href="#cb98-669" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-670"><a href="#cb98-670" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_lstm</span></span>
<span id="cb98-671"><a href="#cb98-671" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in an LSTM</span></span>
<span id="cb98-672"><a href="#cb98-672" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb98-673"><a href="#cb98-673" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - All models so far</span></span>
<span id="cb98-674"><a href="#cb98-674" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - LSTM only</span></span>
<span id="cb98-675"><a href="#cb98-675" aria-hidden="true" tabindex="-1"></a>lstm_loss <span class="op">=</span> pd.DataFrame(history_lstm.history)</span>
<span id="cb98-676"><a href="#cb98-676" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC with gate)"</span>] <span class="op">=</span> history_fc_gated.history[<span class="st">"val_loss"</span>]</span>
<span id="cb98-677"><a href="#cb98-677" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb98-678"><a href="#cb98-678" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss.plot()</span>
<span id="cb98-679"><a href="#cb98-679" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-680"><a href="#cb98-680" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-681"><a href="#cb98-681" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-682"><a href="#cb98-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-683"><a href="#cb98-683" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss[[<span class="st">"loss"</span>, <span class="st">"val_loss"</span>]].plot()</span>
<span id="cb98-684"><a href="#cb98-684" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-685"><a href="#cb98-685" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-686"><a href="#cb98-686" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-687"><a href="#cb98-687" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-688"><a href="#cb98-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-689"><a href="#cb98-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-690"><a href="#cb98-690" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducing... the gatekeepers {#sec-gates}</span></span>
<span id="cb98-691"><a href="#cb98-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-692"><a href="#cb98-692" aria-hidden="true" tabindex="-1"></a>Introducing gates in a model can bring important advantages. According to @lim2021temporal, GLUs:</span>
<span id="cb98-693"><a href="#cb98-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-694"><a href="#cb98-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"... reduce the vanishing gradient problem for deep architectures by providing a linear path for gradients while retaining non-linear capabilities", and</span>
<span id="cb98-695"><a href="#cb98-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"... provide flexibility to suppress any parts of the architecture that are not required for a given dataset".</span>
<span id="cb98-696"><a href="#cb98-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-697"><a href="#cb98-697" aria-hidden="true" tabindex="-1"></a>This section formalises the GLU model of @sec-glu as if it were a single layer. This serves as a building block for the Gated Residual Network (GRN), a group of layers that learns to dynamically adjust the complexity of a larger neural network.</span>
<span id="cb98-698"><a href="#cb98-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-699"><a href="#cb98-699" aria-hidden="true" tabindex="-1"></a>First, the GLU as a layer is introduced below.</span>
<span id="cb98-700"><a href="#cb98-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-703"><a href="#cb98-703" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-704"><a href="#cb98-704" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class glu</span></span>
<span id="cb98-705"><a href="#cb98-705" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: to add to codebase and replace here with an import</span></span>
<span id="cb98-706"><a href="#cb98-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-707"><a href="#cb98-707" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedLinearUnit(keras.Layer):</span>
<span id="cb98-708"><a href="#cb98-708" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-709"><a href="#cb98-709" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-710"><a href="#cb98-710" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb98-711"><a href="#cb98-711" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">|</span><span class="va">None</span><span class="op">=</span><span class="va">None</span>, <span class="co"># Dropout rate</span></span>
<span id="cb98-712"><a href="#cb98-712" aria-hidden="true" tabindex="-1"></a>        use_time_distributed:<span class="bu">bool</span><span class="op">=</span><span class="va">True</span>, <span class="co"># Apply the GLU across all time steps?</span></span>
<span id="cb98-713"><a href="#cb98-713" aria-hidden="true" tabindex="-1"></a>        activation:<span class="bu">str</span><span class="op">|</span>Callable<span class="op">|</span><span class="va">None</span><span class="op">=</span><span class="va">None</span>, <span class="co"># Activation function</span></span>
<span id="cb98-714"><a href="#cb98-714" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-715"><a href="#cb98-715" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-716"><a href="#cb98-716" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Gated Linear Unit dynamically gates input data"</span></span>
<span id="cb98-717"><a href="#cb98-717" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-718"><a href="#cb98-718" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb98-719"><a href="#cb98-719" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-720"><a href="#cb98-720" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_time_distributed <span class="op">=</span> use_time_distributed</span>
<span id="cb98-721"><a href="#cb98-721" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb98-722"><a href="#cb98-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-723"><a href="#cb98-723" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-724"><a href="#cb98-724" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().build(input_shape)</span>
<span id="cb98-725"><a href="#cb98-725" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> keras.layers.Dropout(<span class="va">self</span>.dropout_rate) <span class="cf">if</span> <span class="va">self</span>.dropout_rate <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb98-726"><a href="#cb98-726" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation_layer <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, activation<span class="op">=</span><span class="va">self</span>.activation)</span>
<span id="cb98-727"><a href="#cb98-727" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gate_layer <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, activation<span class="op">=</span><span class="st">'sigmoid'</span>)</span>
<span id="cb98-728"><a href="#cb98-728" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.multiply <span class="op">=</span> keras.layers.Multiply()</span>
<span id="cb98-729"><a href="#cb98-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-730"><a href="#cb98-730" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_time_distributed:</span>
<span id="cb98-731"><a href="#cb98-731" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activation_layer <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.activation_layer)</span>
<span id="cb98-732"><a href="#cb98-732" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gate_layer <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.gate_layer)</span>
<span id="cb98-733"><a href="#cb98-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-734"><a href="#cb98-734" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb98-735"><a href="#cb98-735" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb98-736"><a href="#cb98-736" aria-hidden="true" tabindex="-1"></a>        inputs, </span>
<span id="cb98-737"><a href="#cb98-737" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb98-738"><a href="#cb98-738" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-739"><a href="#cb98-739" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""List of outputs with shape: [</span></span>
<span id="cb98-740"><a href="#cb98-740" aria-hidden="true" tabindex="-1"></a><span class="co">            (batch size, ..., d_model),</span></span>
<span id="cb98-741"><a href="#cb98-741" aria-hidden="true" tabindex="-1"></a><span class="co">            (batch size, ..., d_model)</span></span>
<span id="cb98-742"><a href="#cb98-742" aria-hidden="true" tabindex="-1"></a><span class="co">        ]"""</span></span>
<span id="cb98-743"><a href="#cb98-743" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.dropout <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> training:</span>
<span id="cb98-744"><a href="#cb98-744" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> <span class="va">self</span>.dropout(inputs)</span>
<span id="cb98-745"><a href="#cb98-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-746"><a href="#cb98-746" aria-hidden="true" tabindex="-1"></a>        activation_output <span class="op">=</span> <span class="va">self</span>.activation_layer(inputs)</span>
<span id="cb98-747"><a href="#cb98-747" aria-hidden="true" tabindex="-1"></a>        gate_output <span class="op">=</span> <span class="va">self</span>.gate_layer(inputs)</span>
<span id="cb98-748"><a href="#cb98-748" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.multiply([activation_output, gate_output]), gate_output</span>
<span id="cb98-749"><a href="#cb98-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-750"><a href="#cb98-750" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb98-751"><a href="#cb98-751" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb98-752"><a href="#cb98-752" aria-hidden="true" tabindex="-1"></a>        config.update({</span>
<span id="cb98-753"><a href="#cb98-753" aria-hidden="true" tabindex="-1"></a>            <span class="st">'d_model'</span>: <span class="va">self</span>.d_model,</span>
<span id="cb98-754"><a href="#cb98-754" aria-hidden="true" tabindex="-1"></a>            <span class="st">'dropout_rate'</span>: <span class="va">self</span>.dropout_rate,</span>
<span id="cb98-755"><a href="#cb98-755" aria-hidden="true" tabindex="-1"></a>            <span class="st">'use_time_distributed'</span>: <span class="va">self</span>.use_time_distributed,</span>
<span id="cb98-756"><a href="#cb98-756" aria-hidden="true" tabindex="-1"></a>            <span class="st">'activation'</span>: <span class="va">self</span>.activation</span>
<span id="cb98-757"><a href="#cb98-757" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb98-758"><a href="#cb98-758" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb98-759"><a href="#cb98-759" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-760"><a href="#cb98-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-761"><a href="#cb98-761" aria-hidden="true" tabindex="-1"></a>The GLU is simply:</span>
<span id="cb98-762"><a href="#cb98-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-763"><a href="#cb98-763" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb98-764"><a href="#cb98-764" aria-hidden="true" tabindex="-1"></a>\text{GLU}(x) = \sigma(W_{G} x + b_G) \odot (W_1 x + b_1),</span>
<span id="cb98-765"><a href="#cb98-765" aria-hidden="true" tabindex="-1"></a>$$ {#eq-glu}</span>
<span id="cb98-766"><a href="#cb98-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-767"><a href="#cb98-767" aria-hidden="true" tabindex="-1"></a>with $W_G, W_1 \in \mathbb{R}^{|x| \times \lambda}$ and $b_G, b_1 \in \mathbb{R}^{\lambda}$.</span>
<span id="cb98-768"><a href="#cb98-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-771"><a href="#cb98-771" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-772"><a href="#cb98-772" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_glu</span></span>
<span id="cb98-773"><a href="#cb98-773" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of model with GLU</span></span>
<span id="cb98-774"><a href="#cb98-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-775"><a href="#cb98-775" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb98-776"><a href="#cb98-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-777"><a href="#cb98-777" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb98-778"><a href="#cb98-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-779"><a href="#cb98-779" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),),name<span class="op">=</span><span class="st">"FlattenedLaggedInput"</span>)</span>
<span id="cb98-780"><a href="#cb98-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-781"><a href="#cb98-781" aria-hidden="true" tabindex="-1"></a>gated_features, gate <span class="op">=</span> GatedLinearUnit(d_model<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, use_time_distributed<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"Gate"</span>)(inputs)</span>
<span id="cb98-782"><a href="#cb98-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-783"><a href="#cb98-783" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(gated_features)</span>
<span id="cb98-784"><a href="#cb98-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-785"><a href="#cb98-785" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(output)</span>
<span id="cb98-786"><a href="#cb98-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-787"><a href="#cb98-787" aria-hidden="true" tabindex="-1"></a>nn_glu <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GLUModel"</span>)</span>
<span id="cb98-788"><a href="#cb98-788" aria-hidden="true" tabindex="-1"></a>nn_glu.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-789"><a href="#cb98-789" aria-hidden="true" tabindex="-1"></a>nn_glu.summary()</span>
<span id="cb98-790"><a href="#cb98-790" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-791"><a href="#cb98-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-794"><a href="#cb98-794" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-795"><a href="#cb98-795" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_glu</span></span>
<span id="cb98-796"><a href="#cb98-796" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: GLU model</span></span>
<span id="cb98-797"><a href="#cb98-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-798"><a href="#cb98-798" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_glu, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-799"><a href="#cb98-799" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-800"><a href="#cb98-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-803"><a href="#cb98-803" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-804"><a href="#cb98-804" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fit glu model</span></span>
<span id="cb98-805"><a href="#cb98-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-806"><a href="#cb98-806" aria-hidden="true" tabindex="-1"></a>history_glu <span class="op">=</span> nn_glu.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-807"><a href="#cb98-807" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-808"><a href="#cb98-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-811"><a href="#cb98-811" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-812"><a href="#cb98-812" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_glu</span></span>
<span id="cb98-813"><a href="#cb98-813" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a GLU neural network</span></span>
<span id="cb98-814"><a href="#cb98-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-815"><a href="#cb98-815" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_glu.history).plot()</span>
<span id="cb98-816"><a href="#cb98-816" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-817"><a href="#cb98-817" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-818"><a href="#cb98-818" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-819"><a href="#cb98-819" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-820"><a href="#cb98-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-821"><a href="#cb98-821" aria-hidden="true" tabindex="-1"></a>As mentioned above, the GLU itself actually serves as an important component of a slightly larger group of layers, the GRNs. Now is the time to introduce them.</span>
<span id="cb98-822"><a href="#cb98-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-823"><a href="#cb98-823" aria-hidden="true" tabindex="-1"></a>GRNs are formally:</span>
<span id="cb98-824"><a href="#cb98-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-825"><a href="#cb98-825" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb98-826"><a href="#cb98-826" aria-hidden="true" tabindex="-1"></a>\text{GRN}(x) = \text{LayerNorm}(x + \text{GLU}(W_2 (\text{ELU}(W_1 x + b1 + W_c c )) + b_2)),</span>
<span id="cb98-827"><a href="#cb98-827" aria-hidden="true" tabindex="-1"></a>$$ {#eq-grn}</span>
<span id="cb98-828"><a href="#cb98-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-829"><a href="#cb98-829" aria-hidden="true" tabindex="-1"></a>where $\text{LayerNorm}$ (@ba2016layer) normalises its inputs, ie subtracts its mean and divides by its standard deviation,<span class="ot">[^layernorm]</span> $\text{ELU}$ is the exponential linear unit function (@clevert2015fast). Unlike ReLUs, ELUs allow for negative values, which pushes unit activations closer to zero at a lower computation complexity, and producing more accurate results.</span>
<span id="cb98-830"><a href="#cb98-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-831"><a href="#cb98-831" aria-hidden="true" tabindex="-1"></a>The final component in @eq-grn is $c \in \mathbb{R}^{\lambda}$, or a context vector - more on that below in @sec-timeembed. For the moment, we can use $c=\mathbf{0}$ the zero vector.</span>
<span id="cb98-832"><a href="#cb98-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-833"><a href="#cb98-833" aria-hidden="true" tabindex="-1"></a><span class="ot">[^layernorm]: </span>Normalising a layer helps to avoid the numbers from becoming too large, which is detrimental to gradient transmission and therefore to learning. This is helpful for example when summing up the outputs from intermediary layers.</span>
<span id="cb98-834"><a href="#cb98-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-835"><a href="#cb98-835" aria-hidden="true" tabindex="-1"></a>Put simply, the GRN takes in a certain data $x$ and combines it with a non-linear transformation. This non-linear component goes through an GLU, which learns when to gate and when to let through the non-linear transformation of the data.</span>
<span id="cb98-836"><a href="#cb98-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-837"><a href="#cb98-837" aria-hidden="true" tabindex="-1"></a>The GRN helps keep information only from relevant input variables and keeps the model as simple as possible by only applying non-linearities when relevant.</span>
<span id="cb98-838"><a href="#cb98-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-841"><a href="#cb98-841" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-842"><a href="#cb98-842" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class grn</span></span>
<span id="cb98-843"><a href="#cb98-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-844"><a href="#cb98-844" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GatedResidualNetwork(keras.layers.Layer):</span>
<span id="cb98-845"><a href="#cb98-845" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-846"><a href="#cb98-846" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb98-847"><a href="#cb98-847" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb98-848"><a href="#cb98-848" aria-hidden="true" tabindex="-1"></a>        output_size<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb98-849"><a href="#cb98-849" aria-hidden="true" tabindex="-1"></a>        dropout_rate<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb98-850"><a href="#cb98-850" aria-hidden="true" tabindex="-1"></a>        use_time_distributed<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb98-851"><a href="#cb98-851" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-852"><a href="#cb98-852" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-853"><a href="#cb98-853" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Gated residual network"</span></span>
<span id="cb98-854"><a href="#cb98-854" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GatedResidualNetwork, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-855"><a href="#cb98-855" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb98-856"><a href="#cb98-856" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_size <span class="op">=</span> output_size <span class="cf">if</span> output_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> d_model</span>
<span id="cb98-857"><a href="#cb98-857" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-858"><a href="#cb98-858" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_time_distributed <span class="op">=</span> use_time_distributed</span>
<span id="cb98-859"><a href="#cb98-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-860"><a href="#cb98-860" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-861"><a href="#cb98-861" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GatedResidualNetwork, <span class="va">self</span>).build(input_shape)</span>
<span id="cb98-862"><a href="#cb98-862" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.output_size)</span>
<span id="cb98-863"><a href="#cb98-863" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dense <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model)</span>
<span id="cb98-864"><a href="#cb98-864" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_dense_post <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model)</span>
<span id="cb98-865"><a href="#cb98-865" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_activation <span class="op">=</span> keras.layers.Activation(<span class="st">'elu'</span>)</span>
<span id="cb98-866"><a href="#cb98-866" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dense <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, use_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb98-867"><a href="#cb98-867" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gating_layer <span class="op">=</span> GatedLinearUnit(</span>
<span id="cb98-868"><a href="#cb98-868" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.output_size, </span>
<span id="cb98-869"><a href="#cb98-869" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate, </span>
<span id="cb98-870"><a href="#cb98-870" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">self</span>.use_time_distributed, </span>
<span id="cb98-871"><a href="#cb98-871" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb98-872"><a href="#cb98-872" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add <span class="op">=</span> keras.layers.Add()</span>
<span id="cb98-873"><a href="#cb98-873" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l_norm <span class="op">=</span> keras.layers.LayerNormalization()</span>
<span id="cb98-874"><a href="#cb98-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-875"><a href="#cb98-875" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_time_distributed:</span>
<span id="cb98-876"><a href="#cb98-876" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dense <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.dense)</span>
<span id="cb98-877"><a href="#cb98-877" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_dense <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.hidden_dense)</span>
<span id="cb98-878"><a href="#cb98-878" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_dense <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.context_dense)</span>
<span id="cb98-879"><a href="#cb98-879" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_dense_post <span class="op">=</span> keras.layers.TimeDistributed(<span class="va">self</span>.hidden_dense_post)</span>
<span id="cb98-880"><a href="#cb98-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-881"><a href="#cb98-881" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, additional_context<span class="op">=</span><span class="va">None</span>, training<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb98-882"><a href="#cb98-882" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Setup skip connection</span></span>
<span id="cb98-883"><a href="#cb98-883" aria-hidden="true" tabindex="-1"></a>        skip <span class="op">=</span> <span class="va">self</span>.dense(inputs) <span class="cf">if</span> <span class="va">self</span>.output_size <span class="cf">else</span> inputs</span>
<span id="cb98-884"><a href="#cb98-884" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-885"><a href="#cb98-885" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1st step: eta2</span></span>
<span id="cb98-886"><a href="#cb98-886" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.hidden_dense(inputs)</span>
<span id="cb98-887"><a href="#cb98-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-888"><a href="#cb98-888" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Context handling</span></span>
<span id="cb98-889"><a href="#cb98-889" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> additional_context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb98-890"><a href="#cb98-890" aria-hidden="true" tabindex="-1"></a>            hidden <span class="op">+=</span> <span class="va">self</span>.context_dense(additional_context)</span>
<span id="cb98-891"><a href="#cb98-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-892"><a href="#cb98-892" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.hidden_activation(hidden)</span>
<span id="cb98-893"><a href="#cb98-893" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> <span class="va">self</span>.hidden_dense_post(hidden)</span>
<span id="cb98-894"><a href="#cb98-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-895"><a href="#cb98-895" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2nd step: eta1 and 3rd step</span></span>
<span id="cb98-896"><a href="#cb98-896" aria-hidden="true" tabindex="-1"></a>        gating_layer, gate <span class="op">=</span> <span class="va">self</span>.gating_layer(hidden)</span>
<span id="cb98-897"><a href="#cb98-897" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-898"><a href="#cb98-898" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final step</span></span>
<span id="cb98-899"><a href="#cb98-899" aria-hidden="true" tabindex="-1"></a>        GRN <span class="op">=</span> <span class="va">self</span>.add([skip, gating_layer])</span>
<span id="cb98-900"><a href="#cb98-900" aria-hidden="true" tabindex="-1"></a>        GRN <span class="op">=</span> <span class="va">self</span>.l_norm(GRN)</span>
<span id="cb98-901"><a href="#cb98-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-902"><a href="#cb98-902" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> GRN, gate</span>
<span id="cb98-903"><a href="#cb98-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-904"><a href="#cb98-904" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb98-905"><a href="#cb98-905" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>(GatedResidualNetwork, <span class="va">self</span>).get_config()</span>
<span id="cb98-906"><a href="#cb98-906" aria-hidden="true" tabindex="-1"></a>        config.update({</span>
<span id="cb98-907"><a href="#cb98-907" aria-hidden="true" tabindex="-1"></a>            <span class="st">'d_model'</span>: <span class="va">self</span>.d_model,</span>
<span id="cb98-908"><a href="#cb98-908" aria-hidden="true" tabindex="-1"></a>            <span class="st">'output_size'</span>: <span class="va">self</span>.output_size,</span>
<span id="cb98-909"><a href="#cb98-909" aria-hidden="true" tabindex="-1"></a>            <span class="st">'dropout_rate'</span>: <span class="va">self</span>.dropout_rate,</span>
<span id="cb98-910"><a href="#cb98-910" aria-hidden="true" tabindex="-1"></a>            <span class="st">'use_time_distributed'</span>: <span class="va">self</span>.use_time_distributed</span>
<span id="cb98-911"><a href="#cb98-911" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb98-912"><a href="#cb98-912" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb98-913"><a href="#cb98-913" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-914"><a href="#cb98-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-917"><a href="#cb98-917" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-918"><a href="#cb98-918" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_grn</span></span>
<span id="cb98-919"><a href="#cb98-919" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of GRN</span></span>
<span id="cb98-920"><a href="#cb98-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-921"><a href="#cb98-921" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb98-922"><a href="#cb98-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-923"><a href="#cb98-923" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb98-924"><a href="#cb98-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-925"><a href="#cb98-925" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),),name<span class="op">=</span><span class="st">"FlattenedLaggedInput"</span>)</span>
<span id="cb98-926"><a href="#cb98-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-927"><a href="#cb98-927" aria-hidden="true" tabindex="-1"></a><span class="co"># inputs = keras.layers.Dense(units=dim, activation="relu", name="SummariseInput")(inputs)</span></span>
<span id="cb98-928"><a href="#cb98-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-929"><a href="#cb98-929" aria-hidden="true" tabindex="-1"></a>gated_features, gate <span class="op">=</span> GatedResidualNetwork(d_model<span class="op">=</span><span class="dv">262</span>, use_time_distributed<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"GRN"</span>)(inputs)</span>
<span id="cb98-930"><a href="#cb98-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-931"><a href="#cb98-931" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(gated_features)</span>
<span id="cb98-932"><a href="#cb98-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-933"><a href="#cb98-933" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(output)</span>
<span id="cb98-934"><a href="#cb98-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-935"><a href="#cb98-935" aria-hidden="true" tabindex="-1"></a>nn_grn <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GRNModel"</span>)</span>
<span id="cb98-936"><a href="#cb98-936" aria-hidden="true" tabindex="-1"></a>nn_grn.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-937"><a href="#cb98-937" aria-hidden="true" tabindex="-1"></a>nn_grn.summary()</span>
<span id="cb98-938"><a href="#cb98-938" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-939"><a href="#cb98-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-942"><a href="#cb98-942" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-943"><a href="#cb98-943" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_grn</span></span>
<span id="cb98-944"><a href="#cb98-944" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: GRN model architecture</span></span>
<span id="cb98-945"><a href="#cb98-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-946"><a href="#cb98-946" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_grn, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-947"><a href="#cb98-947" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-948"><a href="#cb98-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-949"><a href="#cb98-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-952"><a href="#cb98-952" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-953"><a href="#cb98-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fitting GRN model</span></span>
<span id="cb98-954"><a href="#cb98-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-955"><a href="#cb98-955" aria-hidden="true" tabindex="-1"></a>history_grn <span class="op">=</span> nn_grn.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-956"><a href="#cb98-956" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-957"><a href="#cb98-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-958"><a href="#cb98-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-961"><a href="#cb98-961" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-962"><a href="#cb98-962" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_grn</span></span>
<span id="cb98-963"><a href="#cb98-963" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a GRN neural network</span></span>
<span id="cb98-964"><a href="#cb98-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-965"><a href="#cb98-965" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_grn.history).plot()</span>
<span id="cb98-966"><a href="#cb98-966" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-967"><a href="#cb98-967" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-968"><a href="#cb98-968" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-969"><a href="#cb98-969" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-970"><a href="#cb98-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-971"><a href="#cb98-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-972"><a href="#cb98-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-973"><a href="#cb98-973" aria-hidden="true" tabindex="-1"></a><span class="fu">## Time to talk about time {#sec-timeembed}</span></span>
<span id="cb98-974"><a href="#cb98-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-975"><a href="#cb98-975" aria-hidden="true" tabindex="-1"></a>Nowcasting, or forecasting for that matter, involves trying to estimate the values of a variable of interest for a certain date. This trivial fact actually offers an important opportunity to improve estimates: it corresponds to something that is *known* about the future, ie about the variable we want to estimate.</span>
<span id="cb98-976"><a href="#cb98-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-977"><a href="#cb98-977" aria-hidden="true" tabindex="-1"></a>But temporal features are not continuous variables as inflation and oil price changes. Because they are categorical, they need to be embedded in a vector space with real data to incorporate continuous variables. This is the goal of this section.</span>
<span id="cb98-978"><a href="#cb98-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-981"><a href="#cb98-981" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-982"><a href="#cb98-982" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: getting temporal features data</span></span>
<span id="cb98-983"><a href="#cb98-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-984"><a href="#cb98-984" aria-hidden="true" tabindex="-1"></a>timefeat <span class="op">=</span> [<span class="st">"month_of_quarter"</span>, <span class="st">"month_of_year"</span>]</span>
<span id="cb98-985"><a href="#cb98-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-986"><a href="#cb98-986" aria-hidden="true" tabindex="-1"></a>y_timefeat_train <span class="op">=</span> {timef:</span>
<span id="cb98-987"><a href="#cb98-987" aria-hidden="true" tabindex="-1"></a>    np.array([</span>
<span id="cb98-988"><a href="#cb98-988" aria-hidden="true" tabindex="-1"></a>        get_timefeat(pd.DataFrame(i.values, index<span class="op">=</span>[i.name], columns<span class="op">=</span>i.index), features<span class="op">=</span>timef)</span>
<span id="cb98-989"><a href="#cb98-989" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> y_train_split_fc[fold][<span class="st">"train"</span>]</span>
<span id="cb98-990"><a href="#cb98-990" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb98-991"><a href="#cb98-991" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> timef <span class="kw">in</span> timefeat</span>
<span id="cb98-992"><a href="#cb98-992" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-993"><a href="#cb98-993" aria-hidden="true" tabindex="-1"></a>y_timefeat_valid <span class="op">=</span> {timef:</span>
<span id="cb98-994"><a href="#cb98-994" aria-hidden="true" tabindex="-1"></a>    np.array([</span>
<span id="cb98-995"><a href="#cb98-995" aria-hidden="true" tabindex="-1"></a>        get_timefeat(pd.DataFrame(i.values, index<span class="op">=</span>[i.name], columns<span class="op">=</span>i.index), features<span class="op">=</span>timef)</span>
<span id="cb98-996"><a href="#cb98-996" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> y_train_split_fc[fold][<span class="st">"valid"</span>]</span>
<span id="cb98-997"><a href="#cb98-997" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb98-998"><a href="#cb98-998" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> timef <span class="kw">in</span> timefeat</span>
<span id="cb98-999"><a href="#cb98-999" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1000"><a href="#cb98-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1001"><a href="#cb98-1001" aria-hidden="true" tabindex="-1"></a>date_range <span class="op">=</span> pd.date_range(start<span class="op">=</span><span class="st">"2024-01-01"</span>, end<span class="op">=</span><span class="st">"2024-12-31"</span>, freq<span class="op">=</span><span class="st">"D"</span>)</span>
<span id="cb98-1002"><a href="#cb98-1002" aria-hidden="true" tabindex="-1"></a>time_feats <span class="op">=</span> get_timefeat(pd.DataFrame(index<span class="op">=</span>date_range), features<span class="op">=</span>timefeat)</span>
<span id="cb98-1003"><a href="#cb98-1003" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1004"><a href="#cb98-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1005"><a href="#cb98-1005" aria-hidden="true" tabindex="-1"></a>When working with categorical data, it is necessary to be explicit about the number of possible different values (eg, there are 7 different days of the week).</span>
<span id="cb98-1006"><a href="#cb98-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1009"><a href="#cb98-1009" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1010"><a href="#cb98-1010" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: set vocab sizes</span></span>
<span id="cb98-1011"><a href="#cb98-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1012"><a href="#cb98-1012" aria-hidden="true" tabindex="-1"></a>vocab_sizes <span class="op">=</span> {col: time_feats[col].nunique() <span class="cf">for</span> col <span class="kw">in</span> time_feats}</span>
<span id="cb98-1013"><a href="#cb98-1013" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_sizes)</span>
<span id="cb98-1014"><a href="#cb98-1014" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1015"><a href="#cb98-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1018"><a href="#cb98-1018" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1019"><a href="#cb98-1019" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_embed</span></span>
<span id="cb98-1020"><a href="#cb98-1020" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of model with embedding</span></span>
<span id="cb98-1021"><a href="#cb98-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1022"><a href="#cb98-1022" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb98-1023"><a href="#cb98-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1024"><a href="#cb98-1024" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb98-1025"><a href="#cb98-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1026"><a href="#cb98-1026" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {</span>
<span id="cb98-1027"><a href="#cb98-1027" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span>k, dtype<span class="op">=</span><span class="st">'int32'</span>)  <span class="co"># Input layer for each feature with shape (1,)</span></span>
<span id="cb98-1028"><a href="#cb98-1028" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()</span>
<span id="cb98-1029"><a href="#cb98-1029" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1030"><a href="#cb98-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1031"><a href="#cb98-1031" aria-hidden="true" tabindex="-1"></a>embedded_layers <span class="op">=</span> {</span>
<span id="cb98-1032"><a href="#cb98-1032" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Embedding(input_dim<span class="op">=</span>v, output_dim<span class="op">=</span>dim, name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>)(inputs[k])</span>
<span id="cb98-1033"><a href="#cb98-1033" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> vocab_sizes.items()</span>
<span id="cb98-1034"><a href="#cb98-1034" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1035"><a href="#cb98-1035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1036"><a href="#cb98-1036" aria-hidden="true" tabindex="-1"></a>combined <span class="op">=</span> keras.layers.Average()(<span class="bu">list</span>(embedded_layers.values()))</span>
<span id="cb98-1037"><a href="#cb98-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1038"><a href="#cb98-1038" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(dim, activation<span class="op">=</span><span class="st">"relu"</span>)(combined)</span>
<span id="cb98-1039"><a href="#cb98-1039" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb98-1040"><a href="#cb98-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1041"><a href="#cb98-1041" aria-hidden="true" tabindex="-1"></a>nn_embed <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">list</span>(inputs.values()), outputs<span class="op">=</span>output)</span>
<span id="cb98-1042"><a href="#cb98-1042" aria-hidden="true" tabindex="-1"></a>nn_embed.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-1043"><a href="#cb98-1043" aria-hidden="true" tabindex="-1"></a>nn_embed.summary()</span>
<span id="cb98-1044"><a href="#cb98-1044" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1045"><a href="#cb98-1045" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1048"><a href="#cb98-1048" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1049"><a href="#cb98-1049" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_embed</span></span>
<span id="cb98-1050"><a href="#cb98-1050" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-label: Architecture of model with embeddings</span></span>
<span id="cb98-1051"><a href="#cb98-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1052"><a href="#cb98-1052" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_embed, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1053"><a href="#cb98-1053" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1054"><a href="#cb98-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1057"><a href="#cb98-1057" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1058"><a href="#cb98-1058" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-embed_loss</span></span>
<span id="cb98-1059"><a href="#cb98-1059" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses of model with embeddings</span></span>
<span id="cb98-1060"><a href="#cb98-1060" aria-hidden="true" tabindex="-1"></a>history_embed <span class="op">=</span> nn_embed.fit(x<span class="op">=</span>y_timefeat_train, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(y_timefeat_valid, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1061"><a href="#cb98-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1062"><a href="#cb98-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1063"><a href="#cb98-1063" aria-hidden="true" tabindex="-1"></a>As before, the losses are plotted below.</span>
<span id="cb98-1064"><a href="#cb98-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1067"><a href="#cb98-1067" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1068"><a href="#cb98-1068" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_embed</span></span>
<span id="cb98-1069"><a href="#cb98-1069" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a simple, fully-connected neural network.</span></span>
<span id="cb98-1070"><a href="#cb98-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1071"><a href="#cb98-1071" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_embed.history).plot()</span>
<span id="cb98-1072"><a href="#cb98-1072" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-1073"><a href="#cb98-1073" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-1074"><a href="#cb98-1074" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1075"><a href="#cb98-1075" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1076"><a href="#cb98-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1077"><a href="#cb98-1077" aria-hidden="true" tabindex="-1"></a>The embeddings themselves can be inspected, as in @fig-embed.</span>
<span id="cb98-1078"><a href="#cb98-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1081"><a href="#cb98-1081" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1082"><a href="#cb98-1082" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-embed</span></span>
<span id="cb98-1083"><a href="#cb98-1083" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Embedding values</span></span>
<span id="cb98-1084"><a href="#cb98-1084" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb98-1085"><a href="#cb98-1085" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Month of the year</span></span>
<span id="cb98-1086"><a href="#cb98-1086" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Quarter of the year</span></span>
<span id="cb98-1087"><a href="#cb98-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1088"><a href="#cb98-1088" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb98-1089"><a href="#cb98-1089" aria-hidden="true" tabindex="-1"></a>plt.imshow(nn_embed.layers[<span class="dv">2</span>].get_weights()[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb98-1090"><a href="#cb98-1090" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>)  <span class="co"># Optional: Add colorbar to show scale</span></span>
<span id="cb98-1091"><a href="#cb98-1091" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Embedding vector elements'</span>)</span>
<span id="cb98-1092"><a href="#cb98-1092" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Month of the quarter'</span>)</span>
<span id="cb98-1093"><a href="#cb98-1093" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1094"><a href="#cb98-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1095"><a href="#cb98-1095" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb98-1096"><a href="#cb98-1096" aria-hidden="true" tabindex="-1"></a>plt.imshow(nn_embed.layers[<span class="dv">3</span>].get_weights()[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb98-1097"><a href="#cb98-1097" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>)  <span class="co"># Optional: Add colorbar to show scale</span></span>
<span id="cb98-1098"><a href="#cb98-1098" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Embedding vector elements'</span>)</span>
<span id="cb98-1099"><a href="#cb98-1099" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Month of the year'</span>)</span>
<span id="cb98-1100"><a href="#cb98-1100" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1101"><a href="#cb98-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1102"><a href="#cb98-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1103"><a href="#cb98-1103" aria-hidden="true" tabindex="-1"></a>To get an intuition how this type of data feeds into a nowcasting model, consider that each nowcasted date has a different combination of month of year and month of quarter. Then the inputs to the model change correspondingly. The time series is plotted below.</span>
<span id="cb98-1104"><a href="#cb98-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1107"><a href="#cb98-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1108"><a href="#cb98-1108" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: ts_embed calculations</span></span>
<span id="cb98-1109"><a href="#cb98-1109" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Time series of embeddings</span></span>
<span id="cb98-1110"><a href="#cb98-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1111"><a href="#cb98-1111" aria-hidden="true" tabindex="-1"></a>y_time_feats <span class="op">=</span> get_timefeat(pd.DataFrame(index<span class="op">=</span>y_train.index), features<span class="op">=</span>timefeat)</span>
<span id="cb98-1112"><a href="#cb98-1112" aria-hidden="true" tabindex="-1"></a>embed_layers <span class="op">=</span> [nn_embed.get_layer(<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>) <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()]</span>
<span id="cb98-1113"><a href="#cb98-1113" aria-hidden="true" tabindex="-1"></a>embed_values <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[e.<span class="bu">input</span> <span class="cf">for</span> e <span class="kw">in</span> embed_layers], outputs<span class="op">=</span>[e.output <span class="cf">for</span> e <span class="kw">in</span> embed_layers])</span>
<span id="cb98-1114"><a href="#cb98-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1115"><a href="#cb98-1115" aria-hidden="true" tabindex="-1"></a>embed_ts <span class="op">=</span> embed_values.predict(<span class="bu">dict</span>(y_time_feats))</span>
<span id="cb98-1116"><a href="#cb98-1116" aria-hidden="true" tabindex="-1"></a>embed_ts <span class="op">=</span> [</span>
<span id="cb98-1117"><a href="#cb98-1117" aria-hidden="true" tabindex="-1"></a>    np.squeeze(i, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb98-1118"><a href="#cb98-1118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> embed_ts</span>
<span id="cb98-1119"><a href="#cb98-1119" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb98-1120"><a href="#cb98-1120" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1121"><a href="#cb98-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1124"><a href="#cb98-1124" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1125"><a href="#cb98-1125" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-embedts</span></span>
<span id="cb98-1126"><a href="#cb98-1126" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Embedding values (last 24 months)</span></span>
<span id="cb98-1127"><a href="#cb98-1127" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb98-1128"><a href="#cb98-1128" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Month of quarter</span></span>
<span id="cb98-1129"><a href="#cb98-1129" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Month of year</span></span>
<span id="cb98-1130"><a href="#cb98-1130" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Average of month of quarter and year embeddings</span></span>
<span id="cb98-1131"><a href="#cb98-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1132"><a href="#cb98-1132" aria-hidden="true" tabindex="-1"></a>window_months <span class="op">=</span> <span class="dv">36</span></span>
<span id="cb98-1133"><a href="#cb98-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1134"><a href="#cb98-1134" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb98-1135"><a href="#cb98-1135" aria-hidden="true" tabindex="-1"></a>plt.imshow(embed_ts[<span class="dv">0</span>][<span class="op">-</span>window_months:].T, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb98-1136"><a href="#cb98-1136" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>) </span>
<span id="cb98-1137"><a href="#cb98-1137" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Months"</span>)</span>
<span id="cb98-1138"><a href="#cb98-1138" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding elements"</span>)</span>
<span id="cb98-1139"><a href="#cb98-1139" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1140"><a href="#cb98-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1141"><a href="#cb98-1141" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb98-1142"><a href="#cb98-1142" aria-hidden="true" tabindex="-1"></a>plt.imshow(embed_ts[<span class="dv">1</span>][<span class="op">-</span>window_months:].T, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb98-1143"><a href="#cb98-1143" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>) </span>
<span id="cb98-1144"><a href="#cb98-1144" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Months"</span>)</span>
<span id="cb98-1145"><a href="#cb98-1145" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding elements"</span>)</span>
<span id="cb98-1146"><a href="#cb98-1146" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1147"><a href="#cb98-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1148"><a href="#cb98-1148" aria-hidden="true" tabindex="-1"></a>avg_embed <span class="op">=</span> np.mean(np.stack(embed_ts, axis<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb98-1149"><a href="#cb98-1149" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb98-1150"><a href="#cb98-1150" aria-hidden="true" tabindex="-1"></a>plt.imshow(avg_embed[<span class="op">-</span>window_months:].T, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb98-1151"><a href="#cb98-1151" aria-hidden="true" tabindex="-1"></a>plt.colorbar(label<span class="op">=</span><span class="st">'Values'</span>) </span>
<span id="cb98-1152"><a href="#cb98-1152" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Months"</span>)</span>
<span id="cb98-1153"><a href="#cb98-1153" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding elements"</span>)</span>
<span id="cb98-1154"><a href="#cb98-1154" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1155"><a href="#cb98-1155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1156"><a href="#cb98-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1157"><a href="#cb98-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1158"><a href="#cb98-1158" aria-hidden="true" tabindex="-1"></a>One way these categorical variables can enrich models with continues variables that were used before is by using the embeddings for a particular instance as additional context, as the $c$ in @eq-grn.</span>
<span id="cb98-1159"><a href="#cb98-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1160"><a href="#cb98-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1163"><a href="#cb98-1163" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1164"><a href="#cb98-1164" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_grn_context</span></span>
<span id="cb98-1165"><a href="#cb98-1165" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: GRN with Embeddings Summary</span></span>
<span id="cb98-1166"><a href="#cb98-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1167"><a href="#cb98-1167" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb98-1168"><a href="#cb98-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1169"><a href="#cb98-1169" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb98-1170"><a href="#cb98-1170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1171"><a href="#cb98-1171" aria-hidden="true" tabindex="-1"></a>inputs_cont <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),), name<span class="op">=</span><span class="st">"FlattenedLaggedInput"</span>)</span>
<span id="cb98-1172"><a href="#cb98-1172" aria-hidden="true" tabindex="-1"></a>continuous <span class="op">=</span> keras.layers.Dense(dim, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs_cont)</span>
<span id="cb98-1173"><a href="#cb98-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1174"><a href="#cb98-1174" aria-hidden="true" tabindex="-1"></a>inputs_time <span class="op">=</span> {</span>
<span id="cb98-1175"><a href="#cb98-1175" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), name<span class="op">=</span>k, dtype<span class="op">=</span><span class="st">'int32'</span>)  <span class="co"># Input layer for each feature with shape (1,)</span></span>
<span id="cb98-1176"><a href="#cb98-1176" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()</span>
<span id="cb98-1177"><a href="#cb98-1177" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1178"><a href="#cb98-1178" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Create embedding layers for each input</span></span>
<span id="cb98-1179"><a href="#cb98-1179" aria-hidden="true" tabindex="-1"></a>embedded_layers <span class="op">=</span> {</span>
<span id="cb98-1180"><a href="#cb98-1180" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Embedding(input_dim<span class="op">=</span>v, output_dim<span class="op">=</span>dim, name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>)(inputs_time[k])</span>
<span id="cb98-1181"><a href="#cb98-1181" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> vocab_sizes.items()</span>
<span id="cb98-1182"><a href="#cb98-1182" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1183"><a href="#cb98-1183" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> keras.layers.Average()(<span class="bu">list</span>(embedded_layers.values()))</span>
<span id="cb98-1184"><a href="#cb98-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1185"><a href="#cb98-1185" aria-hidden="true" tabindex="-1"></a>gated_features, gate <span class="op">=</span> GatedResidualNetwork(d_model<span class="op">=</span>dim, use_time_distributed<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"GRN"</span>)(continuous, additional_context<span class="op">=</span>context)</span>
<span id="cb98-1186"><a href="#cb98-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1187"><a href="#cb98-1187" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(gated_features)</span>
<span id="cb98-1188"><a href="#cb98-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1189"><a href="#cb98-1189" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(output)</span>
<span id="cb98-1190"><a href="#cb98-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1191"><a href="#cb98-1191" aria-hidden="true" tabindex="-1"></a>nn_grn_context <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[inputs_cont, inputs_time], outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GRNwithContextModel"</span>)</span>
<span id="cb98-1192"><a href="#cb98-1192" aria-hidden="true" tabindex="-1"></a>nn_grn_context.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-1193"><a href="#cb98-1193" aria-hidden="true" tabindex="-1"></a>nn_grn_context.summary()</span>
<span id="cb98-1194"><a href="#cb98-1194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1195"><a href="#cb98-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1198"><a href="#cb98-1198" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1199"><a href="#cb98-1199" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_grn_context</span></span>
<span id="cb98-1200"><a href="#cb98-1200" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the GRN model with context</span></span>
<span id="cb98-1201"><a href="#cb98-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1202"><a href="#cb98-1202" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_grn_context, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1203"><a href="#cb98-1203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1204"><a href="#cb98-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1207"><a href="#cb98-1207" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1208"><a href="#cb98-1208" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fitting GRN model with context</span></span>
<span id="cb98-1209"><a href="#cb98-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1210"><a href="#cb98-1210" aria-hidden="true" tabindex="-1"></a>history_grn_context <span class="op">=</span> nn_grn_context.fit(x<span class="op">=</span>[X_train_fc, y_timefeat_train], y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>([X_valid_fc, y_timefeat_valid], y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1211"><a href="#cb98-1211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1212"><a href="#cb98-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1215"><a href="#cb98-1215" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1216"><a href="#cb98-1216" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_grn_context</span></span>
<span id="cb98-1217"><a href="#cb98-1217" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a GRN neural network with context</span></span>
<span id="cb98-1218"><a href="#cb98-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1219"><a href="#cb98-1219" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_grn_context.history).plot()</span>
<span id="cb98-1220"><a href="#cb98-1220" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-1221"><a href="#cb98-1221" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-1222"><a href="#cb98-1222" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1223"><a href="#cb98-1223" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1224"><a href="#cb98-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1225"><a href="#cb98-1225" aria-hidden="true" tabindex="-1"></a><span class="fu">## Encoding continuous variables {#sec-encodcont}</span></span>
<span id="cb98-1226"><a href="#cb98-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1227"><a href="#cb98-1227" aria-hidden="true" tabindex="-1"></a>The intuition on embeddings built from @sec-timeembed can also serve now to think about embedding, or encoding, continuous variables.</span>
<span id="cb98-1228"><a href="#cb98-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1229"><a href="#cb98-1229" aria-hidden="true" tabindex="-1"></a>In contrast to categorical variables like temporal features, or entity identity (ie, which country, or bank, or stock, etc), the possible values are not defined a priori. So there is no way to create a lookup table that will map a given category, say, the month of "September" to a specific vector.</span>
<span id="cb98-1230"><a href="#cb98-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1231"><a href="#cb98-1231" aria-hidden="true" tabindex="-1"></a>However, continuous variables can still be mapped into a vector space defined in $\mathbb{R}$, in a way that encodes useful information. The simplest way to achieve this is by training parameters of a linear transformation of each data point of variable $j$ in time $t$ into $\xi_t^{(j)} \in \mathbb{R}^{\lambda}$. </span>
<span id="cb98-1232"><a href="#cb98-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1235"><a href="#cb98-1235" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1236"><a href="#cb98-1236" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_encodcont</span></span>
<span id="cb98-1237"><a href="#cb98-1237" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of network embedding continuous variable_weights</span></span>
<span id="cb98-1238"><a href="#cb98-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1239"><a href="#cb98-1239" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb98-1240"><a href="#cb98-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1241"><a href="#cb98-1241" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb98-1242"><a href="#cb98-1242" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb98-1243"><a href="#cb98-1243" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb98-1244"><a href="#cb98-1244" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-1245"><a href="#cb98-1245" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb98-1246"><a href="#cb98-1246" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb98-1247"><a href="#cb98-1247" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1248"><a href="#cb98-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1249"><a href="#cb98-1249" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb98-1250"><a href="#cb98-1250" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb98-1251"><a href="#cb98-1251" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb98-1252"><a href="#cb98-1252" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb98-1253"><a href="#cb98-1253" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb98-1254"><a href="#cb98-1254" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb98-1255"><a href="#cb98-1255" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb98-1256"><a href="#cb98-1256" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb98-1257"><a href="#cb98-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1258"><a href="#cb98-1258" aria-hidden="true" tabindex="-1"></a>nn_encodcont <span class="op">=</span> keras.Model(</span>
<span id="cb98-1259"><a href="#cb98-1259" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb98-1260"><a href="#cb98-1260" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb98-1261"><a href="#cb98-1261" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"EncodedContinuousVarsNetwork"</span></span>
<span id="cb98-1262"><a href="#cb98-1262" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-1263"><a href="#cb98-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1264"><a href="#cb98-1264" aria-hidden="true" tabindex="-1"></a>nn_encodcont.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-1265"><a href="#cb98-1265" aria-hidden="true" tabindex="-1"></a>nn_encodcont.summary()</span>
<span id="cb98-1266"><a href="#cb98-1266" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1267"><a href="#cb98-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1270"><a href="#cb98-1270" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1271"><a href="#cb98-1271" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_encodcont</span></span>
<span id="cb98-1272"><a href="#cb98-1272" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the network with encoded continuous variables before the LSTM layers</span></span>
<span id="cb98-1273"><a href="#cb98-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1274"><a href="#cb98-1274" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_encodcont, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1275"><a href="#cb98-1275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1276"><a href="#cb98-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1277"><a href="#cb98-1277" aria-hidden="true" tabindex="-1"></a>Compare this figure with @fig-arch_lstm. Note that while before, each LSTM received as input the original variable, now in @fig-arch_encodcont the inputs to the LSTMs are actually $\lambda=16$ latent variables for each raw data variable we included.</span>
<span id="cb98-1278"><a href="#cb98-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1279"><a href="#cb98-1279" aria-hidden="true" tabindex="-1"></a>The aim of this operation is to allow the model to learn richer nuance from the data in addition to the data point itself.</span>
<span id="cb98-1280"><a href="#cb98-1280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1281"><a href="#cb98-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1284"><a href="#cb98-1284" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1285"><a href="#cb98-1285" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train network with encoded continuous variables</span></span>
<span id="cb98-1286"><a href="#cb98-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1287"><a href="#cb98-1287" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb98-1288"><a href="#cb98-1288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1289"><a href="#cb98-1289" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb98-1290"><a href="#cb98-1290" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb98-1291"><a href="#cb98-1291" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb98-1292"><a href="#cb98-1292" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb98-1293"><a href="#cb98-1293" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb98-1294"><a href="#cb98-1294" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb98-1295"><a href="#cb98-1295" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb98-1296"><a href="#cb98-1296" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb98-1297"><a href="#cb98-1297" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb98-1298"><a href="#cb98-1298" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb98-1299"><a href="#cb98-1299" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb98-1300"><a href="#cb98-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1301"><a href="#cb98-1301" aria-hidden="true" tabindex="-1"></a>history_encodcont <span class="op">=</span> nn_encodcont.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1302"><a href="#cb98-1302" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1303"><a href="#cb98-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1304"><a href="#cb98-1304" aria-hidden="true" tabindex="-1"></a>Now that the network is trained, we can inspect how these embeddings of continuous variables work. Compare @fig-encodcontts below with @fig-embedts-3.</span>
<span id="cb98-1305"><a href="#cb98-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1308"><a href="#cb98-1308" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1309"><a href="#cb98-1309" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-encodcontts</span></span>
<span id="cb98-1310"><a href="#cb98-1310" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Time series of continuous embedding</span></span>
<span id="cb98-1311"><a href="#cb98-1311" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb98-1312"><a href="#cb98-1312" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Monthly data</span></span>
<span id="cb98-1313"><a href="#cb98-1313" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - Daily data</span></span>
<span id="cb98-1314"><a href="#cb98-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1315"><a href="#cb98-1315" aria-hidden="true" tabindex="-1"></a>encodcont_ts_m <span class="op">=</span> nn_encodcont.get_layer(<span class="st">"encoding__m"</span>)(np.expand_dims(lstm_X_train[<span class="st">"m"</span>][:,<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>], axis<span class="op">=</span>(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb98-1316"><a href="#cb98-1316" aria-hidden="true" tabindex="-1"></a>plt.plot(np.squeeze(encodcont_ts_m))</span>
<span id="cb98-1317"><a href="#cb98-1317" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1318"><a href="#cb98-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1319"><a href="#cb98-1319" aria-hidden="true" tabindex="-1"></a>encodcont_ts_m <span class="op">=</span> nn_encodcont.get_layer(<span class="st">"encoding__d"</span>)(np.expand_dims(lstm_X_train[<span class="st">"d"</span>][:,<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>], axis<span class="op">=</span>(<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb98-1320"><a href="#cb98-1320" aria-hidden="true" tabindex="-1"></a>plt.plot(np.squeeze(encodcont_ts_m))</span>
<span id="cb98-1321"><a href="#cb98-1321" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1322"><a href="#cb98-1322" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1323"><a href="#cb98-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1324"><a href="#cb98-1324" aria-hidden="true" tabindex="-1"></a><span class="fu">## Variable selection networks {#sec-varsel}</span></span>
<span id="cb98-1325"><a href="#cb98-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1326"><a href="#cb98-1326" aria-hidden="true" tabindex="-1"></a>Now is a good time to put these elements together in a more directly useful way. In particular, we can consider how it can help select the most informative variables amongst a set of input series (eg, lagged inflation and oil prices in the current toy example.)</span>
<span id="cb98-1327"><a href="#cb98-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1328"><a href="#cb98-1328" aria-hidden="true" tabindex="-1"></a>The Variable Selection Networks (VSN) are components of the TFT that are built with GRNs, taking in each covariate and outputting a weighted average of the covariates, where the weights are learned by the model.</span>
<span id="cb98-1329"><a href="#cb98-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1330"><a href="#cb98-1330" aria-hidden="true" tabindex="-1"></a>VSNs can be used to select amongst continuous, categorical or even static variables. The code below focuses on the continuous variables of the current toy model, $\pi_m$ and $o_d$.</span>
<span id="cb98-1331"><a href="#cb98-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1334"><a href="#cb98-1334" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1335"><a href="#cb98-1335" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class svs</span></span>
<span id="cb98-1336"><a href="#cb98-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1337"><a href="#cb98-1337" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> StaticVariableSelection(keras.Layer):</span>
<span id="cb98-1338"><a href="#cb98-1338" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-1339"><a href="#cb98-1339" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb98-1340"><a href="#cb98-1340" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb98-1341"><a href="#cb98-1341" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>, </span>
<span id="cb98-1342"><a href="#cb98-1342" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-1343"><a href="#cb98-1343" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1344"><a href="#cb98-1344" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Static variable selection network"</span></span>
<span id="cb98-1345"><a href="#cb98-1345" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(StaticVariableSelection, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-1346"><a href="#cb98-1346" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb98-1347"><a href="#cb98-1347" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-1348"><a href="#cb98-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1349"><a href="#cb98-1349" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define GRNs for the transformed embeddings</span></span>
<span id="cb98-1350"><a href="#cb98-1350" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grns_transformed_embeddings <span class="op">=</span> []  <span class="co"># This will be a list of GRN layers</span></span>
<span id="cb98-1351"><a href="#cb98-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1352"><a href="#cb98-1352" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flat <span class="op">=</span> keras.layers.Flatten()</span>
<span id="cb98-1353"><a href="#cb98-1353" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.softmax <span class="op">=</span> keras.layers.Activation(<span class="st">'softmax'</span>)</span>
<span id="cb98-1354"><a href="#cb98-1354" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mult <span class="op">=</span> keras.layers.Multiply()</span>
<span id="cb98-1355"><a href="#cb98-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1356"><a href="#cb98-1356" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-1357"><a href="#cb98-1357" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(StaticVariableSelection, <span class="va">self</span>).build(input_shape)</span>
<span id="cb98-1358"><a href="#cb98-1358" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-1359"><a href="#cb98-1359" aria-hidden="true" tabindex="-1"></a>        num_static <span class="op">=</span> input_shape[<span class="dv">2</span>]</span>
<span id="cb98-1360"><a href="#cb98-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1361"><a href="#cb98-1361" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the GRN for the sparse weights</span></span>
<span id="cb98-1362"><a href="#cb98-1362" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grn_sparse_weights <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-1363"><a href="#cb98-1363" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-1364"><a href="#cb98-1364" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>num_static,</span>
<span id="cb98-1365"><a href="#cb98-1365" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span></span>
<span id="cb98-1366"><a href="#cb98-1366" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-1367"><a href="#cb98-1367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1368"><a href="#cb98-1368" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_static):</span>
<span id="cb98-1369"><a href="#cb98-1369" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create a GRN for each static variable</span></span>
<span id="cb98-1370"><a href="#cb98-1370" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grns_transformed_embeddings.append(</span>
<span id="cb98-1371"><a href="#cb98-1371" aria-hidden="true" tabindex="-1"></a>                GatedResidualNetwork(</span>
<span id="cb98-1372"><a href="#cb98-1372" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.d_model, </span>
<span id="cb98-1373"><a href="#cb98-1373" aria-hidden="true" tabindex="-1"></a>                    use_time_distributed<span class="op">=</span><span class="va">False</span></span>
<span id="cb98-1374"><a href="#cb98-1374" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb98-1375"><a href="#cb98-1375" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-1376"><a href="#cb98-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1377"><a href="#cb98-1377" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb98-1378"><a href="#cb98-1378" aria-hidden="true" tabindex="-1"></a>        _, _, num_static, _ <span class="op">=</span> inputs.shape <span class="co"># batch size / one time step (since it's static) / num static variables / d_model</span></span>
<span id="cb98-1379"><a href="#cb98-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1380"><a href="#cb98-1380" aria-hidden="true" tabindex="-1"></a>        flattened <span class="op">=</span> <span class="va">self</span>.flat(inputs)</span>
<span id="cb98-1381"><a href="#cb98-1381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1382"><a href="#cb98-1382" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute sparse weights</span></span>
<span id="cb98-1383"><a href="#cb98-1383" aria-hidden="true" tabindex="-1"></a>        grn_outputs, _ <span class="op">=</span> <span class="va">self</span>.grn_sparse_weights(flattened, training<span class="op">=</span>training)</span>
<span id="cb98-1384"><a href="#cb98-1384" aria-hidden="true" tabindex="-1"></a>        sparse_weights <span class="op">=</span> <span class="va">self</span>.softmax(grn_outputs)</span>
<span id="cb98-1385"><a href="#cb98-1385" aria-hidden="true" tabindex="-1"></a>        sparse_weights <span class="op">=</span> keras.ops.expand_dims(sparse_weights, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb98-1386"><a href="#cb98-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1387"><a href="#cb98-1387" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute transformed embeddings</span></span>
<span id="cb98-1388"><a href="#cb98-1388" aria-hidden="true" tabindex="-1"></a>        transformed_embeddings <span class="op">=</span> []</span>
<span id="cb98-1389"><a href="#cb98-1389" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_static):</span>
<span id="cb98-1390"><a href="#cb98-1390" aria-hidden="true" tabindex="-1"></a>            embed, _ <span class="op">=</span> <span class="va">self</span>.grns_transformed_embeddings[i](inputs[:, <span class="dv">0</span>, i:i<span class="op">+</span><span class="dv">1</span>, :], training<span class="op">=</span>training)</span>
<span id="cb98-1391"><a href="#cb98-1391" aria-hidden="true" tabindex="-1"></a>            transformed_embeddings.append(embed)</span>
<span id="cb98-1392"><a href="#cb98-1392" aria-hidden="true" tabindex="-1"></a>        transformed_embedding <span class="op">=</span> keras.ops.concatenate(transformed_embeddings, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb98-1393"><a href="#cb98-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1394"><a href="#cb98-1394" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine with sparse weights</span></span>
<span id="cb98-1395"><a href="#cb98-1395" aria-hidden="true" tabindex="-1"></a>        combined <span class="op">=</span> <span class="va">self</span>.mult([sparse_weights, transformed_embedding])</span>
<span id="cb98-1396"><a href="#cb98-1396" aria-hidden="true" tabindex="-1"></a>        static_vec <span class="op">=</span> keras.ops.<span class="bu">sum</span>(combined, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb98-1397"><a href="#cb98-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1398"><a href="#cb98-1398" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> static_vec, sparse_weights</span>
<span id="cb98-1399"><a href="#cb98-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1400"><a href="#cb98-1400" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb98-1401"><a href="#cb98-1401" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>(StaticVariableSelectionLayer, <span class="va">self</span>).get_config()</span>
<span id="cb98-1402"><a href="#cb98-1402" aria-hidden="true" tabindex="-1"></a>        config.update({</span>
<span id="cb98-1403"><a href="#cb98-1403" aria-hidden="true" tabindex="-1"></a>            <span class="st">'d_model'</span>: <span class="va">self</span>.d_model,</span>
<span id="cb98-1404"><a href="#cb98-1404" aria-hidden="true" tabindex="-1"></a>            <span class="st">'dropout_rate'</span>: <span class="va">self</span>.dropout_rate</span>
<span id="cb98-1405"><a href="#cb98-1405" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb98-1406"><a href="#cb98-1406" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb98-1407"><a href="#cb98-1407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1408"><a href="#cb98-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1409"><a href="#cb98-1409" aria-hidden="true" tabindex="-1"></a>Temporal variable selection networks work by considering all variables (of a given frequency) at the same time, together with an (optional) static context embedding, to calculate weights that are multiplied by a processed version of each variable. Breaking this up in steps: </span>
<span id="cb98-1410"><a href="#cb98-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1411"><a href="#cb98-1411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>consider $\xi_t^{(j)} \in \mathbb{R}^{\lambda}$ to be the encoding (as in @tbl-summary_embed or @tbl-summary_encodcont) of the $j$th variable in time $t$, and collect the values at time $t$ for *all* $J$ input variables for that frequency in the flattened vector $\Xi_t$</span>
<span id="cb98-1412"><a href="#cb98-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1413"><a href="#cb98-1413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>collect the static variable encoders for use as context, $c_s \in \mathbb{R}^{\lambda}$,</span>
<span id="cb98-1414"><a href="#cb98-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1415"><a href="#cb98-1415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>then, the selection weights for that frequency are the result of $\nu_t = \text{Softmax}(\text{GRN}(\Xi_t, c_s))  \in \mathbb{R}^{|J|}$$,</span>
<span id="cb98-1416"><a href="#cb98-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1417"><a href="#cb98-1417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>these weights, which sum to one due to the softmax operation, are used on a version of the encoded variables that has itself been further encoded by a variable-specific GRN: $\tilde{\xi}_t = \sum_{j=1}^{J}\nu_t^{(j)} \text{GRN}_j (\xi_t^{(j)}) \in \mathbb{R}^{\lambda}$.</span>
<span id="cb98-1418"><a href="#cb98-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1419"><a href="#cb98-1419" aria-hidden="true" tabindex="-1"></a>In short, the temporal variable selection network takes in data with the shape (batch size / number of time steps / number of variables / embedding dimension $\lambda$) and output a tensor with the shape (batch size / number of time steps / embedding dimension $\lambda$).</span>
<span id="cb98-1420"><a href="#cb98-1420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1423"><a href="#cb98-1423" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1424"><a href="#cb98-1424" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class tvs</span></span>
<span id="cb98-1425"><a href="#cb98-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1426"><a href="#cb98-1426" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TemporalVariableSelection(keras.Layer):</span>
<span id="cb98-1427"><a href="#cb98-1427" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-1428"><a href="#cb98-1428" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb98-1429"><a href="#cb98-1429" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb98-1430"><a href="#cb98-1430" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.</span>, </span>
<span id="cb98-1431"><a href="#cb98-1431" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-1432"><a href="#cb98-1432" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1433"><a href="#cb98-1433" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Temporal variable selection"</span></span>
<span id="cb98-1434"><a href="#cb98-1434" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TemporalVariableSelection, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-1435"><a href="#cb98-1435" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb98-1436"><a href="#cb98-1436" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-1437"><a href="#cb98-1437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1438"><a href="#cb98-1438" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mult <span class="op">=</span> keras.layers.Multiply()</span>
<span id="cb98-1439"><a href="#cb98-1439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1440"><a href="#cb98-1440" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-1441"><a href="#cb98-1441" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TemporalVariableSelection, <span class="va">self</span>).build(input_shape)</span>
<span id="cb98-1442"><a href="#cb98-1442" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.time_steps, <span class="va">self</span>.num_input_vars, <span class="va">self</span>.d_model <span class="op">=</span> input_shape[<span class="dv">1</span>:]</span>
<span id="cb98-1443"><a href="#cb98-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1444"><a href="#cb98-1444" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.var_sel_weights <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-1445"><a href="#cb98-1445" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-1446"><a href="#cb98-1446" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span><span class="va">self</span>.num_input_vars,</span>
<span id="cb98-1447"><a href="#cb98-1447" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-1448"><a href="#cb98-1448" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-1449"><a href="#cb98-1449" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-1450"><a href="#cb98-1450" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.softmax <span class="op">=</span> keras.layers.Activation(<span class="st">'softmax'</span>)</span>
<span id="cb98-1451"><a href="#cb98-1451" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb98-1452"><a href="#cb98-1452" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a GRN for each temporal variable</span></span>
<span id="cb98-1453"><a href="#cb98-1453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grns_transformed_embeddings <span class="op">=</span> [</span>
<span id="cb98-1454"><a href="#cb98-1454" aria-hidden="true" tabindex="-1"></a>            GatedResidualNetwork(</span>
<span id="cb98-1455"><a href="#cb98-1455" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-1456"><a href="#cb98-1456" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-1457"><a href="#cb98-1457" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span></span>
<span id="cb98-1458"><a href="#cb98-1458" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-1459"><a href="#cb98-1459" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_input_vars)</span>
<span id="cb98-1460"><a href="#cb98-1460" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb98-1461"><a href="#cb98-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1462"><a href="#cb98-1462" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb98-1463"><a href="#cb98-1463" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb98-1464"><a href="#cb98-1464" aria-hidden="true" tabindex="-1"></a>        inputs, <span class="co"># List of temporal embeddings, static context</span></span>
<span id="cb98-1465"><a href="#cb98-1465" aria-hidden="true" tabindex="-1"></a>        context<span class="op">=</span><span class="va">None</span>, <span class="co"># Used for static_context in the TFT</span></span>
<span id="cb98-1466"><a href="#cb98-1466" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb98-1467"><a href="#cb98-1467" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1468"><a href="#cb98-1468" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shape of inputs (temporal_embeddings): (batch size / num time steps / num variables / embedding dimension, ie dim_model)</span></span>
<span id="cb98-1469"><a href="#cb98-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1470"><a href="#cb98-1470" aria-hidden="true" tabindex="-1"></a>        flattened_embed <span class="op">=</span> keras.ops.reshape( <span class="co"># \Xi_t</span></span>
<span id="cb98-1471"><a href="#cb98-1471" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb98-1472"><a href="#cb98-1472" aria-hidden="true" tabindex="-1"></a>            (<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.time_steps, <span class="va">self</span>.num_input_vars <span class="op">*</span> <span class="va">self</span>.d_model)</span>
<span id="cb98-1473"><a href="#cb98-1473" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-1474"><a href="#cb98-1474" aria-hidden="true" tabindex="-1"></a>        parallel_variables <span class="op">=</span> keras.ops.split(</span>
<span id="cb98-1475"><a href="#cb98-1475" aria-hidden="true" tabindex="-1"></a>            inputs, </span>
<span id="cb98-1476"><a href="#cb98-1476" aria-hidden="true" tabindex="-1"></a>            indices_or_sections<span class="op">=</span><span class="va">self</span>.num_input_vars,</span>
<span id="cb98-1477"><a href="#cb98-1477" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb98-1478"><a href="#cb98-1478" aria-hidden="true" tabindex="-1"></a>        ) <span class="co"># tensor is shaped this way so that a GRN can be applied to each variable of each batch</span></span>
<span id="cb98-1479"><a href="#cb98-1479" aria-hidden="true" tabindex="-1"></a>        c_s <span class="op">=</span> keras.ops.expand_dims(context, axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">if</span> context <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb98-1480"><a href="#cb98-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1481"><a href="#cb98-1481" aria-hidden="true" tabindex="-1"></a>        <span class="co"># variable weights</span></span>
<span id="cb98-1482"><a href="#cb98-1482" aria-hidden="true" tabindex="-1"></a>        grn_outputs, _ <span class="op">=</span> <span class="va">self</span>.var_sel_weights(flattened_embed, c_s, training<span class="op">=</span>training)</span>
<span id="cb98-1483"><a href="#cb98-1483" aria-hidden="true" tabindex="-1"></a>        variable_weights <span class="op">=</span> <span class="va">self</span>.softmax(grn_outputs)</span>
<span id="cb98-1484"><a href="#cb98-1484" aria-hidden="true" tabindex="-1"></a>        variable_weights <span class="op">=</span> keras.ops.expand_dims(variable_weights, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb98-1485"><a href="#cb98-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1486"><a href="#cb98-1486" aria-hidden="true" tabindex="-1"></a>        transformed_embeddings, _ <span class="op">=</span> [</span>
<span id="cb98-1487"><a href="#cb98-1487" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grns_transformed_embeddings[i](parallel_variables[i], training<span class="op">=</span>training)</span>
<span id="cb98-1488"><a href="#cb98-1488" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_input_vars)</span>
<span id="cb98-1489"><a href="#cb98-1489" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb98-1490"><a href="#cb98-1490" aria-hidden="true" tabindex="-1"></a>        transformed_embeddings <span class="op">=</span> keras.ops.stack(transformed_embeddings, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb98-1491"><a href="#cb98-1491" aria-hidden="true" tabindex="-1"></a>        combined <span class="op">=</span> keras.layers.Multiply()([variable_weights, transformed_embeddings])</span>
<span id="cb98-1492"><a href="#cb98-1492" aria-hidden="true" tabindex="-1"></a>        temporal_vec <span class="op">=</span> keras.<span class="bu">sum</span>(combined, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb98-1493"><a href="#cb98-1493" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-1494"><a href="#cb98-1494" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.variable_weights <span class="op">=</span> variable_weights</span>
<span id="cb98-1495"><a href="#cb98-1495" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> temporal_vec</span>
<span id="cb98-1496"><a href="#cb98-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1497"><a href="#cb98-1497" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_selection_weights(<span class="va">self</span>):</span>
<span id="cb98-1498"><a href="#cb98-1498" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.variable_weights</span>
<span id="cb98-1499"><a href="#cb98-1499" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1500"><a href="#cb98-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1503"><a href="#cb98-1503" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1504"><a href="#cb98-1504" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_vsn</span></span>
<span id="cb98-1505"><a href="#cb98-1505" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of VSN (work in progress)</span></span>
<span id="cb98-1506"><a href="#cb98-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1507"><a href="#cb98-1507" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb98-1508"><a href="#cb98-1508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1509"><a href="#cb98-1509" aria-hidden="true" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb98-1510"><a href="#cb98-1510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1511"><a href="#cb98-1511" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;continuous variables&gt;</span></span>
<span id="cb98-1512"><a href="#cb98-1512" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb98-1513"><a href="#cb98-1513" aria-hidden="true" tabindex="-1"></a>cont_inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span><span class="ss">f"input__freq_</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">"</span>) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb98-1514"><a href="#cb98-1514" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb98-1515"><a href="#cb98-1515" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb98-1516"><a href="#cb98-1516" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb98-1517"><a href="#cb98-1517" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-1518"><a href="#cb98-1518" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb98-1519"><a href="#cb98-1519" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> cont_inputs.items()</span>
<span id="cb98-1520"><a href="#cb98-1520" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1521"><a href="#cb98-1521" aria-hidden="true" tabindex="-1"></a><span class="co"># expand dims to add a dimension for the number of variables (in this case, one)</span></span>
<span id="cb98-1522"><a href="#cb98-1522" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {k: keras.ops.expand_dims(v, axis<span class="op">=</span><span class="dv">2</span>) <span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items()}</span>
<span id="cb98-1523"><a href="#cb98-1523" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;/continuous variables&gt;</span></span>
<span id="cb98-1524"><a href="#cb98-1524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1525"><a href="#cb98-1525" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;categorical variables&gt;</span></span>
<span id="cb98-1526"><a href="#cb98-1526" aria-hidden="true" tabindex="-1"></a>cat_inputs <span class="op">=</span> {</span>
<span id="cb98-1527"><a href="#cb98-1527" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>k, dtype<span class="op">=</span><span class="st">'int32'</span>)  <span class="co"># Input layer for each feature with shape (None, 1) to include a time dimension</span></span>
<span id="cb98-1528"><a href="#cb98-1528" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> vocab_sizes.keys()</span>
<span id="cb98-1529"><a href="#cb98-1529" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1530"><a href="#cb98-1530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1531"><a href="#cb98-1531" aria-hidden="true" tabindex="-1"></a>embedded_layers <span class="op">=</span> {</span>
<span id="cb98-1532"><a href="#cb98-1532" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Embedding(input_dim<span class="op">=</span>v, output_dim<span class="op">=</span>dim, name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">_embedding"</span>)(cat_inputs[k])</span>
<span id="cb98-1533"><a href="#cb98-1533" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> vocab_sizes.items()</span>
<span id="cb98-1534"><a href="#cb98-1534" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1535"><a href="#cb98-1535" aria-hidden="true" tabindex="-1"></a><span class="co"># combined = keras.layers.Average()(list(embedded_layers.values()))</span></span>
<span id="cb98-1536"><a href="#cb98-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1537"><a href="#cb98-1537" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(dim, activation<span class="op">=</span><span class="st">"relu"</span>)(combined)</span>
<span id="cb98-1538"><a href="#cb98-1538" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>)(x)</span>
<span id="cb98-1539"><a href="#cb98-1539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1540"><a href="#cb98-1540" aria-hidden="true" tabindex="-1"></a>nn_embed <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">list</span>(inputs.values()), outputs<span class="op">=</span>output)</span>
<span id="cb98-1541"><a href="#cb98-1541" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;/categorical variables&gt;</span></span>
<span id="cb98-1542"><a href="#cb98-1542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1543"><a href="#cb98-1543" aria-hidden="true" tabindex="-1"></a><span class="co">### &lt;VSN&gt;</span></span>
<span id="cb98-1544"><a href="#cb98-1544" aria-hidden="true" tabindex="-1"></a>flattened <span class="op">=</span> {</span>
<span id="cb98-1545"><a href="#cb98-1545" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Reshape(target_shape<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, v.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">*</span> v.shape[<span class="op">-</span><span class="dv">2</span>]), name<span class="op">=</span><span class="ss">f"reshape__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(v) <span class="co"># \Xi_t</span></span>
<span id="cb98-1546"><a href="#cb98-1546" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items()</span>
<span id="cb98-1547"><a href="#cb98-1547" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1548"><a href="#cb98-1548" aria-hidden="true" tabindex="-1"></a>parallel_variables <span class="op">=</span> {</span>
<span id="cb98-1549"><a href="#cb98-1549" aria-hidden="true" tabindex="-1"></a>    k: keras.ops.split(</span>
<span id="cb98-1550"><a href="#cb98-1550" aria-hidden="true" tabindex="-1"></a>            v, </span>
<span id="cb98-1551"><a href="#cb98-1551" aria-hidden="true" tabindex="-1"></a>            indices_or_sections<span class="op">=</span>v.shape[<span class="dv">2</span>],</span>
<span id="cb98-1552"><a href="#cb98-1552" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb98-1553"><a href="#cb98-1553" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-1554"><a href="#cb98-1554" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items()</span>
<span id="cb98-1555"><a href="#cb98-1555" aria-hidden="true" tabindex="-1"></a>} <span class="co"># each value is a list, with each item of the list corresponding to one variable</span></span>
<span id="cb98-1556"><a href="#cb98-1556" aria-hidden="true" tabindex="-1"></a>var_weights <span class="op">=</span> {</span>
<span id="cb98-1557"><a href="#cb98-1557" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.Activation(<span class="st">'softmax'</span>)(</span>
<span id="cb98-1558"><a href="#cb98-1558" aria-hidden="true" tabindex="-1"></a>        GatedResidualNetwork(</span>
<span id="cb98-1559"><a href="#cb98-1559" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span>dim,</span>
<span id="cb98-1560"><a href="#cb98-1560" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span><span class="dv">1</span>, <span class="co"># number of input variables in each frequency</span></span>
<span id="cb98-1561"><a href="#cb98-1561" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-1562"><a href="#cb98-1562" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="ss">f"variable_selection__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-1563"><a href="#cb98-1563" aria-hidden="true" tabindex="-1"></a>        )(v)[<span class="dv">0</span>]</span>
<span id="cb98-1564"><a href="#cb98-1564" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb98-1565"><a href="#cb98-1565" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> flattened.items()</span>
<span id="cb98-1566"><a href="#cb98-1566" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1567"><a href="#cb98-1567" aria-hidden="true" tabindex="-1"></a>varspecific_grn <span class="op">=</span> {</span>
<span id="cb98-1568"><a href="#cb98-1568" aria-hidden="true" tabindex="-1"></a>    k: keras.ops.concatenate([</span>
<span id="cb98-1569"><a href="#cb98-1569" aria-hidden="true" tabindex="-1"></a>        GatedResidualNetwork(</span>
<span id="cb98-1570"><a href="#cb98-1570" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span>dim,</span>
<span id="cb98-1571"><a href="#cb98-1571" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-1572"><a href="#cb98-1572" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="ss">f"variable_specific__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-1573"><a href="#cb98-1573" aria-hidden="true" tabindex="-1"></a>        )(v[i])[<span class="dv">0</span>]</span>
<span id="cb98-1574"><a href="#cb98-1574" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(v)) <span class="co"># number of input variables in each frequency</span></span>
<span id="cb98-1575"><a href="#cb98-1575" aria-hidden="true" tabindex="-1"></a>    ], axis<span class="op">=</span><span class="dv">2</span>) <span class="co"># along the num_input_vars dimension, resulting in batch size / num time steps / num input vars / embedding dim (lambda)</span></span>
<span id="cb98-1576"><a href="#cb98-1576" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> parallel_variables.items()</span>
<span id="cb98-1577"><a href="#cb98-1577" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1578"><a href="#cb98-1578" aria-hidden="true" tabindex="-1"></a>combined_features <span class="op">=</span> {}</span>
<span id="cb98-1579"><a href="#cb98-1579" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> freqs:</span>
<span id="cb98-1580"><a href="#cb98-1580" aria-hidden="true" tabindex="-1"></a>    broadcasted_weights <span class="op">=</span> keras.ops.tile(keras.ops.expand_dims(var_weights[f], axis<span class="op">=-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, dim))</span>
<span id="cb98-1581"><a href="#cb98-1581" aria-hidden="true" tabindex="-1"></a>    combined_features[f] <span class="op">=</span> keras.ops.<span class="bu">sum</span>(</span>
<span id="cb98-1582"><a href="#cb98-1582" aria-hidden="true" tabindex="-1"></a>        keras.layers.Multiply(name<span class="op">=</span><span class="ss">f"weighting__freq_</span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">"</span>)([broadcasted_weights, varspecific_grn[f]]),</span>
<span id="cb98-1583"><a href="#cb98-1583" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">2</span></span>
<span id="cb98-1584"><a href="#cb98-1584" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb98-1585"><a href="#cb98-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1586"><a href="#cb98-1586" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt;/VSN&gt;</span></span>
<span id="cb98-1587"><a href="#cb98-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1588"><a href="#cb98-1588" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb98-1589"><a href="#cb98-1589" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> combined_features.items():</span>
<span id="cb98-1590"><a href="#cb98-1590" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb98-1591"><a href="#cb98-1591" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb98-1592"><a href="#cb98-1592" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb98-1593"><a href="#cb98-1593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1594"><a href="#cb98-1594" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb98-1595"><a href="#cb98-1595" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"FinalFeatureTransformation"</span>)(encoded_series)</span>
<span id="cb98-1596"><a href="#cb98-1596" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb98-1597"><a href="#cb98-1597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1598"><a href="#cb98-1598" aria-hidden="true" tabindex="-1"></a>nn_vsn <span class="op">=</span> keras.Model(</span>
<span id="cb98-1599"><a href="#cb98-1599" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>cont_inputs <span class="op">|</span> cat_inputs, </span>
<span id="cb98-1600"><a href="#cb98-1600" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb98-1601"><a href="#cb98-1601" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"VSN"</span></span>
<span id="cb98-1602"><a href="#cb98-1602" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-1603"><a href="#cb98-1603" aria-hidden="true" tabindex="-1"></a>nn_vsn.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-1604"><a href="#cb98-1604" aria-hidden="true" tabindex="-1"></a>nn_vsn.summary()</span>
<span id="cb98-1605"><a href="#cb98-1605" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1606"><a href="#cb98-1606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1609"><a href="#cb98-1609" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1610"><a href="#cb98-1610" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_vsn</span></span>
<span id="cb98-1611"><a href="#cb98-1611" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of model with variable selection network</span></span>
<span id="cb98-1612"><a href="#cb98-1612" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_vsn, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1613"><a href="#cb98-1613" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1614"><a href="#cb98-1614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1617"><a href="#cb98-1617" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1618"><a href="#cb98-1618" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train the model with temporal variable selection</span></span>
<span id="cb98-1619"><a href="#cb98-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1620"><a href="#cb98-1620" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split, X_dates <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags, return_dates<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1621"><a href="#cb98-1621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1622"><a href="#cb98-1622" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_vsn(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb98-1623"><a href="#cb98-1623" aria-hidden="true" tabindex="-1"></a>    X_vsn <span class="op">=</span> {}</span>
<span id="cb98-1624"><a href="#cb98-1624" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb98-1625"><a href="#cb98-1625" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb98-1626"><a href="#cb98-1626" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_vsn:</span>
<span id="cb98-1627"><a href="#cb98-1627" aria-hidden="true" tabindex="-1"></a>                X_vsn[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb98-1628"><a href="#cb98-1628" aria-hidden="true" tabindex="-1"></a>            X_vsn[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb98-1629"><a href="#cb98-1629" aria-hidden="true" tabindex="-1"></a>    vsn_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_vsn.items()}</span>
<span id="cb98-1630"><a href="#cb98-1630" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vsn_X</span>
<span id="cb98-1631"><a href="#cb98-1631" aria-hidden="true" tabindex="-1"></a>vsn_X_train <span class="op">=</span> adjust_data_vsn(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb98-1632"><a href="#cb98-1632" aria-hidden="true" tabindex="-1"></a>vsn_X_valid <span class="op">=</span> adjust_data_vsn(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb98-1633"><a href="#cb98-1633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1634"><a href="#cb98-1634" aria-hidden="true" tabindex="-1"></a>y_freq <span class="op">=</span> <span class="st">"m"</span> <span class="co">#&nbsp;only do the date features for the same frequency as the dependent variable</span></span>
<span id="cb98-1635"><a href="#cb98-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1636"><a href="#cb98-1636" aria-hidden="true" tabindex="-1"></a>date_feats <span class="op">=</span> {fold: {chunk: [] <span class="cf">for</span> chunk <span class="kw">in</span> X_dates[fold].keys()} <span class="cf">for</span> fold <span class="kw">in</span> X_dates.keys()}</span>
<span id="cb98-1637"><a href="#cb98-1637" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> X_dates.keys():</span>
<span id="cb98-1638"><a href="#cb98-1638" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> chunk <span class="kw">in</span> X_dates[fold].keys():</span>
<span id="cb98-1639"><a href="#cb98-1639" aria-hidden="true" tabindex="-1"></a>        dates_fold <span class="op">=</span> [i[y_freq] <span class="cf">for</span> i <span class="kw">in</span> X_dates[fold][chunk]]</span>
<span id="cb98-1640"><a href="#cb98-1640" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> dates_fold:</span>
<span id="cb98-1641"><a href="#cb98-1641" aria-hidden="true" tabindex="-1"></a>            to_pad <span class="op">=</span> get_timefeat(pd.DataFrame(index<span class="op">=</span>i), freq<span class="op">=</span>y_freq, features<span class="op">=</span>timefeat).values</span>
<span id="cb98-1642"><a href="#cb98-1642" aria-hidden="true" tabindex="-1"></a>            padded <span class="op">=</span> keras.utils.pad_sequences([to_pad], maxlen<span class="op">=</span>maxlags[y_freq], dtype<span class="op">=</span>np.int32)</span>
<span id="cb98-1643"><a href="#cb98-1643" aria-hidden="true" tabindex="-1"></a>            date_feats[fold][chunk].append(padded)</span>
<span id="cb98-1644"><a href="#cb98-1644" aria-hidden="true" tabindex="-1"></a>cat_input_vars <span class="op">=</span> np.stack([np.squeeze(i) <span class="cf">for</span> i <span class="kw">in</span> date_feats[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]])</span>
<span id="cb98-1645"><a href="#cb98-1645" aria-hidden="true" tabindex="-1"></a><span class="co">#&lt;VSN date padding&gt; -- only to be done in the same frequency as the dependent variable</span></span>
<span id="cb98-1646"><a href="#cb98-1646" aria-hidden="true" tabindex="-1"></a><span class="co"># try:</span></span>
<span id="cb98-1647"><a href="#cb98-1647" aria-hidden="true" tabindex="-1"></a><span class="co">#     if return_dates:</span></span>
<span id="cb98-1648"><a href="#cb98-1648" aria-hidden="true" tabindex="-1"></a><span class="co">#         dates_X[f] = X[f][:ysample_date][:-1].index</span></span>
<span id="cb98-1649"><a href="#cb98-1649" aria-hidden="true" tabindex="-1"></a><span class="co">#     to_pad = X[f][:ysample_date][:-1].values</span></span>
<span id="cb98-1650"><a href="#cb98-1650" aria-hidden="true" tabindex="-1"></a><span class="co"># except KeyError:</span></span>
<span id="cb98-1651"><a href="#cb98-1651" aria-hidden="true" tabindex="-1"></a><span class="co">#     to_pad = np.zeros((1,1))</span></span>
<span id="cb98-1652"><a href="#cb98-1652" aria-hidden="true" tabindex="-1"></a><span class="co"># padded_x[f] = keras.utils.pad_sequences([to_pad], maxlen=maxlags[f], dtype=np.float32)</span></span>
<span id="cb98-1653"><a href="#cb98-1653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1654"><a href="#cb98-1654" aria-hidden="true" tabindex="-1"></a><span class="co"># x_shape = (1, maxlags[f], n_feat[f]) if timedim else (1, maxlags[f] * n_feat[f])</span></span>
<span id="cb98-1655"><a href="#cb98-1655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1656"><a href="#cb98-1656" aria-hidden="true" tabindex="-1"></a><span class="co"># padded_x[f] = padded_x[f].reshape(x_shape)</span></span>
<span id="cb98-1657"><a href="#cb98-1657" aria-hidden="true" tabindex="-1"></a><span class="co">#&lt;/VSN date padding&gt;</span></span>
<span id="cb98-1658"><a href="#cb98-1658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1659"><a href="#cb98-1659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1660"><a href="#cb98-1660" aria-hidden="true" tabindex="-1"></a><span class="co"># y_timefeat_train = {timef:</span></span>
<span id="cb98-1661"><a href="#cb98-1661" aria-hidden="true" tabindex="-1"></a><span class="co">#     np.array([</span></span>
<span id="cb98-1662"><a href="#cb98-1662" aria-hidden="true" tabindex="-1"></a><span class="co">#         get_timefeat(pd.DataFrame(i.values, index=[i.name], columns=i.index), features=timef)</span></span>
<span id="cb98-1663"><a href="#cb98-1663" aria-hidden="true" tabindex="-1"></a><span class="co">#         for i in y_train_split_fc[fold]["train"]</span></span>
<span id="cb98-1664"><a href="#cb98-1664" aria-hidden="true" tabindex="-1"></a><span class="co">#     ])</span></span>
<span id="cb98-1665"><a href="#cb98-1665" aria-hidden="true" tabindex="-1"></a><span class="co">#     for timef in timefeat</span></span>
<span id="cb98-1666"><a href="#cb98-1666" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span>
<span id="cb98-1667"><a href="#cb98-1667" aria-hidden="true" tabindex="-1"></a><span class="co"># y_timefeat_valid = {timef:</span></span>
<span id="cb98-1668"><a href="#cb98-1668" aria-hidden="true" tabindex="-1"></a><span class="co">#     np.array([</span></span>
<span id="cb98-1669"><a href="#cb98-1669" aria-hidden="true" tabindex="-1"></a><span class="co">#         get_timefeat(pd.DataFrame(i.values, index=[i.name], columns=i.index), features=timef)</span></span>
<span id="cb98-1670"><a href="#cb98-1670" aria-hidden="true" tabindex="-1"></a><span class="co">#         for i in y_train_split_fc[fold]["valid"]</span></span>
<span id="cb98-1671"><a href="#cb98-1671" aria-hidden="true" tabindex="-1"></a><span class="co">#     ])</span></span>
<span id="cb98-1672"><a href="#cb98-1672" aria-hidden="true" tabindex="-1"></a><span class="co">#     for timef in timefeat</span></span>
<span id="cb98-1673"><a href="#cb98-1673" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span>
<span id="cb98-1674"><a href="#cb98-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1675"><a href="#cb98-1675" aria-hidden="true" tabindex="-1"></a>history_vsn <span class="op">=</span> nn_vsn.fit(x<span class="op">=</span>vsn_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(vsn_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1676"><a href="#cb98-1676" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1677"><a href="#cb98-1677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1680"><a href="#cb98-1680" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1681"><a href="#cb98-1681" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_vsn</span></span>
<span id="cb98-1682"><a href="#cb98-1682" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses of the model with variable selection</span></span>
<span id="cb98-1683"><a href="#cb98-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1684"><a href="#cb98-1684" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_vsn.history).plot()</span>
<span id="cb98-1685"><a href="#cb98-1685" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-1686"><a href="#cb98-1686" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-1687"><a href="#cb98-1687" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1688"><a href="#cb98-1688" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1689"><a href="#cb98-1689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1690"><a href="#cb98-1690" aria-hidden="true" tabindex="-1"></a><span class="fu">## Now is a(nother) good time to pay attention {#sec-transf}</span></span>
<span id="cb98-1691"><a href="#cb98-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1692"><a href="#cb98-1692" aria-hidden="true" tabindex="-1"></a>The transformer architecture (@vaswani2023attentionneed) is based on an altogether different way to look at time series compared to recurrent neural networks as the LSTMs above. Its basic idea is to look at the whole sequence at the same time instead of sequentially. This allows the network to learn relevant connections between data that might be far off in time.</span>
<span id="cb98-1693"><a href="#cb98-1693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1694"><a href="#cb98-1694" aria-hidden="true" tabindex="-1"></a>Starting with just one frequency for clarity, consider we input time windows of size $N = k + \tau_{\text{max}}$, for $k$ the maximum number of lags and $\tau_{\text{max}}$ the number of forecasted steps. Each of these time steps is associated with a value vector, $v \in \mathbb{R}^{\lambda_{\text{value}}}$; the time series of these values is $\mathbf{V} \in \mathbb{R}^{N \times \lambda_{\text{value}}}$. The self-attention mechanism will scale the $\mathbf{V}$ to reflect the similarity between two other vectors also associated with each time steps.</span>
<span id="cb98-1695"><a href="#cb98-1695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1696"><a href="#cb98-1696" aria-hidden="true" tabindex="-1"></a>These vectors are their keys and queries, respectively $\mathbf{K}, \mathbf{Q} \in \mathbb{R}^{\lambda_{\text{attention}}}$. "Attention" refers to how their similarity is calculated. Essentially, key vectors that are more similar to query vectors will generate larger numbers of "attention"; similarly, if they are orthogonal, this will result in a low or null attention between a pair of time steps. We follow common practice and calculate the attention weights using the dot-product between the key and query vectors, normalised by softmax to sum to one and span only the positive space:</span>
<span id="cb98-1697"><a href="#cb98-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1698"><a href="#cb98-1698" aria-hidden="true" tabindex="-1"></a>$A(\mathbf{Q}, \mathbf{K}) = \text{Softmax}(\mathbf{Q}\mathbf{K}'/\sqrt{\lambda_{\text{attention}}})$.</span>
<span id="cb98-1699"><a href="#cb98-1699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1702"><a href="#cb98-1702" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1703"><a href="#cb98-1703" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class ScaledDotProductAttention</span></span>
<span id="cb98-1704"><a href="#cb98-1704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1705"><a href="#cb98-1705" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScaledDotProductAttention(keras.Layer):</span>
<span id="cb98-1706"><a href="#cb98-1706" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-1707"><a href="#cb98-1707" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-1708"><a href="#cb98-1708" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.0</span>, <span class="co"># Will be ignored if `training=False`</span></span>
<span id="cb98-1709"><a href="#cb98-1709" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-1710"><a href="#cb98-1710" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1711"><a href="#cb98-1711" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ScaledDotProductAttention, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-1712"><a href="#cb98-1712" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-1713"><a href="#cb98-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1714"><a href="#cb98-1714" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-1715"><a href="#cb98-1715" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ScaledDotProductAttention, <span class="va">self</span>).build(input_shape)</span>
<span id="cb98-1716"><a href="#cb98-1716" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> keras.layers.Dropout(rate<span class="op">=</span><span class="va">self</span>.dropout_rate)</span>
<span id="cb98-1717"><a href="#cb98-1717" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> keras.layers.Activation(<span class="st">'softmax'</span>)</span>
<span id="cb98-1718"><a href="#cb98-1718" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dot_22 <span class="op">=</span> keras.layers.Dot(axes<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)) <span class="co"># both inputs need to have the same size in their axis=2 dimensions, in this case, d_model for both</span></span>
<span id="cb98-1719"><a href="#cb98-1719" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dot_21 <span class="op">=</span> keras.layers.Dot(axes<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>)) <span class="co"># the size of the first input's axis=2 dimension needs to match the size of the second input's axis=1 dimension</span></span>
<span id="cb98-1720"><a href="#cb98-1720" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lambda_layer <span class="op">=</span> keras.layers.Lambda(<span class="kw">lambda</span> x: (<span class="op">-</span><span class="fl">1e9</span>) <span class="op">*</span> (<span class="fl">1.</span> <span class="op">-</span> keras.ops.cast(x, <span class="st">'float32'</span>)))</span>
<span id="cb98-1721"><a href="#cb98-1721" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add <span class="op">=</span> keras.layers.Add()</span>
<span id="cb98-1722"><a href="#cb98-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1723"><a href="#cb98-1723" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb98-1724"><a href="#cb98-1724" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-1725"><a href="#cb98-1725" aria-hidden="true" tabindex="-1"></a>        q, <span class="co"># Queries, tensor of shape (?, time, D_model)</span></span>
<span id="cb98-1726"><a href="#cb98-1726" aria-hidden="true" tabindex="-1"></a>        k, <span class="co"># Keys, tensor of shape (?, time, D_model)</span></span>
<span id="cb98-1727"><a href="#cb98-1727" aria-hidden="true" tabindex="-1"></a>        v, <span class="co"># Values, tensor of shape (?, time, D_model)</span></span>
<span id="cb98-1728"><a href="#cb98-1728" aria-hidden="true" tabindex="-1"></a>        mask, <span class="co"># Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)</span></span>
<span id="cb98-1729"><a href="#cb98-1729" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span>, <span class="co"># Whether the layer is being trained or used in inference</span></span>
<span id="cb98-1730"><a href="#cb98-1730" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1731"><a href="#cb98-1731" aria-hidden="true" tabindex="-1"></a>        <span class="co"># returns Tuple (layer outputs, attention weights)</span></span>
<span id="cb98-1732"><a href="#cb98-1732" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> keras.ops.sqrt(keras.ops.cast(keras.ops.shape(k)[<span class="op">-</span><span class="dv">1</span>], dtype<span class="op">=</span><span class="st">'float32'</span>))</span>
<span id="cb98-1733"><a href="#cb98-1733" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> <span class="va">self</span>.dot_22([q, k]) <span class="op">/</span> scale</span>
<span id="cb98-1734"><a href="#cb98-1734" aria-hidden="true" tabindex="-1"></a>        <span class="co">#attention = keras.ops.einsum("bij,bjk-&gt;bik", q, keras.ops.transpose(k, axes=(0, 2, 1))) / scale</span></span>
<span id="cb98-1735"><a href="#cb98-1735" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb98-1736"><a href="#cb98-1736" aria-hidden="true" tabindex="-1"></a>            mmask <span class="op">=</span> <span class="va">self</span>.lambda_layer(mask)</span>
<span id="cb98-1737"><a href="#cb98-1737" aria-hidden="true" tabindex="-1"></a>            attention <span class="op">=</span> <span class="va">self</span>.add([attention, mmask])</span>
<span id="cb98-1738"><a href="#cb98-1738" aria-hidden="true" tabindex="-1"></a>        attention <span class="op">=</span> <span class="va">self</span>.activation(attention)</span>
<span id="cb98-1739"><a href="#cb98-1739" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb98-1740"><a href="#cb98-1740" aria-hidden="true" tabindex="-1"></a>            attention <span class="op">=</span> <span class="va">self</span>.dropout(attention)</span>
<span id="cb98-1741"><a href="#cb98-1741" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.dot_21([attention, v])</span>
<span id="cb98-1742"><a href="#cb98-1742" aria-hidden="true" tabindex="-1"></a>        <span class="co">#output = keras.ops.einsum("btt,btd-&gt;bt", attention, v)</span></span>
<span id="cb98-1743"><a href="#cb98-1743" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention</span>
<span id="cb98-1744"><a href="#cb98-1744" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1745"><a href="#cb98-1745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1748"><a href="#cb98-1748" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1749"><a href="#cb98-1749" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_sdpa</span></span>
<span id="cb98-1750"><a href="#cb98-1750" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of network with attention</span></span>
<span id="cb98-1751"><a href="#cb98-1751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1752"><a href="#cb98-1752" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb98-1753"><a href="#cb98-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1754"><a href="#cb98-1754" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb98-1755"><a href="#cb98-1755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1756"><a href="#cb98-1756" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb98-1757"><a href="#cb98-1757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1758"><a href="#cb98-1758" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb98-1759"><a href="#cb98-1759" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb98-1760"><a href="#cb98-1760" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb98-1761"><a href="#cb98-1761" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-1762"><a href="#cb98-1762" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb98-1763"><a href="#cb98-1763" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb98-1764"><a href="#cb98-1764" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1765"><a href="#cb98-1765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1766"><a href="#cb98-1766" aria-hidden="true" tabindex="-1"></a>Attns <span class="op">=</span> []</span>
<span id="cb98-1767"><a href="#cb98-1767" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb98-1768"><a href="#cb98-1768" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb98-1769"><a href="#cb98-1769" aria-hidden="true" tabindex="-1"></a>    attn, attn_weights <span class="op">=</span> ScaledDotProductAttention(name<span class="op">=</span><span class="ss">f"Attention__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(q<span class="op">=</span>v, k<span class="op">=</span>v, v<span class="op">=</span>v, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb98-1770"><a href="#cb98-1770" aria-hidden="true" tabindex="-1"></a>    Attns.append(attn)</span>
<span id="cb98-1771"><a href="#cb98-1771" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(attn)</span>
<span id="cb98-1772"><a href="#cb98-1772" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb98-1773"><a href="#cb98-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1774"><a href="#cb98-1774" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Concatenate(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb98-1775"><a href="#cb98-1775" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb98-1776"><a href="#cb98-1776" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb98-1777"><a href="#cb98-1777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1778"><a href="#cb98-1778" aria-hidden="true" tabindex="-1"></a>nn_sdpa <span class="op">=</span> keras.Model(</span>
<span id="cb98-1779"><a href="#cb98-1779" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb98-1780"><a href="#cb98-1780" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb98-1781"><a href="#cb98-1781" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"Attention"</span></span>
<span id="cb98-1782"><a href="#cb98-1782" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-1783"><a href="#cb98-1783" aria-hidden="true" tabindex="-1"></a>nn_sdpa.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-1784"><a href="#cb98-1784" aria-hidden="true" tabindex="-1"></a>nn_sdpa.summary()</span>
<span id="cb98-1785"><a href="#cb98-1785" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1786"><a href="#cb98-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1789"><a href="#cb98-1789" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1790"><a href="#cb98-1790" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_sdpa</span></span>
<span id="cb98-1791"><a href="#cb98-1791" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the multi-frequency attention Model</span></span>
<span id="cb98-1792"><a href="#cb98-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1793"><a href="#cb98-1793" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_sdpa, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1794"><a href="#cb98-1794" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1795"><a href="#cb98-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1796"><a href="#cb98-1796" aria-hidden="true" tabindex="-1"></a>Note that in this simple model, the way the different frequencies are combined in @fig-arch_sdpa is by passing the result of the respective self-attention layers (each with their own size along the time dimension due to different frequencies) through their own LSTM layer returning one value. This value is then weighted by a dense network, producing the output.</span>
<span id="cb98-1797"><a href="#cb98-1797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1800"><a href="#cb98-1800" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1801"><a href="#cb98-1801" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train the lstm with attention</span></span>
<span id="cb98-1802"><a href="#cb98-1802" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb98-1803"><a href="#cb98-1803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1804"><a href="#cb98-1804" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb98-1805"><a href="#cb98-1805" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb98-1806"><a href="#cb98-1806" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb98-1807"><a href="#cb98-1807" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb98-1808"><a href="#cb98-1808" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb98-1809"><a href="#cb98-1809" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb98-1810"><a href="#cb98-1810" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb98-1811"><a href="#cb98-1811" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb98-1812"><a href="#cb98-1812" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb98-1813"><a href="#cb98-1813" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb98-1814"><a href="#cb98-1814" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb98-1815"><a href="#cb98-1815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1816"><a href="#cb98-1816" aria-hidden="true" tabindex="-1"></a>history_sdpa <span class="op">=</span> nn_sdpa.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1817"><a href="#cb98-1817" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1818"><a href="#cb98-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1821"><a href="#cb98-1821" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1822"><a href="#cb98-1822" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_sdpa</span></span>
<span id="cb98-1823"><a href="#cb98-1823" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses of the model with attention</span></span>
<span id="cb98-1824"><a href="#cb98-1824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1825"><a href="#cb98-1825" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_sdpa.history).plot()</span>
<span id="cb98-1826"><a href="#cb98-1826" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-1827"><a href="#cb98-1827" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-1828"><a href="#cb98-1828" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1829"><a href="#cb98-1829" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1830"><a href="#cb98-1830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1831"><a href="#cb98-1831" aria-hidden="true" tabindex="-1"></a>Taking this concept of an attention layer further is the interpretable multi-head attention layer. This is just a combination of different scaled dot-product attention layers with the following actions:</span>
<span id="cb98-1832"><a href="#cb98-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1833"><a href="#cb98-1833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>linear transformation of the query, keys and values before being passed to each attention layer,</span>
<span id="cb98-1834"><a href="#cb98-1834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1835"><a href="#cb98-1835" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>concatenation and a linear combination of the results of the layer.</span>
<span id="cb98-1836"><a href="#cb98-1836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1837"><a href="#cb98-1837" aria-hidden="true" tabindex="-1"></a>In effect, this allows the attention layers to attend to different aspects of the time series. For example, one attention head might focus on seasonality issues, while another on non-linearities and jumps in volatility.</span>
<span id="cb98-1838"><a href="#cb98-1838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1839"><a href="#cb98-1839" aria-hidden="true" tabindex="-1"></a>Note that there is no way to know in advance what aspects each head will be focusing on. However, this can be inspected (and subjectively interpreted) ex post.</span>
<span id="cb98-1840"><a href="#cb98-1840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1843"><a href="#cb98-1843" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1844"><a href="#cb98-1844" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class interpretablemultiheadattention</span></span>
<span id="cb98-1845"><a href="#cb98-1845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1846"><a href="#cb98-1846" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InterpretableMultiHeadAttention(keras.Layer):</span>
<span id="cb98-1847"><a href="#cb98-1847" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-1848"><a href="#cb98-1848" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-1849"><a href="#cb98-1849" aria-hidden="true" tabindex="-1"></a>        n_head:<span class="bu">int</span><span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb98-1850"><a href="#cb98-1850" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb98-1851"><a href="#cb98-1851" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.0</span>, <span class="co"># Will be ignored if `training=False`</span></span>
<span id="cb98-1852"><a href="#cb98-1852" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-1853"><a href="#cb98-1853" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1854"><a href="#cb98-1854" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(InterpretableMultiHeadAttention, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-1855"><a href="#cb98-1855" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> n_head</span>
<span id="cb98-1856"><a href="#cb98-1856" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb98-1857"><a href="#cb98-1857" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_qk <span class="op">=</span> <span class="va">self</span>.d_v <span class="op">=</span> d_model <span class="op">//</span> n_head <span class="co"># the original model divides by number of heads</span></span>
<span id="cb98-1858"><a href="#cb98-1858" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-1859"><a href="#cb98-1859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1860"><a href="#cb98-1860" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-1861"><a href="#cb98-1861" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(InterpretableMultiHeadAttention, <span class="va">self</span>).build(input_shape)</span>
<span id="cb98-1862"><a href="#cb98-1862" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-1863"><a href="#cb98-1863" aria-hidden="true" tabindex="-1"></a>        <span class="co"># using the same value layer facilitates interpretability</span></span>
<span id="cb98-1864"><a href="#cb98-1864" aria-hidden="true" tabindex="-1"></a>        vs_layer <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_v, use_bias<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"shared_attn_value"</span>)</span>
<span id="cb98-1865"><a href="#cb98-1865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1866"><a href="#cb98-1866" aria-hidden="true" tabindex="-1"></a>        <span class="co"># creates list of queries, keys and values across heads</span></span>
<span id="cb98-1867"><a href="#cb98-1867" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qs_layers <span class="op">=</span> [keras.layers.Dense(<span class="va">self</span>.d_qk) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head)]</span>
<span id="cb98-1868"><a href="#cb98-1868" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ks_layers <span class="op">=</span> [keras.layers.Dense(<span class="va">self</span>.d_qk) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head)]</span>
<span id="cb98-1869"><a href="#cb98-1869" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vs_layers <span class="op">=</span> [vs_layer <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head)]</span>
<span id="cb98-1870"><a href="#cb98-1870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1871"><a href="#cb98-1871" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> ScaledDotProductAttention(dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate)</span>
<span id="cb98-1872"><a href="#cb98-1872" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_o <span class="op">=</span> keras.layers.Dense(<span class="va">self</span>.d_model, use_bias<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="st">"W_v"</span>) <span class="co"># W_v in Eqs. (14)-(16), output weight matrix to project internal state to the original TFT</span></span>
<span id="cb98-1873"><a href="#cb98-1873" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> keras.layers.Dropout(<span class="va">self</span>.dropout_rate)</span>
<span id="cb98-1874"><a href="#cb98-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1875"><a href="#cb98-1875" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb98-1876"><a href="#cb98-1876" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-1877"><a href="#cb98-1877" aria-hidden="true" tabindex="-1"></a>        q, <span class="co"># Queries, tensor of shape (?, time, d_model)</span></span>
<span id="cb98-1878"><a href="#cb98-1878" aria-hidden="true" tabindex="-1"></a>        k, <span class="co"># Keys, tensor of shape (?, time, d_model)</span></span>
<span id="cb98-1879"><a href="#cb98-1879" aria-hidden="true" tabindex="-1"></a>        v, <span class="co"># Values, tensor of shape (?, time, d_model)</span></span>
<span id="cb98-1880"><a href="#cb98-1880" aria-hidden="true" tabindex="-1"></a>        mask<span class="op">=</span><span class="va">None</span>, <span class="co"># Masking if required (sets Softmax to very large value), tensor of shape (?, time, time)</span></span>
<span id="cb98-1881"><a href="#cb98-1881" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb98-1882"><a href="#cb98-1882" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-1883"><a href="#cb98-1883" aria-hidden="true" tabindex="-1"></a>        heads <span class="op">=</span> []</span>
<span id="cb98-1884"><a href="#cb98-1884" aria-hidden="true" tabindex="-1"></a>        attns <span class="op">=</span> []</span>
<span id="cb98-1885"><a href="#cb98-1885" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_head):</span>
<span id="cb98-1886"><a href="#cb98-1886" aria-hidden="true" tabindex="-1"></a>            qs <span class="op">=</span> <span class="va">self</span>.qs_layers[i](q)</span>
<span id="cb98-1887"><a href="#cb98-1887" aria-hidden="true" tabindex="-1"></a>            ks <span class="op">=</span> <span class="va">self</span>.ks_layers[i](q)</span>
<span id="cb98-1888"><a href="#cb98-1888" aria-hidden="true" tabindex="-1"></a>            vs <span class="op">=</span> <span class="va">self</span>.vs_layers[i](v)</span>
<span id="cb98-1889"><a href="#cb98-1889" aria-hidden="true" tabindex="-1"></a>           </span>
<span id="cb98-1890"><a href="#cb98-1890" aria-hidden="true" tabindex="-1"></a>            head, attn <span class="op">=</span> <span class="va">self</span>.attention(qs, ks, vs, mask, training<span class="op">=</span>training)</span>
<span id="cb98-1891"><a href="#cb98-1891" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> training:</span>
<span id="cb98-1892"><a href="#cb98-1892" aria-hidden="true" tabindex="-1"></a>                head <span class="op">=</span> <span class="va">self</span>.dropout(head)</span>
<span id="cb98-1893"><a href="#cb98-1893" aria-hidden="true" tabindex="-1"></a>            heads.append(head)</span>
<span id="cb98-1894"><a href="#cb98-1894" aria-hidden="true" tabindex="-1"></a>            attns.append(attn)</span>
<span id="cb98-1895"><a href="#cb98-1895" aria-hidden="true" tabindex="-1"></a>        head <span class="op">=</span> keras.ops.stack(heads) <span class="cf">if</span> <span class="va">self</span>.n_head <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> heads[<span class="dv">0</span>]</span>
<span id="cb98-1896"><a href="#cb98-1896" aria-hidden="true" tabindex="-1"></a>        attn <span class="op">=</span> keras.ops.stack(attns)</span>
<span id="cb98-1897"><a href="#cb98-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1898"><a href="#cb98-1898" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> keras.ops.mean(head, axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> <span class="va">self</span>.n_head <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> head <span class="co"># H_tilde</span></span>
<span id="cb98-1899"><a href="#cb98-1899" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.w_o(outputs)</span>
<span id="cb98-1900"><a href="#cb98-1900" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb98-1901"><a href="#cb98-1901" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> <span class="va">self</span>.dropout(outputs)</span>
<span id="cb98-1902"><a href="#cb98-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1903"><a href="#cb98-1903" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs, attn</span>
<span id="cb98-1904"><a href="#cb98-1904" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1905"><a href="#cb98-1905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1908"><a href="#cb98-1908" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1909"><a href="#cb98-1909" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-imha</span></span>
<span id="cb98-1910"><a href="#cb98-1910" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of network with interpretable multi-head attention</span></span>
<span id="cb98-1911"><a href="#cb98-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1912"><a href="#cb98-1912" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb98-1913"><a href="#cb98-1913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1914"><a href="#cb98-1914" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb98-1915"><a href="#cb98-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1916"><a href="#cb98-1916" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb98-1917"><a href="#cb98-1917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1918"><a href="#cb98-1918" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb98-1919"><a href="#cb98-1919" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb98-1920"><a href="#cb98-1920" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb98-1921"><a href="#cb98-1921" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-1922"><a href="#cb98-1922" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb98-1923"><a href="#cb98-1923" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb98-1924"><a href="#cb98-1924" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-1925"><a href="#cb98-1925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1926"><a href="#cb98-1926" aria-hidden="true" tabindex="-1"></a>Attns <span class="op">=</span> []</span>
<span id="cb98-1927"><a href="#cb98-1927" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb98-1928"><a href="#cb98-1928" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb98-1929"><a href="#cb98-1929" aria-hidden="true" tabindex="-1"></a>    attn, attn_weights <span class="op">=</span> InterpretableMultiHeadAttention(name<span class="op">=</span><span class="ss">f"IMHA__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(q<span class="op">=</span>v, k<span class="op">=</span>v, v<span class="op">=</span>v, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb98-1930"><a href="#cb98-1930" aria-hidden="true" tabindex="-1"></a>    Attns.append(attn)</span>
<span id="cb98-1931"><a href="#cb98-1931" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(attn)</span>
<span id="cb98-1932"><a href="#cb98-1932" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb98-1933"><a href="#cb98-1933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1934"><a href="#cb98-1934" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Concatenate(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb98-1935"><a href="#cb98-1935" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb98-1936"><a href="#cb98-1936" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb98-1937"><a href="#cb98-1937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1938"><a href="#cb98-1938" aria-hidden="true" tabindex="-1"></a>nn_imha <span class="op">=</span> keras.Model(</span>
<span id="cb98-1939"><a href="#cb98-1939" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb98-1940"><a href="#cb98-1940" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb98-1941"><a href="#cb98-1941" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"Attention"</span></span>
<span id="cb98-1942"><a href="#cb98-1942" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-1943"><a href="#cb98-1943" aria-hidden="true" tabindex="-1"></a>nn_imha.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-1944"><a href="#cb98-1944" aria-hidden="true" tabindex="-1"></a>nn_imha.summary()</span>
<span id="cb98-1945"><a href="#cb98-1945" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1946"><a href="#cb98-1946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1949"><a href="#cb98-1949" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1950"><a href="#cb98-1950" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_imha</span></span>
<span id="cb98-1951"><a href="#cb98-1951" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the model with multi-frequency interpretable multi-head attention</span></span>
<span id="cb98-1952"><a href="#cb98-1952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1953"><a href="#cb98-1953" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_imha, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1954"><a href="#cb98-1954" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1955"><a href="#cb98-1955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1956"><a href="#cb98-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1959"><a href="#cb98-1959" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1960"><a href="#cb98-1960" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train the lstm with multi-head attention</span></span>
<span id="cb98-1961"><a href="#cb98-1961" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb98-1962"><a href="#cb98-1962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1963"><a href="#cb98-1963" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb98-1964"><a href="#cb98-1964" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb98-1965"><a href="#cb98-1965" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb98-1966"><a href="#cb98-1966" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb98-1967"><a href="#cb98-1967" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb98-1968"><a href="#cb98-1968" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb98-1969"><a href="#cb98-1969" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb98-1970"><a href="#cb98-1970" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb98-1971"><a href="#cb98-1971" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb98-1972"><a href="#cb98-1972" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb98-1973"><a href="#cb98-1973" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb98-1974"><a href="#cb98-1974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1975"><a href="#cb98-1975" aria-hidden="true" tabindex="-1"></a>history_imha <span class="op">=</span> nn_imha.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-1976"><a href="#cb98-1976" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1977"><a href="#cb98-1977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1980"><a href="#cb98-1980" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-1981"><a href="#cb98-1981" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_imha</span></span>
<span id="cb98-1982"><a href="#cb98-1982" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses of the model with interpretable multi-head attention</span></span>
<span id="cb98-1983"><a href="#cb98-1983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1984"><a href="#cb98-1984" aria-hidden="true" tabindex="-1"></a>imha_loss <span class="op">=</span> history_imha.history</span>
<span id="cb98-1985"><a href="#cb98-1985" aria-hidden="true" tabindex="-1"></a>imha_loss[<span class="st">"val_loss_SDPA"</span>] <span class="op">=</span> history_sdpa.history[<span class="st">"val_loss"</span>]</span>
<span id="cb98-1986"><a href="#cb98-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1987"><a href="#cb98-1987" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(imha_loss).plot()</span>
<span id="cb98-1988"><a href="#cb98-1988" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb98-1989"><a href="#cb98-1989" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb98-1990"><a href="#cb98-1990" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb98-1991"><a href="#cb98-1991" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-1992"><a href="#cb98-1992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1993"><a href="#cb98-1993" aria-hidden="true" tabindex="-1"></a><span class="fu">## Complete architecture {#sec-tftmf}</span></span>
<span id="cb98-1994"><a href="#cb98-1994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1995"><a href="#cb98-1995" aria-hidden="true" tabindex="-1"></a>This section uses the following notation, using as much as possible the original TFT paper notation.</span>
<span id="cb98-1996"><a href="#cb98-1996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-1997"><a href="#cb98-1997" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Panel data</span>
<span id="cb98-1998"><a href="#cb98-1998" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>individuals (countries, banks, households, etc) are indexed by $i \in (1, \dots, I)$.</span>
<span id="cb98-1999"><a href="#cb98-1999" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>time steps are indexed by $t \in (t0, t1, \dots, T)$</span>
<span id="cb98-2000"><a href="#cb98-2000" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input variables</span>
<span id="cb98-2001"><a href="#cb98-2001" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>static variables: $s_i \in \mathbb{R}^{m_s}$, for $m_s$ the number of static variables for each individual $i$</span>
<span id="cb98-2002"><a href="#cb98-2002" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>time-dependent variables: $\chi_{i,t} \in \mathbb{R}^{m_{\chi}}$, for $m_{\chi}$ the number of time-dependent variable</span>
<span id="cb98-2003"><a href="#cb98-2003" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model dimensionality</span>
<span id="cb98-2004"><a href="#cb98-2004" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$k(f)$ is the number of lags for each frequency</span>
<span id="cb98-2005"><a href="#cb98-2005" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$\tau_{\text{max}}$ is the number of steps ahead that is forecasted for the dependent variable $y$</span>
<span id="cb98-2006"><a href="#cb98-2006" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$N$ is the number of time steps that feed into the attention layer</span>
<span id="cb98-2007"><a href="#cb98-2007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2010"><a href="#cb98-2010" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-2011"><a href="#cb98-2011" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: class tft</span></span>
<span id="cb98-2012"><a href="#cb98-2012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2013"><a href="#cb98-2013" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TFT(keras.Model):</span>
<span id="cb98-2014"><a href="#cb98-2014" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb98-2015"><a href="#cb98-2015" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-2016"><a href="#cb98-2016" aria-hidden="true" tabindex="-1"></a>        quantiles<span class="op">=</span>[<span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>],</span>
<span id="cb98-2017"><a href="#cb98-2017" aria-hidden="true" tabindex="-1"></a>        d_model:<span class="bu">int</span><span class="op">=</span><span class="dv">16</span>, <span class="co"># Embedding size, $d_\text{model}$</span></span>
<span id="cb98-2018"><a href="#cb98-2018" aria-hidden="true" tabindex="-1"></a>        output_size:<span class="bu">int</span><span class="op">=</span><span class="dv">1</span>, <span class="co"># How many periods to nowcast/forecast?</span></span>
<span id="cb98-2019"><a href="#cb98-2019" aria-hidden="true" tabindex="-1"></a>        n_head:<span class="bu">int</span><span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb98-2020"><a href="#cb98-2020" aria-hidden="true" tabindex="-1"></a>        dropout_rate:<span class="bu">float</span><span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb98-2021"><a href="#cb98-2021" aria-hidden="true" tabindex="-1"></a>        skip_attention:<span class="bu">bool</span><span class="op">=</span><span class="va">False</span>, <span class="co"># Build a partial TFT without attention</span></span>
<span id="cb98-2022"><a href="#cb98-2022" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>kwargs</span>
<span id="cb98-2023"><a href="#cb98-2023" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-2024"><a href="#cb98-2024" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TFT, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb98-2025"><a href="#cb98-2025" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quantiles <span class="op">=</span> quantiles</span>
<span id="cb98-2026"><a href="#cb98-2026" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb98-2027"><a href="#cb98-2027" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_size <span class="op">=</span> output_size</span>
<span id="cb98-2028"><a href="#cb98-2028" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> n_head</span>
<span id="cb98-2029"><a href="#cb98-2029" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb98-2030"><a href="#cb98-2030" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.skip_attention <span class="op">=</span> skip_attention</span>
<span id="cb98-2031"><a href="#cb98-2031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2032"><a href="#cb98-2032" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb98-2033"><a href="#cb98-2033" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(TFT, <span class="va">self</span>).build(input_shape)</span>
<span id="cb98-2034"><a href="#cb98-2034" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-2035"><a href="#cb98-2035" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_layer <span class="op">=</span> InputTFT(</span>
<span id="cb98-2036"><a href="#cb98-2036" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2037"><a href="#cb98-2037" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"input"</span></span>
<span id="cb98-2038"><a href="#cb98-2038" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2039"><a href="#cb98-2039" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.svars <span class="op">=</span> StaticVariableSelection(</span>
<span id="cb98-2040"><a href="#cb98-2040" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2041"><a href="#cb98-2041" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2042"><a href="#cb98-2042" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_variable_selection"</span></span>
<span id="cb98-2043"><a href="#cb98-2043" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2044"><a href="#cb98-2044" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tvars_hist <span class="op">=</span> TemporalVariableSelection(</span>
<span id="cb98-2045"><a href="#cb98-2045" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2046"><a href="#cb98-2046" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2047"><a href="#cb98-2047" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"historical_variable_selection"</span></span>
<span id="cb98-2048"><a href="#cb98-2048" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2049"><a href="#cb98-2049" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.tvars_fut <span class="op">=</span> TemporalVariableSelection(</span>
<span id="cb98-2050"><a href="#cb98-2050" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2051"><a href="#cb98-2051" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2052"><a href="#cb98-2052" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"future_variable_selection"</span></span>
<span id="cb98-2053"><a href="#cb98-2053" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2054"><a href="#cb98-2054" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_s_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-2055"><a href="#cb98-2055" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2056"><a href="#cb98-2056" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2057"><a href="#cb98-2057" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb98-2058"><a href="#cb98-2058" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_variable_selection"</span></span>
<span id="cb98-2059"><a href="#cb98-2059" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2060"><a href="#cb98-2060" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_h_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-2061"><a href="#cb98-2061" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2062"><a href="#cb98-2062" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2063"><a href="#cb98-2063" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb98-2064"><a href="#cb98-2064" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_LSTM_state_h"</span></span>
<span id="cb98-2065"><a href="#cb98-2065" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2066"><a href="#cb98-2066" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_c_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-2067"><a href="#cb98-2067" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2068"><a href="#cb98-2068" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2069"><a href="#cb98-2069" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb98-2070"><a href="#cb98-2070" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_LSTM_state_c"</span></span>
<span id="cb98-2071"><a href="#cb98-2071" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2072"><a href="#cb98-2072" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_e_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-2073"><a href="#cb98-2073" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2074"><a href="#cb98-2074" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2075"><a href="#cb98-2075" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb98-2076"><a href="#cb98-2076" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_for_enrichment_of"</span></span>
<span id="cb98-2077"><a href="#cb98-2077" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2078"><a href="#cb98-2078" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.temporal_features <span class="op">=</span> TemporalFeatures(</span>
<span id="cb98-2079"><a href="#cb98-2079" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2080"><a href="#cb98-2080" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2081"><a href="#cb98-2081" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"LSTM_encoder"</span></span>
<span id="cb98-2082"><a href="#cb98-2082" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2083"><a href="#cb98-2083" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.static_context_enrichment <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-2084"><a href="#cb98-2084" aria-hidden="true" tabindex="-1"></a>            d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2085"><a href="#cb98-2085" aria-hidden="true" tabindex="-1"></a>            dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2086"><a href="#cb98-2086" aria-hidden="true" tabindex="-1"></a>            use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-2087"><a href="#cb98-2087" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"static_context_enrichment"</span></span>
<span id="cb98-2088"><a href="#cb98-2088" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2089"><a href="#cb98-2089" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.skip_attention:</span>
<span id="cb98-2090"><a href="#cb98-2090" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention <span class="op">=</span> InterpretableMultiHeadAttention(</span>
<span id="cb98-2091"><a href="#cb98-2091" aria-hidden="true" tabindex="-1"></a>                n_head<span class="op">=</span><span class="va">self</span>.n_head,</span>
<span id="cb98-2092"><a href="#cb98-2092" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2093"><a href="#cb98-2093" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2094"><a href="#cb98-2094" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"attention_heads"</span></span>
<span id="cb98-2095"><a href="#cb98-2095" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-2096"><a href="#cb98-2096" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention_gating <span class="op">=</span> GatedLinearUnit(</span>
<span id="cb98-2097"><a href="#cb98-2097" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2098"><a href="#cb98-2098" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2099"><a href="#cb98-2099" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-2100"><a href="#cb98-2100" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb98-2101"><a href="#cb98-2101" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"attention_gating"</span></span>
<span id="cb98-2102"><a href="#cb98-2102" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-2103"><a href="#cb98-2103" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attn_grn <span class="op">=</span> GatedResidualNetwork(</span>
<span id="cb98-2104"><a href="#cb98-2104" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2105"><a href="#cb98-2105" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2106"><a href="#cb98-2106" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-2107"><a href="#cb98-2107" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"output_nonlinear_processing"</span></span>
<span id="cb98-2108"><a href="#cb98-2108" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-2109"><a href="#cb98-2109" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.final_skip <span class="op">=</span> GatedLinearUnit(</span>
<span id="cb98-2110"><a href="#cb98-2110" aria-hidden="true" tabindex="-1"></a>                d_model<span class="op">=</span><span class="va">self</span>.d_model,</span>
<span id="cb98-2111"><a href="#cb98-2111" aria-hidden="true" tabindex="-1"></a>                dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2112"><a href="#cb98-2112" aria-hidden="true" tabindex="-1"></a>                use_time_distributed<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb98-2113"><a href="#cb98-2113" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb98-2114"><a href="#cb98-2114" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"final_skip_connection"</span></span>
<span id="cb98-2115"><a href="#cb98-2115" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-2116"><a href="#cb98-2116" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.add <span class="op">=</span> keras.layers.Add()</span>
<span id="cb98-2117"><a href="#cb98-2117" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.l_norm <span class="op">=</span> keras.layers.LayerNormalization()</span>
<span id="cb98-2118"><a href="#cb98-2118" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-2119"><a href="#cb98-2119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flat <span class="op">=</span> keras.layers.Flatten(name<span class="op">=</span><span class="st">"flatten"</span>)</span>
<span id="cb98-2120"><a href="#cb98-2120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2121"><a href="#cb98-2121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2122"><a href="#cb98-2122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layers:</span></span>
<span id="cb98-2123"><a href="#cb98-2123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In order to enforce monotonicity of the quantiles, forecast only the lowest quantile</span></span>
<span id="cb98-2124"><a href="#cb98-2124" aria-hidden="true" tabindex="-1"></a>        <span class="co"># from a base forecast layer, and use output_len - 1 additional layers with ReLU activation</span></span>
<span id="cb98-2125"><a href="#cb98-2125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># to produce the difference between the current quantile and the previous one</span></span>
<span id="cb98-2126"><a href="#cb98-2126" aria-hidden="true" tabindex="-1"></a>        output_len <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.quantiles)</span>
<span id="cb98-2127"><a href="#cb98-2127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2128"><a href="#cb98-2128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.base_output_layer <span class="op">=</span> keras.layers.TimeDistributed(</span>
<span id="cb98-2129"><a href="#cb98-2129" aria-hidden="true" tabindex="-1"></a>            keras.layers.Dense(<span class="dv">1</span>),</span>
<span id="cb98-2130"><a href="#cb98-2130" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"output"</span></span>
<span id="cb98-2131"><a href="#cb98-2131" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2132"><a href="#cb98-2132" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> elu_plus(x):</span>
<span id="cb98-2133"><a href="#cb98-2133" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> keras.activations.elu(x) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb98-2134"><a href="#cb98-2134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2135"><a href="#cb98-2135" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quantile_diff_layers <span class="op">=</span> [</span>
<span id="cb98-2136"><a href="#cb98-2136" aria-hidden="true" tabindex="-1"></a>            keras.layers.TimeDistributed(</span>
<span id="cb98-2137"><a href="#cb98-2137" aria-hidden="true" tabindex="-1"></a>                keras.layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span>elu_plus),</span>
<span id="cb98-2138"><a href="#cb98-2138" aria-hidden="true" tabindex="-1"></a>                name<span class="op">=</span><span class="ss">f"quantile_diff_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-2139"><a href="#cb98-2139" aria-hidden="true" tabindex="-1"></a>            ) </span>
<span id="cb98-2140"><a href="#cb98-2140" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(output_len <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb98-2141"><a href="#cb98-2141" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb98-2142"><a href="#cb98-2142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2143"><a href="#cb98-2143" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb98-2144"><a href="#cb98-2144" aria-hidden="true" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb98-2145"><a href="#cb98-2145" aria-hidden="true" tabindex="-1"></a>        config.update(</span>
<span id="cb98-2146"><a href="#cb98-2146" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb98-2147"><a href="#cb98-2147" aria-hidden="true" tabindex="-1"></a>                <span class="st">"quantiles"</span>: <span class="va">self</span>.quantiles,</span>
<span id="cb98-2148"><a href="#cb98-2148" aria-hidden="true" tabindex="-1"></a>                <span class="st">"d_model"</span>: <span class="va">self</span>.d_model,</span>
<span id="cb98-2149"><a href="#cb98-2149" aria-hidden="true" tabindex="-1"></a>                <span class="st">"output_size"</span>: <span class="va">self</span>.output_size,</span>
<span id="cb98-2150"><a href="#cb98-2150" aria-hidden="true" tabindex="-1"></a>                <span class="st">"n_head"</span>: <span class="va">self</span>.n_head,</span>
<span id="cb98-2151"><a href="#cb98-2151" aria-hidden="true" tabindex="-1"></a>                <span class="st">"dropout_rate"</span> :<span class="va">self</span>.dropout_rate,</span>
<span id="cb98-2152"><a href="#cb98-2152" aria-hidden="true" tabindex="-1"></a>                <span class="st">"skip_attention"</span>:  <span class="va">self</span>.skip_attention,</span>
<span id="cb98-2153"><a href="#cb98-2153" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb98-2154"><a href="#cb98-2154" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2155"><a href="#cb98-2155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> config</span>
<span id="cb98-2156"><a href="#cb98-2156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb98-2157"><a href="#cb98-2157" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(</span>
<span id="cb98-2158"><a href="#cb98-2158" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb98-2159"><a href="#cb98-2159" aria-hidden="true" tabindex="-1"></a>        inputs,</span>
<span id="cb98-2160"><a href="#cb98-2160" aria-hidden="true" tabindex="-1"></a>        training<span class="op">=</span><span class="va">None</span></span>
<span id="cb98-2161"><a href="#cb98-2161" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb98-2162"><a href="#cb98-2162" aria-hidden="true" tabindex="-1"></a>        <span class="co">"Creates the model architecture"</span></span>
<span id="cb98-2163"><a href="#cb98-2163" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-2164"><a href="#cb98-2164" aria-hidden="true" tabindex="-1"></a>        <span class="co"># embedding the inputs</span></span>
<span id="cb98-2165"><a href="#cb98-2165" aria-hidden="true" tabindex="-1"></a>        cont_hist, cat_hist, cat_fut, cat_stat <span class="op">=</span> inputs</span>
<span id="cb98-2166"><a href="#cb98-2166" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(cat_stat.shape) <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb98-2167"><a href="#cb98-2167" aria-hidden="true" tabindex="-1"></a>            cat_stat <span class="op">=</span> keras.ops.expand_dims(cat_stat, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb98-2168"><a href="#cb98-2168" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb98-2169"><a href="#cb98-2169" aria-hidden="true" tabindex="-1"></a>        xi_hist, xi_fut, xi_stat <span class="op">=</span> <span class="va">self</span>.input_layer([cont_hist, cat_hist, cat_fut, cat_stat])</span>
<span id="cb98-2170"><a href="#cb98-2170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2171"><a href="#cb98-2171" aria-hidden="true" tabindex="-1"></a>        <span class="co"># selecting the static covariates</span></span>
<span id="cb98-2172"><a href="#cb98-2172" aria-hidden="true" tabindex="-1"></a>        static_selected_vars, static_selection_weights <span class="op">=</span> <span class="va">self</span>.svars(xi_stat, training<span class="op">=</span>training)</span>
<span id="cb98-2173"><a href="#cb98-2173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2174"><a href="#cb98-2174" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create context vectors from static data</span></span>
<span id="cb98-2175"><a href="#cb98-2175" aria-hidden="true" tabindex="-1"></a>        c_s, _ <span class="op">=</span> <span class="va">self</span>.static_context_s_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for variable selection</span></span>
<span id="cb98-2176"><a href="#cb98-2176" aria-hidden="true" tabindex="-1"></a>        c_h, _ <span class="op">=</span> <span class="va">self</span>.static_context_h_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for LSTM state h</span></span>
<span id="cb98-2177"><a href="#cb98-2177" aria-hidden="true" tabindex="-1"></a>        c_c, _ <span class="op">=</span> <span class="va">self</span>.static_context_c_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for LSTM state c</span></span>
<span id="cb98-2178"><a href="#cb98-2178" aria-hidden="true" tabindex="-1"></a>        c_e, _ <span class="op">=</span> <span class="va">self</span>.static_context_e_grn(static_selected_vars, training<span class="op">=</span>training) <span class="co"># for context enrichment of post-LSTM features</span></span>
<span id="cb98-2179"><a href="#cb98-2179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2180"><a href="#cb98-2180" aria-hidden="true" tabindex="-1"></a>        <span class="co"># temporal variable selection</span></span>
<span id="cb98-2181"><a href="#cb98-2181" aria-hidden="true" tabindex="-1"></a>        hist_selected_vars, hist_selection_weights <span class="op">=</span> <span class="va">self</span>.tvars_hist(</span>
<span id="cb98-2182"><a href="#cb98-2182" aria-hidden="true" tabindex="-1"></a>            [xi_hist, c_s],</span>
<span id="cb98-2183"><a href="#cb98-2183" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb98-2184"><a href="#cb98-2184" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2185"><a href="#cb98-2185" aria-hidden="true" tabindex="-1"></a>        fut_selected_vars, fut_selection_weights <span class="op">=</span> <span class="va">self</span>.tvars_fut(</span>
<span id="cb98-2186"><a href="#cb98-2186" aria-hidden="true" tabindex="-1"></a>            [xi_fut, c_s],</span>
<span id="cb98-2187"><a href="#cb98-2187" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb98-2188"><a href="#cb98-2188" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2189"><a href="#cb98-2189" aria-hidden="true" tabindex="-1"></a>        input_embeddings <span class="op">=</span> keras.ops.concatenate(</span>
<span id="cb98-2190"><a href="#cb98-2190" aria-hidden="true" tabindex="-1"></a>            [hist_selected_vars, fut_selected_vars],</span>
<span id="cb98-2191"><a href="#cb98-2191" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb98-2192"><a href="#cb98-2192" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2193"><a href="#cb98-2193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2194"><a href="#cb98-2194" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.temporal_features(</span>
<span id="cb98-2195"><a href="#cb98-2195" aria-hidden="true" tabindex="-1"></a>            [hist_selected_vars, fut_selected_vars, c_h, c_c],</span>
<span id="cb98-2196"><a href="#cb98-2196" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb98-2197"><a href="#cb98-2197" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2198"><a href="#cb98-2198" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-2199"><a href="#cb98-2199" aria-hidden="true" tabindex="-1"></a>        <span class="co"># static context enrichment</span></span>
<span id="cb98-2200"><a href="#cb98-2200" aria-hidden="true" tabindex="-1"></a>        enriched, _ <span class="op">=</span> <span class="va">self</span>.static_context_enrichment(</span>
<span id="cb98-2201"><a href="#cb98-2201" aria-hidden="true" tabindex="-1"></a>            features, </span>
<span id="cb98-2202"><a href="#cb98-2202" aria-hidden="true" tabindex="-1"></a>            additional_context<span class="op">=</span>keras.ops.expand_dims(c_e, axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb98-2203"><a href="#cb98-2203" aria-hidden="true" tabindex="-1"></a>            training<span class="op">=</span>training</span>
<span id="cb98-2204"><a href="#cb98-2204" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb98-2205"><a href="#cb98-2205" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.skip_attention:</span>
<span id="cb98-2206"><a href="#cb98-2206" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> get_decoder_mask(enriched)</span>
<span id="cb98-2207"><a href="#cb98-2207" aria-hidden="true" tabindex="-1"></a>            attn_output, self_attn <span class="op">=</span> <span class="va">self</span>.attention(</span>
<span id="cb98-2208"><a href="#cb98-2208" aria-hidden="true" tabindex="-1"></a>                q<span class="op">=</span>enriched,</span>
<span id="cb98-2209"><a href="#cb98-2209" aria-hidden="true" tabindex="-1"></a>                k<span class="op">=</span>enriched,</span>
<span id="cb98-2210"><a href="#cb98-2210" aria-hidden="true" tabindex="-1"></a>                v<span class="op">=</span>enriched,</span>
<span id="cb98-2211"><a href="#cb98-2211" aria-hidden="true" tabindex="-1"></a>                mask<span class="op">=</span>mask,</span>
<span id="cb98-2212"><a href="#cb98-2212" aria-hidden="true" tabindex="-1"></a>                training<span class="op">=</span>training</span>
<span id="cb98-2213"><a href="#cb98-2213" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb98-2214"><a href="#cb98-2214" aria-hidden="true" tabindex="-1"></a>            attn_output, _ <span class="op">=</span> <span class="va">self</span>.attention_gating(attn_output)</span>
<span id="cb98-2215"><a href="#cb98-2215" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.add([enriched, attn_output])</span>
<span id="cb98-2216"><a href="#cb98-2216" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.l_norm(output)</span>
<span id="cb98-2217"><a href="#cb98-2217" aria-hidden="true" tabindex="-1"></a>            output, _ <span class="op">=</span> <span class="va">self</span>.attn_grn(output)</span>
<span id="cb98-2218"><a href="#cb98-2218" aria-hidden="true" tabindex="-1"></a>            output, _ <span class="op">=</span> <span class="va">self</span>.final_skip(output)</span>
<span id="cb98-2219"><a href="#cb98-2219" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.add([features, output])</span>
<span id="cb98-2220"><a href="#cb98-2220" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb98-2221"><a href="#cb98-2221" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> enriched</span>
<span id="cb98-2222"><a href="#cb98-2222" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.l_norm(output)</span>
<span id="cb98-2223"><a href="#cb98-2223" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-2224"><a href="#cb98-2224" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Base quantile output</span></span>
<span id="cb98-2225"><a href="#cb98-2225" aria-hidden="true" tabindex="-1"></a>        base_output <span class="op">=</span> output[<span class="va">Ellipsis</span>,hist_selected_vars.shape[<span class="dv">1</span>]:,:]</span>
<span id="cb98-2226"><a href="#cb98-2226" aria-hidden="true" tabindex="-1"></a>        base_quantile <span class="op">=</span> <span class="va">self</span>.base_output_layer(base_output)</span>
<span id="cb98-2227"><a href="#cb98-2227" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb98-2228"><a href="#cb98-2228" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Additional layers for remaining quantiles</span></span>
<span id="cb98-2229"><a href="#cb98-2229" aria-hidden="true" tabindex="-1"></a>        quantile_outputs <span class="op">=</span> [base_quantile]</span>
<span id="cb98-2230"><a href="#cb98-2230" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.quantiles) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb98-2231"><a href="#cb98-2231" aria-hidden="true" tabindex="-1"></a>            quantile_diff <span class="op">=</span> <span class="va">self</span>.quantile_diff_layers[i](base_output)</span>
<span id="cb98-2232"><a href="#cb98-2232" aria-hidden="true" tabindex="-1"></a>            quantile_output <span class="op">=</span> quantile_outputs[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> quantile_diff</span>
<span id="cb98-2233"><a href="#cb98-2233" aria-hidden="true" tabindex="-1"></a>            quantile_outputs.append(quantile_output)</span>
<span id="cb98-2234"><a href="#cb98-2234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2235"><a href="#cb98-2235" aria-hidden="true" tabindex="-1"></a>        final_output <span class="op">=</span> keras.ops.concatenate(quantile_outputs, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb98-2236"><a href="#cb98-2236" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb98-2237"><a href="#cb98-2237" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> final_output</span>
<span id="cb98-2238"><a href="#cb98-2238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-2239"><a href="#cb98-2239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2242"><a href="#cb98-2242" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb98-2243"><a href="#cb98-2243" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-summary_tft</span></span>
<span id="cb98-2244"><a href="#cb98-2244" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of TFT Model</span></span>
<span id="cb98-2245"><a href="#cb98-2245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2246"><a href="#cb98-2246" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb98-2247"><a href="#cb98-2247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2248"><a href="#cb98-2248" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>]</span>
<span id="cb98-2249"><a href="#cb98-2249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2250"><a href="#cb98-2250" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb98-2251"><a href="#cb98-2251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2252"><a href="#cb98-2252" aria-hidden="true" tabindex="-1"></a>encoded_inputs <span class="op">=</span> {</span>
<span id="cb98-2253"><a href="#cb98-2253" aria-hidden="true" tabindex="-1"></a>    k: keras.layers.TimeDistributed(</span>
<span id="cb98-2254"><a href="#cb98-2254" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(dim),</span>
<span id="cb98-2255"><a href="#cb98-2255" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="ss">f"encoding__</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb98-2256"><a href="#cb98-2256" aria-hidden="true" tabindex="-1"></a>    )(v)</span>
<span id="cb98-2257"><a href="#cb98-2257" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k, v <span class="kw">in</span> inputs.items()</span>
<span id="cb98-2258"><a href="#cb98-2258" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb98-2259"><a href="#cb98-2259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2260"><a href="#cb98-2260" aria-hidden="true" tabindex="-1"></a>Attns <span class="op">=</span> []</span>
<span id="cb98-2261"><a href="#cb98-2261" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb98-2262"><a href="#cb98-2262" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> encoded_inputs.items():</span>
<span id="cb98-2263"><a href="#cb98-2263" aria-hidden="true" tabindex="-1"></a>    attn, attn_weights <span class="op">=</span> ScaledDotProductAttention(name<span class="op">=</span><span class="ss">f"Attention__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(q<span class="op">=</span>v, k<span class="op">=</span>v, v<span class="op">=</span>v, mask<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb98-2264"><a href="#cb98-2264" aria-hidden="true" tabindex="-1"></a>    Attns.append(attn)</span>
<span id="cb98-2265"><a href="#cb98-2265" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(attn)</span>
<span id="cb98-2266"><a href="#cb98-2266" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb98-2267"><a href="#cb98-2267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2268"><a href="#cb98-2268" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Concatenate(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb98-2269"><a href="#cb98-2269" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb98-2270"><a href="#cb98-2270" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb98-2271"><a href="#cb98-2271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2272"><a href="#cb98-2272" aria-hidden="true" tabindex="-1"></a>nn_sdpa <span class="op">=</span> keras.Model(</span>
<span id="cb98-2273"><a href="#cb98-2273" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb98-2274"><a href="#cb98-2274" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb98-2275"><a href="#cb98-2275" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"Attention"</span></span>
<span id="cb98-2276"><a href="#cb98-2276" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-2277"><a href="#cb98-2277" aria-hidden="true" tabindex="-1"></a>nn_sdpa.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb98-2278"><a href="#cb98-2278" aria-hidden="true" tabindex="-1"></a>nn_sdpa.summary()</span>
<span id="cb98-2279"><a href="#cb98-2279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2280"><a href="#cb98-2280" aria-hidden="true" tabindex="-1"></a>tft <span class="op">=</span> TFT()</span>
<span id="cb98-2281"><a href="#cb98-2281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb98-2282"><a href="#cb98-2282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2283"><a href="#cb98-2283" aria-hidden="true" tabindex="-1"></a><span class="fu">## Nowcasting inflation with a simple model {#sec-nowcast}</span></span>
<span id="cb98-2284"><a href="#cb98-2284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2285"><a href="#cb98-2285" aria-hidden="true" tabindex="-1"></a>It all boils down to this: using past data, coupled with higher frequency current data, to nowcast current values. Continuing with the simplistic example in this page, now it is time to nowcast monthly inflation from its lags and from daily changes in oil prices.</span>
<span id="cb98-2286"><a href="#cb98-2286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2287"><a href="#cb98-2287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2288"><a href="#cb98-2288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2289"><a href="#cb98-2289" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb98-2290"><a href="#cb98-2290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-2291"><a href="#cb98-2291" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb98-2292"><a href="#cb98-2292" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bis-med-it/gingado/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>