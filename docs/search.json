[
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Data augmentation",
    "section": "",
    "text": "gingado provides data augmentation functionalities that can help users to augment their datasets with a time series dimension. This can be done both on a stand-alone basis as the user incorporates new data on top of the original dataset, or as part of a scikit-learn Pipeline that also includes other steps like data transformation and model estimation."
  },
  {
    "objectID": "augmentation.html#compatibility-with-scikit-learn",
    "href": "augmentation.html#compatibility-with-scikit-learn",
    "title": "Data augmentation",
    "section": "Compatibility with scikit-learn",
    "text": "Compatibility with scikit-learn\nAs mentioned above, gingado’s transformers are built to be compatible with scikit-learn. The code below demonstrates this compatibility.\nFirst, we create the example dataset. In this case, it comprises the daily foreign exchange rate of selected currencies to the Euro. The Brazilian Real (BRL) is chosen for this example as the dependent variable.\n\nfrom gingado.utils import load_SDMX_data, Lag\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\nX = load_SDMX_data(\n    sources={'ECB': 'EXR'}, \n    keys={'FREQ': 'D', 'CURRENCY': ['EUR', 'AUD', 'BRL', 'CAD', 'CHF', 'GBP', 'JPY', 'SGD', 'USD']},\n    params={\"startPeriod\": 2003}\n    )\n# drop rows with empty values\nX.dropna(inplace=True)\n# adjust column names in this simple example for ease of understanding:\n# remove parts related to source and dataflow names\nX.columns = X.columns.str.replace(\"ECB__EXR_D__\", \"\").str.replace(\"__EUR__SP00__A\", \"\")\nX = Lag(lags=1, jump=0, keep_contemporaneous_X=True).fit_transform(X)\ny = X.pop('BRL')\n# retain only the lagged variables in the X variable\nX = X[X.columns[X.columns.str.contains('_lag_')]]\n\nQuerying data from ECB's dataflow 'EXR' - Exchange Rates...\n\n\n\nX_train, X_test = X.iloc[:-1], X.tail(1)\ny_train, y_test = y.iloc[:-1], y.tail(1)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n((5414, 8), (5414,), (1, 8), (1,))\n\n\nNext, the data augmentation object provided by gingado adds more data. In this case, for brevity only one dataflow from one source is listed. If users want to add more SDMX sources, simply add more keys to the dictionary. And if users want data from all dataflows from a given source provided the keys and parameters such as frequency and dates match, the value should be set to 'all', as in {'ECB': ['CISS'], 'BIS': 'all'}.\n\ntest_src = {'ECB': ['CISS'], 'BIS': ['WS_CBPOL_D']}\n\nX_train__fit_transform = AugmentSDMX(sources=test_src).fit_transform(X=X_train)\nX_train__fit_then_transform = AugmentSDMX(sources=test_src).fit(X=X_train).transform(X=X_train, training=True)\n\nassert X_train__fit_transform.shape == X_train__fit_then_transform.shape\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\n\n\nThis is the dataset now after this particular augmentation:\n\nprint(f\"No of columns: {len(X_train__fit_transform.columns)} {X_train__fit_transform.columns}\")\nX_train__fit_transform\n\nNo of columns: 69 Index(['AUD_lag_1', 'BRL_lag_1', 'CAD_lag_1', 'CHF_lag_1', 'GBP_lag_1',\n       'JPY_lag_1', 'SGD_lag_1', 'USD_lag_1',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX', 'BIS__WS_CBPOL_D_D__CH',\n       'BIS__WS_CBPOL_D_D__CL', 'BIS__WS_CBPOL_D_D__CN',\n       'BIS__WS_CBPOL_D_D__CO', 'BIS__WS_CBPOL_D_D__CZ',\n       'BIS__WS_CBPOL_D_D__DK', 'BIS__WS_CBPOL_D_D__GB',\n       'BIS__WS_CBPOL_D_D__HK', 'BIS__WS_CBPOL_D_D__HR',\n       'BIS__WS_CBPOL_D_D__HU', 'BIS__WS_CBPOL_D_D__ID',\n       'BIS__WS_CBPOL_D_D__IL', 'BIS__WS_CBPOL_D_D__IN',\n       'BIS__WS_CBPOL_D_D__IS', 'BIS__WS_CBPOL_D_D__JP',\n       'BIS__WS_CBPOL_D_D__AR', 'BIS__WS_CBPOL_D_D__KR',\n       'BIS__WS_CBPOL_D_D__MA', 'BIS__WS_CBPOL_D_D__MK',\n       'BIS__WS_CBPOL_D_D__MX', 'BIS__WS_CBPOL_D_D__BR',\n       'BIS__WS_CBPOL_D_D__MY', 'BIS__WS_CBPOL_D_D__NO',\n       'BIS__WS_CBPOL_D_D__NZ', 'BIS__WS_CBPOL_D_D__PE',\n       'BIS__WS_CBPOL_D_D__PH', 'BIS__WS_CBPOL_D_D__CA',\n       'BIS__WS_CBPOL_D_D__PL', 'BIS__WS_CBPOL_D_D__AU',\n       'BIS__WS_CBPOL_D_D__RO', 'BIS__WS_CBPOL_D_D__RS',\n       'BIS__WS_CBPOL_D_D__RU', 'BIS__WS_CBPOL_D_D__SA',\n       'BIS__WS_CBPOL_D_D__SE', 'BIS__WS_CBPOL_D_D__TH',\n       'BIS__WS_CBPOL_D_D__TR', 'BIS__WS_CBPOL_D_D__US',\n       'BIS__WS_CBPOL_D_D__XM', 'BIS__WS_CBPOL_D_D__ZA'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nAUD_lag_1\nBRL_lag_1\nCAD_lag_1\nCHF_lag_1\nGBP_lag_1\nJPY_lag_1\nSGD_lag_1\nUSD_lag_1\nECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX\n...\nBIS__WS_CBPOL_D_D__RO\nBIS__WS_CBPOL_D_D__RS\nBIS__WS_CBPOL_D_D__RU\nBIS__WS_CBPOL_D_D__SA\nBIS__WS_CBPOL_D_D__SE\nBIS__WS_CBPOL_D_D__TH\nBIS__WS_CBPOL_D_D__TR\nBIS__WS_CBPOL_D_D__US\nBIS__WS_CBPOL_D_D__XM\nBIS__WS_CBPOL_D_D__ZA\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-01-03\n1.8554\n3.6770\n1.6422\n1.4528\n0.65200\n124.40\n1.8188\n1.0446\n0.021899\n0.043292\n...\nNaN\n9.5\nNaN\nNaN\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-06\n1.8440\n3.6112\n1.6264\n1.4555\n0.65000\n124.56\n1.8132\n1.0392\n0.020801\n0.039924\n...\n19.75\n9.5\nNaN\n2.0\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-07\n1.8281\n3.5145\n1.6383\n1.4563\n0.64950\n124.40\n1.8210\n1.0488\n0.019738\n0.038084\n...\n19.75\n9.5\nNaN\n2.0\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-08\n1.8160\n3.5139\n1.6257\n1.4565\n0.64960\n124.82\n1.8155\n1.0425\n0.019947\n0.040338\n...\n19.75\n9.5\n21.0\n2.0\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n2003-01-09\n1.8132\n3.4405\n1.6231\n1.4586\n0.64950\n124.90\n1.8102\n1.0377\n0.017026\n0.040535\n...\n19.75\n9.5\n21.0\n2.0\n3.75\n1.75\n44.0\n1.250\n2.75\n13.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-02-14\n1.6526\n5.3465\n1.4511\n0.9481\n0.85098\n161.17\n1.4510\n1.0793\n0.111582\n0.098751\n...\n7.00\n6.5\n16.0\n6.0\n4.00\n2.50\n45.0\n5.375\n4.50\n8.25\n\n\n2024-02-15\n1.6521\n5.3069\n1.4509\n0.9493\n0.85258\n161.28\n1.4451\n1.0713\n0.102352\n0.094350\n...\n7.00\n6.5\n16.0\n6.0\n4.00\n2.50\n45.0\n5.375\n4.50\n8.25\n\n\n2024-02-16\n1.6525\n5.3401\n1.4547\n0.9484\n0.85635\n161.26\n1.4472\n1.0743\n0.104774\n0.093517\n...\n7.00\n6.5\n16.0\n6.0\n4.00\n2.50\n45.0\n5.375\n4.50\n8.25\n\n\n2024-02-19\n1.6517\n5.3551\n1.4518\n0.9491\n0.85605\n161.88\n1.4500\n1.0768\n0.101870\n0.089907\n...\n7.00\n6.5\n16.0\n6.0\n4.00\n2.50\n45.0\n5.375\n4.50\n8.25\n\n\n2024-02-20\n1.6479\n5.3433\n1.4522\n0.9492\n0.85448\n161.59\n1.4503\n1.0776\n0.102598\n0.087116\n...\n7.00\n6.5\n16.0\n6.0\n4.00\n2.50\n45.0\n5.375\n4.50\n8.25\n\n\n\n\n5414 rows × 69 columns\n\n\n\n\nPipeline\nAugmentSDMX can also be part of a Pipeline object, which minimises operational errors during modelling and avoids using testing data during training:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\npipeline = Pipeline([\n    ('augmentation', AugmentSDMX(sources={'BIS': 'WS_CBPOL_D'})),\n    ('imp', IterativeImputer(max_iter=10)),\n    ('forest', RandomForestRegressor())\n], verbose=True)\n\n\n\nTuning the data augmentation to enhance model performance\nAnd since AugmentSDMX can be included in a Pipeline, it can also be fine-tuned by parameter search techniques (such as grid search), further helping users make the best of available data to enhance performance of their models.\n\n\n\n\n\n\nTip\n\n\n\nUsers can cache the data augmentation step to avoid repeating potentially lengthy data downloads. See the memory argument in the sklearn.pipeline.Pipeline documentation.\n\n\n\ngrid = GridSearchCV(\n    estimator=pipeline,\n    param_grid={\n        'augmentation': ['passthrough', AugmentSDMX(sources={'ECB': 'CISS'})]\n    },\n    verbose=2,\n    cv=TimeSeriesSplit(n_splits=2)\n    )\n\ny_pred_grid = grid.fit(X_train, y_train).predict(X_test)\n\nFitting 2 folds for each of 2 candidates, totalling 4 fits\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.6s\n[CV] END ...........................augmentation=passthrough; total time=   1.6s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   3.7s\n[CV] END ...........................augmentation=passthrough; total time=   3.7s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  25.6s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.5s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   4.8s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time=  46.0s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  30.2s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.6s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=  12.2s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time= 2.0min\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   5.7s\n\n\n\ngrid.best_params_\n\n{'augmentation': 'passthrough'}\n\n\n\nprint(f\"In this particular case, the best model was achieved by {'not ' if grid.best_params_['augmentation'] == 'passthrough' else ''}using the data augmentation.\")\n\nIn this particular case, the best model was achieved by not using the data augmentation.\n\n\n\nprint(f\"The last value in the training dataset was {y_train.tail(1).to_numpy()}. The predicted value was {y_pred_grid}, and the actual value was {y_test.to_numpy()}.\")\n\nThe last value in the training dataset was [5.3521]. The predicted value was [5.341226], and the actual value was [5.3253]."
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Automatic benchmark model",
    "section": "",
    "text": "A Benchmark object has a similar API to a sciki-learn estimator: you build an instance with the desired arguments, and fit it to the data at a later moment. Benchmarks is a convenience wrapper for reading the training data, passing it through a simplified pipeline consisting of data imputation and a standard scalar, and then the benchmark function calibrated with a grid search.\nA gingado Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a compare method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This compare method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators.\nBenchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark’s components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning."
  },
  {
    "objectID": "benchmark.html#scoring",
    "href": "benchmark.html#scoring",
    "title": "Automatic benchmark model",
    "section": "Scoring",
    "text": "Scoring\nClassificationBenchmark and RegressionBenchmark use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with gingado Benchmark objects can be found here."
  },
  {
    "objectID": "benchmark.html#data-split",
    "href": "benchmark.html#data-split",
    "title": "Automatic benchmark model",
    "section": "Data split",
    "text": "Data split\ngingado benchmarks rely on hyperparameter tuning to discover the benchmark specification that is most likely to perform better with the user data. This tuning in turn depends on a data splitting strategy for the cross-validation. By default, gingado uses StratifiedShuffleSplit (in classification problems) or ShuffleSplit (in regression problems) if the data is not time series and TimeSeriesSplit otherwise.\nThe user may overrun these defaults either by directly setting the parameter cv or default_cv when instanciating the gingado benchmark class. The difference is that default_cv is only used after gingado checks that the data is not a time series (if a time dimension exists, then TimeSeriesSplit is used).\n\nX, y = make_classification()\nbm_cls = ClassificationBenchmark(cv=TimeSeriesSplit(n_splits=3)).fit(X, y)\nassert bm_cls.benchmark.n_splits_ == 3\n\nX, y = make_regression()\nbm_reg = RegressionBenchmark(default_cv=ShuffleSplit(n_splits=7)).fit(X, y)\nassert bm_reg.benchmark.n_splits_ == 7\n\nPlease refer to this page for more information on the different Splitter classes available on scikit-learn, and this page for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a Benchmark object.\nUsers that wish to use specific parameters should include the actual Splitter object as the parameter, as done with the n_splits parameter in the chunk above."
  },
  {
    "objectID": "benchmark.html#custom-benchmarks",
    "href": "benchmark.html#custom-benchmarks",
    "title": "Automatic benchmark model",
    "section": "Custom benchmarks",
    "text": "Custom benchmarks\ngingado provides users with two Benchmark objects out of the box: ClassificationBenchmark and RegressionBenchmark, to be used depending on the task at hand. Both classes derive from a base class ggdBenchmark, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n\nthe simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a Benchmark object.\nif the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from ggdBenchmark and either implements custom fit, predict and score methods, or also subclasses from scikit-learn’s BaseEstimator.\n\nIn any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the fit method should call self._fit on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended)."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets for economic research",
    "section": "",
    "text": "load_BarroLee_1994 (return_tuple: bool = True)\n\nLoads the dataset used in R. Barro and J.-W. Lee's \"Sources of Economic Growth\" (1994).\n\nArgs:\n    return_tuple (bool):  Whether to return the data in a tuple or jointly in a single pandas\n                          DataFrame.\n\nReturns:\n    pandas.DataFrame or tuple: If `return_tuple` is True, returns a tuple of (X, y), where `X` is a\n    DataFrame of independent variables and `y` is a Series of the dependent variable. If False,\n    returns a single DataFrame with both independent and dependent variables.\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n&lt;Axes: title={'center': 'GDP growth'}, ylabel='Frequency'&gt;"
  },
  {
    "objectID": "datasets.html#real-datasets",
    "href": "datasets.html#real-datasets",
    "title": "Datasets for economic research",
    "section": "",
    "text": "load_BarroLee_1994 (return_tuple: bool = True)\n\nLoads the dataset used in R. Barro and J.-W. Lee's \"Sources of Economic Growth\" (1994).\n\nArgs:\n    return_tuple (bool):  Whether to return the data in a tuple or jointly in a single pandas\n                          DataFrame.\n\nReturns:\n    pandas.DataFrame or tuple: If `return_tuple` is True, returns a tuple of (X, y), where `X` is a\n    DataFrame of independent variables and `y` is a Series of the dependent variable. If False,\n    returns a single DataFrame with both independent and dependent variables.\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n&lt;Axes: title={'center': 'GDP growth'}, ylabel='Frequency'&gt;"
  },
  {
    "objectID": "datasets.html#simulated-datasets",
    "href": "datasets.html#simulated-datasets",
    "title": "Datasets for economic research",
    "section": "Simulated datasets",
    "text": "Simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nAll of the functions creating simulated datasets have a parameter random_state that allow for reproducible random numbers.\n\n\n\nfrom gingado.datasets import make_causal_effect\n\n\nmake_causal_effect\n\nmake_causal_effect (n_samples: int = 100, n_features: int = 100, pretreatment_outcome=&lt;function &lt;lambda&gt; at 0x00000266AB180040&gt;, treatment_propensity=&lt;function &lt;lambda&gt; at 0x00000266B2F9D1F0&gt;, treatment_assignment=&lt;function &lt;lambda&gt; at 0x00000266B2F9D160&gt;, treatment=&lt;function &lt;lambda&gt; at 0x00000266B2F9D280&gt;, treatment_effect=&lt;function &lt;lambda&gt; at 0x00000266B2F9D310&gt;, bias: float = 0, noise: float = 0, random_state=None, return_propensity: bool = False, return_assignment: bool = False, return_treatment_value: bool = False, return_treatment_effect: bool = True, return_pretreatment_y: bool = False, return_as_dict: bool = False)\n\nGenerates a simulated dataset to analyze causal effects of a treatment on an outcome variable.\n\nArgs:\n    n_samples (int): Number of observations in the dataset.\n    n_features (int): Number of covariates for each observation.\n    pretreatment_outcome (function): Function to generate outcome variable before any treatment.\n    treatment_propensity (function or float): Function to generate treatment propensity or a fixed value for each observation.\n    treatment_assignment (function): Function to determine treatment assignment based on propensity.\n    treatment (function): Function to determine the magnitude of treatment for each treated observation.\n    treatment_effect (function): Function to calculate the effect of treatment on the outcome variable.\n    bias (float): Constant value added to the outcome variable.\n    noise (float): Standard deviation of the noise added to the outcome variable. If 0, no noise is added.\n    random_state (int, RandomState instance, or None): Seed or numpy random state instance for reproducibility.\n    return_propensity (bool): If True, returns the treatment propensity for each observation.\n    return_assignment (bool): If True, returns the treatment assignment status for each observation.\n    return_treatment_value (bool): If True, returns the treatment value for each observation.\n    return_treatment_effect (bool): If True, returns the treatment effect for each observation.\n    return_pretreatment_y (bool): If True, returns the outcome variable of each observation before treatment effect.\n    return_as_dict (bool): If True, returns the results as a dictionary; otherwise, returns as a list.\n    \nReturns:\n    A dictionary or list containing the simulated dataset components specified by the return flags.\nmake_causal_effect creates a dataset for when the question of interest is related to the causal effects of a treatment. For example, for a simulated dataset, we can check that \\(Y_i\\) corresponds to the sum of the treatment effects plus the component that does not depend on the treatment:\n\n causal_sim = make_causal_effect(\n    n_samples=2000,\n    n_features=100,\n    return_propensity=True,\n    return_treatment_effect=True, \n    return_pretreatment_y=True, \n    return_as_dict=True)\n\n assert not np.any(np.round(causal_sim['y'] - causal_sim['pretreatment_y'] - causal_sim['treatment_effect'], decimals=13))\n\n\nPre-treatment outcome\nThe pre-treatment outcome \\(Y_i|X_i\\) (the part of the outcome variable that is not dependent on the treatment) might be defined by the user. This corresponds to the value of the outcome for any untreated observations. The function should always take at least two arguments: X and bias, even if one of them is unused; bias is the constant. The argument is zero by default but can be set by the user to be another value.\n\ncausal_sim = make_causal_effect(\n    bias=0.123,\n    pretreatment_outcome=lambda X, bias: bias,\n    return_assignment=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim['y'][causal_sim['treatment_assignment'] == 0] == 0.123)\n\nIf the outcome depends on specific columns of \\(X\\), this can be implemented as shown below.\n\ncausal_sim = make_causal_effect(\n    pretreatment_outcome=lambda X, bias: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias\n)\n\nAnd of course, the outcome might also have a random component.\nIn these cases (and in other parts of this function), when the user wants to use the same random number generator as the other parts of the function, the function must have an argment rng for the NumPy random number generator used in other parts of the function.\n\ncausal_sim_1 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\ncausal_sim_2 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim_1['X'].reshape(-1, 1) == causal_sim_2['X'].reshape(-1, 1))\nassert all(causal_sim_1['y'] == causal_sim_2['y'])\nassert all(causal_sim_1['pretreatment_y'] == causal_sim_2['pretreatment_y'])\n\n\n\nTreatment propensity\nThe treatment propensity of observations may all be the same, in which case treatment_propensity is a floating number between 0 and 1.\n\nsame_propensity_sim = make_causal_effect(\n    n_samples=485,\n    treatment_propensity=0.3,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nassert np.unique(same_propensity_sim['propensity']) == 0.3\nassert len(same_propensity_sim['propensity']) == 485\n\nOr it might depend on the observation’s covariates, with the user passing a function with an argument ‘X’.\n\nheterogenous_propensities_sim = make_causal_effect(\n    n_samples=1000,\n    treatment_propensity=lambda X: 0.3 + (X[:, 0] &gt; 0) * 0.2,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Heterogenously distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(heterogenous_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\nThe propensity can also be randomly allocated, together with covariate dependence or not. Note that even if the propensity is completely random and does not depend on covariates, the function must still use the argument X to calculate a random vector with the appropriate size.\n\nrandom_propensities_sim = make_causal_effect(\n    n_samples=50000,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Randomly distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(random_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\n\n\nTreatment assignment\nAs seen above, every observation has a given treatment propensity - the chance that they are treated. Users can define how this propensity translates into actual treatment with the argument treatment_assignment. This argument takes a function, which must have an argument called propensity.\nThe default value for this argument is a function returning 1s with probability propensity and 0s otherwise. Any other function should always return either 0s or 1s for the data simulator to work as expected.\n\ncausal_sim = make_causal_effect(\n    treatment_assignment=lambda propensity: np.random.binomial(1, propensity)\n)\n\nWhile the case above is likely to be the most useful in practice, this argument accepts more complex relationships between an observation’s propensity and the actual treatment assignment.\nFor example, if treatment is subject to rationing, then one could simulate data with 10 observations where only the samples with the highest (say, 3) propensity scores get treated, as below:\n\nrationed_treatment_sim = make_causal_effect(\n    n_samples=10,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    treatment_assignment=lambda propensity: propensity &gt;= propensity[np.argsort(propensity)][-3],\n    return_propensity=True,\n    return_assignment=True,\n    return_as_dict=True\n)\n\n\nrationed_treatment = pd.DataFrame(\n    np.column_stack((rationed_treatment_sim['propensity'], rationed_treatment_sim['treatment_assignment'])),\n    columns = ['propensity', 'assignment']\n    )\n\n\nrationed_treatment.sort_values('propensity')\n\n\n\n\n\n\n\n\npropensity\nassignment\n\n\n\n\n4\n0.025481\n0.0\n\n\n6\n0.025927\n0.0\n\n\n0\n0.237511\n0.0\n\n\n5\n0.257700\n0.0\n\n\n1\n0.424010\n0.0\n\n\n9\n0.431970\n0.0\n\n\n3\n0.465772\n0.0\n\n\n2\n0.560694\n1.0\n\n\n7\n0.568330\n1.0\n\n\n8\n0.948788\n1.0\n\n\n\n\n\n\n\n\n\nTreatment value\nThe treatment argument indicates the magnitude of the treatment for each observation assigned for treatment. Its value is always a function that must have an argument called assignment, as in the first example below.\nIn the simplest case, the treatment is a binary variable indicating whether or not a variable was treated. In other words, the treatment is the same as the assignment, as in the default value.\nBut users can also simulate data with heterogenous treatment, conditional on assignment. This is done by including a pararemeter X in the function, as shown in the second example below.\n\nbinary_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment: assignment,\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nassert sum(binary_treatment_sim['treatment_assignment'] - binary_treatment_sim['treatment_value'][0]) == 0\n\nHeterogenous treatments may occur in settings where treatment intensity, conditional on assignment, varies across observations. Please note the following:\n\nthe heterogenous treatment amount may or may not depend on covariates, but either way, if treatment values are heterogenous, then X needs to be an argument of the function passed to treatment, if nothing else to make sure the shapes match; and\nif treatments are heterogenous, then it is important to multiply the treatment value with the assignment argument to ensure that observations that are not assigned to be treated are indeed not treated (the function will return an AssertionError otherwise).\n\n\nhetereogenous_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment, X: assignment * np.random.uniform(size=X.shape[0]),\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nIn contrast to the function above, in the chunk below the function make_causal_effect fails because a treatment value is also assigned to observations that were not assigned for treatment.\n\ntest_fail(\n    make_causal_effect, \n    kwargs=dict(treatment=lambda assignment, X: assignment + np.random.uniform(size=X.shape[0]))\n)\n\n\n\nTreatment effect\nThe treatment effect can be homogenous, ie, is doesn’t depend on any other characteristic of the individual observations (in other words, does not depend on \\(X_i\\)), or heterogenous (where the treatment effect on \\(Y_i\\) does depend on each observation’s \\(X_i\\)). This can be done by specifying the causal relationship through a lambda function, as below:\n\nhomogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value: treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (homogenous_effects_sim['treatment_effect'] == homogenous_effects_sim['treatment_value']).all()\n\nheterogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value, X: np.maximum(X[:, 1], 0) * treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (heterogenous_effects_sim['treatment_effect'] != heterogenous_effects_sim['treatment_value']).any()"
  },
  {
    "objectID": "datasets.html#references",
    "href": "datasets.html#references",
    "title": "Datasets for economic research",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Model documentation",
    "section": "",
    "text": "Each user has a specific documentation need, ranging from simply logging the model training to a more complex description of the model pipeline with a discusson of the model outcomes. gingado addresses this variety of needs by offering a class of objects, “Documenters”, that facilitate model documentation. A base class facilitates the creation of generic ways to document models, and gingado includes two specific model documentation templates off-the-shelf as described below.\nThe model documentation is performed by Documenters, objects that subclass from the base class ggdModelDocumentation. This base class offers code that can be used by any Documenter to read the model in question, format the information according to a template and save the resulting documentation in a JSON format. Documenters save the underlying information using the JSON format. With the JSON documentation file at hand, the user can then deploy existing third-party libraries to transform the information stored in JSON into a variety of formats (eg, HTML, PDF) as needed.\nOne current area of development is the automatic filing of some fields related to the model. The objective is to automatise documentation of the information that can be fetched automatically from the model, leaving time for the analyst to concentrate on other tasks, such as considering the ethical implications of the machine learning model being trained."
  },
  {
    "objectID": "documentation.html#modelcard",
    "href": "documentation.html#modelcard",
    "title": "Model documentation",
    "section": "ModelCard",
    "text": "ModelCard\nModelCard - the model documentation template inspired by the work of Mitchell et al. (2018) already comes with gingado. Its template can be used by users as is, or tweaked according to each need. The ModelCard template can also serve as inspiration for any custom documentation needs. Users with documentation needs beyond the out-of-the-box solutions provided by gingado can create their own class of Documenters (more information on that below), and compatibility with these custom documentation routines with the rest of the code is ensured. Users are encouraged to submit a pull request with their own documentation models subclassing ggdModelDocumentation if these custom templates can also benefit other users.\nLike all gingado Documenters, a ModelCard is can be easily created on a standalone basis as shown below, or as part of a gingado.ggdBenchmark object.\n\nmodel_doc = ModelCard()\n\nBy default, it autofills the template with the current date and time. Users can add other information to be automatically added by a customised Documenter object.\n\nmodel_doc_with_autofill = ModelCard(autofill=True)\nmodel_doc_no_autofill = ModelCard(autofill=False)\n\nBelow is a comparison of the model_details section of the model document, with and without the autofill.\n\nmodel_doc_with_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': '2024-02-22 14:53:19 ',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nmodel_doc_no_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': 'Model date',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nModelCard\n\nModelCard (file_path: 'str' = '', autofill: 'bool' = True, indent_level: 'int | None' = 2)\n\nA gingado Documenter based on @ModelCards\n\nautofill_template\n\nautofill_template (self)\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#forecastcard",
    "href": "documentation.html#forecastcard",
    "title": "Model documentation",
    "section": "ForecastCard",
    "text": "ForecastCard\nForecastCard is a model documentation template inspired by Mitchell et al. (2018), but with fields that are more specifically targeted towards forecasting or nowcasting use cases.\nBecause a ForecastCard Documenter object is targeted to forecasting and nowcasting models, it contains some specialised fields, as illustrated below.\n\nmodel_doc = ForecastCard()\n\nmodel_doc.show_template()\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"variable\": \"Variable(s) being forecasted or nowcasted\",\n    \"jurisdiction\": \"Jurisdiction(s) of the variable being forecasted or nowcasted\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"pipeline\": \"Description of the pipeline steps being used\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"econometric_model\": \"Information about the econometric model or technique\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"estimation_approaches\": \"How are the evaluation metrics calculated? Include information on the cross-validation approach, if used\"\n  },\n  \"data\": {\n    \"field_description\": \"Details on the dataset(s) used for the training and evaluation of the model\",\n    \"datasets\": \"Datasets\",\n    \"preprocessing\": \"Preprocessing\",\n    \"cutoff_date\": \"Cut-off date that separates training from evaluation data\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"Ethical considerations that went into model development, surfacing ethical challenges and solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"risks_and_harms\": \"What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n\n\nmodel_doc.show_json()\n\n{'model_details': {'variable': 'Variable(s) being forecasted or nowcasted',\n  'jurisdiction': 'Jurisdiction(s) of the variable being forecasted or nowcasted',\n  'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-22 14:53:20 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'pipeline': 'Description of the pipeline steps being used',\n  'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n  'econometric_model': 'Information about the econometric model or technique',\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'estimation_approaches': 'How are the evaluation metrics calculated? Include information on the cross-validation approach, if used'},\n 'data': {'datasets': 'Datasets',\n  'preprocessing': 'Preprocessing',\n  'cutoff_date': 'Cut-off date that separates training from evaluation data'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\n\nForecastCard\n\nForecastCard (file_path: 'str' = '', autofill: 'bool' = True, indent_level: 'int | None' = 2)\n\nA gingado Documenter for forecasting or nowcasting use cases\n\nautofill_template\n\nautofill_template (self)\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#preliminaries",
    "href": "documentation.html#preliminaries",
    "title": "Model documentation",
    "section": "Preliminaries",
    "text": "Preliminaries\nThe mock dataset below is used to construct models using different libraries, to demonstrate how they are read by Documenters.\n\nfrom sklearn.datasets import make_classification\n\n\n# some mock up data\nX, y = make_classification()\n\nX.shape, y.shape\n\n((100, 20), (100,))"
  },
  {
    "objectID": "documentation.html#gingado-benchmark",
    "href": "documentation.html#gingado-benchmark",
    "title": "Model documentation",
    "section": "gingado Benchmark",
    "text": "gingado Benchmark\n\nfrom gingado.benchmark import ClassificationBenchmark\n\n\n# the gingado benchmark\ngingado_clf = ClassificationBenchmark(verbose_grid=1).fit(X, y)\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n\n\n\n# a new instance of ModelCard is created and used to document the model\nmodel_doc_gingado = ModelCard()\nmodel_doc_gingado.read_model(gingado_clf.benchmark)\nprint(model_doc_gingado.show_json()['model_details']['info'])\n\n# but given that gingado Benchmark objects already document the best model at every fit, we can check that they are equal:\nassert model_doc_gingado.show_json()['model_details']['info'] == gingado_clf.model_documentation.show_json()['model_details']['info']\n\n{'_estimator_type': 'classifier', 'best_estimator_': RandomForestClassifier(n_estimators=250, oob_score=True), 'best_index_': 1, 'best_params_': {'max_features': 'sqrt', 'n_estimators': 250}, 'best_score_': 0.8800000000000001, 'classes_': array([0, 1]), 'cv_results_': {'mean_fit_time': array([0.1762069 , 0.45980067, 0.17040405, 0.45701041, 0.19690371,\n       0.49231734]), 'std_fit_time': array([0.0101552 , 0.05892754, 0.01201183, 0.06162824, 0.01183725,\n       0.03546673]), 'mean_score_time': array([0.00539758, 0.01099873, 0.0051007 , 0.01120093, 0.00460131,\n       0.01079695]), 'std_score_time': array([0.00080164, 0.00141299, 0.00083226, 0.00124878, 0.00049118,\n       0.00098192]), 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n             mask=[False, False, False, False, False, False],\n       fill_value='?',\n            dtype=object), 'params': [{'max_features': 'sqrt', 'n_estimators': 100}, {'max_features': 'sqrt', 'n_estimators': 250}, {'max_features': 'log2', 'n_estimators': 100}, {'max_features': 'log2', 'n_estimators': 250}, {'max_features': None, 'n_estimators': 100}, {'max_features': None, 'n_estimators': 250}], 'split0_test_score': array([0.8, 0.8, 0.8, 0.8, 0.7, 0.7]), 'split1_test_score': array([0.9, 0.9, 0.9, 0.9, 0.8, 0.8]), 'split2_test_score': array([0.8, 0.8, 0.8, 0.8, 0.8, 0.8]), 'split3_test_score': array([1. , 1. , 1. , 1. , 1. , 0.9]), 'split4_test_score': array([0.7, 0.7, 0.7, 0.7, 0.8, 0.8]), 'split5_test_score': array([0.9, 0.9, 0.9, 0.9, 0.8, 0.8]), 'split6_test_score': array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9]), 'split7_test_score': array([1., 1., 1., 1., 1., 1.]), 'split8_test_score': array([1., 1., 1., 1., 1., 1.]), 'split9_test_score': array([0.6, 0.8, 0.8, 0.8, 0.8, 0.8]), 'mean_test_score': array([0.86, 0.88, 0.88, 0.88, 0.86, 0.85]), 'std_test_score': array([0.12806248, 0.09797959, 0.09797959, 0.09797959, 0.10198039,\n       0.09219544]), 'rank_test_score': array([4, 1, 1, 1, 4, 6])}, 'multimetric_': False, 'n_features_in_': 20, 'n_splits_': 10, 'refit_time_': 0.4230809211730957, 'scorer_': &lt;sklearn.metrics._scorer._PassthroughScorer object at 0x0000027D4E9FA0A0&gt;}"
  },
  {
    "objectID": "documentation.html#scikit-learn",
    "href": "documentation.html#scikit-learn",
    "title": "Model documentation",
    "section": "scikit-learn",
    "text": "scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nsklearn_clf = RandomForestClassifier().fit(X, y)\n\n\nmodel_doc_sklearn = ModelCard()\nmodel_doc_sklearn.read_model(sklearn_clf)\nprint(model_doc_sklearn.show_json()['model_details']['info'])\n\n{'_estimator_type': 'classifier', 'classes_': array([0, 1]), 'estimator_': DecisionTreeClassifier(), 'estimators_': [DecisionTreeClassifier(max_features='sqrt', random_state=1350714431), DecisionTreeClassifier(max_features='sqrt', random_state=430634535), DecisionTreeClassifier(max_features='sqrt', random_state=218166437), DecisionTreeClassifier(max_features='sqrt', random_state=540713047), DecisionTreeClassifier(max_features='sqrt', random_state=53370647), DecisionTreeClassifier(max_features='sqrt', random_state=1852074463), DecisionTreeClassifier(max_features='sqrt', random_state=1194035741), DecisionTreeClassifier(max_features='sqrt', random_state=797143155), DecisionTreeClassifier(max_features='sqrt', random_state=1027054987), DecisionTreeClassifier(max_features='sqrt', random_state=387811444), DecisionTreeClassifier(max_features='sqrt', random_state=442134494), DecisionTreeClassifier(max_features='sqrt', random_state=1475863423), DecisionTreeClassifier(max_features='sqrt', random_state=1542578600), DecisionTreeClassifier(max_features='sqrt', random_state=1197421293), DecisionTreeClassifier(max_features='sqrt', random_state=1001422141), DecisionTreeClassifier(max_features='sqrt', random_state=1702896346), DecisionTreeClassifier(max_features='sqrt', random_state=1955008506), DecisionTreeClassifier(max_features='sqrt', random_state=1344891007), DecisionTreeClassifier(max_features='sqrt', random_state=1395141149), DecisionTreeClassifier(max_features='sqrt', random_state=608696580), DecisionTreeClassifier(max_features='sqrt', random_state=531608306), DecisionTreeClassifier(max_features='sqrt', random_state=632982477), DecisionTreeClassifier(max_features='sqrt', random_state=1261998838), DecisionTreeClassifier(max_features='sqrt', random_state=42040129), DecisionTreeClassifier(max_features='sqrt', random_state=1982656340), DecisionTreeClassifier(max_features='sqrt', random_state=491494913), DecisionTreeClassifier(max_features='sqrt', random_state=1486351631), DecisionTreeClassifier(max_features='sqrt', random_state=325585563), DecisionTreeClassifier(max_features='sqrt', random_state=506355410), DecisionTreeClassifier(max_features='sqrt', random_state=92514021), DecisionTreeClassifier(max_features='sqrt', random_state=625653897), DecisionTreeClassifier(max_features='sqrt', random_state=428604530), DecisionTreeClassifier(max_features='sqrt', random_state=1111046695), DecisionTreeClassifier(max_features='sqrt', random_state=1683254466), DecisionTreeClassifier(max_features='sqrt', random_state=832647221), DecisionTreeClassifier(max_features='sqrt', random_state=1583536151), DecisionTreeClassifier(max_features='sqrt', random_state=2040969429), DecisionTreeClassifier(max_features='sqrt', random_state=461687375), DecisionTreeClassifier(max_features='sqrt', random_state=490417382), DecisionTreeClassifier(max_features='sqrt', random_state=702218285), DecisionTreeClassifier(max_features='sqrt', random_state=140450015), DecisionTreeClassifier(max_features='sqrt', random_state=1321567040), DecisionTreeClassifier(max_features='sqrt', random_state=215491519), DecisionTreeClassifier(max_features='sqrt', random_state=306551002), DecisionTreeClassifier(max_features='sqrt', random_state=194820889), DecisionTreeClassifier(max_features='sqrt', random_state=1044948431), DecisionTreeClassifier(max_features='sqrt', random_state=531787375), DecisionTreeClassifier(max_features='sqrt', random_state=443115688), DecisionTreeClassifier(max_features='sqrt', random_state=2025577590), DecisionTreeClassifier(max_features='sqrt', random_state=1647089852), DecisionTreeClassifier(max_features='sqrt', random_state=1790579057), DecisionTreeClassifier(max_features='sqrt', random_state=566150597), DecisionTreeClassifier(max_features='sqrt', random_state=493270713), DecisionTreeClassifier(max_features='sqrt', random_state=777097755), DecisionTreeClassifier(max_features='sqrt', random_state=899388107), DecisionTreeClassifier(max_features='sqrt', random_state=1890234104), DecisionTreeClassifier(max_features='sqrt', random_state=1635335653), DecisionTreeClassifier(max_features='sqrt', random_state=1758086609), DecisionTreeClassifier(max_features='sqrt', random_state=1988566232), DecisionTreeClassifier(max_features='sqrt', random_state=1553688749), DecisionTreeClassifier(max_features='sqrt', random_state=1326489971), DecisionTreeClassifier(max_features='sqrt', random_state=468168296), DecisionTreeClassifier(max_features='sqrt', random_state=1083515976), DecisionTreeClassifier(max_features='sqrt', random_state=1463215127), DecisionTreeClassifier(max_features='sqrt', random_state=1187864021), DecisionTreeClassifier(max_features='sqrt', random_state=1171176170), DecisionTreeClassifier(max_features='sqrt', random_state=528795425), DecisionTreeClassifier(max_features='sqrt', random_state=526385551), DecisionTreeClassifier(max_features='sqrt', random_state=691817660), DecisionTreeClassifier(max_features='sqrt', random_state=620780009), DecisionTreeClassifier(max_features='sqrt', random_state=284065499), DecisionTreeClassifier(max_features='sqrt', random_state=642749492), DecisionTreeClassifier(max_features='sqrt', random_state=1203645187), DecisionTreeClassifier(max_features='sqrt', random_state=95781576), DecisionTreeClassifier(max_features='sqrt', random_state=1030647338), DecisionTreeClassifier(max_features='sqrt', random_state=1631535816), DecisionTreeClassifier(max_features='sqrt', random_state=889047269), DecisionTreeClassifier(max_features='sqrt', random_state=1921064597), DecisionTreeClassifier(max_features='sqrt', random_state=67383526), DecisionTreeClassifier(max_features='sqrt', random_state=537753925), DecisionTreeClassifier(max_features='sqrt', random_state=308262463), DecisionTreeClassifier(max_features='sqrt', random_state=304684572), DecisionTreeClassifier(max_features='sqrt', random_state=1351918324), DecisionTreeClassifier(max_features='sqrt', random_state=12640671), DecisionTreeClassifier(max_features='sqrt', random_state=1055065058), DecisionTreeClassifier(max_features='sqrt', random_state=1368763873), DecisionTreeClassifier(max_features='sqrt', random_state=153628578), DecisionTreeClassifier(max_features='sqrt', random_state=2116721383), DecisionTreeClassifier(max_features='sqrt', random_state=1412204620), DecisionTreeClassifier(max_features='sqrt', random_state=1495111840), DecisionTreeClassifier(max_features='sqrt', random_state=185444011), DecisionTreeClassifier(max_features='sqrt', random_state=1914958364), DecisionTreeClassifier(max_features='sqrt', random_state=153694605), DecisionTreeClassifier(max_features='sqrt', random_state=1675736732), DecisionTreeClassifier(max_features='sqrt', random_state=1513251194), DecisionTreeClassifier(max_features='sqrt', random_state=239765532), DecisionTreeClassifier(max_features='sqrt', random_state=34407827), DecisionTreeClassifier(max_features='sqrt', random_state=781986240), DecisionTreeClassifier(max_features='sqrt', random_state=1248372015), DecisionTreeClassifier(max_features='sqrt', random_state=1805870535)], 'estimators_samples_': [array([13, 42, 25,  4, 29, 56, 95, 40, 84, 98, 46, 35, 93, 63, 29,  6, 46,\n       27, 49, 79, 14,  0, 53,  9, 75, 64, 14, 61, 83, 33, 71, 10, 54, 14,\n       86, 69, 38, 62, 35, 96,  5, 61, 87, 44, 55, 83, 30, 81, 51,  3, 68,\n       54, 13, 36, 53, 81, 82, 82, 96, 48, 69,  4, 53,  3, 50, 85, 63, 75,\n       88, 44,  4, 78, 18,  4, 74,  9, 85, 44, 86, 68, 74, 40, 10, 12, 66,\n        6, 21, 23, 89, 80, 18, 39, 30,  8, 93, 84, 40, 67, 68, 57]), array([79, 76, 64,  4, 60, 97, 97, 47, 33, 88, 34, 28, 96, 17, 99, 70, 36,\n       79, 80, 19, 47, 59, 15,  3, 47, 87, 83, 72, 93,  2, 79, 19, 76, 38,\n       35,  5, 54,  3, 50, 93, 88, 25, 65, 35, 55, 95, 19, 76, 86, 46, 23,\n       88, 19, 16, 36, 64,  7, 80, 57,  8, 64, 42,  3, 80, 76,  6, 92, 92,\n        1, 45,  5, 75, 47, 77, 41, 73, 37, 95, 76, 73, 22, 83, 46, 93, 94,\n       76, 84, 39, 65, 92, 21, 82, 81,  8, 72, 43, 32, 80, 71, 73]), array([ 1, 31, 32, 64, 75, 97, 13, 36, 41, 21, 29,  9, 52, 49,  1,  7, 81,\n       91, 98, 70,  0, 75, 64, 99, 66, 38, 86, 48, 12, 40,  2, 64, 86, 70,\n       78,  3, 71, 84, 89, 27, 24, 42, 84, 74, 93, 38, 50, 77, 50, 68, 68,\n       67, 15, 69, 65, 78, 42, 21, 29, 78, 31, 45, 94, 31, 32, 23, 29, 59,\n       61, 55, 87,  3, 40, 74, 86, 68, 68, 75, 84, 80, 10,  9, 54, 74, 47,\n       82, 94, 48, 88, 61, 41, 53, 19, 92, 64, 29, 94, 27, 50, 88]), array([41, 98, 95,  6, 24, 38, 73, 32, 59, 21, 93, 31, 89,  0, 64, 77, 26,\n       57, 88, 23, 42, 33, 13, 67,  9, 82,  5, 53, 81, 79, 85, 19, 99, 90,\n       34, 65, 31, 89, 56,  9, 84, 70, 41, 18, 28, 37, 93, 38, 42, 54, 28,\n        2, 37, 44, 52, 40, 20, 19, 27, 91, 46, 34, 90, 72, 86, 87, 65, 65,\n       73, 50, 83, 64, 66, 56, 95, 10, 76, 81, 37, 93, 40,  3, 80, 79, 12,\n       67, 66, 61, 22, 34, 99, 23, 18,  6, 20, 77, 56, 56, 50, 17]), array([50, 63,  8, 71, 81, 50, 99,  6,  7,  6, 11, 41, 22, 18, 31, 49, 13,\n       72, 99, 75, 62, 36, 72, 73, 35, 51, 78, 44,  4, 19, 88,  2, 50,  8,\n       96, 86, 59, 86,  6, 64, 73, 34, 96, 62, 55, 76, 21, 98, 22, 35, 87,\n        7, 74, 89, 45, 42, 53, 97, 94, 49, 99,  2, 96, 50, 53,  1, 17, 46,\n       13, 78, 16, 61, 49,  0, 69, 62, 89,  8, 60, 17, 65, 97,  1, 27, 68,\n       48,  0, 22, 82, 40, 52, 27, 65, 72, 67, 83, 70, 33, 74, 15]), array([79, 89, 21, 15, 20, 43, 83, 77, 50, 81, 65, 96, 32, 25, 27, 24, 92,\n       97, 63,  5,  4, 85, 25, 70, 49, 74, 90, 63, 23, 88, 95, 32, 73, 50,\n       56, 16, 27, 10, 25, 55, 20, 61, 64, 50, 99, 57, 16, 87, 38, 60, 46,\n       66, 75, 62, 34, 32, 64, 51, 70, 41, 39,  2, 76, 51, 24, 68, 39, 38,\n       37, 97, 75, 92, 68, 83, 33, 39,  2, 11, 79, 50, 92, 36,  3,  1,  3,\n       10, 46, 20, 23,  8, 28, 93, 18, 76, 79, 42, 28, 60,  4, 13]), array([75, 97, 83, 84, 38,  9, 14, 39, 11, 95, 99, 36, 39, 20, 27, 51, 99,\n       56, 45, 35, 47, 43, 95, 60, 96, 85, 45, 16, 32, 24, 96,  3, 56, 66,\n       43, 38,  1, 51, 72, 97, 72, 16, 51, 12, 34, 30, 12, 73, 11, 71, 93,\n       81, 33, 78, 49, 93, 77, 50, 25, 96, 60, 23, 94, 20, 23, 88, 36, 72,\n       85, 40, 16, 66, 28, 30,  5, 18, 67, 12, 92, 39, 71, 25, 10, 91,  8,\n       83, 12, 49, 20, 33, 48, 62, 49, 46, 10, 13, 76, 40, 28, 71]), array([18, 75, 37,  1, 29, 81, 93, 46, 51, 93,  2, 18, 60, 84, 78, 39, 41,\n       64, 84, 77, 96, 91, 96, 98, 64, 53, 82, 87, 16, 85, 74, 83, 48, 60,\n       46, 83, 47, 20, 32, 48, 31, 56, 25, 31, 40, 37, 27, 62, 80, 42, 55,\n       78, 51, 55, 95, 84, 87, 82, 92, 88, 80, 30, 75, 25, 49, 20, 87, 25,\n       55, 34,  8, 45, 91, 49, 94, 90, 70,  2, 42,  3, 11, 23, 82, 25, 29,\n       56, 48, 59, 20, 14, 79,  1, 22,  2, 50, 63, 70, 78, 40,  7]), array([53, 50, 60, 71, 67, 98, 94, 47, 37, 23, 83,  5,  2, 73, 33, 74,  0,\n       39, 46, 29, 14,  6, 26,  8, 77, 96, 48, 21, 41, 63,  9, 33, 46, 87,\n       88, 96, 66, 98, 33, 95, 60, 46, 93, 65, 75,  0, 16, 24, 21,  8, 21,\n       97, 22, 72, 38, 35,  0, 29,  0, 43, 91, 31, 74, 22, 83, 41, 41, 29,\n       85,  8, 99, 72, 65, 18, 51, 89, 80, 87, 82, 98, 40, 83, 71, 37,  8,\n       71, 69, 45, 80, 55, 68, 74, 47,  3, 60, 83, 35, 26, 13, 33]), array([88, 94, 66, 47, 31,  2, 49, 53, 48, 90, 83, 97, 84, 18, 60,  6, 59,\n       84, 33, 38, 28,  1, 72, 30, 58, 64, 83, 27, 17,  1, 88, 11, 12, 46,\n       65, 95, 61, 69, 12, 36, 59, 58, 78,  8, 81, 18, 19, 36, 33, 22, 56,\n       19, 13,  7, 74, 76, 95, 69, 54, 23, 99, 74, 84, 74, 44, 85, 11, 72,\n       24, 16, 86, 93, 84, 39, 14, 75, 75, 29, 28, 11, 68, 49, 33, 47, 58,\n       10, 24, 78, 61, 42, 32, 18, 17, 95, 23,  3, 80, 91, 41, 40]), array([23, 11, 37, 12, 22, 72, 14, 63, 47,  7, 81, 67, 19, 17, 14, 83, 51,\n       97, 49, 13, 96, 50, 26, 99, 32, 49, 80, 44, 67, 28, 76,  8,  5,  9,\n       84, 36, 52, 69, 84, 63, 59, 80, 38, 18, 18, 71, 31, 32, 62, 98, 76,\n        3, 27, 84, 65, 54, 53,  9, 56, 87, 74, 14, 98, 39,  6, 24, 98, 17,\n       63, 17, 25,  0, 87, 50, 82, 44, 42,  4, 64, 51, 67, 85, 11, 14, 38,\n       89, 50, 79, 68, 39, 70, 81, 65, 24, 62, 90, 30, 58, 96, 19]), array([83, 41, 69, 94, 72, 20, 93, 95,  6, 89,  2, 38, 75, 30, 66, 68, 76,\n       89, 89, 41, 57, 31, 61, 12, 66, 72, 12, 39, 18, 90, 39, 55, 37, 76,\n       72, 15, 25, 88, 70, 40, 42, 20, 20, 56, 96, 28, 29, 19, 21, 59, 35,\n       82, 75,  1, 62,  8, 52, 23, 99, 30, 99,  2, 60,  2, 14, 94, 33, 90,\n       74, 14, 92, 14, 15, 65, 38, 98, 69, 66, 46, 15, 50, 47, 91, 21, 61,\n       50, 90, 47, 96, 28,  0, 96, 36, 90,  2, 97, 72, 83, 84, 80]), array([ 4, 21, 98, 84, 87, 21, 93, 38, 85,  5, 98, 79, 79, 37, 57, 28, 46,\n       30, 55, 72,  2, 62,  8, 18,  7, 21, 54, 50, 41, 17, 40, 54, 10, 69,\n       94, 80, 53, 68, 64, 14, 14, 62, 88, 99, 81, 77,  4, 63, 92, 78, 18,\n       26, 77,  3, 59, 63, 19, 61,  6, 80, 37, 29, 16, 78,  9, 47, 72, 15,\n       38, 85,  4, 15, 92, 21,  1, 69, 79, 99, 59, 94, 50, 78, 80, 56, 76,\n       35, 98, 61, 49, 21, 76, 22, 94, 18, 11, 83, 15, 37, 65, 81]), array([27, 22, 54, 18, 79,  2, 31, 19, 63, 90, 14, 29, 21, 61, 14, 25, 19,\n       99, 91, 49, 20, 21, 61, 47, 81, 36, 43, 27, 76, 32, 47, 78, 59, 64,\n       36, 58, 31, 21, 19, 79, 87, 68, 36, 90,  6, 39,  6, 57, 26, 86, 31,\n       51, 34, 30, 56, 91, 13, 25, 32, 53, 54, 63,  8,  9, 72, 41, 52, 69,\n       39, 41, 83,  4, 34,  2, 46, 28, 17, 77, 60, 43, 94, 46, 82, 80,  3,\n       52, 78, 53, 34, 18, 61,  4,  7,  8, 42, 43, 52,  5, 66, 48]), array([48,  0, 26, 92, 84, 41, 24, 67, 83, 58, 65, 63, 40, 34, 81, 35, 58,\n       10, 76,  7, 99, 11, 61, 97, 74, 50, 61, 53, 72, 38, 80, 72, 37, 81,\n       24, 39, 80, 38,  9, 75, 11, 61, 46, 38, 38, 15, 24,  6, 98,  4, 65,\n       18, 39,  5, 54,  3, 36, 19, 26, 65, 16, 16, 60,  0, 50,  0, 43, 64,\n       64, 50, 33, 84,  5, 90, 43, 22, 92, 92, 38, 19, 19, 67, 76, 29,  5,\n       93, 21,  2, 66, 27, 60, 12, 13, 16, 32,  7, 32, 49, 18, 62]), array([94,  4, 71, 87, 68, 58, 17, 97, 44,  5, 37,  5, 10, 80, 64, 88,  9,\n        8, 87, 58, 20, 77, 23, 83, 66, 50, 54, 46, 30, 22, 60, 71, 98, 68,\n       26, 80, 79, 48,  4, 94, 76, 70, 99, 67, 56, 91, 11, 39, 21, 52, 48,\n        0, 91, 90, 18, 36, 84, 79, 59, 63, 97,  9, 63, 70, 94, 93, 97, 26,\n       28, 87,  1, 62, 41, 62,  9, 76, 27, 42, 98, 58, 57, 74, 65, 82, 71,\n       54, 11, 70, 95,  7, 81, 91,  1, 81, 50, 40, 59, 85, 47, 95]), array([28, 47, 69, 76,  2, 58, 61,  2, 21, 97, 49, 43, 13, 11, 35, 83, 91,\n       65, 55, 64, 27, 84, 49, 25, 77,  8, 42, 98,  7,  3, 56, 16, 92, 65,\n       70,  3, 25, 57, 84, 30, 93,  4, 65, 78, 80, 92, 29, 51,  9, 30, 48,\n       30, 55, 63, 96,  4, 58, 81, 78, 87, 22, 83, 56, 21, 58, 55, 95,  1,\n       62, 12, 88, 46, 54, 56, 27, 37, 17, 23, 34, 83, 91, 80, 97, 37, 50,\n       93, 64, 19, 21, 47, 83, 76, 56,  0, 74, 27, 27,  6, 25, 51]), array([33, 15, 86, 25, 55, 35,  3, 52, 13, 29, 53, 48, 74, 45, 33, 41, 80,\n       75,  3, 75, 88, 94, 80,  8, 28, 10, 74, 59, 63, 49, 49, 11, 55, 11,\n       27, 61,  0, 39, 26, 18, 44, 28, 83, 64, 50, 79, 60, 41, 87, 29, 34,\n       41, 58, 61, 48, 66, 44, 56, 25, 87, 47, 39, 39,  6, 55,  4, 10,  3,\n       53, 12, 18, 50,  8, 40, 88, 77, 26, 47, 29, 71, 52, 77, 66, 83, 81,\n       64, 26, 81,  5, 58, 37, 23, 35, 47, 87, 33, 61, 85, 59, 32]), array([23, 74, 46,  6, 33, 35, 45, 73, 97, 97, 71, 27, 12, 28,  4, 87, 43,\n       86, 33, 96, 24, 90, 96,  7, 57, 80, 44, 99, 56, 65, 62, 84, 85, 27,\n        1, 15, 87, 36, 24, 11, 68, 88, 83, 60, 46, 80, 88, 64, 73, 64, 96,\n       43, 17, 90, 67, 99, 54,  4, 95, 57, 84, 99, 77, 79, 82, 69, 92, 48,\n       99, 43, 59, 28, 32, 55, 17, 37, 57, 66, 27, 70, 56, 71, 85, 92, 32,\n       20,  6, 39, 91, 90, 67, 50, 42,  9, 63, 76, 14, 73, 89, 21]), array([24, 53, 74, 59, 57, 91, 65, 76, 40, 38, 26, 83, 77, 22, 23, 99, 17,\n       92, 90, 11, 33, 45, 20,  7,  1, 80, 98,  2, 77, 13, 55, 35, 90, 11,\n       35, 44, 54, 25,  7, 81, 28, 11, 56, 17, 84, 15, 92, 61, 82, 14, 28,\n       73, 10, 16, 24, 42, 45, 50, 44, 75,  7, 55, 21, 59, 36, 96, 60, 18,\n       11, 77, 13, 49,  1, 49,  6, 33, 74, 76, 77, 27, 45, 72, 77, 51, 63,\n       21, 38, 75, 69, 15, 93,  9, 39, 61, 42, 81, 32,  9,  3, 93]), array([18, 46,  2, 58, 44, 16, 98, 99,  6, 51, 28, 52, 55, 80, 85, 28, 48,\n       60, 27, 91,  1, 33, 60, 26, 37,  8, 94, 51, 36,  5, 45, 44, 23, 58,\n       36, 16, 73, 73,  8, 15, 67, 91,  7, 18, 11, 13, 13, 27, 33, 28, 66,\n        7, 39, 76, 10, 43, 85, 41, 90, 47, 32, 86, 54,  7, 97, 49, 93, 97,\n       38, 11, 74, 65, 73, 94, 54, 23, 90, 24, 43, 43, 57, 90, 25, 34, 56,\n       62, 64, 24, 58, 12, 14, 29, 45, 54, 77,  1,  1, 87, 34, 59]), array([78, 93, 37, 92, 57, 35, 92, 25, 37, 35, 99, 80, 33, 35, 23, 46, 62,\n       53,  2, 33, 74, 75, 30, 26, 12, 24, 10, 45, 32, 93, 63, 13, 21, 71,\n       65, 71, 45, 44, 32, 29, 91, 11, 95, 94, 25,  0, 92, 96, 21, 94, 83,\n       34,  4, 41, 34, 39, 48, 24, 15,  8, 47, 70, 61, 13,  3, 85, 51, 21,\n       65, 76, 75, 47, 77,  6, 93, 70, 80,  7, 37, 64, 52,  3, 19, 45,  5,\n       96, 20, 40, 77, 38, 82, 24, 53, 30, 34, 51, 75, 39, 34, 18]), array([80, 63, 57, 16, 81, 86, 58, 79,  7, 64,  5, 43, 65, 59, 89, 62, 61,\n       48, 33,  4, 41, 85, 57, 23, 53, 18, 58, 67, 22, 31, 45, 93, 10, 49,\n       56, 98, 47, 14, 40, 44, 23, 64, 67,  6, 62, 29, 74, 49, 24, 60, 64,\n       50, 22, 80, 55, 89, 36, 65, 59, 40, 12, 57, 37, 16, 83, 33, 70, 59,\n       75, 62, 32, 48, 17, 29, 47,  6, 41, 66, 70, 46,  4,  4, 90, 69, 14,\n       47, 20, 44, 83, 24, 16, 33, 89, 54, 51, 58, 26, 12, 86, 35]), array([72, 70, 85, 83, 51, 35, 68,  9,  3, 90, 90, 69, 53, 22,  4, 30, 10,\n       48,  8, 95, 94, 60, 21, 25, 98, 72, 83,  1, 95, 65, 11, 43, 86, 77,\n       98, 10,  3, 72, 64, 16, 25, 16,  4, 15, 38, 15, 85, 98, 54, 94, 87,\n       23, 44, 20, 44, 68, 56, 49, 11, 82, 13, 92, 54, 87, 59, 36, 28, 39,\n       14, 70, 58, 45, 57, 92, 87, 29, 92, 68, 51, 28, 51, 44, 38, 79, 55,\n       84, 32, 84, 90, 92, 21, 14, 67, 66, 91, 99, 33, 72, 56, 44]), array([75, 57, 97, 37, 29, 95, 49, 61, 68, 36, 98, 65, 65, 65, 12,  4, 70,\n       32,  5, 30,  7, 10, 56, 88, 51, 88, 81, 53, 42, 45, 51, 53, 28, 29,\n       92, 84, 29, 75, 87, 44, 49, 37, 36, 78, 85, 64,  3,  8,  1, 28, 27,\n       54, 60, 32, 79, 85,  1, 57, 46, 62, 71, 82, 30, 34, 73, 84, 87,  0,\n       66, 78, 90, 95, 48, 92,  2, 16, 32, 26, 83, 47, 79, 92, 61, 64,  1,\n       95, 37,  6, 74, 99, 47,  6, 27, 18, 39, 73, 15, 91, 52, 12]), array([11, 69, 68, 88, 97,  6, 68, 64, 30, 13, 92,  9, 50, 57, 32, 73, 99,\n       65, 24, 55, 14,  1, 55, 77, 76, 53, 13,  7, 60, 72, 91, 77, 26, 75,\n       37, 25,  5, 80, 54, 65, 75, 39,  0,  9, 47, 88, 33, 45, 96, 77, 56,\n       35, 46, 22, 28,  4,  4, 42,  8, 97, 55, 77, 44, 68, 95, 99, 74, 66,\n       41, 85, 64,  1, 67, 36, 45,  6, 67, 72, 27, 44, 95, 95, 71, 66,  9,\n       51, 71, 28, 14, 56, 44, 97, 99, 62, 93, 93, 84, 96, 75, 37]), array([89, 72, 63, 48, 81,  9, 53, 42, 32, 43, 95, 23, 60, 77, 40, 29, 42,\n       75, 78,  9, 51, 83, 72, 81, 11, 84, 64, 33, 72, 23, 68, 72, 19, 77,\n       89, 73, 17, 70, 93, 12, 67, 87, 11, 31, 20, 50,  3,  4, 12, 22, 23,\n       64, 97, 90, 36, 19, 21, 59, 85, 31, 41, 15, 79, 93,  8, 24, 27, 20,\n       40,  7, 32, 21, 47, 46, 15, 56, 42, 38, 12, 61,  0, 45, 50, 60, 77,\n       76, 11, 21, 30, 75, 92, 72, 13, 37, 66, 53, 79, 50,  2, 54]), array([39, 58, 57, 29, 32, 88, 19, 98, 25, 98, 40, 30, 25, 12, 99,  0, 32,\n       18,  2, 63, 31, 27, 94, 31, 23,  9, 66, 44, 42, 76, 77, 88, 16, 68,\n       86, 30, 79, 30, 27, 68, 61, 97, 98, 70, 11, 60, 49, 23, 56, 24, 93,\n       74, 86, 80, 46, 89, 87, 85,  1,  7, 71, 34, 10, 92, 86, 81, 99, 54,\n        9, 80, 74, 45, 29, 16, 81, 17, 26, 58, 84, 22, 78, 90, 62, 87, 51,\n       27, 35, 82, 55, 17, 27, 41, 98,  4,  7, 13, 91, 62, 66, 37]), array([85, 78,  4, 35, 34, 88, 40, 47, 98, 93,  8, 73, 48,  3, 95, 57, 96,\n       81,  4, 47, 72, 21, 53, 73, 71, 43, 61, 20, 90, 59, 35, 62, 41, 38,\n       24, 78, 72,  2, 72, 58, 93, 50,  1,  7, 39, 25, 99, 77, 82, 30,  0,\n       59,  8, 32, 61, 85, 67, 20, 56, 86, 23, 45, 52, 55, 55, 16, 80, 29,\n       66, 63, 14, 82, 63, 11, 76, 27, 53,  4, 69, 18, 30,  8, 46, 32, 34,\n       87, 38, 42, 46, 17, 21, 66, 90, 33, 57, 45, 56, 15,  0, 79]), array([ 3, 16, 71, 16,  0, 40,  7, 69, 34, 56, 85, 12, 72, 35,  6, 17, 94,\n       54, 77, 41, 93, 19, 95, 37, 87, 29, 13, 42, 43, 66, 47, 33, 71, 86,\n        0, 66, 62, 50, 74, 70, 27, 10, 61, 83, 17, 33,  2, 18, 42, 37, 87,\n       87, 66, 48, 18, 42, 50, 19, 18,  5, 46,  6, 15,  7, 71, 41, 74, 63,\n       16, 38,  5, 60, 98, 96, 77, 99, 32, 60, 68, 27, 53, 89, 22, 59, 88,\n       98,  7, 44, 41, 38, 95, 37, 55, 52, 27, 37, 63, 91, 62, 24]), array([ 9, 13, 92, 22, 42, 17, 29, 15, 21,  6, 43, 88, 86, 26, 76, 81, 67,\n       94, 91, 95, 81, 94, 17, 29, 97, 82, 81, 25, 48, 21, 29, 29, 19, 78,\n       87, 50, 88, 60, 63, 98, 72, 78, 17, 97, 62, 35, 32, 87, 75, 16, 75,\n        8, 64, 69, 93, 95, 95, 61,  9, 77, 37, 82,  6, 37, 56, 51, 25, 45,\n       63, 99,  0, 90, 79, 34, 92, 40, 18, 89,  3,  5, 39, 54, 12, 70, 83,\n       20, 48,  0,  1, 54, 31, 76,  0, 99, 73, 65, 28, 76, 44, 46]), array([14,  1, 52,  9, 54, 89, 86, 45, 57, 63, 40, 17, 51, 47, 60, 62, 93,\n       29, 74, 14, 65, 76, 42, 37, 65, 77,  1, 83,  3, 14, 64, 31,  7,  3,\n       65, 69, 87,  8, 26, 58, 24, 60, 91, 45, 57, 99,  8, 78,  5, 45,  3,\n       89, 76, 79, 89,  0,  7, 14, 86, 84, 97, 40, 76, 11, 89, 33, 31, 33,\n       54, 66,  4, 79, 46, 19, 37, 90, 46, 65, 97, 61, 88, 86, 19, 73, 63,\n       79, 39, 53, 52, 31, 76, 96, 28,  9, 49, 29, 57, 66, 63, 25]), array([28, 95, 36, 53, 51,  6,  8,  8, 24, 72, 76, 72, 69, 66, 87, 52, 39,\n       85, 43, 99, 45, 92, 93, 66, 78, 77, 14, 38, 65, 25, 17, 99, 58, 19,\n       96, 99, 49, 69, 62, 13, 93,  7, 95, 78, 95, 47, 56,  4, 66, 46, 55,\n       67, 91, 36, 12, 35, 38, 28, 89, 73, 53, 21, 78, 74,  9,  1, 21, 62,\n       17, 32, 77, 90, 13, 52, 56, 70, 94, 94, 17,  9, 41, 89, 99, 49, 47,\n       16, 46, 72, 98, 88, 11, 18, 51, 60, 57, 53, 49, 69, 77, 96]), array([55, 63, 14, 17, 44, 84, 49, 33, 86, 76, 45,  6, 46, 60, 95, 46,  7,\n       55, 28, 78, 56, 50, 75, 63, 60,  7, 12, 91, 84, 16, 38, 51,  1, 63,\n       40, 77, 54, 84, 50, 20, 97, 79,  4, 19, 50, 13, 21, 29, 10,  1, 45,\n       76,  7, 21, 25, 30,  3, 93, 51, 70, 87,  5, 93, 77, 49, 39, 10, 82,\n       40, 35,  2, 73, 72, 46, 21, 14,  9, 91, 85, 44, 56, 18, 95, 25, 21,\n       50, 37, 64, 36, 55, 16, 22, 48, 70, 97, 57, 83,  7, 13, 34]), array([61, 49, 92, 23, 42, 73, 26, 89, 20, 74,  8, 93, 62, 43, 60, 97, 68,\n       42, 72, 32, 57, 78,  4, 46, 90, 99, 79, 66, 70, 20, 17, 64, 16, 80,\n       59, 85,  5, 29, 38, 69, 70, 72, 90, 51, 94, 93, 26, 39, 76, 68, 55,\n        5,  5, 46, 63, 83, 14, 28, 86, 49, 51, 96, 15, 35, 47, 53, 36, 42,\n       87, 51, 31, 45, 65, 24, 19, 81, 47, 24, 10, 78, 89,  5, 51, 24,  2,\n       70, 23,  9, 93, 41, 43, 24, 12, 99, 47, 34, 52, 90, 88, 89]), array([89, 15, 37, 13, 54, 51, 85, 78, 22, 28, 68, 46, 42, 31, 94, 49, 46,\n       28, 30, 26, 76, 33, 74, 65, 92, 55, 70,  1, 52, 25, 28, 14, 51, 47,\n       55,  9, 62,  3, 78, 16, 21, 18, 65,  8, 29, 34, 52, 57, 36, 40, 81,\n       24, 56, 95, 38, 23, 73,  6, 22, 37, 89, 23, 19, 59, 80, 29, 23, 63,\n       65, 18, 53, 14, 78, 14, 67, 69, 86,  9, 58, 97, 27, 28, 21, 81,  8,\n       91, 68, 17, 94, 48, 95, 77, 93, 99, 47, 75, 48,  3, 81,  9]), array([34, 75, 39, 50, 24, 23, 72, 87, 86, 11, 78, 71,  8, 13, 20, 57, 35,\n       89, 89, 34, 28, 48, 16, 55, 91, 70, 59, 93, 35, 89, 50, 61, 64, 73,\n       11, 14, 60, 94, 23, 99, 42, 16, 79, 92, 79,  6, 13,  0, 20, 39,  0,\n       99, 90, 94, 28, 63, 53, 81, 55, 15, 22, 98,  7, 20, 23, 81, 33, 11,\n       69, 86, 63, 81, 25, 36, 14,  5, 68, 49, 99, 47, 43, 69, 65, 17, 61,\n       50, 30, 20, 10, 24,  7, 35, 12, 74, 37, 40,  8, 10,  6, 42]), array([54, 93, 46, 20, 87, 83, 85, 60,  3, 22,  3, 85, 22, 80, 34, 54, 51,\n       18,  0, 67, 87, 55, 38, 61, 48, 75, 30, 30, 96, 87, 76, 43, 67, 14,\n        8, 26, 26,  6, 58, 15, 93, 38, 29,  3,  2, 22, 84, 32, 69, 42, 26,\n       56, 73,  1, 64, 82, 16, 70, 66, 94, 49,  3, 24, 42, 69, 32, 58, 77,\n       66, 78, 60, 50, 31, 66, 62, 17, 37, 22, 68, 95, 83,  7, 87, 15, 51,\n       67, 10, 25, 93,  9, 31, 26, 81, 31, 58, 77, 82, 86, 42, 83]), array([37, 48, 16, 58, 11,  2, 54, 52, 20, 65, 37, 27, 62, 15, 92, 49, 30,\n       69, 55, 31, 61, 48, 72, 95, 74, 87, 95, 35, 25, 81, 73, 96, 66, 54,\n       90, 23, 24, 73,  9, 42, 85, 10, 89, 40, 95, 14, 56, 83, 15, 92, 41,\n        7, 77, 50, 43, 95, 22, 99, 89, 37, 66, 95, 56, 25, 51, 51, 80, 94,\n       12, 84, 12, 92, 73, 45, 25, 79,  4, 61, 82, 66,  7, 36, 61, 40, 49,\n       16, 56, 21, 90,  4, 10, 58,  7, 68, 19, 11, 41, 63, 88, 70]), array([45,  0, 73, 90, 94, 68, 24, 91, 43, 73, 59, 79, 84, 40, 76,  5, 90,\n       34, 45, 55, 44, 81, 81,  9, 83, 64, 24, 34, 71, 67, 81, 89, 20, 84,\n       19, 67, 49,  0, 15, 55, 47, 54, 46, 40, 13, 88,  1, 98, 67, 92, 66,\n       60, 20, 69, 13, 18, 26, 11, 24,  5, 76, 64, 60, 79, 65, 66, 54, 36,\n       86,  5, 66, 99, 69, 83, 96, 70, 21, 60, 51, 99, 23, 82, 37, 21,  8,\n       24, 40, 44, 83, 11, 94, 27, 71, 93, 55, 17, 97, 25, 14, 31]), array([15, 87, 80, 81, 30, 65, 66, 73,  3, 81, 26, 21, 82, 26, 13, 27, 61,\n       19,  7, 85, 37, 38, 41, 80, 43, 71, 11, 23, 24,  7, 78, 95, 62, 83,\n       33, 97,  7, 83, 34, 74, 80,  7, 28, 47, 56, 96, 17, 18, 87, 86, 19,\n       59,  9, 33, 86, 91, 16,  3, 70, 91, 46, 75, 87, 70, 83,  3,  4,  0,\n        3, 98, 92, 26, 45, 43, 98, 65,  2, 23, 47, 10, 46, 22, 84,  4,  8,\n       69, 38, 13, 75, 21, 92, 91, 48, 27, 46, 32, 99, 21,  0, 49]), array([37, 26, 67, 68, 45, 51, 66, 58, 72, 46, 23, 66, 23, 76, 82,  0, 40,\n       39, 65, 66, 15,  6, 67, 37, 92, 61, 11, 12, 50, 35, 13, 96, 65, 61,\n       19, 33, 57, 76, 43, 31, 61, 39, 26, 62, 64, 68, 84,  1, 68,  7, 76,\n       54, 55, 29, 83, 89, 70, 94, 29, 85, 67, 76, 50, 10, 10, 82, 94, 53,\n       15, 98, 30, 18,  3, 21, 15, 31, 10, 27,  3, 79, 13, 58, 68, 73, 71,\n       65, 20, 43, 75, 79, 84, 33, 29, 46, 81, 13, 21, 47, 73, 81]), array([92,  4,  1, 69,  5, 33, 80, 20, 69, 31, 24, 11, 46, 82, 76, 11, 66,\n       12, 46, 13, 48, 23, 53, 16, 75, 40, 16, 94, 42, 92, 20, 39, 64, 97,\n       19,  2, 85,  5, 83, 58, 15, 29, 17, 26, 92,  1, 71, 55, 76, 40, 82,\n       66, 21, 63, 44, 20, 24, 61,  4, 13, 81,  6, 15,  6, 18, 11, 23, 33,\n       14, 46, 42, 88, 51, 81, 93, 15, 34, 80,  1, 18, 99, 99, 75, 23, 64,\n       13, 48, 71,  5, 19, 32, 26, 82, 88,  4, 44, 68, 97,  3, 94]), array([98, 11, 82, 23, 91, 31, 53, 41, 88, 41, 65, 65, 20, 32,  3, 49, 82,\n       62, 71, 21, 19, 19,  1, 77, 50,  4, 94, 90, 48, 21, 25, 64, 60, 61,\n        6, 85, 67, 89, 44, 24, 36, 46, 56, 31, 81, 16, 48, 37, 33, 57, 91,\n       90, 37, 54, 18, 68, 23, 82, 83, 66, 16, 69, 71, 16, 32, 68, 62, 90,\n       95, 18, 91, 80, 66,  3, 93, 57, 34, 69, 44, 89,  7, 60, 99, 11, 18,\n       90, 83, 37, 29, 10, 30, 40, 12, 27, 57, 48,  1, 39, 56, 76]), array([65, 51, 31, 45, 67, 38, 30, 75, 75, 47, 22, 58, 48, 74, 29, 51, 96,\n       23, 58,  9, 53, 40, 48, 33, 31, 13,  0, 12, 90, 86, 35, 37, 44, 21,\n       64, 84, 23, 20, 87, 16, 33, 91, 94,  5, 76, 99, 71,  2,  9, 18, 39,\n       50, 57, 78, 44, 94, 94, 33, 74, 26, 80, 93, 92, 93, 72, 64, 10, 86,\n       76, 69,  6, 60, 91, 93, 70, 61, 38,  4, 93, 27,  9,  2, 27, 60, 23,\n       73,  6, 20, 77,  4, 30, 53, 54, 80, 14, 47, 27,  0, 90, 66]), array([96, 31, 87, 62, 62, 77, 33, 62, 55, 84, 27, 54, 15, 45, 89, 27, 98,\n        6, 82, 65, 44, 22, 67, 33, 93, 51, 72, 79, 67, 38, 16, 78, 74, 12,\n       81, 55,  7, 88,  8,  3, 65, 53, 80, 90, 96, 82, 14, 24, 10, 16, 92,\n       20, 20, 71, 95, 36,  5, 11, 62, 11, 62, 21,  8, 55, 32, 38, 84, 35,\n       45, 93, 40, 77, 38,  9, 93, 22,  5,  4, 95, 16, 16, 44, 35, 64, 85,\n       22, 28, 66, 13, 97,  5, 11, 56, 22, 76, 38, 95, 72, 56, 65]), array([65, 74, 38, 43, 70, 39, 67, 75,  8, 86, 55, 22,  0, 70, 68, 76, 31,\n       42,  7, 19, 90, 56, 58, 35, 81, 36, 96, 91, 94, 57, 10, 38, 62, 14,\n       44,  5, 13, 70, 47, 41, 53, 63, 90, 33,  5, 69, 17, 99, 86, 49, 96,\n       61, 50, 43, 47, 86, 14, 58, 10, 32, 53, 42, 70, 99, 54, 50, 12, 98,\n       61, 46, 92, 37, 16, 44, 50, 97, 57,  8,  0, 39, 17, 10, 41, 17, 21,\n       76, 74, 42, 37, 33, 23, 35, 80, 79, 47, 25, 98, 43, 27, 77]), array([77, 28, 89, 15, 46, 35, 50, 84, 73, 65, 12, 44, 62, 91, 42, 12, 86,\n       16, 25, 20, 97, 56, 56, 61, 61, 85, 16, 16, 23, 68, 33, 36, 79, 43,\n       53, 49, 78, 74, 42, 11, 67, 91, 81, 51, 36, 96, 87, 65, 52, 15, 53,\n       57, 93, 84, 67, 17,  2, 15, 96, 80, 36,  1, 43, 96, 22,  2, 22, 92,\n       54, 88, 29, 39, 35, 27, 38, 93, 61, 55, 38, 94, 37, 35, 11, 81,  6,\n       93, 73, 31, 24, 90, 77,  6, 75, 51, 59, 13, 63, 18, 92, 80]), array([12, 86, 45, 61, 49, 88, 21, 94,  0, 58, 27, 22, 96, 96, 89, 94, 35,\n       72, 96, 15, 96, 89, 14, 48, 64, 52, 29,  2, 19, 65, 63, 27, 80, 82,\n       33, 15, 26, 75, 55, 80, 12, 48, 73, 75,  9, 48,  4, 98, 60, 51, 95,\n       32,  3, 99, 86, 68, 83,  5, 77, 92, 14, 39, 91, 74, 19, 54, 25, 93,\n       38, 37, 41,  8, 97, 10, 85, 92, 62, 10,  2, 77, 24, 54, 27, 52, 20,\n       97, 61, 70,  1, 37, 87, 66, 95, 50, 33, 24, 59, 62, 16, 94]), array([55, 34, 65, 21, 17, 37, 98, 71, 74, 51,  7, 20, 60, 37,  6, 52, 71,\n       93, 48, 94, 84, 64, 32, 93, 36, 93, 23,  8, 24,  5, 83,  7, 17, 48,\n        8, 90, 31, 53, 55,  5, 74, 27, 64, 76, 58, 55, 87, 27, 40,  4,  3,\n       24, 65, 33, 35, 36, 50, 96, 33, 61, 91, 87, 22, 53, 12, 17, 11, 76,\n       69,  7, 33, 41, 55, 56,  7, 77, 49, 10, 93, 70, 43, 82, 25, 17, 44,\n       44, 98, 28,  0, 58, 66, 30, 60, 54, 50, 69, 87, 36, 59, 56]), array([94, 42, 49, 54, 45, 41, 31, 50, 88, 71,  3, 23, 31,  5, 90, 21, 81,\n       40, 58, 54, 52, 35, 87, 56, 28, 24, 75, 71, 10,  5, 90, 23, 37, 34,\n       66, 39, 11, 82,  5, 69, 28, 21, 82, 13, 83, 77, 23, 45, 79, 14, 16,\n       72, 49, 22, 41, 52, 63, 41,  6, 24, 99, 32, 89, 62, 41,  3, 63, 52,\n       42, 87, 40, 65, 49, 83, 10, 46, 49, 66, 63, 39, 41, 87, 94, 80, 54,\n       15, 91, 75, 97, 70, 30, 90, 64, 69, 40, 39, 47, 36, 94, 92]), array([ 2, 48, 56, 45,  2, 38,  5, 23, 29, 48, 68, 35, 47, 40, 11, 16, 73,\n       37, 73, 45, 97, 43, 92, 86, 12, 88, 74, 23, 42,  6, 45, 83, 34, 36,\n       94, 87, 64, 46, 92, 69, 33, 76, 88, 68, 36, 30, 90, 74, 21, 35, 28,\n        6, 78, 77, 64, 69, 60, 95, 74, 75, 51, 76, 46, 75, 74, 55, 90,  2,\n       56, 57, 62, 50, 87, 12, 93, 26, 18, 81, 97, 76, 84, 71,  5,  0, 16,\n       41, 29, 86, 97, 74, 46, 38, 50, 84, 55, 89, 93, 18, 78, 11]), array([49, 65, 73,  3, 43, 86, 26, 82, 40, 84, 99, 52, 34, 49, 53, 60, 40,\n       22, 61,  0, 28, 95, 77, 39, 42, 89, 40, 30, 37,  7, 87, 30, 29, 32,\n       16, 50,  1, 43, 52, 26, 48, 30, 83, 88, 26, 52, 24, 62, 25, 23, 62,\n       43, 64,  3,  1, 20, 78, 68, 36, 91, 96, 78, 85, 13, 35, 57, 26, 40,\n       76, 47, 33, 24, 19, 70, 68, 18, 25, 41, 32, 85, 73,  9, 64, 94, 64,\n       74, 87, 34, 25, 77, 59, 25, 57, 62, 61, 41, 82, 52, 21, 19]), array([74, 91,  6, 88, 26, 64, 29,  4, 27, 10, 73, 84, 42, 55, 41, 44, 76,\n       95, 56, 90, 98, 14, 13, 35, 26, 57, 54, 41, 73, 28,  9, 23, 95, 58,\n       16, 96, 52, 12,  6, 26, 31, 81, 74, 97, 82, 93, 18,  8, 14, 18, 67,\n        9, 15, 14, 32, 10, 20, 87, 71, 97, 91, 10, 15, 82, 52,  1, 82, 68,\n       97, 88, 15, 32, 59, 14, 96,  1, 34, 40, 79, 78, 56, 35, 15, 43,  7,\n       79, 90, 44, 66, 71,  6,  0, 91, 50,  4, 57, 40, 70, 67, 44]), array([ 3, 98, 38, 83,  6, 95, 39, 77, 94, 87, 77, 31, 87,  0, 85, 49, 32,\n        6, 93,  1, 54, 35, 25,  9, 20, 81, 56,  3, 41,  5,  6, 33, 54, 85,\n       21, 56, 60, 23, 45, 38, 14,  0, 10, 63, 10, 66, 60, 28, 12, 67, 95,\n       88, 70, 32, 51, 58, 41, 60, 36, 42, 19, 35, 82, 48, 68, 18, 21, 94,\n       61, 57,  3, 36, 59, 12, 60, 13, 36, 70,  4, 20, 53, 49, 51, 40, 30,\n       68, 39, 21, 32,  0, 91, 46, 96, 61, 76,  8, 18, 51, 23, 73]), array([19, 24,  4, 17, 16, 81, 29, 96,  8, 90, 41, 68, 26, 65, 55, 59,  8,\n       78, 24, 96, 68, 45, 46, 71, 51, 89, 79, 35, 85, 65, 38, 35, 59,  2,\n       97, 39, 80, 11, 51, 83, 55, 71, 53, 31, 79,  3, 76,  0, 74, 78,  6,\n       71,  7, 65, 26, 85, 99, 60, 75, 83, 47,  2, 55, 46, 35, 17, 47, 58,\n       56, 57, 34, 39,  2, 18, 87, 75,  8, 13, 62, 49, 78, 75, 78, 46, 37,\n       15, 91, 63, 37, 39, 43,  0, 48, 98, 71, 51, 71, 47, 99, 51]), array([15, 82, 60, 77, 34,  0, 12, 46, 17, 42, 74, 69, 19, 29, 16,  2, 39,\n       33, 63,  4, 63, 42, 66, 76, 45, 80, 58, 74, 42, 25, 64, 42, 22, 75,\n       34, 44, 55, 39, 43, 60,  2, 55, 52,  2, 67, 22, 19, 31, 80, 66, 61,\n       35, 89, 27, 12, 23, 90, 55, 10, 52, 18, 31, 12, 89, 65, 16, 52, 36,\n       56, 94, 25, 93, 94, 93,  3, 29, 98, 12, 30, 42, 34, 95, 39, 69, 44,\n       19, 30, 56, 69, 33, 74, 89, 66, 11, 18, 65, 13, 28, 15, 72]), array([45, 26, 67, 12, 65, 24, 46, 80, 57, 69, 69, 57, 77, 41,  1, 61, 82,\n       48, 60, 57, 44, 23, 99, 75, 47, 31, 77, 56, 15, 79, 41, 13, 11, 21,\n       97, 65,  5, 17, 27, 31,  2, 48, 65, 15, 75, 44, 41, 45, 33, 39, 31,\n       55, 18, 26, 64, 85, 82, 47, 23, 36, 69, 12, 42, 55, 29, 12, 14, 91,\n       44, 58, 19, 70, 38, 55, 55, 11, 87, 95, 14, 94, 21, 79, 83, 38, 85,\n       55, 78, 81,  7, 54, 69, 95, 48, 36, 43, 23, 82, 42, 77, 76]), array([12,  4, 55, 86, 61, 40, 21, 97, 42, 54, 99, 19, 91, 22, 56, 56, 22,\n       29, 82, 77, 83, 90, 89, 65,  3, 67, 64, 49, 58,  8,  3, 32, 87, 29,\n        5, 36, 36, 67, 67, 94,  0, 29, 68, 61, 53, 28, 23, 75, 89, 96, 31,\n       11, 34, 99, 90, 90, 89, 13, 36, 69,  4, 57, 48, 28, 84,  4, 63, 75,\n       82, 88, 98, 69, 63, 54, 26, 34, 20, 36, 77, 81, 39, 14, 96,  4, 74,\n       26, 70,  1, 68, 41, 97,  0, 91, 73, 88, 65, 20, 80, 98, 53]), array([95, 59, 91, 84,  1, 34, 39, 43, 54, 12, 52, 11, 49, 17, 49, 82, 67,\n       95,  6, 35,  2, 87, 34, 84, 83,  4, 31, 85,  9,  7, 19, 24, 45, 28,\n        8, 25, 66, 91, 36, 50, 81,  4, 51, 38, 85, 44, 66, 50, 50, 91,  2,\n       59, 24, 93, 32, 94, 84, 30,  4,  8,  3, 87, 31, 29, 20, 89, 29,  8,\n       79, 84, 15, 95, 45, 82, 38, 29, 10, 32, 96, 70, 38, 54, 51, 42,  8,\n       72, 21, 95, 58, 19, 48, 54, 80,  0, 88, 49, 77, 99, 98, 72]), array([40, 71, 61, 33, 90, 31,  8, 44, 96, 52, 98, 21, 61, 19,  0, 99, 88,\n       38, 22, 75, 16, 74, 35, 61, 43, 76, 78, 68, 98, 31, 28, 40, 36, 49,\n       39, 67, 74, 30, 28, 83, 77,  7, 93, 45, 32, 50, 83, 15, 43, 79, 90,\n       77, 12, 38, 83, 89, 56,  0, 38, 20, 25, 57, 83, 25, 25, 66, 81, 19,\n       65, 96, 96,  6, 64, 52, 77, 93,  8, 14,  7, 11, 35, 56, 57, 60, 91,\n       16, 98, 16, 88, 55, 64,  4, 40, 90, 97, 36, 88, 52, 56, 72]), array([37, 46, 92, 23, 41, 88, 90, 79, 69, 35, 54, 64, 18, 60, 69, 14, 57,\n        8, 78, 65, 16, 41,  4,  5, 55, 46, 47, 70, 33, 52, 80, 33, 40, 67,\n       77, 28, 88, 23,  2, 39, 37, 89, 46, 30, 45,  6, 93, 56, 79, 28, 14,\n       98, 88, 76, 22, 59,  4, 53, 76, 74, 50,  4, 80, 57, 55,  1, 78, 43,\n       85, 64, 15, 23, 10, 49, 17, 75,  2, 78, 44, 50, 12, 18,  4, 41, 51,\n       88,  0, 32, 95, 99, 88, 72, 74, 22,  9, 36, 59, 60, 13, 25]), array([78, 87, 11,  6, 54, 97, 66, 92, 15, 50, 42, 97, 39,  7, 97, 16,  0,\n       31, 69, 70, 46, 44, 61, 43, 13, 64, 89, 89, 38, 26, 28, 73, 42, 82,\n       51, 66, 13, 94, 17, 37, 27, 12, 92,  0, 35, 47,  1, 23, 40, 55, 73,\n       12, 85, 82, 66, 75, 71, 57, 25, 57, 72, 11, 95, 63, 73, 25, 40, 16,\n       22, 51, 27, 42, 45, 44, 76,  5, 40, 97, 72, 27, 89, 32, 89,  9, 11,\n       82, 56,  9, 96,  7, 92, 93, 57, 50, 99, 88, 60,  6, 40, 88]), array([65, 95, 61, 81, 38, 74, 41, 80, 64, 30, 74,  4, 82, 32, 66, 52, 88,\n       24, 46, 39,  3, 77, 26, 39, 70, 82, 17, 60, 75, 49, 95, 60, 88, 17,\n        7, 21, 76, 70, 91, 76, 29, 28, 71, 77, 38, 60, 92, 95, 86, 86, 40,\n       92, 15, 67, 18, 46, 90, 53, 23, 24, 44, 11, 32, 70, 19, 58, 25, 25,\n       99, 18, 25, 65, 70,  1, 86, 70, 48, 31,  1, 79, 78, 99, 66, 78, 43,\n       78, 91,  8, 88, 14,  3, 11, 58, 25, 94, 78, 98, 23, 83, 61]), array([73, 21, 52, 11, 95,  5, 22, 31, 27, 86, 73,  0, 95, 29, 53, 36, 35,\n       82, 35, 39, 48,  7,  1, 18, 59, 66, 22, 50, 97,  6, 29, 92, 17, 95,\n       95, 30,  9, 11, 61, 19, 11, 89, 15, 56, 83, 33, 77,  8, 70,  1,  6,\n        3, 48, 19, 72, 26, 11, 67, 30, 33, 48, 93, 19, 40, 62, 92,  3,  3,\n       85, 62, 89, 32, 64, 24, 75, 18, 16, 18, 58, 40, 29, 67, 49, 91, 25,\n       56, 34, 78, 20, 34, 18, 81, 98, 33, 64, 94, 33, 55, 60, 92]), array([37, 50, 30, 28, 46, 95, 74, 87, 50, 52, 80, 48, 89, 78, 41, 10, 59,\n       99, 70, 74, 34, 94, 64,  2, 12, 74, 91, 16, 85, 53, 32, 93, 49, 46,\n       11, 16, 96, 64, 45, 40, 66, 29, 57, 19, 32, 33,  0, 39, 14, 55, 50,\n       32, 74, 55, 65, 22, 57, 55, 44, 29, 70, 88, 68, 17, 42, 84, 65, 61,\n       33, 49,  8, 87, 67, 28, 18, 55, 74, 34, 23, 13, 37, 78, 38, 26, 81,\n       15, 64, 47, 24, 62, 77, 19, 49, 55, 71,  3, 24, 72, 46,  7]), array([97, 20, 21, 59, 52,  8, 59, 76, 37, 36, 26, 73, 65, 72,  5, 73, 40,\n       40,  6, 75, 86, 67, 69, 57, 51,  1, 39, 67, 61, 27, 35, 74, 89, 99,\n       43, 29, 32, 90, 59, 45, 89, 75, 39, 10, 11, 99, 70, 58, 17, 18, 58,\n       82, 74, 57, 64, 97, 71, 31, 73, 22, 68, 40, 30, 67,  9, 60, 36, 35,\n       50, 52, 37, 67, 25, 38, 62, 65, 32, 48, 25,  3, 50, 91, 91, 37, 82,\n       19, 97, 54, 22, 52, 43, 34, 12, 97, 21, 16, 48, 27,  6, 81]), array([33, 50, 70, 74, 43, 92, 64, 79, 39, 61, 33, 72, 21,  6, 39, 89, 77,\n       27, 12, 51,  6, 40, 13, 30, 17,  4, 35, 22, 18, 54, 76, 74, 70, 88,\n       92, 13,  6, 23, 49, 82, 20, 87, 33, 81, 95, 30, 83, 55,  8,  5, 84,\n       68, 18, 73, 97, 23, 37, 86, 24, 36, 36, 87, 27, 37, 48, 87, 70, 59,\n       97, 46,  2, 18, 38, 61, 51, 14, 78, 38, 89, 44, 97, 74, 52, 54, 16,\n        2,  1, 30, 36, 43, 59, 89, 46,  6, 73, 87, 48, 92, 88,  0]), array([26, 19, 36, 56, 70, 75, 43, 93, 12, 24, 27, 96, 56, 16,  4, 79, 19,\n       26,  5, 47, 77, 21, 38, 71, 76,  2, 22, 45, 25,  2, 75, 77, 36, 14,\n       21, 24, 39, 53,  2, 42, 39, 51, 55, 32, 93, 85, 22,  7, 22, 35, 69,\n       59, 77, 54, 11,  1, 97, 62, 99, 34, 88, 75, 66, 25, 31, 78, 10, 36,\n        8, 78, 92, 50, 47, 48,  5, 97,  0, 86, 45,  9, 24, 20, 19, 95, 87,\n       79, 26,  9, 69, 28, 65, 22,  3, 28, 25, 53, 45, 91, 24, 38]), array([40,  2, 15, 75, 89,  1, 63, 54, 38, 90,  9, 91, 11, 59, 37, 44, 21,\n       74, 72, 54, 23, 28, 14, 26, 27, 98, 53, 83, 38, 64, 21, 24, 97, 84,\n       17, 83, 91, 86, 62, 66, 80, 50,  5, 93, 63, 71, 48, 48, 82, 57, 64,\n        8, 42, 75, 65, 15, 25, 85, 26,  9, 97,  3, 20, 58, 10, 86, 17, 58,\n       39, 28, 71, 10, 83,  9, 13, 95, 86, 91, 19, 35, 68, 72, 66, 35, 58,\n       69, 75, 82, 20, 12, 87, 84, 23, 38, 71, 90, 20, 76, 83, 31]), array([12, 90, 97, 99, 28, 77, 37, 99, 70,  2, 77, 68, 61, 80,  3, 72, 71,\n       12, 88, 18, 24, 22, 14, 68, 15, 65, 96, 49, 67, 35, 93,  7, 79, 70,\n       90, 16, 98, 94, 26, 26, 29, 60, 56, 60, 63, 44, 70, 49, 15, 93, 20,\n        3, 40, 47, 59, 16, 28, 55,  8, 45, 51, 74, 40, 35, 81, 70, 53, 54,\n       55, 92,  4, 72, 59, 29, 86, 51, 15, 39, 92, 92, 80, 13, 44, 81, 24,\n       30, 50, 25, 13, 67, 84, 33, 22, 96, 93, 81, 17, 29, 34, 60]), array([63, 52, 32, 16, 46, 30, 52, 68, 96, 84, 61, 34, 61, 66, 64, 21, 82,\n       94, 86, 89, 54, 16, 66, 64, 80, 47, 19, 98,  6, 66, 18, 81, 96, 54,\n       47, 34,  4, 66, 55, 92, 94,  6, 28,  6, 75, 90, 72, 71, 36, 29,  0,\n       32, 52, 63, 95, 60, 23, 43, 76, 50, 94, 39, 68,  3, 83, 98, 87, 35,\n        8, 37,  3, 52, 78, 65, 39, 96, 86, 12, 68, 34, 91,  6, 91, 90, 59,\n       90, 51, 47, 28,  4,  8, 31, 10,  4, 56,  0, 27, 43, 44, 78]), array([83,  6, 51, 64, 22, 74, 21, 43, 70,  3, 39, 34, 95, 77, 90, 34, 21,\n       13, 90, 20,  2, 50, 66, 40, 66, 15, 91, 76, 72, 21,  5, 44,  1, 40,\n       10, 13, 73,  5, 17,  6, 31, 84, 67, 93, 13, 19, 97, 42, 17, 99, 79,\n       17,  9, 74, 54,  2, 34, 21, 21, 99, 67, 53, 39,  0, 18,  1, 34, 56,\n       45, 45, 28, 43, 47, 73, 58, 82, 67, 97,  6, 52, 93, 63, 69, 54, 95,\n       61, 47,  6, 19, 40, 27, 56, 65,  5, 60, 55, 97, 19, 86, 92]), array([14, 45,  8, 96,  6, 60, 75, 97, 32, 30, 77, 15, 83, 40, 64, 71, 65,\n       78, 11, 53, 30, 80, 33, 28, 68, 73, 11, 82, 32, 13, 84, 96, 79, 61,\n        3, 10,  8, 10, 82, 96, 54, 97,  1, 39, 40, 31,  6,  0, 44, 65, 35,\n       98, 13, 61, 76, 23, 87, 66, 70,  5, 64, 89, 21, 23,  3, 76, 85,  4,\n       78, 66,  7, 27, 76, 76,  8, 32, 99, 19, 88, 10, 55, 73, 37, 61, 91,\n       94, 31, 72, 50, 22, 31, 50, 45, 47,  7, 96, 82, 21, 74, 91]), array([73, 52, 86, 35, 49, 35,  2, 16,  6,  0, 47, 23, 89, 37, 51, 45, 89,\n       92, 20,  8, 82,  5, 93, 59, 40, 17, 83, 99, 90, 78, 25, 52, 48, 66,\n       30, 66, 22,  9, 51, 50, 82, 83, 63, 67, 96, 70, 39, 84, 98, 17, 13,\n       75, 47, 74, 47, 52, 28,  9, 32,  6, 41, 20, 35, 82,  1, 65, 50, 96,\n       22, 97, 18, 50,  2, 35, 41, 10, 87, 39, 47, 72, 25, 85, 87, 43, 82,\n       56, 84, 55, 10,  0, 65, 76, 47, 31, 81, 17, 67,  2, 56, 18]), array([62, 96, 35, 65, 27, 61, 48, 98, 96, 30,  1, 71,  1, 87, 63, 98, 75,\n       89, 63, 62, 12, 25, 85, 55, 45, 51, 66, 68, 58, 45, 56,  9, 64,  5,\n       23, 22,  9, 48, 46, 58, 64, 10, 71, 41, 70, 84, 52, 56, 46, 58, 91,\n        7,  0, 21, 55, 35, 74, 46,  6, 95, 71, 99, 59, 46,  1,  7, 81, 16,\n       88,  4, 12, 56, 13, 55, 10, 36, 52, 33, 36, 21, 95, 31, 96, 14, 69,\n       75, 66, 77, 21, 21, 62, 55, 49, 30, 73, 69, 37, 87, 55, 50]), array([43, 88, 68, 44, 22, 32, 58,  3, 21, 81,  2, 42, 76, 97, 70, 75, 38,\n       57, 27, 68, 21, 10, 20, 63, 80, 13, 81, 84, 72,  9, 47, 24, 76, 32,\n       53, 20, 61, 58, 13, 22,  6, 71, 60, 18, 59, 24, 46, 10, 83, 34, 40,\n       33, 68, 48, 87, 33, 29, 98, 13,  5, 35, 29, 49, 86, 53, 90, 66, 96,\n       98, 53, 21, 91, 14, 49, 82, 65, 99, 79, 52, 31, 54, 60, 68, 42, 97,\n       77, 19, 96, 93, 34, 92, 30, 91, 37, 19, 53, 58, 90, 10, 95]), array([88, 34, 92,  8, 76, 35,  4, 59, 48, 10,  6, 97, 83, 52, 44, 76, 78,\n       15, 26, 69,  2, 22, 80, 60, 50, 93, 99, 42, 49, 14, 83, 42, 79, 91,\n       57, 28, 82, 59, 88, 46,  4, 98, 30, 43, 35,  8, 87, 28, 71, 67, 70,\n       62, 98, 25, 91, 96, 28, 16, 27, 98, 53,  7, 14, 33, 94, 38, 42, 88,\n       68,  7, 56,  2, 64, 53, 60, 37, 25, 13, 23, 59, 85, 42, 20, 79,  8,\n       46, 11, 64, 26, 37, 27, 65, 95, 81, 32, 56, 88, 22, 20, 66]), array([39, 62, 27, 28, 46, 37, 10, 42, 81,  9, 32, 83, 28, 69, 53, 60, 80,\n       61, 96, 68, 73, 61, 88, 96, 55, 93, 10, 26, 99, 57, 16, 55, 19,  5,\n       28,  7, 88, 24, 75, 20, 41, 68, 19, 39, 46, 94, 56, 17, 71, 50, 60,\n       29, 53, 16,  6, 59,  2, 19, 62, 91, 43, 63, 53, 33, 49, 28,  4, 24,\n       47, 30, 73, 95, 86, 90, 65, 20, 20, 97, 38, 73, 50, 31, 31, 27, 56,\n       32, 77, 55, 88, 69, 62,  7, 61, 75, 17,  4, 45, 43, 80, 21]), array([69, 18, 45, 48, 53, 40, 59, 88, 78, 30, 41, 58, 69, 89, 14, 59, 26,\n        1, 67, 33, 82, 71, 90, 16, 30, 38, 43, 52, 25, 99, 20, 33, 38, 10,\n       99, 17, 39, 54, 64, 46, 72, 72, 80, 46, 83, 86, 65, 22,  2, 98, 98,\n       14, 76, 65, 72, 79, 62, 12, 19, 61, 62, 29, 19, 35, 32,  3, 78, 39,\n       26, 30, 41,  5, 64, 52, 45, 25, 97, 25, 60, 87, 18, 43, 93, 29, 60,\n       98, 83,  6, 81, 57, 34, 17, 92, 84, 40, 11, 40, 42, 26, 72]), array([17,  4, 96, 24, 11, 91, 25,  3, 46, 34, 65, 73, 65, 59, 86, 67, 52,\n       45, 52, 48, 29, 76, 70,  6, 54, 58, 76, 39, 26, 89, 52, 40,  3, 12,\n       86, 67, 74, 16, 75, 65, 75, 68, 43, 70, 72, 73,  6,  7, 41, 73, 73,\n       68, 90, 97,  8,  0, 66, 14, 44, 38, 32, 72, 66, 53, 84,  9, 15,  0,\n       99,  7, 27, 62, 67, 93, 36, 65, 80, 24, 30, 18,  7, 41, 16, 57, 10,\n       57, 60, 42, 67, 89, 70, 28, 95, 79, 12, 94,  7, 29, 93, 89]), array([10,  4, 96, 16, 67, 73, 45, 39, 21, 90, 22, 31, 23, 54, 47, 50, 53,\n       53, 18, 19, 45, 56, 91, 28, 26, 39, 73,  4, 35, 18, 46,  8, 39, 56,\n       41, 83, 64, 76, 74, 26, 46, 48,  0, 31, 96, 36, 91, 37, 94, 97, 12,\n       79, 40, 60, 29, 90, 73, 24, 53, 95, 12, 12, 15, 17, 12,  8,  5, 99,\n       55, 66, 33, 95, 57, 47, 20, 64, 54, 81, 33, 72, 87, 85, 33, 12, 37,\n       43, 70, 29, 43, 97,  5, 68, 54,  6, 35, 69,  4, 30, 72, 23]), array([45, 39, 79, 48, 59, 15, 58, 15, 64, 60, 74,  5, 19, 70, 24, 94, 35,\n       56, 83, 50, 80, 89, 39, 43, 60, 96, 72, 77, 59, 56, 95, 27,  2, 93,\n       41, 24, 24, 59, 26, 34, 60, 76, 91, 20, 70, 45, 11, 34, 13,  0, 66,\n       13, 64, 49, 69, 96, 29, 59, 86, 83,  5, 55, 90, 27, 33, 51,  2, 59,\n       23, 12, 32, 23, 61, 49,  5, 17, 11, 84, 55, 62, 81, 50, 40, 75, 64,\n       37, 55, 97, 63, 53, 40, 86, 45, 48, 80,  7, 54,  4, 46, 89]), array([81, 61, 21, 30,  1, 62, 73, 50, 84, 55, 60, 85, 65, 94, 60, 83, 65,\n       11, 36, 74, 50, 14, 50, 21,  4, 39, 43, 73, 51, 66, 83, 57, 18, 67,\n       57, 80, 78, 65, 71, 96, 20, 22, 13, 60, 75, 29, 75, 39, 94, 82, 14,\n       35, 96, 28,  0,  7,  7, 68, 79, 31, 93,  1, 15, 30, 14, 93, 53,  2,\n       86, 29, 98, 59, 78,  5, 32, 86, 48, 49, 98, 24, 89,  0, 22, 25, 60,\n       24, 95, 37, 40, 32, 73, 74, 77, 90, 65, 89, 16, 25, 87, 38]), array([47, 40, 25, 73, 56, 34, 12,  1, 44, 81, 21, 31, 33, 26, 14, 60,  4,\n       52, 79,  3, 11, 80, 18, 41,  7, 21, 37,  5, 33, 88, 49, 79,  9, 99,\n       67, 95, 97, 42, 40, 64,  2, 84, 66, 49, 25, 77, 25,  9, 34, 79, 55,\n       70,  3, 89, 93, 59, 75, 81, 75, 61, 11, 25, 68, 54, 69, 13, 31, 12,\n       89, 52, 78, 85, 48, 52, 14,  8, 37, 41, 96, 66, 21, 63,  5, 62, 96,\n        3, 56, 15, 72, 97, 15,  9,  2, 11, 76, 14, 98, 64, 26, 49]), array([91, 96, 58, 84, 88, 34, 79, 31, 56, 80, 56, 22, 30, 42,  8, 47, 17,\n       78, 25,  0, 96, 97,  7, 38, 63, 23, 84, 37,  2, 98, 53, 13,  9, 77,\n       64, 17, 23, 77, 94, 60, 38, 20, 76, 86, 10,  1, 34, 99, 46, 22, 21,\n       57,  0, 20, 99, 92, 64, 87, 31, 73, 26,  5, 67, 69, 80, 27, 98, 35,\n       35, 88, 28, 19, 48, 54, 48, 48, 24, 51, 90, 71, 18, 18, 87, 87, 87,\n       39, 26,  1, 83, 23, 56, 62, 24, 36, 65, 23, 61,  3, 83, 45]), array([79, 56, 66, 33, 30, 96, 74, 45,  5,  5, 20, 52, 80, 14, 68, 74, 94,\n       23, 27, 88, 15, 85, 50, 75, 43, 88, 16, 49, 26, 47, 27, 60,  2, 74,\n       66,  8,  4, 50, 17,  9, 97, 91, 96, 70, 45, 11, 69, 12, 65, 81, 54,\n       39, 69, 85,  4, 98, 10,  1,  8, 23,  0, 92,  0, 66, 26, 69, 18, 30,\n       22, 35, 58, 24, 12, 82, 61, 80, 33,  5, 94, 52, 76, 28, 27, 21, 12,\n       72, 42, 39, 87, 87, 64,  6, 49, 77,  6,  2, 87, 18, 45, 39]), array([88, 82, 97, 13, 53, 50, 13, 31, 23, 94,  3,  4, 98, 93, 85, 63, 46,\n       62, 86, 99, 71, 35, 93, 43, 97, 73, 55,  8, 84, 13, 28, 32, 83, 32,\n       93,  2, 44, 44, 66, 98, 85,  2,  9, 89, 92,  4, 79, 44, 72, 41, 16,\n       41, 72, 68,  0, 68, 18, 26, 77, 98, 98, 94, 51, 15, 72, 21, 14, 53,\n       58, 31, 83,  3, 76, 64, 29,  0, 54, 54, 80, 77, 65, 90, 21,  6, 38,\n        8, 29, 67, 80, 97, 66, 27, 12, 96, 41, 85, 30, 36, 14, 47]), array([41, 73, 26, 62, 34, 64, 95, 74, 71, 44, 15, 84, 39, 23, 44, 96, 57,\n       12,  6, 69, 66, 95, 28, 20, 29, 61, 32, 49, 13, 23, 68, 61, 70, 87,\n        8, 49, 37, 20, 85, 18, 57, 49, 74, 41, 85, 75, 32,  0, 19, 66, 76,\n       36, 30, 45, 98, 90, 33, 26,  1, 84, 86, 14, 47, 26, 20, 25, 76, 67,\n       10, 58, 60, 21, 98, 53, 86, 64,  7, 87, 10, 58, 17, 79, 44, 22, 69,\n       97, 77, 56, 23, 76, 66, 80, 71, 59, 67, 44, 78,  5, 52, 26]), array([38, 75, 41, 87, 44, 37, 56, 97, 68, 63, 13, 22, 57, 89, 54, 89, 59,\n       57,  6, 93, 18,  8, 47,  2, 66, 52, 22, 75, 17,  4, 45,  9, 16, 65,\n       62, 59, 52, 66, 12, 18,  8, 85, 37, 43, 96,  1, 39, 36, 79, 78, 64,\n       30, 50, 70, 72, 78,  8, 72,  4, 40, 96, 39, 37, 69, 13, 27, 82, 79,\n       82, 69, 66, 92, 87, 32, 85, 60, 33, 81,  4, 60, 60, 12, 11, 34, 61,\n       65, 24, 60, 48, 27, 52, 61,  5, 55, 84, 58, 11, 36, 13, 61]), array([99, 98, 40, 86, 93, 59, 51, 57, 63, 84, 13,  0, 32, 34, 24, 92,  5,\n       71, 64, 97, 59, 75, 64, 35, 98, 77, 55, 82, 30, 44, 36, 67, 42, 57,\n       75, 79, 66, 12, 78,  1, 41, 66,  8, 60, 79,  9, 31,  9, 75, 37, 87,\n       44, 46, 77, 77, 25, 13, 21, 82, 28, 66, 19,  5, 59, 74, 11, 30, 56,\n       72, 43, 16, 44, 35, 91, 40, 29, 87, 54, 11, 11, 34, 49, 60, 96, 78,\n       83, 40, 90, 20, 31, 10, 14, 36, 76, 49,  4,  5, 10, 48, 66]), array([ 6, 97, 97, 74, 31, 43, 52, 92, 98, 13, 47, 71, 46, 70, 20,  2, 49,\n       79, 94, 56, 47, 86, 68, 44, 29, 40, 64, 98, 73, 42, 81, 71, 71, 52,\n       74, 77, 33, 65, 38, 97, 36, 81, 64, 91, 89, 83, 12, 33, 63, 17, 67,\n       39, 11, 57, 34, 73, 17, 60, 92, 53, 15, 73,  0, 28, 18, 85, 47, 25,\n        3, 62, 37, 58, 23, 11, 35, 80, 70, 70, 77, 41, 89, 11, 58, 13, 61,\n       69, 24,  9, 19, 58, 98, 46, 56, 28,  2, 22, 95, 49, 26, 50]), array([48, 56, 71, 21, 90, 38,  7, 70, 90, 26, 44, 97, 76, 16, 62, 41, 48,\n       51, 70, 21, 91, 80, 69, 67, 20, 77, 39, 43,  7, 95, 68, 57, 78, 72,\n       87, 91, 99, 17, 44, 66, 66,  8, 17, 88, 31, 67, 17, 97, 56, 44, 10,\n       89, 63, 41, 35, 85, 27,  7, 24, 73, 32, 34, 80,  7,  6, 37, 16, 59,\n       52, 29, 47, 65, 47, 74, 50, 40, 40, 60, 32, 36,  5, 75, 72, 64, 91,\n       52, 94,  6,  7, 72, 18, 52, 88, 20, 22, 28, 27, 48,  5, 15]), array([32, 19, 59, 15, 30, 29, 44, 17, 78, 11, 62, 76, 89, 12,  3, 14, 55,\n       73, 50, 72, 69,  6, 18, 21,  7, 85, 66, 88, 97,  6, 82, 48, 61, 48,\n       49, 60, 90,  8, 16,  3,  1,  2,  8, 28, 33, 35, 86, 94, 61, 58, 18,\n       10, 65, 73, 99, 87, 80, 64, 83, 45, 89, 75, 88, 23,  5, 17, 18, 34,\n        1, 85, 79, 26, 51, 39, 69, 66, 47,  4, 17, 23, 33, 80, 27, 70, 44,\n       43, 25, 40, 57, 14, 28, 14, 28, 33,  2, 28, 71, 17, 59, 20]), array([19, 57, 57, 79, 92, 45, 96, 25, 68, 33, 69, 31, 94, 81, 82,  6, 42,\n       84, 92, 43, 97, 58, 92, 72, 47, 71, 82, 65, 84, 64, 42,  8,  9, 61,\n       40, 38, 29, 73, 66, 14, 69, 17, 89, 16, 49, 75, 40, 75,  1,  9, 52,\n       54, 61, 43, 19, 76, 12, 67, 39, 52, 42, 69, 69, 78, 35, 35, 55, 42,\n       23, 98,  3, 16, 55, 21, 40, 80, 92, 71, 24, 90,  5, 11, 68, 68, 23,\n       29, 25, 14, 21, 14, 82, 51, 50, 33, 31, 10, 36, 83, 81, 69]), array([63, 70, 86, 34, 55, 80, 82, 14, 14, 60, 26,  4, 99,  3, 80, 87, 55,\n       88, 44, 59, 62, 59, 23, 91, 38, 96, 52, 57, 81, 12, 37, 58,  9, 57,\n       89, 95, 54, 11,  7, 35, 88, 22, 68, 59, 78, 56, 35, 82, 42, 19, 68,\n       30, 59, 11, 35, 33, 76,  3, 21, 30, 95, 84, 95, 53,  4, 34, 12, 11,\n       56, 82, 22, 48, 38, 76, 37, 81,  6, 51, 91, 42, 67, 68, 26, 48, 79,\n       49, 48, 14, 29, 56, 97, 87, 33, 76,  9, 12, 19, 62, 19, 21]), array([19, 91, 72, 28, 77, 95, 73, 38, 58, 79, 97, 61, 98, 76, 77, 64, 32,\n       33, 53, 84, 40, 57, 61,  8, 33, 76, 85, 28, 99, 94, 95,  8, 66, 88,\n       61, 42, 95, 25, 15, 96, 14, 44,  7, 48, 54, 13, 39, 16, 22, 73, 81,\n       72, 53, 66, 47, 94, 38, 39, 78, 50, 23, 25,  8, 73, 39, 12, 41, 19,\n       32, 10, 90, 14, 22, 25, 98, 97, 81, 84, 12, 67, 90, 64, 59, 97,  0,\n       90, 25,  3, 33, 17, 87, 90, 97, 12, 70, 19, 69, 34, 11, 78]), array([88,  4, 24, 37, 53, 18, 47, 93, 71, 67, 40, 69, 64, 55, 57, 91,  3,\n       28, 91, 99, 79, 26, 70, 16, 89, 74, 33, 62, 82, 50, 97,  2, 52, 77,\n       14, 59, 44, 32, 71, 34, 56, 37,  3, 15, 87, 81, 76, 12, 64, 89, 76,\n       41, 33, 68, 54, 47, 98, 97,  2, 18, 83, 25, 75, 10, 96,  2, 70, 93,\n       95, 97, 34, 90, 31, 17, 48, 10,  8, 72, 20, 73, 31, 52, 89, 66, 49,\n       70,  6, 61, 57, 16, 22, 80, 26, 44, 78, 49,  4, 93, 79, 45]), array([65, 41, 28,  3, 72, 18,  5, 21,  3, 30, 38, 89, 74, 18,  4, 21, 70,\n       64, 55, 24, 66, 22, 37, 78,  4, 18, 28, 24,  3, 93, 71,  5, 32, 22,\n       86, 89, 72, 43,  2, 40, 12,  8,  8, 62, 28, 64, 11, 24,  6,  0, 46,\n       99, 91, 93, 10, 64, 69, 24, 98, 86, 93, 41,  5, 40, 42, 63, 67, 38,\n       94, 60, 44, 60, 16, 23, 68,  4, 16, 19, 34, 24, 74, 48, 64, 16, 57,\n       69, 69, 30, 66,  2,  5, 31,  1,  5, 42, 35, 32, 63, 58, 28]), array([11, 22, 14, 91, 61, 77, 89, 31,  4, 14, 35, 97,  9, 16, 68, 38, 87,\n       95, 23, 92, 32, 13, 32,  8,  8, 67, 46, 27, 86, 81, 99,  5, 24, 53,\n       21, 41, 51, 55, 82, 12, 96, 31, 95, 40, 13, 23, 90, 35, 81, 93, 80,\n        9, 39, 61,  0, 49, 50, 57, 88, 99, 91, 75, 93, 73, 11, 41, 95, 28,\n       53, 33,  4, 58, 11, 42, 90, 75, 79, 89, 60, 44, 15, 47, 27, 41, 25,\n       35,  4,  2, 98, 23, 88, 45, 17, 20, 17, 99, 91, 53, 74, 64])], 'feature_importances_': array([0.01961227, 0.01788882, 0.0177844 , 0.02662382, 0.03526735,\n       0.01605791, 0.01625369, 0.02484259, 0.01516394, 0.22031669,\n       0.02988917, 0.01990195, 0.35851355, 0.04824655, 0.01184591,\n       0.01972861, 0.02501337, 0.02912331, 0.02353892, 0.02438721]), 'n_classes_': 2, 'n_features_in_': 20, 'n_outputs_': 1}"
  },
  {
    "objectID": "documentation.html#keras",
    "href": "documentation.html#keras",
    "title": "Model documentation",
    "section": "Keras",
    "text": "Keras\n\nfrom tensorflow import keras\n\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\nkeras_clf = keras.Sequential()\nkeras_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(20,)))\nkeras_clf.add(keras.layers.Dense(8, activation='relu'))\nkeras_clf.add(keras.layers.Dense(1, activation='sigmoid'))\nkeras_clf.compile(optimizer='sgd', loss='binary_crossentropy')\nkeras_clf.fit(X, y, batch_size=10, epochs=10)\n\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nEpoch 1/10\nWARNING:tensorflow:From C:\\Users\\st004186\\Envs\\gingado\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n 1/10 [==&gt;...........................] - ETA: 7s - loss: 0.6940\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 1s 2ms/step - loss: 0.6796\nEpoch 2/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6566\nEpoch 3/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.7110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6382\nEpoch 4/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4833\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6218\nEpoch 5/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5453\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.6062\nEpoch 6/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.6784\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 1ms/step - loss: 0.5924\nEpoch 7/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.3930\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.5790\nEpoch 8/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4872\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.5672\nEpoch 9/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.5532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.5550\nEpoch 10/10\n 1/10 [==&gt;...........................] - ETA: 0s - loss: 0.4532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 [==============================] - 0s 2ms/step - loss: 0.5448\n\n\n&lt;keras.src.callbacks.History at 0x27d58298910&gt;\n\n\n\nmodel_doc_keras = ModelCard()\nmodel_doc_keras.read_model(keras_clf)\nmodel_doc_keras.show_json()['model_details']['info']\n\n'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 20], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"dense_input\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"batch_input_shape\": [null, 20], \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 20]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 16]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 8]}}]}, \"keras_version\": \"2.15.0\", \"backend\": \"tensorflow\"}'"
  },
  {
    "objectID": "documentation.html#other-models",
    "href": "documentation.html#other-models",
    "title": "Model documentation",
    "section": "Other models",
    "text": "Other models\nNative support for automatic documentation of other model types, such as from fastai, pytorch is expected to be available in future versions. Until then, any models coded form scratch by the user as well as any other model can be documented by passing the information as an argument to the Documenter’s fill_model_info method. This can be done with a string or dictionary. For example:\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.len = self.X.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass PytorchNet(torch.nn.Module):\n    def __init__(self):\n        super(PytorchNet, self).__init__()\n        self.layer1 = torch.nn.Linear(20, 16)\n        self.layer2 = torch.nn.Linear(16, 8)\n        self.layer3 = torch.nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        x = torch.sigmoid(self.layer3(x))\n        return x\n\npytorch_clf = PytorchNet()\n\ndataloader = MockDataset(X, y)\n\n\nloss_func = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(pytorch_clf.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        _X, _y = data\n        optimizer.zero_grad()\n        y_pred_epoch = pytorch_clf(_X)\n        loss = loss_func(y_pred_epoch, _y.reshape(1))\n        loss.backward()\n        optimizer.step()\n\n\nmodel_doc_pytorch = ModelCard()\nmodel_doc_pytorch.fill_model_info(\"This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation\")\nmodel_doc_pytorch.show_json()['model_details']['info']\n\n'This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation'"
  },
  {
    "objectID": "estimators.html",
    "href": "estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "In many instances, economists are interested in using machine learning models for specific purposes that go beyond their ability to predict variables to a good accuracy. For example:\nThe gingado.estimators module contains machine learning algorithms adapted to enable the types of analyses described above. More estimators can be expected over time.\nFor more academic discussions of machine learning methods in economics covering a broad range of topics, see Athey and Imbens (2019)."
  },
  {
    "objectID": "estimators.html#clustering",
    "href": "estimators.html#clustering",
    "title": "Estimators",
    "section": "Clustering",
    "text": "Clustering\nThe clustering algorithms used below are not themselves adapted from the general use methods. Rather, the functions offer convenience functionalities to find and retain the other variables in the same cluster.\nThese variables are usually entities (individuals, countries, stocks, etc) in a larger population.\nThe gingado clustering routines are designed to allow users standalone usage, or a seamless integration as part of a pipeline.\nThere are three levels of sophistication that users can choose from:\n\nusing the off-the-shelf clustering routines provided by gingado, which were selected to be applied cross various use cases;\nselecting an existing clustering routine from the scikit-learn.cluster module; or\ndesigning their own clustering algorithm.\n\n\nFindCluster\n\nFindCluster (cluster_alg: '[BaseEstimator, ClusterMixin]' = AffinityPropagation(), auto_document: 'ggdModelDocumentation' = &lt;class 'gingado.model_documentation.ModelCard'&gt;, random_state: 'int | None' = None)\n\nRetain only the columns of `X` that are in the same cluster as `y`.\n\nArgs:\n    cluster_alg (BaseEstimator|ClusterMixin): An instance of the clustering algorithm to use.\n    auto_document (ggdModelDocumentation): gingado Documenter template to facilitate model documentation.\n    random_state (int|None): The random seed to be used by the algorithm, if relevant. Defaults to None.\n\nfit\n\nfit (self, X, y)\n\nFit `FindCluster`.\n\nArgs:\n    X: The population of entities, organized in columns.\n    y: The entity of interest.\n\n\ntransform\n\ntransform (self, X) -&gt; 'np.array'\n\nKeep only the entities in `X` that belong to the same cluster as `y`.\n\nArgs:\n    X: The population of entities, organized in columns.\n\nReturns:\n    np.array: Columns of `X` that are in the same cluster as `y`.\n\n\nfit_transform\n\nfit_transform (self, X, y) -&gt; 'np.array'\n\nFit a `FindCluster` object and keep only the entities in `X` that belong to the same cluster as `y`.\n\nArgs:\n    X: The population of entities, organized in columns.\n    y: The entity of interest.\n\nReturns:\n    np.array: Columns of `X` that are in the same cluster as `y`.\n\n\ndocument\n\ndocument (self, documenter: 'ggdModelDocumentation | None' = None)\n\nDocument the `FindCluster` model using the template in `documenter`.\n\nArgs:\n    documenter (ggdModelDocumentation|None): A gingado Documenter or the documenter set in `auto_document` if None.\n        Defaults to None.\n\n\nExample: finding similar countries\nThe Barro and Lee (1994) dataset is used to illustrate the use of FindCluster. It is a country-level dataset. Let’s use it to answer the following question: for some specific country, what other countries are the closest to it considering the data available?\nFirst, we import the data:\n\nfrom gingado.datasets import load_BarroLee_1994\n\nThe data is organized by rows: each row is a different country, and the variables are organised in columns.\nThe dataset is originally organised for a regression of GDP growth (here denoted y) on the covariates (X). This is not what we want to do in this case. So instead of keeping GDP as a separate variable, the next step is to include it in the X DataFrame.\n\nX, y = load_BarroLee_1994()\nX['gdp'] = y\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\ngdp\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n-0.024336\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n0.100473\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n0.067051\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n0.064089\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n0.027930\n\n\n\n\n5 rows × 63 columns\n\n\n\nNow we remove the first column (an identifier) and transpose the DataFrame, so that countries are organized in columns.\nEach country is identified by a number: 0, 1, …\n\nX = X.iloc[:, 1:]\ncountries = X.T\ncountries.columns = ['country_' + str(c) for c in countries.columns]\ncountries.head()\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n\n\n5 rows × 90 columns\n\n\n\nSuppose we are interested in country No 13. What other countries are similar to it?\nFirst, country No 13 needs to be carved out of the DataFrame with the other countries.\nSecond, we can now pass the larger DataFrame and country 13’s data separately to an instance of FindCluster.\n\ncountry_of_interest = countries.pop('country_13')\n\n\nsimilar = FindCluster(AffinityPropagation(convergence_iter=5000))\nsimilar\n\nFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FindClusteriNot fittedFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000)) cluster_alg: AffinityPropagationAffinityPropagation(convergence_iter=5000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(convergence_iter=5000) \n\n\n\nsame_cluster = similar.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster.equals(similar.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster\n\n\n\n\n\n\n\n\ncountry_2\ncountry_9\ncountry_41\ncountry_48\ncountry_49\ncountry_52\ncountry_60\ncountry_64\ncountry_66\n\n\n\n\ngdpsh465\n8.895082\n8.151910\n7.360740\n6.469250\n5.762051\n9.224933\n8.346168\n7.655864\n7.830028\n\n\nbmp1l\n0.000000\n0.148400\n0.418100\n0.538800\n0.600500\n0.000000\n0.319900\n0.134500\n0.488000\n\n\nfreeop\n0.204244\n0.110885\n0.218471\n0.153491\n0.151848\n0.204244\n0.110885\n0.164598\n0.136287\n\n\nfreetar\n0.009186\n0.028579\n0.027087\n0.043888\n0.024100\n0.009186\n0.028579\n0.044446\n0.046730\n\n\nh65\n0.260000\n0.145000\n0.032000\n0.015000\n0.002000\n0.393000\n0.272000\n0.080000\n0.146000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.174100\n0.052400\n0.190500\n0.069200\n0.148400\n0.255800\n0.062500\n0.052500\n0.076400\n\n\nim1\n0.175000\n0.052300\n0.225700\n0.074800\n0.186400\n0.241200\n0.057800\n0.057200\n0.086600\n\n\nxr65\n1.082000\n2.119000\n3.949000\n0.348000\n7.367000\n1.017000\n36.603000\n30.929000\n40.500000\n\n\ntot1\n-0.010040\n0.007584\n0.205768\n0.035226\n0.007548\n0.018636\n0.014286\n-0.004592\n-0.007018\n\n\ngdp\n0.067051\n0.039147\n0.016775\n-0.048712\n0.024477\n0.050757\n-0.034045\n0.046010\n-0.011384\n\n\n\n\n62 rows × 9 columns\n\n\n\nThe default clustering algorithm used by FindCluster is affinity propagation (Frey and Dueck 2007). It is the algorithm of choice because of it combines several desireable characteristics, in particular: - the number of clusters is data-driven instad of set by the user, - the number of entities in each cluster is also chosen by the model, - all entities are part of a cluster, and - each cluster might have a different number of entities.\nHowever, we may want to try different clustering algorithms. Let’s compare the result above with the same analyses using DBSCAN (Ester et al. 1996).\n\nfrom sklearn.cluster import DBSCAN\n\n\nsimilar_dbscan = FindCluster(cluster_alg=DBSCAN())\nsimilar_dbscan\n\nFindCluster(cluster_alg=DBSCAN())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FindClusteriNot fittedFindCluster(cluster_alg=DBSCAN()) cluster_alg: DBSCANDBSCAN()  DBSCAN?Documentation for DBSCANDBSCAN() \n\n\n\nsame_cluster_dbscan = similar_dbscan.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster_dbscan.equals(similar_dbscan.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster_dbscan\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n0.063400\n0.034200\n0.086400\n0.059400\n0.052400\n...\n0.166200\n0.259700\n0.104400\n0.286600\n0.129600\n0.440700\n0.166900\n0.323800\n0.184500\n0.187600\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n0.076200\n0.042800\n0.093100\n0.046000\n0.052300\n...\n0.161700\n0.228800\n0.179600\n0.350000\n0.145800\n0.425700\n0.220100\n0.313400\n0.194000\n0.200700\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n1.000000\n12.499000\n7.000000\n1.000000\n2.119000\n...\n4.286000\n2.460000\n32.051000\n0.452000\n652.850000\n2.529000\n25.553000\n4.152000\n0.452000\n0.886000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n-0.001747\n0.009092\n0.011630\n0.008169\n0.007584\n...\n-0.006642\n-0.003241\n-0.034352\n-0.001660\n-0.046278\n-0.011883\n-0.039080\n0.005175\n-0.029551\n-0.036482\n\n\ngdp\n-0.024336\n0.100473\n0.067051\n0.064089\n0.027930\n0.046407\n0.067332\n0.020978\n0.033551\n0.039147\n...\n0.038095\n0.034213\n0.052759\n0.038416\n0.031895\n0.031196\n0.034096\n0.046900\n0.039773\n0.040642\n\n\n\n\n62 rows × 89 columns\n\n\n\nAs illustrated above, the results can be quite different. In this case, affinity propagation converged to more tightly defined clusters, while DBSCAN selected a cluster that contains almost all other countries (therefore, not useful in this particular case).\nNote that model documentation is already jumpstarted when the cluster is fit. A glimpse of the current template, including the questions in the documentation template that have been automatically filled, are shown below.\n\nsimilar.model_documentation.show_json()\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-22 14:54:45 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'clusterer',\n   'affinity_matrix_': array([[-4.23373922e+08, -5.97375771e+07, -5.35974361e+07, ...,\n           -1.92434215e+09, -8.60822083e+07, -3.77976931e+07],\n          [-5.97375771e+07, -4.23373922e+08, -2.26471602e+08, ...,\n           -2.66217555e+09, -2.43057326e+06, -1.92555486e+08],\n          [-5.35974361e+07, -2.26471602e+08, -4.23373922e+08, ...,\n           -1.33575671e+09, -2.75395788e+08, -1.37934978e+06],\n          ...,\n          [-1.92434215e+09, -2.66217555e+09, -1.33575671e+09, ...,\n           -4.23373922e+08, -2.82418157e+09, -1.42280304e+09],\n          [-8.60822083e+07, -2.43057326e+06, -2.75395788e+08, ...,\n           -2.82418157e+09, -4.23373922e+08, -2.37881124e+08],\n          [-3.77976931e+07, -1.92555486e+08, -1.37934978e+06, ...,\n           -1.42280304e+09, -2.37881124e+08, -4.23373922e+08]]),\n   'cluster_centers_': array([[ 6.82979374e+00,  6.14100000e-01,  3.13509000e-01, ...,\n            5.25000000e-01,  5.75000000e-03,  1.00472567e-01],\n          [ 8.89508153e+00,  0.00000000e+00,  2.04244000e-01, ...,\n            1.08200000e+00, -1.00400000e-02,  6.70514822e-02],\n          [ 7.56527528e+00,  1.99700000e-01,  2.48714000e-01, ...,\n            6.62500000e+00, -2.19500000e-03,  6.40891662e-02],\n          ...,\n          [ 8.33254894e+00,  0.00000000e+00,  3.45485000e-01, ...,\n            4.52000000e-01, -1.66000000e-03,  3.84156381e-02],\n          [ 8.86531163e+00,  0.00000000e+00,  2.82939000e-01, ...,\n            4.52000000e-01, -2.95510000e-02,  3.97733722e-02],\n          [ 8.91233857e+00,  0.00000000e+00,  1.50366000e-01, ...,\n            8.86000000e-01, -3.64820000e-02,  4.06415381e-02]]),\n   'cluster_centers_indices_': array([ 1,  2,  3,  4,  5,  7,  8, 10, 13, 14, 16, 18, 19, 25, 27, 32, 35,\n          39, 42, 45, 46, 49, 50, 52, 53, 55, 57, 58, 60, 62, 67, 68, 69, 71,\n          76, 82, 87, 88], dtype=int64),\n   'feature_names_in_': array(['gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65', 'hf65',\n          'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65', 'mort65',\n          'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n          'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65',\n          'highm65', 'highf65', 'highc65', 'highcm65', 'highcf65', 'human65',\n          'humanm65', 'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65',\n          'nom65', 'nof65', 'pinstab1', 'pop65', 'worker65', 'pop1565',\n          'pop6565', 'sec65', 'secm65', 'secf65', 'secc65', 'seccm65',\n          'seccf65', 'syr65', 'syrm65', 'syrf65', 'teapri65', 'teasec65',\n          'ex1', 'im1', 'xr65', 'tot1', 'gdp'], dtype=object),\n   'labels_': array([29,  0,  1,  2,  3,  4, 18,  5,  6,  1,  7, 30, 14,  8,  9, 29, 10,\n          29, 11, 12, 12, 18, 29, 36, 18, 13, 18, 14, 29, 36, 36, 14, 15, 36,\n          29, 16, 18, 14, 36, 17,  1, 14, 18, 29, 29, 19, 20,  1,  1, 21, 22,\n           1, 23, 24, 21, 25, 36, 26, 27,  1, 28, 12, 29,  1, 14,  1, 29, 30,\n          31, 32, 12, 33, 18, 29, 30, 18, 34, 14, 18, 36, 36, 29, 35, 36, 29,\n          29, 14, 36, 37,  1], dtype=int64),\n   'n_features_in_': 62,\n   'n_iter_': 200},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nFindCluster can also be used as part of a pipeline. In this case, only the entities in the same cluster as the entity of interest will continue on to the next steps of the estimation.\n\nfrom gingado.benchmark import RegressionBenchmark\nfrom sklearn.pipeline import Pipeline\n\n\npipe = Pipeline([\n    ('cluster', FindCluster(AffinityPropagation(convergence_iter=5000))),\n    ('rf', RegressionBenchmark())\n])\n\n\npipe.fit(X=countries, y=country_of_interest)\n\nPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))]) cluster: FindClusterFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000)) cluster_alg: AffinityPropagationAffinityPropagation(convergence_iter=5000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(convergence_iter=5000) rf: RegressionBenchmarkRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True)"
  },
  {
    "objectID": "estimators.html#comparative-case-studies",
    "href": "estimators.html#comparative-case-studies",
    "title": "Estimators",
    "section": "Comparative case studies",
    "text": "Comparative case studies\n\nMachineControl\n\nMachineControl (cluster_alg: '[BaseEstimator, ClusterMixin] | None' = AffinityPropagation(), estimator: 'BaseEstimator' = RegressionBenchmark(), manifold: 'BaseEstimator' = TSNE(), with_placebo: 'bool' = True, auto_document: 'ggdModelDocumentation' = &lt;class 'gingado.model_documentation.ModelCard'&gt;, random_state: 'int | None' = None)\n\nSynthetic controls with machine learning methods\n\nArgs:\n    cluster_alg (BaseEstimator | ClusterMixin | None): An instance of the clustering algorithm to use, or None to retain all entities.\n    estimator (BaseEstimator): Method to weight the control entities.\n    manifold (BaseEstimator): Algorithm for manifold learning.\n    with_placebo (bool): Include placebo estimations during prediction?\n    auto_document (ggdModelDocumentation): gingado Documenter template to facilitate model documentation.\n    random_state (int | None): The random seed to be used by the algorithm, if relevant.\n\nfit\n\nfit (self, X: 'pd.DataFrame', y: 'pd.DataFrame | pd.Series')\n\nFit the `MachineControl` model.\n\nArgs:\n    X (pd.DataFrame): A pandas DataFrame with pre-intervention data of shape (n_samples, n_control_entities).\n    y (pd.DataFrame | pd.Series): A pandas DataFrame or Series with pre-intervention data of shape (n_samples,).\n\n\npredict\n\npredict (self, X: 'pd.DataFrame', y: 'pd.DataFrame | pd.Series')\n\nCalculate the model predictions before and after the intervention.\n\nArgs:\n    X (pd.DataFrame): A pandas DataFrame with complete time series (pre- and post-intervention) of shape (n_samples, n_control_entities).\n    y (pd.DataFrame | pd.Series): A pandas DataFrame or Series with complete time series of shape (n_samples,).\n\n\nget_controls\n\nget_controls (self)\n\nGet the list of control entities\n\n\ndocument\n\ndocument (self, documenter: 'ggdModelDocumentation | None' = None)\n\nDocument the `MachineControl` model using the template in `documenter`.\n\nArgs:\n    documenter (ggdModelDocumentation | None): A gingado Documenter or the documenter set in `auto_document` if None.\n\n\nBrief econometric description\nThe goal of MachineControl is to estimate:\n\\[\n\\tau_t = Y_{1, t}^{I} - Y_{1, t}^{N}, t &gt; T0\n\\]\nwhere:\n\n\\(\\tau\\) is the effect on entity \\(i=1\\) of the intervention of interest\nwithout loss of generality, \\(i=1\\) is an entity that has undergone the intervention of interest, amongst \\(N\\) total entities\ntime period \\(T0\\) is a date in which the intervention occurred\nsuperscript \\(I\\) in an outcome variable denotes the occurence of the intervention, whereas superscript \\(N\\) is absence of intervention\nfor \\(t &gt; T0\\), \\(Y_{i, t}^{I}\\) is observed while \\(Y_{i, t}^{N}\\) must be estimated because it is a counterfacual.\n\n\\(Y_{i, t}^{N}\\) is calculated from the values of the other entities, \\(i \\neq 1\\). Collect this data in a vector \\(\\mathbb{Y}_{-1, t}^{N}\\). Then, following Doudchenko and Imbens (2016):\n\\[\n\\hat{Y}_{i, t}^{N} = f^*(\\mathbb{Y}_{-1, t}^{N}),\n\\]\nwith the star (\\(*\\)) superscript on the function \\(f(\\cdot)\\) representing that it was trained only with data up until the intervention date. The exact form of \\(f(\\cdot)\\) depends on the argument estimator. A general use estimator is the random forest (Breiman 2001).\nThe panel data itself might be the whole population in the data, or a subset when using the whole population might be too cumbersome to run analyses (eg, if the data contains too many entities). One way to select this subsample of control units without including subjective judgment in the data is quantitatilve. The control units are selected through a clustering algorithm (argument cluster_arg). One cluster algorithm that can be used is affinity propagation (Frey and Dueck 2007).\nTo finalise, the quality of the synthetic control can be assessed in many ways. One fully data-driven way to achieve this is by using manifold learning: lower-dimensional embeddings of a higher-dimensional data. A preferred manifold learning algorithm is t-SNE (Van der Maaten and Hinton 2008).\nThe relative distance between embeddings and the target centre, as well as the control and the target, represent the chance that a better feasible control (either from real or combined) will materialise. The intuition behind this test is:\n\nlet \\(d_{i,j}\\) be the Euclidean distance between the embeddings (2d points) of entities \\(i\\) and \\(j\\)\nif only a very small percentage of \\(d_{1, j \\in (2, ..., N)}\\) are lower than \\(d_{1, \\text{Synthetic control}}\\), than the synthetic control produced with \\(f(\\cdot)\\) is indeed a formula that provides one of the best alternative.\n\nMain references:\n\nAbadie and Gardeazabal (2003)\nAbadie, Diamond, and Hainmueller (2010)\nAbadie, Diamond, and Hainmueller (2015)\nDoudchenko and Imbens (2016)\nAbadie (2021)\n\n\n\nExample: impact of labour reform on productivity\nSee Machine controls: Synthetic controls with machine learning."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\n\n\n\nread_attr (obj)\n\nReads and yields the type and values of fitted attributes from an object.\n\nArgs:\n    obj: Object from which attributes will be read.\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=2099199461),\n    DecisionTreeRegressor(max_features=1.0, random_state=520470076),\n    DecisionTreeRegressor(max_features=1.0, random_state=1821101536)]},\n  {'estimators_samples_': [array([1, 0]), array([1, 1]), array([1, 1])]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-model-documentation",
    "href": "utils.html#support-for-model-documentation",
    "title": "Utils",
    "section": "",
    "text": "get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\n\n\n\nread_attr (obj)\n\nReads and yields the type and values of fitted attributes from an object.\n\nArgs:\n    obj: Object from which attributes will be read.\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=2099199461),\n    DecisionTreeRegressor(max_features=1.0, random_state=520470076),\n    DecisionTreeRegressor(max_features=1.0, random_state=1821101536)]},\n  {'estimators_samples_': [array([1, 0]), array([1, 1]), array([1, 1])]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-time-series",
    "href": "utils.html#support-for-time-series",
    "title": "Utils",
    "section": "Support for time series",
    "text": "Support for time series\nObjects of the class Lag are similar to scikit-learn’s transformers.\n\nLag\n\nLag (lags=1, jump=0, keep_contemporaneous_X=False)\n\nA transformer for lagging variables.\n\nArgs:\n    lags (int): The number of lags to apply.\n    jump (int): The number of initial observations to skip before applying the lag.\n    keep_contemporaneous_X (bool): Whether to keep the contemporaneous values of X in the output.\n\n\nfit\n\nfit (self, X: numpy.ndarray, y=None)\n\nFits the Lag transformer.\n\nArgs:\n    X (np.ndarray): Array-like data of shape (n_samples, n_features).\n    y: Array-like data of shape (n_samples,) or (n_samples, n_targets) or None.\n    \nReturns:\n    self: A fitted version of the `Lag` instance.\n\n\ntransform\n\ntransform (self, X: numpy.ndarray)\n\nApplies the lag transformation to the dataset `X`.\n\nArgs:\n    X (np.ndarray): Array-like data of shape (n_samples, n_features).\n    \nReturns:\n    A lagged version of `X`.\n\n\nfit_transform\n\nfit_transform (self, X, y=None, **fit_params)\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.\nThe code below demonstrates how Lag works in practice. Note in particular that, because Lag is a transformer, it can be used as part of a scikit-learn’s Pipeline.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nrandomX = np.random.rand(15, 2)\nrandomY = np.random.rand(15)\n\nlags = 3\njump = 2\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lagger', Lag(lags=lags, jump=jump, keep_contemporaneous_X=False))\n]).fit_transform(randomX, randomY)\n\nBelow we confirm that the lagger removes the correct number of rows corresponding to the lagged observations:\n\nassert randomX.shape[0] - lags - jump == pipe.shape[0]\n\nAnd because Lag is a transformer, its parameters (lags and jump) can be calibrated using hyperparameter tuning to achieve the best performance for a model."
  },
  {
    "objectID": "utils.html#support-for-data-augmentation-with-sdmx",
    "href": "utils.html#support-for-data-augmentation-with-sdmx",
    "title": "Utils",
    "section": "Support for data augmentation with SDMX",
    "text": "Support for data augmentation with SDMX\n\n\n\n\n\n\nNote\n\n\n\nplease note that working with SDMX may take some minutes depending on the amount of information you are downloading.\n\n\n\nlist_SDMX_sources\n\nlist_SDMX_sources ()\n\nFetches the list of SDMX sources.\n\nReturns:\n    The list of codes representing the SDMX sources available for data download.\n\nsources = list_SDMX_sources()\nprint(sources)\n\nassert len(sources) &gt; 0\n# all elements are of type 'str'\nassert sum([isinstance(src, str) for src in sources]) == len(sources)\n\n['ABS', 'ABS_XML', 'BBK', 'BIS', 'CD2030', 'ECB', 'EC_COMP', 'EC_EMPL', 'EC_GROW', 'ESTAT', 'ILO', 'IMF', 'INEGI', 'INSEE', 'ISTAT', 'LSD', 'NB', 'NBB', 'OECD', 'SGR', 'SPC', 'STAT_EE', 'UNICEF', 'UNSD', 'WB', 'WB_WDI']\n\n\n\n\nlist_all_dataflows\n\nlist_all_dataflows (codes_only: bool = False, return_pandas: bool = True)\n\nLists all SDMX dataflows. Note: When using as a parameter to an `AugmentSDMX` object\nor to the `load_SDMX_data` function, set `codes_only=True`\"\n\nArgs:\n    codes_only (bool): Whether to return only the dataflow codes.\n    return_pandas (bool): Whether to return the result in a pandas DataFrame format.\n    \nReturns:\n    All available dataflows for all SDMX sources.\n\ndflows = list_all_dataflows(return_pandas=False)\n\nassert isinstance(dflows, dict)\nall_sources = list_SDMX_sources()\nassert len([s for s in dflows.keys() if s in all_sources]) == len(dflows.keys())\n\nlist_all_dataflows returns by default a pandas Series, facilitating data discovery by users like so:\n\ndflows = list_all_dataflows(return_pandas=True)\nassert type(dflows) == pd.core.series.Series\n\ndflows\n\nABS_XML  ABORIGINAL_POP_PROJ                 Projected population, Aboriginal and Torres St...\n         ABORIGINAL_POP_PROJ_REMOTE          Projected population, Aboriginal and Torres St...\n         ABS_ABORIGINAL_POPPROJ_INDREGION    Projected population, Aboriginal and Torres St...\n         ABS_ACLD_LFSTATUS                   Australian Census Longitudinal Dataset (ACLD):...\n         ABS_ACLD_TENURE                     Australian Census Longitudinal Dataset (ACLD):...\n                                                                   ...                        \nUNSD     DF_UNData_UNFCC                                                       SDMX_GHG_UNDATA\nWB       DF_WITS_Tariff_TRAINS                                WITS - UNCTAD TRAINS Tariff Data\n         DF_WITS_TradeStats_Development                             WITS TradeStats Devlopment\n         DF_WITS_TradeStats_Tariff                                      WITS TradeStats Tariff\n         DF_WITS_TradeStats_Trade                                        WITS TradeStats Trade\nName: dataflow, Length: 2960, dtype: object\n\n\nThis format allows for more easily searching dflows by source:\n\nlist_all_dataflows(codes_only=True, return_pandas=True)\n\nABS_XML  0                 ABORIGINAL_POP_PROJ\n         1          ABORIGINAL_POP_PROJ_REMOTE\n         2    ABS_ABORIGINAL_POPPROJ_INDREGION\n         3                   ABS_ACLD_LFSTATUS\n         4                     ABS_ACLD_TENURE\n                            ...               \nUNSD     3                     DF_UNData_UNFCC\nWB       0               DF_WITS_Tariff_TRAINS\n         1      DF_WITS_TradeStats_Development\n         2           DF_WITS_TradeStats_Tariff\n         3            DF_WITS_TradeStats_Trade\nName: dataflow, Length: 10580, dtype: object\n\n\n\ndflows['BIS']\n\nWS_CBPOL_D                                    Policy rates daily\nWS_CBPOL_M                                  Policy rates monthly\nWS_CBS_PUB                              BIS consolidated banking\nWS_CPMI_CASHLESS                   CPMI cashless payments (T5-6)\nWS_CPMI_CT1                       CPMI comparative tables type 1\nWS_CPMI_CT2                       CPMI comparative tables type 2\nWS_CPMI_DEVICES                             CPMI payment devices\nWS_CPMI_INSTITUTIONS                           CPMI institutions\nWS_CPMI_MACRO                                         CPMI Macro\nWS_CPMI_PARTICIPANTS                           CPMI participants\nWS_CPMI_SYSTEMS         CPMI systems (T8-9-11-13-14-16-17-18-19)\nWS_CPP                                Commercial property prices\nWS_CREDIT_GAP                             BIS credit-to-GDP gaps\nWS_DEBT_SEC2_PUB                             BIS debt securities\nWS_DER_OTC_TOV                          OTC derivatives turnover\nWS_DPP                      Detailed residential property prices\nWS_DSR                                    BIS debt service ratio\nWS_EER_D                      BIS effective exchange rates daily\nWS_EER_M                    BIS effective exchange rates monthly\nWS_GLI                               Global liquidity indicators\nWS_LBS_D_PUB                              BIS locational banking\nWS_LONG_CPI                             BIS long consumer prices\nWS_NA_SEC_DSS                         Debt securities statistics\nWS_OTC_DERIV2                        OTC derivatives outstanding\nWS_SPP                      BIS property prices: selected series\nWS_TC                            BIS long series on total credit\nWS_XRU                           US dollar exchange rates, m,q,a\nWS_XRU_D                         US dollar exchange rates, daily\nWS_XTD_DERIV                         Exchange traded derivatives\nName: dataflow, dtype: object\n\n\nOr the user can search dataflows by their human-readable name instead of their code. For example, this is one way to see if any dataflow has information on interest rates:\n\ndflows[dflows.str.contains('Interest rates', case=False)]\n\nBBK  BBSDI    Discount interest rates pursuant to section 25...\n     BBSEI    Expectation of inflation rate and expected rea...\nECB  RIR                                  Retail Interest Rates\nName: dataflow, dtype: object\n\n\nThe function load_SDMX_data is a convenience function that downloads data from SDMX sources (and any specific dataflows passed as arguments) if they match the key and parameters set by the user.\n\n\nload_SDMX_data\n\nload_SDMX_data (sources: dict, keys: dict, params: dict, verbose: bool = True)\n\nLoads datasets from SDMX.\n\nArgs:\n    sources (dict): A dictionary with the sources and dataflows per source.\n    keys (dict): The keys to be used in the SDMX query.\n    params (dict): The parameters to be used in the SDMX query.\n    verbose (bool): Whether to communicate download steps to the user.\n    \nReturns:\n    A pandas DataFrame with data from SDMX or None if no data matches the sources, keys, and parameters.\n\ndf = load_SDMX_data(sources={'ECB': 'CISS', 'BIS': 'WS_CBPOL_D'}, keys={'FREQ': 'D'}, params={'startPeriod': 2003})\n\nassert type(df) == pd.DataFrame\nassert df.shape[0] &gt; 0\nassert df.shape[1] &gt; 0\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily..."
  },
  {
    "objectID": "barrolee1994.html",
    "href": "barrolee1994.html",
    "title": "Using gingado to understand economic growth",
    "section": "",
    "text": "This notebook showcases one possible use of gingado by estimating economic growth across countries, using the dataset studied by Barro and Lee (1994). You can run this notebook interactively, by clicking on the appropriate link above.\nThis dataset has been widely studied in economics. Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021) are two studies of this dataset that are most related to machine learning.\nThis notebook will use gingado to compare quickly setup a well-performing machine learning model and use its results as evidence to support the conditional convergence hypothesis; compare different classes of models (and their combination in a single model), and use and document the best performing alternative.\nBecause the notebook is for pedagogical purposes only, please bear in mind some aspects of the machine learning workflow (such as carefully thinking about the cross-validation strategy) are glossed over in this notebook. Also, only the key academic references are cited; more references can be found in the papers mentioned in this example.\nFor a more thorough description of gingado, please refer to the package’s website and to the academic material about it."
  },
  {
    "objectID": "barrolee1994.html#setting-the-stage",
    "href": "barrolee1994.html#setting-the-stage",
    "title": "Using gingado to understand economic growth",
    "section": "Setting the stage",
    "text": "Setting the stage\nWe will import packages as the work progresses. This will help highlight the specific steps in the workflow that gingado can be helpful with.\n\n\nCode\nimport pandas as pd\n\n\nThe data is available in the online annex to Giannone, Lenza, and Primiceri (2021). In that paper, this dataset corresponds to what the authors call “macro2”. The original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\n\nCode\nfrom gingado.datasets import load_BarroLee_1994\n\nX, y = load_BarroLee_1994()\n\n\nThe dataset contains explanatory variables representing per-capita growth between 1960 and 1985, for 90 countries.\n\n\nCode\nX.columns\n\n\nIndex(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65',\n       'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65',\n       'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n       'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65', 'highm65',\n       'highf65', 'highc65', 'highcm65', 'highcf65', 'human65', 'humanm65',\n       'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65', 'nom65', 'nof65',\n       'pinstab1', 'pop65', 'worker65', 'pop1565', 'pop6565', 'sec65',\n       'secm65', 'secf65', 'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65',\n       'syrf65', 'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'],\n      dtype='object')\n\n\n\n\nCode\nX.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nUnnamed: 0\n0.000000\n1.000000\n2.000000\n3.000000\n4.000000\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n\n\n...\n...\n...\n...\n...\n...\n\n\nteasec65\n17.300000\n18.000000\n20.700000\n22.700000\n17.600000\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n\n\n\n\n62 rows × 5 columns\n\n\n\nThe outcome variable is represented here:\n\n\nCode\ny.plot.hist(bins=90, title='GDP growth')\n\n\n&lt;Axes: title={'center': 'GDP growth'}, ylabel='Frequency'&gt;"
  },
  {
    "objectID": "barrolee1994.html#establishing-a-benchmark-model",
    "href": "barrolee1994.html#establishing-a-benchmark-model",
    "title": "Using gingado to understand economic growth",
    "section": "Establishing a benchmark model",
    "text": "Establishing a benchmark model\nGenerally speaking, it is a good idea to establish a benchmark model at the first stages of development of the machine learning model. gingado offers a class of automatic benchmarks that can be used off-the-shelf depending on the task at hand: RegressionBenchmark and ClassificationBenchmark. It is also good to keep in mind that more advanced users can create their own benchmark on top of a base class provided by gingado: ggdBenchmark.\nFor this application, since we are interested in running a regression task, we will use RegressionBenchmark:\n\n\nCode\nfrom gingado.benchmark import RegressionBenchmark\n\n\nWhat this object does is the following:\n\nit creates a random forest\nthree different versions of the random forest are trained on the user data\nthe version that performs better is chosen as the benchmark\nright after it is trained, the benchmark is documented using gingado’s ModelCard documenter.\n\nThe user can easily change the parameters above. For example, instead of a random forest the user might prefer a neural network as the benchmark. Or, in lieu of the default parameters provided by gingado, users might have their own idea of what could be a reasonable parameter space to search.\nRandom forests are chosen as the go-to benchmark algorithm because of their reasonably good performance in a wide variety of settings, the fact that they don’t require much data transformation (ie, normalising the data to have zero mean and one standard deviation), and by virtue of their relatively transparency about the importance of each regressor.\nThe first step is to initialise the benchmark object. At this time, we pass some arguments about how we want it to behave. In this case, we set the verbosity level to produce output related to each alternative considered. Then we fit it to the data.\n\n\nCode\n#####\n#####\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X, y)\n\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor() \n\n\n\n\nCode\nbenchmark = RegressionBenchmark(verbose_grid=2)\nbenchmark.fit(X, y)\n\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n\n\nRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    verbose_grid=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RegressionBenchmarkiFittedRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    verbose_grid=2) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True) \n\n\nAs we can see above, with a few lines we have trained a random forest on the dataset. In this case, the benchmark was the better of six versions of the random forest, according to the default hyperparameters: 100 and 250 estimators were alternated with models for which the maximum number of regressors analysed by individual trees changesd fom the maximum, a square root and a log of the number of regressors. They were each trained using a 5-fold cross-validation.\nLet’s see which one was the best performing in this case, and hence our benchmark model:\n\n\nCode\npd.DataFrame(benchmark.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.180656\n0.433933\n0.157642\n0.414226\n0.366611\n0.977491\n\n\nstd_fit_time\n0.005786\n0.032523\n0.00374\n0.042034\n0.003828\n0.033327\n\n\nmean_score_time\n0.0051\n0.010402\n0.005008\n0.010905\n0.004615\n0.011018\n\n\nstd_score_time\n0.00054\n0.000663\n0.000449\n0.000831\n0.000477\n0.001094\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n0.488231\n0.452523\n0.541315\n0.510664\n0.558099\n0.589899\n\n\nsplit1_test_score\n0.094273\n0.16525\n0.145656\n0.162487\n-0.030948\n0.082421\n\n\nsplit2_test_score\n0.445283\n0.346525\n0.353413\n0.338097\n0.435735\n0.396551\n\n\nsplit3_test_score\n0.304017\n0.364367\n0.268699\n0.23055\n0.491401\n0.520112\n\n\nsplit4_test_score\n-0.069719\n-0.017022\n0.009186\n-0.007687\n-0.077036\n-0.128026\n\n\nsplit5_test_score\n0.462468\n0.425335\n0.342554\n0.489489\n0.343625\n0.414221\n\n\nsplit6_test_score\n0.094724\n0.015118\n-0.023456\n0.040244\n-0.082972\n-0.009906\n\n\nsplit7_test_score\n0.170793\n0.074973\n0.150148\n0.160753\n0.252658\n0.256297\n\n\nsplit8_test_score\n0.276371\n0.27026\n0.225229\n0.204153\n0.333868\n0.390889\n\n\nsplit9_test_score\n0.05785\n0.120482\n0.165312\n0.070969\n0.466614\n0.261208\n\n\nmean_test_score\n0.232429\n0.221781\n0.217806\n0.219972\n0.269104\n0.277366\n\n\nstd_test_score\n0.182848\n0.163604\n0.159851\n0.168701\n0.233137\n0.220907\n\n\nrank_test_score\n3\n4\n6\n5\n2\n1\n\n\n\n\n\n\n\nThe values above are calculated with \\(R^2\\), the default scoring function for a random forest from the scikit-learn package. Suppose that instead we would like a benchmark model that is optimised on the maximum error, ie a benchmark that minimises the worst deviation from prediction to ground truth for all the sample. These are the steps that we would take. Note that a more complete list of ready-made scoring parameters and how to create your own function can be found here.\n\n\nCode\nbenchmark_lower_worsterror = RegressionBenchmark(scoring='max_error', verbose_grid=2)\nbenchmark_lower_worsterror.fit(X, y)\n\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n\n\nRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    scoring='max_error', verbose_grid=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RegressionBenchmarkiFittedRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    scoring='max_error', verbose_grid=2) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True) \n\n\n\n\nCode\npd.DataFrame(benchmark_lower_worsterror.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.190659\n0.432832\n0.167223\n0.414907\n0.406831\n0.973284\n\n\nstd_fit_time\n0.029873\n0.02885\n0.013814\n0.025611\n0.040256\n0.056742\n\n\nmean_score_time\n0.004899\n0.011402\n0.005108\n0.011512\n0.005494\n0.010704\n\n\nstd_score_time\n0.000944\n0.00229\n0.000824\n0.002419\n0.001916\n0.001099\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n-0.141305\n-0.138216\n-0.128377\n-0.136267\n-0.151263\n-0.148743\n\n\nsplit1_test_score\n-0.138384\n-0.136289\n-0.126467\n-0.13922\n-0.147547\n-0.14993\n\n\nsplit2_test_score\n-0.071616\n-0.066368\n-0.064452\n-0.069436\n-0.083584\n-0.076518\n\n\nsplit3_test_score\n-0.03612\n-0.041857\n-0.036953\n-0.036624\n-0.047582\n-0.038582\n\n\nsplit4_test_score\n-0.058005\n-0.049058\n-0.05539\n-0.052265\n-0.052295\n-0.048199\n\n\nsplit5_test_score\n-0.095987\n-0.089092\n-0.089168\n-0.088889\n-0.106108\n-0.103971\n\n\nsplit6_test_score\n-0.045346\n-0.05428\n-0.0472\n-0.046268\n-0.049821\n-0.045507\n\n\nsplit7_test_score\n-0.056903\n-0.063544\n-0.065105\n-0.063613\n-0.068414\n-0.060603\n\n\nsplit8_test_score\n-0.071264\n-0.070998\n-0.079182\n-0.073086\n-0.067835\n-0.068566\n\n\nsplit9_test_score\n-0.064886\n-0.067567\n-0.065912\n-0.0654\n-0.061306\n-0.054018\n\n\nmean_test_score\n-0.077982\n-0.077727\n-0.075821\n-0.077107\n-0.083575\n-0.079464\n\n\nstd_test_score\n0.034497\n0.032178\n0.029336\n0.033314\n0.036836\n0.039113\n\n\nrank_test_score\n4\n3\n1\n2\n6\n5\n\n\n\n\n\n\n\nNow we even have two benchmark models.\nWe could further tweak and adjust them, but one of the ideas behind having a benchmark is that it is simple and easy to set up.\nLet’s retain only the first benchmark, for simplicity, and now look at the predictions, comparing them to the original growth values.\n\n\nCode\ny_pred = benchmark.predict(X)\n\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(\n        x='y', y='y_pred',\n         grid=True, \n         title='Actual and predicted outcome',\n         xlabel='actual GDP growth',\n         ylabel='predicted GDP growth')\n\n\n&lt;Axes: title={'center': 'Actual and predicted outcome'}, xlabel='actual GDP growth', ylabel='predicted GDP growth'&gt;\n\n\n\n\n\nAnd now a histogram of the benchmark’s errors:\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30, title='Residual')\n\n\n&lt;Axes: title={'center': 'Residual'}, ylabel='Frequency'&gt;\n\n\n\n\n\nSince the benchmark is a random forest model, we can see what are the most important regressors, measured as the average reduction in impurity across the trees in the random forest that actually use that particular regressor. They are scaled so that the sum for all features is one. Higher importance amounts indicate that that particular regressor is a more important contributor to the final prediction.\n\n\nCode\nregressor_importance = pd.DataFrame(\n    benchmark.benchmark.best_estimator_.feature_importances_, \n    index=X.columns, \n    columns=[\"Importance\"]\n    )\n\nregressor_importance.sort_values(by=\"Importance\", ascending=False) \\\n    .plot.bar(figsize=(20, 8), title='Regressor importance')\n\n\n&lt;Axes: title={'center': 'Regressor importance'}&gt;\n\n\n\n\n\nFrom the graph above, we can see that the regressor bmp1l (black-market premium on foreign exchange) predominates. Interestingly, Belloni, Chernozhukov, and Hansen (2011) using squared-root lasso also find this regressor to be important."
  },
  {
    "objectID": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "href": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "title": "Using gingado to understand economic growth",
    "section": "Testing the conditional converge hypothesis",
    "text": "Testing the conditional converge hypothesis\nNow we can leverage our automatic benchmark model to test the conditional converge hypothesis - ie, the preposition that countries with lower starting GDP tend to grow faster than other comparable countries. In other words, this hypothesis predicts that when GDP growth is regressed on the level of past GDP and on an adequate set of covariates \\(X\\), the coefficient on past GDP levels are negative.\nSince we have the results for the importance of each regressor in separating countries by their growth result, we can compare the estimated coefficient for GDP levels in regressions that include different regressors in the vector \\(X\\). To maintain this example a simple exercise, the following three models are estimated:\n\n\\(X\\) contains the five most important regressors, as estimated by the benchmark model (see the graph above)\n\\(X\\) contains the five least important regressors, from the same estimation as above\n\\(X\\) is the empty set - in other words, this is a simple equation on GDP growth on GDP levels\n\nA result that would be consistent with the conditionality of the conditional convergence hypothesis is the first equation resulting in a negative coefficient for starting GDP, while the following two equations may not necessarily be successful in identifying a negative coefficient. This is because the least important regressors are not likely to have sufficient predictive power to separate countries into comparable groups.\nThe five more and less important regressors are:\n\n\nCode\ntop_five = regressor_importance.sort_values(by=\"Importance\", ascending=False).head(5)\nbottom_five = regressor_importance.sort_values(by=\"Importance\", ascending=True).head(5)\n\ntop_five, bottom_five\n\n\n(            Importance\n bmp1l         0.168681\n pop6565       0.110742\n teasec65      0.050551\n Unnamed: 0    0.039252\n gpop1         0.035134,\n           Importance\n highc65     0.002367\n s65         0.002580\n hm65        0.002730\n humanf65    0.002939\n human65     0.002979)\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n\n\n\nCode\ngdp_level = 'gdpsh465'\n\n\n\n\nCode\nX_topfive = X[[gdp_level] + list(top_five.index)]\nX_topfive = sm.add_constant(X_topfive)\nX_topfive.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nbmp1l\npop6565\nteasec65\nUnnamed: 0\ngpop1\n\n\n\n\n0\n1.0\n6.591674\n0.2837\n0.027591\n17.3\n0\n0.0203\n\n\n1\n1.0\n6.829794\n0.6141\n0.035637\n18.0\n1\n0.0185\n\n\n2\n1.0\n8.895082\n0.0000\n0.076685\n20.7\n2\n0.0188\n\n\n3\n1.0\n7.565275\n0.1997\n0.031039\n22.7\n3\n0.0345\n\n\n4\n1.0\n7.162397\n0.1740\n0.026281\n17.6\n4\n0.0310\n\n\n\n\n\n\n\n\n\nCode\nX_bottomfive = X[[gdp_level] + list(bottom_five.index)]\nX_bottomfive = sm.add_constant(X_bottomfive)\nX_bottomfive.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nhighc65\ns65\nhm65\nhumanf65\nhuman65\n\n\n\n\n0\n1.0\n6.591674\n0.09\n0.04\n0.013\n0.043\n0.301\n\n\n1\n1.0\n6.829794\n0.63\n0.16\n0.032\n0.257\n0.706\n\n\n2\n1.0\n8.895082\n4.50\n0.56\n0.325\n8.384\n8.317\n\n\n3\n1.0\n7.565275\n2.11\n0.24\n0.070\n3.807\n3.833\n\n\n4\n1.0\n7.162397\n0.45\n0.17\n0.027\n1.720\n1.900\n\n\n\n\n\n\n\n\n\nCode\nX_onlyGDPlevel = sm.add_constant(X[gdp_level])\nX_onlyGDPlevel.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\n\n\n\n\n0\n1.0\n6.591674\n\n\n1\n1.0\n6.829794\n\n\n2\n1.0\n8.895082\n\n\n3\n1.0\n7.565275\n\n\n4\n1.0\n7.162397\n\n\n\n\n\n\n\n\n\nCode\nmodels = dict(\n    topfive = sm.OLS(y, X_topfive).fit(),\n    bottomfive = sm.OLS(y, X_bottomfive).fit(),\n    onlyGDPlevel = sm.OLS(y, X_onlyGDPlevel).fit()\n)\n\n\n\n\nCode\ncoefs = pd.DataFrame({name: model.conf_int().loc[gdp_level] for name, model in models.items()})\ncoefs.loc[0.5] = [model.params[gdp_level] for _, model in models.items()]\ncoefs = coefs.sort_index().reset_index(drop=True)\ncoefs.index = ['[0.025', 'coef on GDP levels', '0.975]']\ncoefs\n\n\n\n\n\n\n\n\n\ntopfive\nbottomfive\nonlyGDPlevel\n\n\n\n\n[0.025\n-0.033143\n-0.022365\n-0.010810\n\n\ncoef on GDP levels\n-0.016378\n0.001215\n0.001317\n\n\n0.975]\n0.000387\n0.024795\n0.013444\n\n\n\n\n\n\n\nThe equation using the top five regressors in explanatory power yielded a coefficient that is statistically speaking negative under the usual confidence interval levels. In contrast, the regression using the bottom five regressors failed to maintain that level of statistical significance (although the coefficient point estimate was still negative). And finally the regression on GDP level solely resulted, as in the past literature, on a point estimate that is also statistically not different than zero.\nThese results above offer a different way to add evidence to the conditional convergence hypothesis. In particular, with the help of gingado’s RegressionBenchmark model, it is possible to identify which covariates can meaningfully serve as covariates in a growth equation from those that cannot. This is important because if the covariate selection for some reason included only variables with little explanatory power instead of the most relevant ones, an economist might erroneously reach a different conclusion."
  },
  {
    "objectID": "barrolee1994.html#model-documentation",
    "href": "barrolee1994.html#model-documentation",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nImportantly for model documentation, the benchmark already has some baseline documentation set up. If the user wishes, they can use that as a basis to document their model. Note that the output is in a raw format that is suitable for machine reading and writing. Intermediary and advanced users may wish to use that format to construct personalised forms, documents, etc.\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-22 15:03:03 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': RandomForestRegressor(max_features=None, n_estimators=250, oob_score=True),\n   'best_index_': 5,\n   'best_params_': {'max_features': None, 'n_estimators': 250},\n   'best_score_': 0.2773664821278155,\n   'cv_results_': {'mean_fit_time': array([0.18065588, 0.4339329 , 0.1576416 , 0.41422613, 0.36661069,\n           0.97749126]),\n    'std_fit_time': array([0.00578563, 0.03252342, 0.00374015, 0.04203366, 0.00382845,\n           0.03332728]),\n    'mean_score_time': array([0.00510006, 0.01040199, 0.00500774, 0.01090472, 0.00461476,\n           0.01101763]),\n    'std_score_time': array([0.00053967, 0.00066316, 0.00044908, 0.00083058, 0.00047684,\n           0.00109365]),\n    'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n                 mask=[False, False, False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'params': [{'max_features': 'sqrt', 'n_estimators': 100},\n     {'max_features': 'sqrt', 'n_estimators': 250},\n     {'max_features': 'log2', 'n_estimators': 100},\n     {'max_features': 'log2', 'n_estimators': 250},\n     {'max_features': None, 'n_estimators': 100},\n     {'max_features': None, 'n_estimators': 250}],\n    'split0_test_score': array([0.48823084, 0.45252256, 0.54131489, 0.51066358, 0.55809881,\n           0.58989893]),\n    'split1_test_score': array([ 0.09427254,  0.16524974,  0.1456563 ,  0.16248727, -0.03094832,\n            0.08242116]),\n    'split2_test_score': array([0.44528289, 0.34652534, 0.35341344, 0.33809688, 0.43573451,\n           0.39655133]),\n    'split3_test_score': array([0.30401701, 0.36436664, 0.26869911, 0.23055014, 0.49140112,\n           0.5201117 ]),\n    'split4_test_score': array([-0.06971919, -0.01702215,  0.00918628, -0.00768706, -0.07703643,\n           -0.12802637]),\n    'split5_test_score': array([0.46246775, 0.42533494, 0.3425543 , 0.48948866, 0.34362499,\n           0.41422112]),\n    'split6_test_score': array([ 0.09472367,  0.01511834, -0.02345592,  0.04024407, -0.0829722 ,\n           -0.00990625]),\n    'split7_test_score': array([0.17079347, 0.074973  , 0.15014766, 0.16075304, 0.25265825,\n           0.25629658]),\n    'split8_test_score': array([0.27637099, 0.27025966, 0.22522851, 0.20415296, 0.33386846,\n           0.39088863]),\n    'split9_test_score': array([0.05785005, 0.12048241, 0.16531236, 0.07096945, 0.46661358,\n           0.26120801]),\n    'mean_test_score': array([0.232429  , 0.22178105, 0.21780569, 0.2199719 , 0.26910428,\n           0.27736648]),\n    'std_test_score': array([0.18284834, 0.16360441, 0.15985088, 0.16870103, 0.23313736,\n           0.22090744]),\n    'rank_test_score': array([3, 4, 6, 5, 2, 1])},\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 10,\n   'refit_time_': 1.138108491897583,\n   'scorer_': &lt;sklearn.metrics._scorer._PassthroughScorer at 0x1ad6720e2b0&gt;},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nSince there is some information in the model documentation that was automatically added, we might want to concentrate on the fields in the model card that are yet to be answered. Actually, this is the purpose of gingado’s automatic documentation: to afford users more time so they can invest, if they want, on model documentation.\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nLet’s fill some information:\n\n\nCode\nbenchmark.model_documentation.fill_info({\n    'intended_use': {\n        'primary_uses': 'This model is trained for pedagogical uses only.',\n        'primary_users': 'Everyone is welcome to follow the description showing the development of this benchmark.'\n    }\n})\n\n\nNote the format, based on a Python dictionary. In particular, the open_questions method results include keys divided by double underscores. As seen above, these should be interpreted as different levels of the documentation template, leading to a nested dictionary.\nNow when we confirm that the questions answered above are no longer “open questions”:\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nIf we want, at any time we can save the documentation to a local JSON file, as well as read another document."
  },
  {
    "objectID": "barrolee1994.html#trying-out-model-alternatives",
    "href": "barrolee1994.html#trying-out-model-alternatives",
    "title": "Using gingado to understand economic growth",
    "section": "Trying out model alternatives",
    "text": "Trying out model alternatives\nThe benchmark model may be enough for some analyses, or maybe the user is interested in using the benchmark to explore the data and have an understanding of the importance of each regressor, to concentrate their work on data that can be meaningful for their purposes. But oftentimes a user will want to seek a machine learning model that performs as well as possible.\nFor users that want to manually create other models, gingado allows the possibility of comparing them with the benchmark. If the user model is better, it becomes the new benchmark!\nFor the following analyses, we will use K-fold as cross-validation, with 5 splits of the sample.\n\nFirst candidate: a gradient boosting tree\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n\n\nCode\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.25],\n    'max_depth': [3, 6, 9]\n}\n\nreg_gradbooster = GradientBoostingRegressor()\n\ngradboosterg_grid = GridSearchCV(\n    reg_gradbooster,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n\n\n\n\nCode\ny_pred = gradboosterg_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n&lt;Axes: xlabel='y', ylabel='y_pred'&gt;\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n\nSecond candidate: lasso\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nparam_grid = {\n    'alpha': [0.5, 1, 1.25],\n}\n\nreg_lasso = Lasso(fit_intercept=True)\n\nlasso_grid = GridSearchCV(\n    reg_lasso,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n\n\n\n\nCode\ny_pred = lasso_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n&lt;Axes: xlabel='y', ylabel='y_pred'&gt;\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "href": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "title": "Using gingado to understand economic growth",
    "section": "Comparing the models with the benchmark",
    "text": "Comparing the models with the benchmark\ngingado allows users to compare different candidate models with the existing benchmark in a very simple way: using the compare method.\n\n\nCode\ncandidates = [gradboosterg_grid, lasso_grid]\nbenchmark.compare(X, y, candidates)\n\n\nFitting 10 folds for each of 4 candidates, totalling 40 fits\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  26.3s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  24.7s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  25.1s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  24.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  24.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  24.8s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  24.8s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  25.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  25.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  25.5s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.6s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.3s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.1s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.8s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.3s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   4.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.2s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   3.8s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  28.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  28.7s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  28.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  27.7s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  29.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  30.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  28.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  29.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  27.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.2s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.7s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  28.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.8s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\n[CV] END ................max_features=None, n_estimators=250; total time=   1.0s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.9s\nBenchmark updated!\nNew benchmark:\nPipeline(steps=[('candidate_estimator',\n                 GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                              estimator=RandomForestRegressor(oob_score=True),\n                              param_grid={'max_features': ['sqrt', 'log2',\n                                                           None],\n                                          'n_estimators': [100, 250]},\n                              verbose=2))])\n\n\nThe output above clearly indicates that after evaluating the models - and their ensemble together with the existing benchmark - at least one of them was better than the current benchmark. Therefore, it will now be the new benchmark.\n\n\nCode\ny_pred = benchmark.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n&lt;Axes: xlabel='y', ylabel='y_pred'&gt;\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "barrolee1994.html#model-documentation-1",
    "href": "barrolee1994.html#model-documentation-1",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nAfter this process, we can now see how the model documentation was updated automatically:\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-22 15:13:48 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': Pipeline(steps=[('candidate_estimator',\n                    GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                 estimator=RandomForestRegressor(oob_score=True),\n                                 param_grid={'max_features': ['sqrt', 'log2',\n                                                              None],\n                                             'n_estimators': [100, 250]},\n                                 verbose=2))]),\n   'best_index_': 0,\n   'best_params_': {'candidate_estimator': GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                 estimator=RandomForestRegressor(oob_score=True),\n                 param_grid={'max_features': ['sqrt', 'log2', None],\n                             'n_estimators': [100, 250]},\n                 verbose=2),\n    'candidate_estimator__cv': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n    'candidate_estimator__error_score': nan,\n    'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n    'candidate_estimator__estimator__bootstrap': True,\n    'candidate_estimator__estimator__ccp_alpha': 0.0,\n    'candidate_estimator__estimator__criterion': 'squared_error',\n    'candidate_estimator__estimator__max_depth': None,\n    'candidate_estimator__estimator__max_features': 1.0,\n    'candidate_estimator__estimator__max_leaf_nodes': None,\n    'candidate_estimator__estimator__max_samples': None,\n    'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n    'candidate_estimator__estimator__min_samples_leaf': 1,\n    'candidate_estimator__estimator__min_samples_split': 2,\n    'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n    'candidate_estimator__estimator__monotonic_cst': None,\n    'candidate_estimator__estimator__n_estimators': 100,\n    'candidate_estimator__estimator__n_jobs': None,\n    'candidate_estimator__estimator__oob_score': True,\n    'candidate_estimator__estimator__random_state': None,\n    'candidate_estimator__estimator__verbose': 0,\n    'candidate_estimator__estimator__warm_start': False,\n    'candidate_estimator__n_jobs': None,\n    'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n     'max_features': ['sqrt', 'log2', None]},\n    'candidate_estimator__pre_dispatch': '2*n_jobs',\n    'candidate_estimator__refit': True,\n    'candidate_estimator__return_train_score': False,\n    'candidate_estimator__scoring': None,\n    'candidate_estimator__verbose': 2},\n   'best_score_': -0.052280020300051966,\n   'cv_results_': {'mean_fit_time': array([25.21748474,  3.60295188,  0.05851879, 28.7659409 ]),\n    'std_fit_time': array([0.54436684, 0.34820704, 0.00269353, 0.92445408]),\n    'mean_score_time': array([0.01011012, 0.00240815, 0.00190234, 0.01321754]),\n    'std_score_time': array([0.00262368, 0.0004848 , 0.00030165, 0.00349578]),\n    'param_candidate_estimator': masked_array(data=[GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                    estimator=RandomForestRegressor(oob_score=True),\n                                    param_grid={'max_features': ['sqrt', 'log2', None],\n                                                'n_estimators': [100, 250]},\n                                    verbose=2)                                                                       ,\n                       GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                                    param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                                                'max_depth': [3, 6, 9]},\n                                    verbose=2)                                       ,\n                       GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                                    verbose=2)                                                         ,\n                       VotingRegressor(estimators=[('candidate_1',\n                                                    GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                                                 estimator=RandomForestRegressor(oob_score=True),\n                                                                 param_grid={'max_features': ['sqrt',\n                                                                                              'log2',\n                                                                                              None],\n                                                                             'n_estimators': [100,\n                                                                                              250]},\n                                                                 verbose=2)),\n                                                   ('candidate_2',\n                                                    GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                                 n_jobs=-1,\n                                                                 param_grid={'learning_rate': [0.01,\n                                                                                               0.1,\n                                                                                               0.25],\n                                                                             'max_depth': [3, 6, 9]},\n                                                                 verbose=2)),\n                                                   ('candidate_3',\n                                                    GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                                 param_grid={'alpha': [0.5, 1, 1.25]},\n                                                                 verbose=2))])                                                                    ],\n                 mask=[False, False, False, False],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__cv': masked_array(data=[ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                       None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__error_score': masked_array(data=[nan, nan, nan, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator': masked_array(data=[RandomForestRegressor(oob_score=True),\n                       GradientBoostingRegressor(), Lasso(), --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__bootstrap': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__ccp_alpha': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__criterion': masked_array(data=['squared_error', 'friedman_mse', --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_depth': masked_array(data=[None, 3, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_features': masked_array(data=[1.0, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_leaf_nodes': masked_array(data=[None, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_samples': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_impurity_decrease': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_samples_leaf': masked_array(data=[1, 1, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_samples_split': masked_array(data=[2, 2, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__min_weight_fraction_leaf': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__monotonic_cst': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_estimators': masked_array(data=[100, 100, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_jobs': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__oob_score': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__random_state': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__verbose': masked_array(data=[0, 0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__warm_start': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__n_jobs': masked_array(data=[None, -1, -1, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__param_grid': masked_array(data=[{'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]},\n                       {'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]},\n                       {'alpha': [0.5, 1, 1.25]}, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__pre_dispatch': masked_array(data=['2*n_jobs', '2*n_jobs', '2*n_jobs', --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__refit': masked_array(data=[True, True, True, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__return_train_score': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__scoring': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__verbose': masked_array(data=[2, 2, 2, --],\n                 mask=[False, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__alpha': masked_array(data=[--, 0.9, 1.0, --],\n                 mask=[ True, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__init': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__learning_rate': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__loss': masked_array(data=[--, 'squared_error', --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__n_iter_no_change': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__subsample': masked_array(data=[--, 1.0, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__tol': masked_array(data=[--, 0.0001, 0.0001, --],\n                 mask=[ True, False, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__validation_fraction': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__copy_X': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__fit_intercept': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__max_iter': masked_array(data=[--, --, 1000, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__positive': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__precompute': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'param_candidate_estimator__estimator__selection': masked_array(data=[--, --, 'cyclic', --],\n                 mask=[ True,  True, False,  True],\n           fill_value='?',\n                dtype=object),\n    'params': [{'candidate_estimator': GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                   estimator=RandomForestRegressor(oob_score=True),\n                   param_grid={'max_features': ['sqrt', 'log2', None],\n                               'n_estimators': [100, 250]},\n                   verbose=2),\n      'candidate_estimator__cv': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n      'candidate_estimator__estimator__bootstrap': True,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'squared_error',\n      'candidate_estimator__estimator__max_depth': None,\n      'candidate_estimator__estimator__max_features': 1.0,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__max_samples': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__monotonic_cst': None,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_jobs': None,\n      'candidate_estimator__estimator__oob_score': True,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': None,\n      'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n       'max_features': ['sqrt', 'log2', None]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                   param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                               'max_depth': [3, 6, 9]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': GradientBoostingRegressor(),\n      'candidate_estimator__estimator__alpha': 0.9,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'friedman_mse',\n      'candidate_estimator__estimator__init': None,\n      'candidate_estimator__estimator__learning_rate': 0.1,\n      'candidate_estimator__estimator__loss': 'squared_error',\n      'candidate_estimator__estimator__max_depth': 3,\n      'candidate_estimator__estimator__max_features': None,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_iter_no_change': None,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__subsample': 1.0,\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__validation_fraction': 0.1,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'learning_rate': [0.01, 0.1, 0.25],\n       'max_depth': [3, 6, 9]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': Lasso(),\n      'candidate_estimator__estimator__alpha': 1.0,\n      'candidate_estimator__estimator__copy_X': True,\n      'candidate_estimator__estimator__fit_intercept': True,\n      'candidate_estimator__estimator__max_iter': 1000,\n      'candidate_estimator__estimator__positive': False,\n      'candidate_estimator__estimator__precompute': False,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__selection': 'cyclic',\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'alpha': [0.5, 1, 1.25]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': VotingRegressor(estimators=[('candidate_1',\n                                   GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                                estimator=RandomForestRegressor(oob_score=True),\n                                                param_grid={'max_features': ['sqrt',\n                                                                             'log2',\n                                                                             None],\n                                                            'n_estimators': [100,\n                                                                             250]},\n                                                verbose=2)),\n                                  ('candidate_2',\n                                   GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                n_jobs=-1,\n                                                param_grid={'learning_rate': [0.01,\n                                                                              0.1,\n                                                                              0.25],\n                                                            'max_depth': [3, 6, 9]},\n                                                verbose=2)),\n                                  ('candidate_3',\n                                   GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                param_grid={'alpha': [0.5, 1, 1.25]},\n                                                verbose=2))])}],\n    'split0_test_score': array([-0.04375761,  0.11588478, -0.3013862 ,  0.15052963]),\n    'split1_test_score': array([-0.44061565, -0.8533899 , -0.44245366, -0.65248371]),\n    'split2_test_score': array([ 0.12820278,  0.18465863, -0.06540152,  0.13742974]),\n    'split3_test_score': array([ 0.147597  ,  0.26305786, -0.20556609,  0.1969557 ]),\n    'split4_test_score': array([ 0.05419552, -0.34731968,  0.1152951 , -0.00468908]),\n    'split5_test_score': array([-0.87849578, -1.20795204, -5.72700107, -1.19795522]),\n    'split6_test_score': array([-0.18576575, -0.21015057, -0.46969364, -0.05576571]),\n    'split7_test_score': array([0.39361204, 0.35311566, 0.01627654, 0.30232847]),\n    'split8_test_score': array([ 0.16682922,  0.10472204, -0.04758619,  0.0964844 ]),\n    'split9_test_score': array([ 0.13539802, -0.0302966 , -0.07093014,  0.12540423]),\n    'mean_test_score': array([-0.05228002, -0.16276698, -0.71984469, -0.09017616]),\n    'std_test_score': array([0.34888991, 0.4837963 , 1.67902879, 0.44490693]),\n    'rank_test_score': array([1, 3, 4, 2])},\n   'feature_names_in_': array(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65',\n          'hm65', 'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65',\n          'fert65', 'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1',\n          'invsh41', 'geetot1', 'geerec1', 'gde1', 'govwb1', 'govsh41',\n          'gvxdxe41', 'high65', 'highm65', 'highf65', 'highc65', 'highcm65',\n          'highcf65', 'human65', 'humanm65', 'humanf65', 'hyr65', 'hyrm65',\n          'hyrf65', 'no65', 'nom65', 'nof65', 'pinstab1', 'pop65',\n          'worker65', 'pop1565', 'pop6565', 'sec65', 'secm65', 'secf65',\n          'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65', 'syrf65',\n          'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'], dtype=object),\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 10,\n   'refit_time_': 26.721534252166748,\n   'scorer_': &lt;sklearn.metrics._scorer._PassthroughScorer at 0x1ad68ac8a60&gt;},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nAnd as before, any remaining open questions can be viewed and answered using the same methods as above."
  },
  {
    "objectID": "barrolee1994.html#references",
    "href": "barrolee1994.html#references",
    "title": "Using gingado to understand economic growth",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37."
  },
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "Using gingado to forecast financial series",
    "section": "",
    "text": "This notebook illustrates the use of gingado to build models for forecasting, using foreign exchange (FX) rate movements as an example. Please note that the results or the model should not be taken as investment advice.\nForecasting exchange rates is notoriously difficult (Rossi (2013) and references therein).\nThis exercise will illustrate various functionalities provided by gingado:\nUnlike most scripts that concentrate the package imports at the beginning, this walkthrough will import as needed, to better highlight where each contribution of gingado is used in the workflow.\nFirst, we will use gingado to run a simple example with the following characteristics:"
  },
  {
    "objectID": "forecast.html#downloading-fx-rates",
    "href": "forecast.html#downloading-fx-rates",
    "title": "Using gingado to forecast financial series",
    "section": "Downloading FX rates",
    "text": "Downloading FX rates\nIn this exercise, we will concentrate on the bilateral FX rates between the 🇺🇸 US Dollar (USD) and the 🇧🇷 Brazilian Real (BRL), 🇨🇦 Canadian Dollar (CAD), 🇨🇭 Swiss Franc (CHF), 🇪🇺 Euro (EUR), 🇬🇧 British Pound (GBP), 🇯🇵 Japanese Yen (JPY) and 🇲🇽 Mexican Peso (MXN).\nThe rates are standardised to measure the units in foreign currency bought by one USD. Therefore, positive returns represent USD is more valued compared to the other currency, and vice-versa.\n\n\nCode\nfrom gingado.utils import load_SDMX_data\n\n\n\n\nCode\ndf = load_SDMX_data(\n    sources={'BIS': 'WS_XRU_D'},\n    keys={\n        'FREQ': 'D', \n        'CURRENCY': ['BRL', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'MXN'],\n        'REF_AREA': ['BR', 'CA', 'CH', 'XM', 'GB', 'JP', 'MX']\n        },\n    params={'startPeriod': 2003}\n)\n\n\nQuerying data from BIS's dataflow 'WS_XRU' - US dollar exchange rates, m,q,a...\nthis dataflow does not have data in the desired frequency and time period.\nQuerying data from BIS's dataflow 'WS_XRU_D' - US dollar exchange rates, daily...\n\n\nThe code below simplifies the column names by removing the identification of the SDMX sources, dataflows and keys and replacing it with the usual code for the bilateral exchange rates.\n\n\nCode\nprint(\"Original column names:\")\nprint(df.columns)\n\ndf.columns = ['USD' + col.split('_')[9] for col in df.columns]\n\nprint(\"New column names:\")\nprint(df.columns)\n\n\nOriginal column names:\nIndex(['BIS__WS_XRU_D_D__MX__MXN__A', 'BIS__WS_XRU_D_D__JP__JPY__A',\n       'BIS__WS_XRU_D_D__XM__EUR__A', 'BIS__WS_XRU_D_D__CH__CHF__A',\n       'BIS__WS_XRU_D_D__BR__BRL__A', 'BIS__WS_XRU_D_D__GB__GBP__A',\n       'BIS__WS_XRU_D_D__CA__CAD__A'],\n      dtype='object')\nNew column names:\nIndex(['USDMXN', 'USDJPY', 'USDEUR', 'USDCHF', 'USDBRL', 'USDGBP', 'USDCAD'], dtype='object')\n\n\nThe dataset looks like this so far (most recent 5 rows displayed only):\n\n\nCode\ndf.tail()\n\n\n\n\n\n\n\n\n\nUSDMXN\nUSDJPY\nUSDEUR\nUSDCHF\nUSDBRL\nUSDGBP\nUSDCAD\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n2024-02-13\n17.073288\n149.328268\n0.926526\n0.878440\n4.953674\n0.788455\n1.344483\n\n\n2024-02-14\n17.134229\n150.546066\n0.933445\n0.886120\n4.953701\n0.795837\n1.354336\n\n\n2024-02-15\n17.086661\n150.107046\n0.930839\n0.882807\n4.970772\n0.797124\n1.354091\n\n\n2024-02-16\n17.044577\n150.334324\n0.928678\n0.881408\n4.973161\n0.794994\n1.348254\n\n\n2024-02-19\n17.046956\n149.953601\n0.927988\n0.880846\n4.958519\n0.792947\n1.347624\n\n\n\n\n\n\n\nWe are interested in the percentage change from the previous day.\n\n\nCode\nFX_rate_changes = df.pct_change()\nFX_rate_changes.dropna(inplace=True)\n\n\n\n\nCode\nFX_rate_changes.plot(subplots=True, layout=(4, 2), figsize=(15, 15), sharex=True, title='Selected daily FX rate changes')\n\n\narray([[&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "forecast.html#augmenting-the-dataset",
    "href": "forecast.html#augmenting-the-dataset",
    "title": "Using gingado to forecast financial series",
    "section": "Augmenting the dataset",
    "text": "Augmenting the dataset\nWe will complement the FX rates data with two other datasets:\n\ndaily central bank policy rates from the Bank for International Settlements (BIS) (2017), and\nthe daily Composite Indicator of Systemic Stress (CISS), created by Hollo, Kremer, and Lo Duca (2012) and updated by the European Central Bank (ECB).\n\n\n\nCode\nfrom gingado.augmentation import AugmentSDMX\n\n\n\n\nCode\nX = AugmentSDMX(sources={'BIS': 'WS_CBPOL_D', 'ECB': 'CISS'}).fit_transform(FX_rate_changes)\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL_D' - Policy rates daily...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n\n\n\n\n\n\nNote\n\n\n\nit is acceptable in gingado to pass the variable of interest (the “y”, or in this case, FX_rate_changes) as the X argument in fit_transform. This is because this series will also be merged with the additional, augmented data and subsequently lagged along with it.\n\n\nYou can see below that the column names for the newly added columns reflect the source (BIS or ECB), the dataflow (separated from the source by a double underline), and then the specific keys to the series, which are specific to each dataflow.\n\n\nCode\nX.columns\n\n\nIndex(['USDMXN', 'USDJPY', 'USDEUR', 'USDCHF', 'USDBRL', 'USDGBP', 'USDCAD',\n       'BIS__WS_CBPOL_D_D__CH', 'BIS__WS_CBPOL_D_D__CL',\n       'BIS__WS_CBPOL_D_D__CN', 'BIS__WS_CBPOL_D_D__CO',\n       'BIS__WS_CBPOL_D_D__CZ', 'BIS__WS_CBPOL_D_D__DK',\n       'BIS__WS_CBPOL_D_D__GB', 'BIS__WS_CBPOL_D_D__HK',\n       'BIS__WS_CBPOL_D_D__HR', 'BIS__WS_CBPOL_D_D__HU',\n       'BIS__WS_CBPOL_D_D__ID', 'BIS__WS_CBPOL_D_D__IL',\n       'BIS__WS_CBPOL_D_D__IN', 'BIS__WS_CBPOL_D_D__IS',\n       'BIS__WS_CBPOL_D_D__JP', 'BIS__WS_CBPOL_D_D__AR',\n       'BIS__WS_CBPOL_D_D__KR', 'BIS__WS_CBPOL_D_D__MA',\n       'BIS__WS_CBPOL_D_D__MK', 'BIS__WS_CBPOL_D_D__MX',\n       'BIS__WS_CBPOL_D_D__BR', 'BIS__WS_CBPOL_D_D__MY',\n       'BIS__WS_CBPOL_D_D__NO', 'BIS__WS_CBPOL_D_D__NZ',\n       'BIS__WS_CBPOL_D_D__PE', 'BIS__WS_CBPOL_D_D__PH',\n       'BIS__WS_CBPOL_D_D__CA', 'BIS__WS_CBPOL_D_D__PL',\n       'BIS__WS_CBPOL_D_D__AU', 'BIS__WS_CBPOL_D_D__RO',\n       'BIS__WS_CBPOL_D_D__RS', 'BIS__WS_CBPOL_D_D__RU',\n       'BIS__WS_CBPOL_D_D__SA', 'BIS__WS_CBPOL_D_D__SE',\n       'BIS__WS_CBPOL_D_D__TH', 'BIS__WS_CBPOL_D_D__TR',\n       'BIS__WS_CBPOL_D_D__US', 'BIS__WS_CBPOL_D_D__XM',\n       'BIS__WS_CBPOL_D_D__ZA', 'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\nBefore proceeding, we also include a differentiated version of the central bank policy data. It will be sparse, since these changes occur infrequently for most central banks, but it can help the model uncover how FX rate changes respond to central bank policy changes.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nX_diff = X.loc[:, X.columns.str.contains(\"BIS__WS_CBPOL_D\", case=False)].diff()\nX_diff.columns = [col + \"_diff\" for col in X_diff.columns]\nX = pd.concat([X, X_diff], axis=1)\n\n\nThis is how the data looks like now. Note that the names of the added columns reflect the source, dataflow and keys, all separated by underlines (the source is separated from the dataflow by two underlines at all cases). For example, the last key is the jurisdiction of the central bank.\nWe will keep all the newly added variables - even those that are from countries not in the currency list. This is because the model may uncover any relationship of interest between central bank policies from other countries and each particular currency pair.\n\n\nCode\nX.describe().transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUSDMXN\n5461.0\n0.000124\n0.008089\n-0.070532\n-0.003982\n-0.000213\n0.003857\n0.118877\n\n\nUSDJPY\n5461.0\n0.000061\n0.006081\n-0.044831\n-0.003006\n0.000151\n0.003205\n0.032901\n\n\nUSDEUR\n5461.0\n0.000011\n0.005712\n-0.039574\n-0.003121\n0.000000\n0.003042\n0.048493\n\n\nUSDCHF\n5461.0\n-0.000063\n0.006389\n-0.139149\n-0.003251\n0.000011\n0.003225\n0.085326\n\n\nUSDBRL\n5461.0\n0.000117\n0.010476\n-0.080226\n-0.005706\n-0.000047\n0.005351\n0.120503\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nBIS__WS_CBPOL_D_D__TH_diff\n5460.0\n0.000137\n0.033666\n-1.000000\n0.000000\n0.000000\n0.000000\n0.500000\n\n\nBIS__WS_CBPOL_D_D__TR_diff\n5460.0\n0.000183\n0.309450\n-4.250000\n0.000000\n0.000000\n0.000000\n8.500000\n\n\nBIS__WS_CBPOL_D_D__US_diff\n5460.0\n0.000755\n0.041192\n-1.000000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D_D__XM_diff\n5460.0\n0.000321\n0.031443\n-0.750000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D_D__ZA_diff\n5460.0\n-0.000962\n0.062475\n-1.500000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\n\n\n107 rows × 8 columns\n\n\n\nThe policy rates for some central banks have less observations than the others, as seen above.\nBecause some data are missing, we will impute data for the missing dates, by simply propagating the last valid observation, and when that is not possible, replacing the missing information with a “0”.\n\n\nCode\nX.fillna(method='pad', inplace=True)\nX.fillna(value=0, inplace=True)\n\n\nNow is a good time to start the model documentation. For this, we can use the standard model card that already comes with gingado.\nThe goal is to facilitate economists who want to make model documentation a part of their normal workflow.\n\n\nCode\nfrom gingado.model_documentation import ModelCard\n\n\n\n\nCode\nmodel_doc = ModelCard()\nmodel_doc.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nAs an example, we can add the following information to the model:\n\n\nCode\nmodel_doc.fill_info({\n    'intended_use': {\n        'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n        'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'\n    },\n    'metrics': {\n        'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.'\n    },\n    'ethical_considerations': {\n        'sensitive_data': 'These models were not trained with sensitive data.',\n        'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.'\n    },\n    \n})"
  },
  {
    "objectID": "forecast.html#lagging-the-regressors",
    "href": "forecast.html#lagging-the-regressors",
    "title": "Using gingado to forecast financial series",
    "section": "Lagging the regressors",
    "text": "Lagging the regressors\nThis model will not include any contemporaneous variable. Therefore, all regresors must be lagged.\nFor illustration purposes, we use 5 lags in this exercise.\n\n\nCode\nfrom gingado.utils import Lag\n\n\n\n\nCode\nn_lags = 5\n\nX_lagged = Lag(lags=n_lags).fit_transform(X)\nX_lagged\n\ny = FX_rate_changes[n_lags:]\n\n\nNow is a good opportunity to check by how much we have increased our regressor space:\n\n\nCode\npd.Series({\n    \"FX rates only\": y.shape[1],\n    \"... with augmentation_\": X.shape[1],\n    \"... lagged\": X_lagged.shape[1]\n})\n\n\nFX rates only               7\n... with augmentation_    107\n... lagged                535\ndtype: int64"
  },
  {
    "objectID": "forecast.html#training-the-models",
    "href": "forecast.html#training-the-models",
    "title": "Using gingado to forecast financial series",
    "section": "Training the models",
    "text": "Training the models\nOur dataset is now complete. Before using it to train the models, we hold out the most recent data to serve as our testing dataset, so we can compare our models with real out-of-sample information.\nWe can choose, say, 1st January 2022.\n\n\nCode\ncutoff = '2020-01-01'\n\nX_train, X_test = X_lagged[:cutoff], X_lagged[cutoff:]\ny_train, y_test = y[:cutoff], y[cutoff:]\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'training_data': \n    {'training_data': \n        \"\"\"\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\n        * foreign exchange rates\n        * central bank policy rates\n        * an estimated indicator for systemic stress\n        The training and evaluation datasets are the same time series, only different windows in time.\"\"\"\n    }\n})\n\n\nThe current status of the documentation is:\n\n\nCode\npd.Series(model_doc.show_json())\n\n\nmodel_details              {'developer': 'Person or organisation developi...\nintended_use               {'primary_uses': 'These models are simplified ...\nfactors                    {'relevant': 'Relevant factors', 'evaluation':...\nmetrics                    {'performance_measures': 'Consistent with most...\nevaluation_data            {'datasets': 'Datasets', 'motivation': 'Motiva...\ntraining_data              {'training_data': '\n        The training data ...\nquant_analyses             {'unitary': 'Unitary results', 'intersectional...\nethical_considerations     {'sensitive_data': 'These models were not trai...\ncaveats_recommendations    {'caveats': 'For example, did the results sugg...\ndtype: object\n\n\n\nCreating a random walk benchmark\nRossi (2013) highlights that few predictors beat the random walk without drift model. This is a good opportunity to showcase how we can use gingado’s in-built base class ggdBenchmark to build our customised benchmark model, in this case a random walk.\nThe calculation of the random walk benchmark is very simple. Still, creating a gingado benchmark offers some advantages: it is easier to compare alternative models, and the model documentation is done more seamlessly.\nA custom benchmark model must implement the following steps:\n\nsub-class ggdBenchmark (or alternatively implement its methods)\ndefine an estimator that is compatible with scikit-learn’s API:\n\nat the very least, it has a fit method that returns self\n\n\nIf the user is relying on a custom estimator - like in this case, a random walk estimator to align with the literature - then this custom estimator also has some requirements:\n\nit should ideally subclass scikit-learn’s BaseEstimator (mostly for the get_params / set_params methods)\nthree methods are necessary:\n\nfit, which should at least create an attribute ending in an underline (“_“), so that gingado knows it is fitted\npredict\nscore\n\n\n\n\nCode\nimport numpy as np\nfrom gingado.benchmark import ggdBenchmark\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\n\nCode\nclass RandomWalkEstimator(BaseEstimator):\n    def __init__(self, scoring='neg_root_mean_squared_error'):\n        self.scoring = scoring\n    \n    def fit(self, X, y=None):\n        self.n_samples_ = X.shape[0]\n        return self\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n    def score(self, X, y, sample_weight=None):\n        from sklearn.metrics import mean_squared_error\n        y_pred = self.predict(X)\n        return mean_squared_error(y, y_pred, sample_weight=sample_weight, squared=False)\n\n    def forecast(self, forecast_horizon=1):\n        self.forecast_horizon = forecast_horizon\n        return np.zeros(self.forecast_horizon)\n\nclass RandomWalkBenchmark(ggdBenchmark):\n    def __init__(\n        self, \n        estimator=RandomWalkEstimator(), \n        auto_document=ModelCard,\n        cv=TimeSeriesSplit(n_splits=10, test_size=60), \n        ensemble_method=VotingRegressor, \n        verbose_grid=None):\n        self.estimator=estimator\n        self.auto_document=auto_document\n        self.cv=cv\n        self.ensemble_method=ensemble_method\n        self.verbose_grid=verbose_grid\n\n    def fit(self, X, y=None):\n        self.benchmark=self.estimator\n        self.benchmark.fit(X, y)\n        return self\n\n\n\n\nTraining the candidate models\nNow that we have a benchmark, we can create candidate models that will try to beat it.\nIn this simplified example, we will choose only two: a random forest, an AdaBoost regressor and a Lasso model. Their hyperparameters are not particularly important for the example, but of course they could be fine-tuned as well.\nIn the language of Rossi (2013), the models below are one “single-equation, lagged fundamental model” for each currency.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nforest = RandomForestRegressor(n_estimators=250, max_features='log2').fit(X_train, y_train['USDBRL'])\nadaboost = AdaBoostRegressor(n_estimators=150).fit(X_train, y_train['USDBRL'])\nlasso = Lasso(alpha=0.1).fit(X_train, y_train['USDBRL'])\n\nrw = RandomWalkBenchmark().fit(X_train, y_train['USDBRL'])\n\n\nWe can now compare the model results, using the test dataset we held out previously.\nNote that we must pass the criterion against which we are comparing the forecasts.\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nresults = rw.compare_fitted_candidates(\n    X_test, y_test['USDBRL'],\n    candidates=[forest, adaboost, lasso],\n    scoring_func=mean_squared_error)\n\npd.Series(results)\n\n\nRandomWalkEstimator()                                           0.000109\nRandomForestRegressor(max_features='log2', n_estimators=250)    0.000115\nAdaBoostRegressor(n_estimators=150)                             0.000112\nLasso(alpha=0.1)                                                0.000109\ndtype: float64\n\n\nAs mentioned above, benchmarks can facilitate the model documentation. In addition to the broader documentation that is already ongoing, each benchmark object create their own where they store model information. We can use that for the broader documentation.\nIn our case, the only parameter we created above during fit is the number of samples: not a particularly informative variable but it was included just for illustration purposes. In any case, the parameter appears in the “model_details” section, item “info”, of the benchmark’s rw documentation. Similarly, the parameters of more fully-fledged estimators also appear in that section.\n\n\nCode\nrw.document()\n\nrw.model_documentation.show_json()['model_details']['info']\n\n\n{'n_samples_': 4394}\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'model_details': {'info': rw.model_documentation.show_json()['model_details']['info']}\n})\n\n\n\n\nCode\nmodel_doc.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2024-02-22 15:17:30 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'n_samples_': 4394},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': '\\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\\n        * foreign exchange rates\\n        * central bank policy rates\\n        * an estimated indicator for systemic stress\\n        The training and evaluation datasets are the same time series, only different windows in time.'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'These models were not trained with sensitive data.',\n  'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nWe can save the documentation to disk in JSON format with model_doc.save_json(), or parse it to create other documents (eg, a PDF file) using third-party libraries."
  },
  {
    "objectID": "forecast.html#references",
    "href": "forecast.html#references",
    "title": "Using gingado to forecast financial series",
    "section": "References",
    "text": "References\n\n\nBank for International Settlements. 2017. “Recent Enhancements to the BIS Statistics.” BIS Quarterly Review. Vol. September. https://www.bis.org/publ/qtrpdf/r_qt1709c.htm.\n\n\nHollo, Daniel, Manfred Kremer, and Marco Lo Duca. 2012. “CISS-a Composite Indicator of Systemic Stress in the Financial System.”\n\n\nRossi, Barbara. 2013. “Exchange Rate Predictability.” Journal of Economic Literature 51 (4): 1063–1119."
  },
  {
    "objectID": "machine_controls.html",
    "href": "machine_controls.html",
    "title": "Machine controls",
    "section": "",
    "text": "This notebook illustrates the use of MachineControl, the gingado estimator that calculates a synthetic control model with machine learning techniques."
  },
  {
    "objectID": "machine_controls.html#setting",
    "href": "machine_controls.html#setting",
    "title": "Machine controls",
    "section": "Setting",
    "text": "Setting\nUse of the MachineControl estimator is illustrated with an admittedly simplistic estimation of the impact of softening labour regulation on output per worker, measured in constant 2017 international US dollars PPP.\nMore specifically, the example below focuses on Brazil’s 2017 labour reforms (Law No 13,467/2017). The reform markedly deregulated labour markets, with the purpose of increasing productivity and thereby unlocking growth. Some of its main points are:\n\nprominence of collective bargaining between firms and employees over statutory “blanket” provisions\nlower costs for employers of employment termination without just cause\ndiscouraging of labour litigation by employees, previously diagnosed as being excessive and contributing to clogging the judicial system\n\nThe reform was enacted in July 2017 and went into effect in November of the same year."
  },
  {
    "objectID": "machine_controls.html#using-machine-learning",
    "href": "machine_controls.html#using-machine-learning",
    "title": "Machine controls",
    "section": "Using machine learning",
    "text": "Using machine learning\nMachineControl does the following:\n\nautomatically select a group of countries from a global list to form a smaller set of control countries\nestimate a GDP value for “synthetic Brazil” using pre-enactment data on the outcome of interest\ncheck the statistical quality of the synthetic control\ncalculates the difference between post-reforms actual Brazilian GDP growth to synthetic Brazil’s to measure the effect of the labour reform.\n\n\nNote: There are many other variables that would be interesting for this study as well, such as various labour market indicators.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom gingado.utils import list_all_dataflows, load_SDMX_data\nfrom gingado.estimators import MachineControl\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.manifold import TSNE"
  },
  {
    "objectID": "machine_controls.html#downloading-the-data",
    "href": "machine_controls.html#downloading-the-data",
    "title": "Machine controls",
    "section": "Downloading the data",
    "text": "Downloading the data\nFirst, we list all dataflows obtainable with SDMX.\n\n\nCode\ndflows = list_all_dataflows(return_pandas=True)\n\n\n\n\nCode\ndflows[dflows.str.contains('per worker', case=False)]\n\n\nILO  DF_GDP_205U_NOC_NB    Output per worker (GDP constant 2015 US $) -- ...\n     DF_GDP_211P_NOC_NB    Output per worker (GDP constant 2017 internati...\n     DF_SDG_A821_NOC_RT    SDG indicator 8.2.1 - Annual growth rate of ou...\n     DF_SDG_B821_NOC_RT    SDG indicator 8.2.1 - Annual growth rate of ou...\nName: dataflow, dtype: object\n\n\nLet’s get the data on output per worker provided by the International Labour Organisation (ILO):\n\n\nCode\noutcome_var = load_SDMX_data(\n    sources={'ILO': 'DF_GDP_211P_NOC_NB'}, \n    keys={'FREQ': 'A'}, \n    params={'startPeriod': 2000, 'endPeriod': 2022}\n)\n\n\nQuerying data from ILO's dataflow 'DF_GDP_211P_NOC_NB' - Output per worker (GDP constant 2017 international $ at PPP) -- ILO modelled estimates, Nov. 2023...\n\n\nThis dataflow provides one series for each country, as seen below.\n\n\nCode\ncol_fields = pd.DataFrame([c.split(\"__\") for c in outcome_var.columns])\nfor col in col_fields.columns:\n    print(\"No of unique values for column No\", col, \": \", col_fields[col].nunique())\nprint(\"\\nFirst five rows:\")\nprint(col_fields.head())\n\n\nNo of unique values for column No 0 :  1\nNo of unique values for column No 1 :  274\nNo of unique values for column No 2 :  1\nNo of unique values for column No 3 :  1\n\nFirst five rows:\n     0                       1  2            3\n0  ILO  DF_GDP_211P_NOC_NB_AFG  A  GDP_211P_NB\n1  ILO  DF_GDP_211P_NOC_NB_AGO  A  GDP_211P_NB\n2  ILO  DF_GDP_211P_NOC_NB_ALB  A  GDP_211P_NB\n3  ILO  DF_GDP_211P_NOC_NB_ARE  A  GDP_211P_NB\n4  ILO  DF_GDP_211P_NOC_NB_ARG  A  GDP_211P_NB\n\n\nBecause 271 is much higher than the number of countries that usually report international statistics (close to 200), it is likely some 70 columns or more correspond to aggregations, typically used in statistics for convenience (eg, one code that encompasses all countries in the European Union, etc). We need to take them out, in case Brazil is a constituent of any of those aggregations.\nOne common way of spotting these aggregations in international statistics is finding the country codes that begin with “X”.\n\n\nCode\nlen([c for c in outcome_var.columns if \"DF_GDP_211P_NOC_NB_X\" in c])\n\n\n85\n\n\nAlso, we will take out other countries that have undergone labour reforms more or less around the same time. From Serra, Bottega, and Sanches (2022), I am aware of Argentina 🇦🇷, Costa Rica 🇨🇷, Paraguay 🇵🇾, and Uruguay 🇺🇾.\nIt is important to note, though, that these countries above were named by Serra, Bottega, and Sanches (2022) because their synthetic control donor pool consistent of geographically close countries. Other countries might have also enacted labour reforms in that period. For expositional purposes, we can assume no further country needs to be taken out of the same.\n\n\nCode\ncol_filter = [\n    c for c in outcome_var.columns\n    if \"DF_GDP_211P_NOC_NB_X\" not in c\n    and \"DF_GDP_211P_NOC_NB_ARG\" not in c\n    and \"DF_GDP_211P_NOC_NB_CRI\" not in c\n    and \"DF_GDP_211P_NOC_NB_PAR\" not in c\n    and \"DF_GDP_211P_NOC_NB_URY\" not in c\n]\n\nX = outcome_var[col_filter]\nX = X.dropna(axis=1)\n\n# cleaning out the name to remove the constant portions across all countries\nX.columns = [c.replace(\"ILO__DF_GDP_211P_NOC_NB_\", \"\") \\\n    .replace(\"__A__GDP_211P_NB\", \"\") for c in X.columns]\n\ncol_BRA = [c for c in X.columns if c == \"BRA\"]\ny = X.pop(col_BRA[0])\n\nassert X.shape[0] == y.shape[0]\n\n\nThis is what the series of annual output per worker in Brazil looks like. A vertical line marking November 2017, when the labour reforms entered into force.\n\n\nCode\nlaw_date = '2017-11-11'\nylabel = 'PPP 2017 US$ per worker'\n\nax = y.plot(legend=False)\nax.axvline(x=law_date, color='r', linestyle='--')\nplt.ylabel(ylabel)\nplt.xlabel('')\nplt.title('Labour productivity in Brazil')\n\n\nText(0.5, 1.0, 'Labour productivity in Brazil')\n\n\n\n\n\nBefore creating the MachineControl object, a final comment on the intervention date.\nSince the reforms were enacted and entered into force in the same year of 2017, we can be conservative and consider: - pre-intervention data up to end-2016 - post-intervention data from 2018 onwards\n\n\nCode\nX_pre, y_pre = X[:'2016-12-31'], y[:'2016-12-31']\n\nassert X_pre.shape[0] == y_pre.shape[0]\nX_pre.shape, y_pre.shape\n\n\n((17, 184), (17,))"
  },
  {
    "objectID": "machine_controls.html#using-the-machinecontrol-object",
    "href": "machine_controls.html#using-the-machinecontrol-object",
    "title": "Machine controls",
    "section": "Using the MachineControl object",
    "text": "Using the MachineControl object\nThe code chunk below shows how a MachineControl object can be created.\nAs illustrated below, users can not only choose the clustering, estimator and manifold learning algorithms that best suit their needs, but also pass specific arguments to each of these elements.\n\n\nCode\nsynth_BR = MachineControl(\n    cluster_alg=AffinityPropagation(max_iter=10_000),\n    estimator=RandomForestRegressor(),\n    manifold=TSNE(perplexity=5)\n)\n\n\nLet’s train and then inspect the MachineControl object:\n\n\nCode\nsynth_BR.fit(X_pre, y_pre)\n\n\nMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. MachineControliFittedMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5)) cluster_alg: AffinityPropagationAffinityPropagation(max_iter=10000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(max_iter=10000) estimator: RandomForestRegressorRandomForestRegressor()  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor() manifold: TSNETSNE(perplexity=5)  TSNE?Documentation for TSNETSNE(perplexity=5) \n\n\nAs shown above, the machine control estimator comprises:\n\nA clustering algorithm that selects the donor pool from the larger population (affinity propagation)\nA supervised learning algorithm that will use the donor pool to estimate contemporanous values for Brazil (random forest)\nA manifold learning algorithm that summarises the different time series in a 2-dimensional embedding space, enabling easier comparison of Brazil with each other country and with the synthetic control (t-SNE)\n\nAt this stage, we can extract the list of donor countries, ie, those that will be used in the control.\n\n\nCode\n\" \".join(synth_BR.donor_pool_)\n\n\n'BGR BIH BLR BLZ BRB COL CUB DOM EGY FJI GUY IRQ KAZ LCA MUS NAM PSE SRB SWZ TUN VCT YEM ZAF'\n\n\nThese countries are: Albania 🇦🇱, Bulgaria 🇧🇬, Bosnia and Herzegovina 🇧🇦, Belarus 🇧🇾, Barbados 🇧🇧, Botswana 🇧🇼, Colombia 🇨🇴, Cuba 🇨🇺, Dominican Republic 🇩🇴, Algeria 🇩🇿, Egypt 🇪🇬, Fiji 🇫🇯, Guyana 🇬🇾, Iraq 🇮🇶, Kazakhstan 🇰🇿, Saint Lucia 🇱🇨, North Macedonia 🇲🇰, Mauritius 🇲🇺, Namibia 🇳🇦, State of Palestine 🇵🇸, Serbia 🇷🇸, Eswatini 🇸🇿, Tunisia 🇹🇳, Saint Vincent and the Grenadines 🇻🇨, Yemen 🇾🇪 and South Africa 🇿🇦.\n\nNote: This list does not imply all countries contribute equally to estimating the synthetic version of Brazil. In fact, some might even end up not even contributing in the estimating equation. The selected list also does not imply a causal explanation from any of those countries into the Brazilian dynamics. They are merely closest to Brazil in this clustering exercise, and as such, likely to be a good predictor at the same time period.\n\nAnd here is how the outcome variable for these countries (grey) compares with Brazil’s (red).\n\n\nCode\nax = X_pre[synth_BR.donor_pool_].plot(legend=False, color=\"grey\", linewidth=0.5)\ny_pre.plot(ax=ax, color=\"red\", linewidth=2.5)\n\n\n&lt;Axes: xlabel='TIME_PERIOD'&gt;\n\n\n\n\n\nPlotting the result of the manifold learning underscores whether the synthetic control seems indeed to come from a similar space in the data distribution as the entity of interest.\n\n\nCode\ncolors = [(0.5, 0.5, 0.5, 0.35)] * X.shape[1] + ['red', 'blue']\n\nfig, ax = plt.subplots()\nax.scatter(\n    synth_BR.manifold_embed_[:, 0], synth_BR.manifold_embed_[:, 1],\n    color=colors,\n    )\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nsynth_BR.control_quality_test_ / 100\n\n\n0.019520585509500647\n\n\nAs can be seen in the graph above, the actual Brazil 🇧🇷 (blue) and the machine controls (red) are almost juxtaposed.\nTo confirm objectively that the control is good, the Euclidean distance between the embeddings for the control and the actual outcome are one of the lowest, more specifically at the 2% percentile.\nTogether, both point to a strong signal that the control managed to replicate the pre-intervention outcome.\nSo now it’s time to look at the results in the years following the event."
  },
  {
    "objectID": "machine_controls.html#results",
    "href": "machine_controls.html#results",
    "title": "Machine controls",
    "section": "Results",
    "text": "Results\n\n\nCode\nax = synth_BR.predict(X, y).plot(legend=False)\nax.axvline(x='2017-11-11', color='r', linestyle='--')\nplt.ylabel('PPP 2017 US$ per worker')\nplt.xlabel('')\nplt.title(\"Labour productivity in Brazil\")\nplt.show()\n\n\n\n\n\n\nTreatment effect\nThe main result is shown below, corresponding to the difference between actual and estimated value for the whole time series (from 2000). If the red line (Brazil) stays close to the other ones - placebo estimations.\n\n\nCode\nall_diffs = pd.concat([\n    synth_BR.placebo_diff_,\n    synth_BR.diff_\n], axis=1)\ncolors = ['grey'] * synth_BR.placebo_diff_.shape[1] + ['red']\ny_label = 'PPP 2017 US$ per worker'\neffect_title = \"Effect of deregulation on labour productivity in Brazil\"\n\nax = all_diffs.plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()\n\n\n\n\n\nIt seems there is some outlier effect with one of the control countries.\n\n\nCode\nsynth_BR.placebo_diff_.loc['2022-01-01'].sort_values(ascending=False).index[0]\n\n\n'GUY'\n\n\nAccording to the code above, it is Guyana 🇬🇾. Now plotting without this country:\n\n\n\nCode\ncolors = ['grey'] * (synth_BR.placebo_diff_.shape[1] - 1)+ ['red']\nax = all_diffs[[c for c in all_diffs.columns if c != \"GUY\"]].plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()"
  },
  {
    "objectID": "machine_controls.html#interpretation",
    "href": "machine_controls.html#interpretation",
    "title": "Machine controls",
    "section": "Interpretation",
    "text": "Interpretation\nThe above results hint at a null effect of the reforms on labour productivity after training cutoff date (black vertical line). If anything, the effect squarely close to zero throughout the years following the reform (red vertical line), even as the productivity for the majority of control countries actually went up in the years before the pandemic."
  },
  {
    "objectID": "dataset_transformation.html",
    "href": "dataset_transformation.html",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('gingado/data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/data/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "dataset_transformation.html#barro-and-lee-1994",
    "href": "dataset_transformation.html#barro-and-lee-1994",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('gingado/data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/data/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to gingado",
    "section": "",
    "text": "Welcome, and thank you for your interest in contributing to gingado! Whether it’s reporting issues, suggesting new features, or contributing to the code, documentation, or tests, your involvement is highly appreciated.\n\n\n\n\nTo get started with contributing to gingado, you need to set up your development environment. This includes installing necessary tools and configuring your system to work efficiently with our codebase.\nTo install the dependencies to work with gingado, you need to install the execution dependencies and the development dependencies of gingado:\npip install -r requirements.txt\npip install -r dev_requirements.txt\nTo work with the documentation an tests, we use quarto. You can either use RStudio or Visual Studio Code with the Quarto extension installed for editing .qmd files which are used for generating documentation. Here’s how you can set up your environment:\n\nInstall Quarto: If you haven’t already, install Quarto from Quarto’s official website. Follow the instructions for your operating system.\nConfigure Your Editor:\n\nFor RStudio: Quarto is integrated with RStudio. Ensure you have the latest version of RStudio to work with Quarto seamlessly.\nFor Visual Studio Code: Install the Quarto extension from the Visual Studio Code marketplace. This extension provides support for .qmd files, including syntax highlighting and preview capabilities.\n\n\n\n\n\nIf you encounter a bug, have suggestions, or want to propose new functionalities:\n\nCheck Existing Issues: Ensure the bug or suggestion hasn’t been reported/mentioned before by searching under Issues on GitHub.\nCreate a New Issue: If no existing issue addresses the problem or suggestion, please create a new issue, providing a descriptive title, a clear description, and as much relevant information as possible. For bugs, include a code sample or an executable test case demonstrating the expected behavior that is not occurring, along with complete error messages.\n\n\n\n\n\n\nTo contribute changes to the codebase, including documentation and tests, follow these guidelines:\n\nDocument New Features: Clearly document new functions or classes in the .qmd files. Write clear descriptions, specify expected inputs and outputs, and include any relevant information to understand the functionality.\nInclude Tests: Implement tests for new functionalities as part of the .qmd files to ensure the integrity and reliability of the code. Make sure your tests cover the expected behavior and edge cases.\n\n\n\n\n\nFocused PRs: Each PR should be focused on a single topic. Avoid combining unrelated changes.\nSeparate Style and Functional Changes: Do not mix style changes with functional changes in the same PR.\nPreserve File Style: Avoid adding or removing vertical whitespace unnecessarily. Keep the original style of the files you edit.\nDevelopment Process: Do not use a submitted PR as a development playground. If additional work is needed, consider closing the PR, completing the work, and then submitting a new PR.\nResponding to Feedback: If your PR requires changes based on feedback, continue committing to the same PR unless the changes are substantial. In that case, it might be better to start a new PR.\n\n\n\n\n\nWhen contributing to the documentation, ensure your contributions are made within .qmd files. This is essential for the changes to be correctly reflected in the generated documentation through Quarto.\n\n\n\n\nBy contributing to gingado, you are part of a community that values collaboration, innovation, and learning. We look forward to your contributions and are excited to see what we can build together."
  },
  {
    "objectID": "CONTRIBUTING.html#getting-started",
    "href": "CONTRIBUTING.html#getting-started",
    "title": "Contributing to gingado",
    "section": "",
    "text": "To get started with contributing to gingado, you need to set up your development environment. This includes installing necessary tools and configuring your system to work efficiently with our codebase.\nTo install the dependencies to work with gingado, you need to install the execution dependencies and the development dependencies of gingado:\npip install -r requirements.txt\npip install -r dev_requirements.txt\nTo work with the documentation an tests, we use quarto. You can either use RStudio or Visual Studio Code with the Quarto extension installed for editing .qmd files which are used for generating documentation. Here’s how you can set up your environment:\n\nInstall Quarto: If you haven’t already, install Quarto from Quarto’s official website. Follow the instructions for your operating system.\nConfigure Your Editor:\n\nFor RStudio: Quarto is integrated with RStudio. Ensure you have the latest version of RStudio to work with Quarto seamlessly.\nFor Visual Studio Code: Install the Quarto extension from the Visual Studio Code marketplace. This extension provides support for .qmd files, including syntax highlighting and preview capabilities.\n\n\n\n\n\nIf you encounter a bug, have suggestions, or want to propose new functionalities:\n\nCheck Existing Issues: Ensure the bug or suggestion hasn’t been reported/mentioned before by searching under Issues on GitHub.\nCreate a New Issue: If no existing issue addresses the problem or suggestion, please create a new issue, providing a descriptive title, a clear description, and as much relevant information as possible. For bugs, include a code sample or an executable test case demonstrating the expected behavior that is not occurring, along with complete error messages.\n\n\n\n\n\n\nTo contribute changes to the codebase, including documentation and tests, follow these guidelines:\n\nDocument New Features: Clearly document new functions or classes in the .qmd files. Write clear descriptions, specify expected inputs and outputs, and include any relevant information to understand the functionality.\nInclude Tests: Implement tests for new functionalities as part of the .qmd files to ensure the integrity and reliability of the code. Make sure your tests cover the expected behavior and edge cases.\n\n\n\n\n\nFocused PRs: Each PR should be focused on a single topic. Avoid combining unrelated changes.\nSeparate Style and Functional Changes: Do not mix style changes with functional changes in the same PR.\nPreserve File Style: Avoid adding or removing vertical whitespace unnecessarily. Keep the original style of the files you edit.\nDevelopment Process: Do not use a submitted PR as a development playground. If additional work is needed, consider closing the PR, completing the work, and then submitting a new PR.\nResponding to Feedback: If your PR requires changes based on feedback, continue committing to the same PR unless the changes are substantial. In that case, it might be better to start a new PR.\n\n\n\n\n\nWhen contributing to the documentation, ensure your contributions are made within .qmd files. This is essential for the changes to be correctly reflected in the generated documentation through Quarto."
  },
  {
    "objectID": "CONTRIBUTING.html#your-contributions-make-a-difference",
    "href": "CONTRIBUTING.html#your-contributions-make-a-difference",
    "title": "Contributing to gingado",
    "section": "",
    "text": "By contributing to gingado, you are part of a community that values collaboration, innovation, and learning. We look forward to your contributions and are excited to see what we can build together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to gingado!",
    "section": "",
    "text": "gingado seeks to facilitate the use of machine learning in economic and finance use cases, while promoting good practices. This package aims to be suitable for beginners and advanced users alike. Use cases may range from simple data retrievals to experimentation with machine learning algorithms to more complex model pipelines used in production."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to gingado!",
    "section": "Overview",
    "text": "Overview\ngingado is a free, open source library built different functionalities:\n\ndata augmentation, to add more data from official sources, improving the machine models being trained by the user;\nrelevant datasets, both real and simulamed, to allow for easier model development and comparison;\nautomatic benchmark model, to assess candidate models against a reasonably well-performant model;\n(new!) machine learning-based estimators, to help answer questions of academic or practical importance;\nsupport for model documentation, to embed documentation and ethical considerations in the model development phase; and\nutilities, including tools to allow for lagging variables in a straightforward way.\n\nEach of these functionalities builds on top of the previous one. They can be used on a stand-alone basis, together, or even as part of a larger pipeline from data input to model training to documentation!\n\n\n\n\n\n\nTip\n\n\n\nNew functionalities are planned over time, so consider checking frequently on gingado for the latest toolsets."
  },
  {
    "objectID": "index.html#design-principles",
    "href": "index.html#design-principles",
    "title": "Welcome to gingado!",
    "section": "Design principles",
    "text": "Design principles\nThe choices made during development of gingado derive from the following principles, in no particular order:\n\nflexibility: users can use gingado out of the box or build custom processes on top of it;\ncompatibility: gingado works well with other widely used libraries in machine learning, such as scikit-learn and pandas; and\nresponsibility: gingado facilitates and promotes model documentation, including ethical considerations, as part of the machine learning development workflow."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to gingado!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\ngingado’s API is inspired on the following libraries:\n\nscikit-learn (Buitinck et al. 2013)\nkeras (website here and also, this essay)\nfastai (Howard and Gugger 2020)\n\nIn addition, gingado is developed and maintained using quarto."
  },
  {
    "objectID": "index.html#presentations-talks-papers",
    "href": "index.html#presentations-talks-papers",
    "title": "Welcome to gingado!",
    "section": "Presentations, talks, papers",
    "text": "Presentations, talks, papers\nThe most current version of the paper describing gingado is here. The paper and other material about gingado (ie, slide decks, papers) in this dedicated repository. Interested users are welcome to visit the repository and comment on the drafts or slide decks, preferably by opening an issue. I also store in this repository suggestions I receive as issues, so users can see what others commented (anonymously unless requested) and comment along as well!"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Welcome to gingado!",
    "section": "Install",
    "text": "Install\nTo install gingado, simply run the following code on the terminal:\n$ pip install gingado\nIf you use this package in your work, please cite it as below:\nAraujo, Douglas KG (2023): “gingado: a machine learning library focused on economics and finance”, BIS Working Paper No 1122.\n@techreport{gingado,\n    author = {Araujo, Douglas KG},\n    title = {gingado: a machine learning library focused on economics and finance},\n    series = {BIS Working Paper},\n    type = {Working Paper},\n    institution = {Bank for International Settlements},\n    year = {2023},\n    number = {1122}\n}"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Welcome to gingado!",
    "section": "References",
    "text": "References\n\n\nBuitinck, Lars, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, et al. 2013. “API Design for Machine Learning Software: Experiences from the Scikit-Learn Project.” CoRR abs/1309.0238. http://arxiv.org/abs/1309.0238.\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108."
  }
]