[
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "Security Policy",
    "section": "",
    "text": "We are committed to maintaining the security of our software. However, our resources are limited to providing security patches only for the latest combination of minor and major versions of our software.\n\n\n\nWe take the security of our software seriously. If you believe you have found a security vulnerability in our software, we encourage you to report it to us as soon as possible. Please follow these steps:\n\nDo Not Publish the Vulnerability: Publicly disclosing a vulnerability can put the entire community at risk. We ask that you do not share or publicize an unresolved vulnerability to/with third parties.\nReport Confidentially: Please email us at contact.gingado@bis.org with the details of the vulnerability. The report should include:\n\nA description of the vulnerability and its potential impact.\nSteps to reproduce or proof-of-concept (PoC).\nAny relevant screenshots or output.\n\nResponse and Collaboration: Our security team will review your report and may contact you for further information. Once the vulnerability is confirmed, we will work with you to assess and understand its impact and develop a mitigation or fix.\nAcknowledgment: After the vulnerability has been resolved, we will acknowledge your contribution in our release notes, unless you prefer to remain anonymous.\n\n\n\n\nWhen a vulnerability is discovered, either through internal processes or via an external report, the following process will be followed:\n\nVulnerability Assessment: Our security team will assess the severity and impact of the vulnerability.\nPatch Development: A patch will be developed for the latest supported version.\nRelease and Notification: Once the patch is ready, it will be released as part of a new version. We will notify users of the need to update through our communication channels (e.g., repository release notes).\nBackporting: In exceptional cases, where a vulnerability has a high impact, we may consider backporting the patch to earlier versions. This decision will be made on a case-by-case basis.\n\nThank you for helping us keep our software secure.\n\nThis policy is subject to change at the discretion of the project maintainers."
  },
  {
    "objectID": "SECURITY.html#supported-versions",
    "href": "SECURITY.html#supported-versions",
    "title": "Security Policy",
    "section": "",
    "text": "We are committed to maintaining the security of our software. However, our resources are limited to providing security patches only for the latest combination of minor and major versions of our software."
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "Security Policy",
    "section": "",
    "text": "We take the security of our software seriously. If you believe you have found a security vulnerability in our software, we encourage you to report it to us as soon as possible. Please follow these steps:\n\nDo Not Publish the Vulnerability: Publicly disclosing a vulnerability can put the entire community at risk. We ask that you do not share or publicize an unresolved vulnerability to/with third parties.\nReport Confidentially: Please email us at contact.gingado@bis.org with the details of the vulnerability. The report should include:\n\nA description of the vulnerability and its potential impact.\nSteps to reproduce or proof-of-concept (PoC).\nAny relevant screenshots or output.\n\nResponse and Collaboration: Our security team will review your report and may contact you for further information. Once the vulnerability is confirmed, we will work with you to assess and understand its impact and develop a mitigation or fix.\nAcknowledgment: After the vulnerability has been resolved, we will acknowledge your contribution in our release notes, unless you prefer to remain anonymous."
  },
  {
    "objectID": "SECURITY.html#security-patch-release-process",
    "href": "SECURITY.html#security-patch-release-process",
    "title": "Security Policy",
    "section": "",
    "text": "When a vulnerability is discovered, either through internal processes or via an external report, the following process will be followed:\n\nVulnerability Assessment: Our security team will assess the severity and impact of the vulnerability.\nPatch Development: A patch will be developed for the latest supported version.\nRelease and Notification: Once the patch is ready, it will be released as part of a new version. We will notify users of the need to update through our communication channels (e.g., repository release notes).\nBackporting: In exceptional cases, where a vulnerability has a high impact, we may consider backporting the patch to earlier versions. This decision will be made on a case-by-case basis.\n\nThank you for helping us keep our software secure.\n\nThis policy is subject to change at the discretion of the project maintainers."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to gingado!",
    "section": "",
    "text": "gingado seeks to facilitate the use of machine learning in economic and finance use cases, while promoting good practices. This package aims to be suitable for beginners and advanced users alike. Use cases may range from simple data retrievals to experimentation with machine learning algorithms to more complex model pipelines used in production."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Welcome to gingado!",
    "section": "Overview",
    "text": "Overview\ngingado is a free, open source library built different functionalities:\n\ndata augmentation, to add data from official sources, improving the machine models being trained by the user;\nrelevant datasets, both real and simulated, to allow for easier model development and comparison;\nautomatic benchmark model, to assess candidate models against a reasonably well-performant model;\nmachine learning-based estimators, to help answer questions of academic or practical importance;\nsupport for model documentation, to embed documentation and ethical considerations in the model development phase; and\nutilities, including tools to allow for lagging variables in a straightforward way.\n\nEach of these functionalities builds on top of the previous one. They can be used on a stand-alone basis, together, or even as part of a larger pipeline from data input to model training to documentation!\n\n\n\n\n\n\nTip\n\n\n\nNew functionalities are planned over time, so consider checking frequently on gingado for the latest toolsets."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Welcome to gingado!",
    "section": "Install",
    "text": "Install\n\n\n\n\n\n\nNote\n\n\n\nPlease make sure you have read and understood the license disclaimer in the NOTES.md file in our GitHub repository before using gingado.\n\n\nTo install gingado, simply run the following code on the terminal:\n$ pip install gingado"
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "Welcome to gingado!",
    "section": "Attribution",
    "text": "Attribution\nIf you use this package in your work, please consider citing Araujo (2023).\nIn BibTeX format:\n@techreport{gingado,\n    author = {Araujo, Douglas KG},\n    title = {gingado: a machine learning library focused on economics and finance},\n    series = {BIS Working Paper},\n    type = {Working Paper},\n    institution = {Bank for International Settlements},\n    year = {2023},\n    number = {1122}\n}\nOver time, new tools that are described in specific papers might be added (eg, a machine learning-based econometric estimator). Please consider citing them as well if used in your work. Specific information, if any, can be found in the documentation."
  },
  {
    "objectID": "index.html#design-principles",
    "href": "index.html#design-principles",
    "title": "Welcome to gingado!",
    "section": "Design principles",
    "text": "Design principles\nThe choices made during development of gingado derive from the following principles, in no particular order:\n\nflexibility: users can use gingado out of the box or build custom processes on top of it;\ncompatibility: gingado works well with other widely used libraries in machine learning, such as scikit-learn and pandas; and\nresponsibility: gingado facilitates and promotes model documentation, including ethical considerations, as part of the machine learning development workflow.\n\nFor more information about gingado, please read the paper."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to gingado!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\ngingado’s API is inspired on the following libraries:\n\nscikit-learn (Buitinck et al. 2013)\nkeras (website here and also, this essay)\nfastai (Howard and Gugger 2020)\n\nIn addition, gingado is developed and maintained using quarto."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Welcome to gingado!",
    "section": "References",
    "text": "References\n\n\nAraujo, Douglas KG. 2023. “Gingado: A Machine Learning Library Focused on Economics and Finance.” Working Paper 1122. BIS Working Paper. Bank for International Settlements.\n\n\nBuitinck, Lars, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, et al. 2013. “API Design for Machine Learning Software: Experiences from the Scikit-Learn Project.” CoRR abs/1309.0238. http://arxiv.org/abs/1309.0238.\n\n\nHoward, Jeremy, and Sylvain Gugger. 2020. “Fastai: A Layered API for Deep Learning.” Information 11 (2). https://doi.org/10.3390/info11020108."
  },
  {
    "objectID": "compliance.html",
    "href": "compliance.html",
    "title": "Open Source Software Licence Compliance Note",
    "section": "",
    "text": "The BIS has independently authored the software program: gingado. It relies on multiple third-party software modules listed in requirements.txt.\ngingado is licensed under the Apache License Version 2.0 and is therefore provided with no warranty. To comply with the terms of the licences covering the third-party components, gingado must be installed with the considerations below, any other installation method may not be compliant with the relevant third-party licences.\n\n\nFor a licence compliant installation, gingado must be installed using the package installer for Python (pip) using the –no-binary flag. An example installation command is:\npip install gingado --no-binary :all:\n\n\n\n\nPlease note that usage of the –no-binary flag will increase the complexity of the installation (such as requiring building modules for some components from source). Please refer to third-party documentation for additional guidance; and\nPlease be aware that compliance materials may be placed into temporary directories by pip; and\nWhen resolving dependencies for gingado, pip may automatically use a later version of a dependency. For convenience, the BIS has provided verified-requirements.txt which contains fixed version numbers to prevent this behaviour. An example installation using this file is: pip install -r verified-requirements.txt --no-binary :all:"
  },
  {
    "objectID": "compliance.html#installation-considerations",
    "href": "compliance.html#installation-considerations",
    "title": "Open Source Software Licence Compliance Note",
    "section": "",
    "text": "For a licence compliant installation, gingado must be installed using the package installer for Python (pip) using the –no-binary flag. An example installation command is:\npip install gingado --no-binary :all:"
  },
  {
    "objectID": "compliance.html#further-information",
    "href": "compliance.html#further-information",
    "title": "Open Source Software Licence Compliance Note",
    "section": "",
    "text": "Please note that usage of the –no-binary flag will increase the complexity of the installation (such as requiring building modules for some components from source). Please refer to third-party documentation for additional guidance; and\nPlease be aware that compliance materials may be placed into temporary directories by pip; and\nWhen resolving dependencies for gingado, pip may automatically use a later version of a dependency. For convenience, the BIS has provided verified-requirements.txt which contains fixed version numbers to prevent this behaviour. An example installation using this file is: pip install -r verified-requirements.txt --no-binary :all:"
  },
  {
    "objectID": "machine_controls.html",
    "href": "machine_controls.html",
    "title": "Machine controls",
    "section": "",
    "text": "This notebook illustrates the use of MachineControl, the gingado estimator that calculates a synthetic control model with machine learning techniques."
  },
  {
    "objectID": "machine_controls.html#setting",
    "href": "machine_controls.html#setting",
    "title": "Machine controls",
    "section": "Setting",
    "text": "Setting\nUse of the MachineControl estimator is illustrated with an admittedly simplistic estimation of the impact of softening labour regulation on output per worker, measured in constant 2017 international US dollars PPP.\nMore specifically, the example below focuses on Brazil’s 2017 labour reforms (Law No 13,467/2017). The reform markedly deregulated labour markets, with the purpose of increasing productivity and thereby unlocking growth. Some of its main points are:\n\nprominence of collective bargaining between firms and employees over statutory “blanket” provisions\nlower costs for employers of employment termination without just cause\ndiscouraging of labour litigation by employees, previously diagnosed as being excessive and contributing to clogging the judicial system\n\nThe reform was enacted in July 2017 and went into effect in November of the same year."
  },
  {
    "objectID": "machine_controls.html#using-machine-learning",
    "href": "machine_controls.html#using-machine-learning",
    "title": "Machine controls",
    "section": "Using machine learning",
    "text": "Using machine learning\nMachineControl does the following:\n\nautomatically select a group of countries from a global list to form a smaller set of control countries\nestimate a GDP value for “synthetic Brazil” using pre-enactment data on the outcome of interest\ncheck the statistical quality of the synthetic control\ncalculates the difference between post-reforms actual Brazilian GDP growth to synthetic Brazil’s to measure the effect of the labour reform.\n\n\nNote: There are many other variables that would be interesting for this study as well, such as various labour market indicators.\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom gingado.utils import list_all_dataflows, load_SDMX_data\nfrom gingado.estimators import MachineControl\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.manifold import TSNE"
  },
  {
    "objectID": "machine_controls.html#downloading-the-data",
    "href": "machine_controls.html#downloading-the-data",
    "title": "Machine controls",
    "section": "Downloading the data",
    "text": "Downloading the data\nFirst, we list all dataflows obtainable with SDMX.\n\n\nCode\ndflows = list_all_dataflows(return_pandas=True)\n\n\n\n\nCode\ndflows[dflows.str.contains('per worker', case=False)]\n\n\nILO   DF_GDP_205U_NOC_NB                             Output per worker (GDP constant 2015 US $) -- ...\n      DF_GDP_211P_NOC_NB                             Output per worker (GDP constant 2021 internati...\n      DF_SDG_0821_NOC_RT                             SDG indicator 8.2.1 - Annual growth rate of ou...\nOECD  OECD.ELS.SAE:DSD_HW@DF_AVG_ANN_HRS_WKD(1.0)      Average annual hours actually worked per worker\nName: dataflow, dtype: object\n\n\nLet’s get the data on output per worker provided by the International Labour Organisation (ILO):\n\n\nCode\noutcome_var = load_SDMX_data(\n    sources={'ILO': 'DF_GDP_211P_NOC_NB'}, \n    keys={'FREQ': 'A'}, \n    params={'startPeriod': 2000, 'endPeriod': 2022}\n)\n\n\nQuerying data from ILO's dataflow 'DF_GDP_211P_NOC_NB' - Output per worker (GDP constant 2021 international $ at PPP) -- ILO modelled estimates, Nov. 2024...\n\n\nThis dataflow provides one series for each country, as seen below.\n\n\nCode\ncol_fields = pd.DataFrame([c.split(\"__\") for c in outcome_var.columns])\nfor col in col_fields.columns:\n    print(\"No of unique values for column No\", col, \": \", col_fields[col].nunique())\nprint(\"\\nFirst five rows:\")\nprint(col_fields.head())\n\n\nNo of unique values for column No 0 :  1\nNo of unique values for column No 1 :  276\nNo of unique values for column No 2 :  1\nNo of unique values for column No 3 :  1\n\nFirst five rows:\n     0                       1  2            3\n0  ILO  DF_GDP_211P_NOC_NB_AFG  A  GDP_211P_NB\n1  ILO  DF_GDP_211P_NOC_NB_AGO  A  GDP_211P_NB\n2  ILO  DF_GDP_211P_NOC_NB_ALB  A  GDP_211P_NB\n3  ILO  DF_GDP_211P_NOC_NB_ARE  A  GDP_211P_NB\n4  ILO  DF_GDP_211P_NOC_NB_ARG  A  GDP_211P_NB\n\n\nBecause 271 is much higher than the number of countries that usually report international statistics (close to 200), it is likely some 70 columns or more correspond to aggregations, typically used in statistics for convenience (eg, one code that encompasses all countries in the European Union, etc). We need to take them out, in case Brazil is a constituent of any of those aggregations.\nOne common way of spotting these aggregations in international statistics is finding the country codes that begin with “X”.\n\n\nCode\nlen([c for c in outcome_var.columns if \"DF_GDP_211P_NOC_NB_X\" in c])\n\n\n87\n\n\nAlso, we will take out other countries that have undergone labour reforms more or less around the same time. From Serra, Bottega, and Sanches (2022), I am aware of Argentina 🇦🇷, Costa Rica 🇨🇷, Paraguay 🇵🇾, and Uruguay 🇺🇾.\nIt is important to note, though, that these countries above were named by Serra, Bottega, and Sanches (2022) because their synthetic control donor pool consistent of geographically close countries. Other countries might have also enacted labour reforms in that period. For expositional purposes, we can assume no further country needs to be taken out of the same.\n\n\nCode\ncol_filter = [\n    c for c in outcome_var.columns\n    if \"DF_GDP_211P_NOC_NB_X\" not in c\n    and \"DF_GDP_211P_NOC_NB_ARG\" not in c\n    and \"DF_GDP_211P_NOC_NB_CRI\" not in c\n    and \"DF_GDP_211P_NOC_NB_PAR\" not in c\n    and \"DF_GDP_211P_NOC_NB_URY\" not in c\n]\n\nX = outcome_var[col_filter]\nX = X.dropna(axis=1)\n\n# cleaning out the name to remove the constant portions across all countries\nX.columns = [c.replace(\"ILO__DF_GDP_211P_NOC_NB_\", \"\") \\\n    .replace(\"__A__GDP_211P_NB\", \"\") for c in X.columns]\n\ncol_BRA = [c for c in X.columns if c == \"BRA\"]\ny = X.pop(col_BRA[0])\n\nassert X.shape[0] == y.shape[0]\n\n\nThis is what the series of annual output per worker in Brazil looks like. A vertical line marking November 2017, when the labour reforms entered into force.\n\n\nCode\nlaw_date = '2017-11-11'\nylabel = 'PPP 2017 US$ per worker'\n\nax = y.plot(legend=False)\nax.axvline(x=law_date, color='r', linestyle='--')\nplt.ylabel(ylabel)\nplt.xlabel('')\nplt.title('Labour productivity in Brazil')\n\n\nText(0.5, 1.0, 'Labour productivity in Brazil')\n\n\n\n\n\n\n\n\n\nBefore creating the MachineControl object, a final comment on the intervention date.\nSince the reforms were enacted and entered into force in the same year of 2017, we can be conservative and consider: - pre-intervention data up to end-2016 - post-intervention data from 2018 onwards\n\n\nCode\nX_pre, y_pre = X[:'2016-12-31'], y[:'2016-12-31']\n\nassert X_pre.shape[0] == y_pre.shape[0]\nX_pre.shape, y_pre.shape\n\n\n((17, 184), (17,))"
  },
  {
    "objectID": "machine_controls.html#using-the-machinecontrol-object",
    "href": "machine_controls.html#using-the-machinecontrol-object",
    "title": "Machine controls",
    "section": "Using the MachineControl object",
    "text": "Using the MachineControl object\nThe code chunk below shows how a MachineControl object can be created.\nAs illustrated below, users can not only choose the clustering, estimator and manifold learning algorithms that best suit their needs, but also pass specific arguments to each of these elements.\n\n\nCode\nsynth_BR = MachineControl(\n    cluster_alg=AffinityPropagation(max_iter=10_000),\n    estimator=RandomForestRegressor(),\n    manifold=TSNE(perplexity=5)\n)\n\n\nLet’s train and then inspect the MachineControl object:\n\n\nCode\nsynth_BR.fit(X_pre, y_pre)\n\n\nMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. MachineControliFittedMachineControl(cluster_alg=AffinityPropagation(max_iter=10000),\n               estimator=RandomForestRegressor(), manifold=TSNE(perplexity=5)) cluster_alg: AffinityPropagationAffinityPropagation(max_iter=10000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(max_iter=10000) estimator: RandomForestRegressorRandomForestRegressor()  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor() manifold: TSNETSNE(perplexity=5)  TSNE?Documentation for TSNETSNE(perplexity=5) \n\n\nAs shown above, the machine control estimator comprises:\n\nA clustering algorithm that selects the donor pool from the larger population (affinity propagation)\nA supervised learning algorithm that will use the donor pool to estimate contemporanous values for Brazil (random forest)\nA manifold learning algorithm that summarises the different time series in a 2-dimensional embedding space, enabling easier comparison of Brazil with each other country and with the synthetic control (t-SNE)\n\nAt this stage, we can extract the list of donor countries, ie, those that will be used in the control.\n\n\nCode\n\" \".join(synth_BR.donor_pool_)\n\n\n'BGR BIH BLR BWA CUB DOM EGY IRN JOR KAZ LBN LCA MDV MEX MKD MUS NAM PAN SRB STP TUN ZAF'\n\n\nThese countries are: Albania 🇦🇱, Bulgaria 🇧🇬, Bosnia and Herzegovina 🇧🇦, Belarus 🇧🇾, Barbados 🇧🇧, Botswana 🇧🇼, Colombia 🇨🇴, Cuba 🇨🇺, Dominican Republic 🇩🇴, Algeria 🇩🇿, Egypt 🇪🇬, Fiji 🇫🇯, Guyana 🇬🇾, Iraq 🇮🇶, Kazakhstan 🇰🇿, Saint Lucia 🇱🇨, North Macedonia 🇲🇰, Mauritius 🇲🇺, Namibia 🇳🇦, State of Palestine 🇵🇸, Serbia 🇷🇸, Eswatini 🇸🇿, Tunisia 🇹🇳, Saint Vincent and the Grenadines 🇻🇨, Yemen 🇾🇪 and South Africa 🇿🇦.\n\nNote: This list does not imply all countries contribute equally to estimating the synthetic version of Brazil. In fact, some might even end up not even contributing in the estimating equation. The selected list also does not imply a causal explanation from any of those countries into the Brazilian dynamics. They are merely closest to Brazil in this clustering exercise, and as such, likely to be a good predictor at the same time period.\n\nAnd here is how the outcome variable for these countries (grey) compares with Brazil’s (red).\n\n\nCode\nax = X_pre[synth_BR.donor_pool_].plot(legend=False, color=\"grey\", linewidth=0.5)\ny_pre.plot(ax=ax, color=\"red\", linewidth=2.5)\n\n\n\n\n\n\n\n\n\nPlotting the result of the manifold learning underscores whether the synthetic control seems indeed to come from a similar space in the data distribution as the entity of interest.\n\n\nCode\ncolors = [(0.5, 0.5, 0.5, 0.35)] * X.shape[1] + ['red', 'blue']\n\nfig, ax = plt.subplots()\nax.scatter(\n    synth_BR.manifold_embed_[:, 0], synth_BR.manifold_embed_[:, 1],\n    color=colors,\n    )\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nsynth_BR.control_quality_test_ / 100\n\n\nnp.float64(0.014806594082073521)\n\n\nAs can be seen in the graph above, the actual Brazil 🇧🇷 (blue) and the machine controls (red) are almost juxtaposed.\nTo confirm objectively that the control is good, the Euclidean distance between the embeddings for the control and the actual outcome are one of the lowest, more specifically at the 2% percentile.\nTogether, both point to a strong signal that the control managed to replicate the pre-intervention outcome.\nSo now it’s time to look at the results in the years following the event."
  },
  {
    "objectID": "machine_controls.html#results",
    "href": "machine_controls.html#results",
    "title": "Machine controls",
    "section": "Results",
    "text": "Results\n\n\nCode\nax = synth_BR.predict(X, y).plot(legend=False)\nax.axvline(x='2017-11-11', color='r', linestyle='--')\nplt.ylabel('PPP 2017 US$ per worker')\nplt.xlabel('')\nplt.title(\"Labour productivity in Brazil\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTreatment effect\nThe main result is shown below, corresponding to the difference between actual and estimated value for the whole time series (from 2000). If the red line (Brazil) stays close to the other ones - placebo estimations.\n\n\nCode\nall_diffs = pd.concat([\n    synth_BR.placebo_diff_,\n    synth_BR.diff_\n], axis=1)\ncolors = ['grey'] * synth_BR.placebo_diff_.shape[1] + ['red']\ny_label = 'PPP 2017 US$ per worker'\neffect_title = \"Effect of deregulation on labour productivity in Brazil\"\n\nax = all_diffs.plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()\n\n\n\n\n\n\n\n\n\nIt seems there is some outlier effect with one of the control countries.\n\n\nCode\nsynth_BR.placebo_diff_.loc['2022-01-01'].sort_values(ascending=False).index[0]\n\n\n'EGY'\n\n\nAccording to the code above, it is Guyana 🇬🇾. Now plotting without this country:\n\n\n\nCode\ncolors = ['grey'] * (synth_BR.placebo_diff_.shape[1] - 1)+ ['red']\nax = all_diffs[[c for c in all_diffs.columns if c != \"GUY\"]].plot(legend=False, color=colors)\nax.axvline(x='2016-12-31', color='black', linestyle='solid')\nax.axvline(x='2017-11-11', color='red', linestyle='--')\nax.axhline(y=0, color='black', linewidth=1.2)\nplt.ylabel(y_label)\nplt.xlabel('')\nplt.title(effect_title)\nplt.show()"
  },
  {
    "objectID": "machine_controls.html#interpretation",
    "href": "machine_controls.html#interpretation",
    "title": "Machine controls",
    "section": "Interpretation",
    "text": "Interpretation\nThe above results hint at a null effect of the reforms on labour productivity after training cutoff date (black vertical line). If anything, the effect squarely close to zero throughout the years following the reform (red vertical line), even as the productivity for the majority of control countries actually went up in the years before the pandemic."
  },
  {
    "objectID": "barrolee1994.html",
    "href": "barrolee1994.html",
    "title": "Using gingado to understand economic growth",
    "section": "",
    "text": "This notebook showcases one possible use of gingado by estimating economic growth across countries, using the dataset studied by Barro and Lee (1994). You can run this notebook interactively, by clicking on the appropriate link above.\nThis dataset has been widely studied in economics. Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021) are two studies of this dataset that are most related to machine learning.\nThis notebook will use gingado to compare quickly setup a well-performing machine learning model and use its results as evidence to support the conditional convergence hypothesis; compare different classes of models (and their combination in a single model), and use and document the best performing alternative.\nBecause the notebook is for pedagogical purposes only, please bear in mind some aspects of the machine learning workflow (such as carefully thinking about the cross-validation strategy) are glossed over in this notebook. Also, only the key academic references are cited; more references can be found in the papers mentioned in this example."
  },
  {
    "objectID": "barrolee1994.html#setting-the-stage",
    "href": "barrolee1994.html#setting-the-stage",
    "title": "Using gingado to understand economic growth",
    "section": "Setting the stage",
    "text": "Setting the stage\nWe will import packages as the work progresses. This will help highlight the specific steps in the workflow that gingado can be helpful with.\n\n\nCode\nimport pandas as pd\n\n\nThe data is available in the online annex to Giannone, Lenza, and Primiceri (2021). In that paper, this dataset corresponds to what the authors call “macro2”. The original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\n\n\nCode\nfrom gingado.datasets import load_BarroLee_1994\n\nX, y = load_BarroLee_1994()\n\n\nThe dataset contains explanatory variables representing per-capita growth between 1960 and 1985, for 90 countries.\n\n\nCode\nX.columns\n\n\nIndex(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65',\n       'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65',\n       'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n       'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65', 'highm65',\n       'highf65', 'highc65', 'highcm65', 'highcf65', 'human65', 'humanm65',\n       'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65', 'nom65', 'nof65',\n       'pinstab1', 'pop65', 'worker65', 'pop1565', 'pop6565', 'sec65',\n       'secm65', 'secf65', 'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65',\n       'syrf65', 'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'],\n      dtype='object')\n\n\n\n\nCode\nX.head().T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nUnnamed: 0\n0.000000\n1.000000\n2.000000\n3.000000\n4.000000\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n\n\n...\n...\n...\n...\n...\n...\n\n\nteasec65\n17.300000\n18.000000\n20.700000\n22.700000\n17.600000\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n\n\n\n\n62 rows × 5 columns\n\n\n\nThe outcome variable is represented here:\n\n\nCode\ny.plot.hist(bins=90, title='GDP growth')"
  },
  {
    "objectID": "barrolee1994.html#establishing-a-benchmark-model",
    "href": "barrolee1994.html#establishing-a-benchmark-model",
    "title": "Using gingado to understand economic growth",
    "section": "Establishing a benchmark model",
    "text": "Establishing a benchmark model\nGenerally speaking, it is a good idea to establish a benchmark model at the first stages of development of the machine learning model. gingado offers a class of automatic benchmarks that can be used off-the-shelf depending on the task at hand: RegressionBenchmark and ClassificationBenchmark. It is also good to keep in mind that more advanced users can create their own benchmark on top of a base class provided by gingado: ggdBenchmark.\nFor this application, since we are interested in running a regression task, we will use RegressionBenchmark:\n\n\nCode\nfrom gingado.benchmark import RegressionBenchmark\n\n\nWhat this object does is the following:\n\nit creates a random forest\nthree different versions of the random forest are trained on the user data\nthe version that performs better is chosen as the benchmark\nright after it is trained, the benchmark is documented using gingado’s ModelCard documenter.\n\nThe user can easily change the parameters above. For example, instead of a random forest the user might prefer a neural network as the benchmark. Or, in lieu of the default parameters provided by gingado, users might have their own idea of what could be a reasonable parameter space to search.\nRandom forests are chosen as the go-to benchmark algorithm because of their reasonably good performance in a wide variety of settings, the fact that they don’t require much data transformation (ie, normalising the data to have zero mean and one standard deviation), and by virtue of their relatively transparency about the importance of each regressor.\nThe first step is to initialise the benchmark object. At this time, we pass some arguments about how we want it to behave. In this case, we set the verbosity level to produce output related to each alternative considered. Then we fit it to the data.\n\n\nCode\n#####\n#####\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X, y)\n\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor() \n\n\n\n\nCode\nbenchmark = RegressionBenchmark(verbose_grid=2)\nbenchmark.fit(X, y)\n\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n\n\nRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    verbose_grid=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RegressionBenchmarkiFittedRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    verbose_grid=2) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True) \n\n\nAs we can see above, with a few lines we have trained a random forest on the dataset. In this case, the benchmark was the better of six versions of the random forest, according to the default hyperparameters: 100 and 250 estimators were alternated with models for which the maximum number of regressors analysed by individual trees changesd fom the maximum, a square root and a log of the number of regressors. They were each trained using a 5-fold cross-validation.\nLet’s see which one was the best performing in this case, and hence our benchmark model:\n\n\nCode\npd.DataFrame(benchmark.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.081632\n0.199995\n0.075092\n0.187042\n0.214161\n0.512574\n\n\nstd_fit_time\n0.003303\n0.004798\n0.00268\n0.002429\n0.006796\n0.0039\n\n\nmean_score_time\n0.002252\n0.004852\n0.002056\n0.0047\n0.0025\n0.004951\n\n\nstd_score_time\n0.000404\n0.000709\n0.000155\n0.000642\n0.000501\n0.000787\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n-0.685392\n-0.391721\n-0.948843\n-0.683873\n-0.629106\n-0.454698\n\n\nsplit1_test_score\n-0.895469\n-0.779989\n-0.900981\n-0.931069\n-0.623699\n-0.689593\n\n\nsplit2_test_score\n0.367596\n0.431917\n0.256544\n0.319657\n0.566962\n0.646417\n\n\nsplit3_test_score\n-0.057115\n0.000795\n-0.102078\n-0.087003\n0.012035\n-0.008196\n\n\nsplit4_test_score\n0.043378\n0.04427\n-0.06196\n-0.019205\n-0.040598\n-0.039951\n\n\nsplit5_test_score\n0.126446\n0.303863\n0.328339\n0.334566\n0.2475\n0.26604\n\n\nsplit6_test_score\n0.134727\n0.087939\n0.059286\n0.058916\n0.12938\n0.288939\n\n\nsplit7_test_score\n-1.235767\n-1.313177\n-1.090751\n-1.124379\n-1.30537\n-1.142031\n\n\nsplit8_test_score\n0.149101\n0.146561\n0.093801\n0.147215\n0.090569\n0.104888\n\n\nsplit9_test_score\n0.114809\n0.066065\n0.124151\n-0.021437\n0.15243\n0.068269\n\n\nmean_test_score\n-0.193768\n-0.140348\n-0.224249\n-0.200661\n-0.13999\n-0.095992\n\n\nstd_test_score\n0.513138\n0.510266\n0.511326\n0.494349\n0.52206\n0.49838\n\n\nrank_test_score\n4\n3\n6\n5\n2\n1\n\n\n\n\n\n\n\nThe values above are calculated with \\(R^2\\), the default scoring function for a random forest from the scikit-learn package. Suppose that instead we would like a benchmark model that is optimised on the maximum error, ie a benchmark that minimises the worst deviation from prediction to ground truth for all the sample. These are the steps that we would take. Note that a more complete list of ready-made scoring parameters and how to create your own function can be found here.\n\n\nCode\nbenchmark_lower_worsterror = RegressionBenchmark(scoring='max_error', verbose_grid=2)\nbenchmark_lower_worsterror.fit(X, y)\n\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n\n\nRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    scoring='max_error', verbose_grid=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RegressionBenchmarkiFittedRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                    scoring='max_error', verbose_grid=2) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True) \n\n\n\n\nCode\npd.DataFrame(benchmark_lower_worsterror.benchmark.cv_results_).T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nmean_fit_time\n0.079291\n0.196221\n0.074016\n0.187242\n0.210768\n0.527957\n\n\nstd_fit_time\n0.001336\n0.001498\n0.001389\n0.005723\n0.00311\n0.010984\n\n\nmean_score_time\n0.002\n0.00501\n0.001905\n0.004451\n0.0022\n0.004608\n\n\nstd_score_time\n0.000003\n0.000594\n0.000195\n0.000471\n0.000399\n0.000438\n\n\nparam_max_features\nsqrt\nsqrt\nlog2\nlog2\nNone\nNone\n\n\nparam_n_estimators\n100\n250\n100\n250\n100\n250\n\n\nparams\n{'max_features': 'sqrt', 'n_estimators': 100}\n{'max_features': 'sqrt', 'n_estimators': 250}\n{'max_features': 'log2', 'n_estimators': 100}\n{'max_features': 'log2', 'n_estimators': 250}\n{'max_features': None, 'n_estimators': 100}\n{'max_features': None, 'n_estimators': 250}\n\n\nsplit0_test_score\n-0.085467\n-0.091153\n-0.0904\n-0.084083\n-0.088967\n-0.08769\n\n\nsplit1_test_score\n-0.092157\n-0.091021\n-0.082699\n-0.090982\n-0.102708\n-0.104783\n\n\nsplit2_test_score\n-0.106007\n-0.104499\n-0.098513\n-0.101979\n-0.092124\n-0.08801\n\n\nsplit3_test_score\n-0.09431\n-0.100525\n-0.093991\n-0.105385\n-0.087987\n-0.084116\n\n\nsplit4_test_score\n-0.076174\n-0.077168\n-0.082671\n-0.086901\n-0.077802\n-0.080576\n\n\nsplit5_test_score\n-0.142032\n-0.143486\n-0.135608\n-0.14305\n-0.150366\n-0.153509\n\n\nsplit6_test_score\n-0.101488\n-0.094475\n-0.101747\n-0.100747\n-0.088365\n-0.087726\n\n\nsplit7_test_score\n-0.106873\n-0.10398\n-0.105304\n-0.101232\n-0.079848\n-0.087725\n\n\nsplit8_test_score\n-0.123041\n-0.124119\n-0.1214\n-0.123711\n-0.122154\n-0.122765\n\n\nsplit9_test_score\n-0.064935\n-0.064271\n-0.06124\n-0.062287\n-0.04405\n-0.044849\n\n\nmean_test_score\n-0.099248\n-0.09947\n-0.097357\n-0.100036\n-0.093437\n-0.094175\n\n\nstd_test_score\n0.021166\n0.021198\n0.019798\n0.020926\n0.02664\n0.027079\n\n\nrank_test_score\n4\n5\n3\n6\n1\n2\n\n\n\n\n\n\n\nNow we even have two benchmark models.\nWe could further tweak and adjust them, but one of the ideas behind having a benchmark is that it is simple and easy to set up.\nLet’s retain only the first benchmark, for simplicity, and now look at the predictions, comparing them to the original growth values.\n\n\nCode\ny_pred = benchmark.predict(X)\n\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(\n        x='y', y='y_pred',\n         grid=True, \n         title='Actual and predicted outcome',\n         xlabel='actual GDP growth',\n         ylabel='predicted GDP growth')\n\n\n\n\n\n\n\n\n\nAnd now a histogram of the benchmark’s errors:\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30, title='Residual')\n\n\n\n\n\n\n\n\n\nSince the benchmark is a random forest model, we can see what are the most important regressors, measured as the average reduction in impurity across the trees in the random forest that actually use that particular regressor. They are scaled so that the sum for all features is one. Higher importance amounts indicate that that particular regressor is a more important contributor to the final prediction.\n\n\nCode\nregressor_importance = pd.DataFrame(\n    benchmark.benchmark.best_estimator_.feature_importances_, \n    index=X.columns, \n    columns=[\"Importance\"]\n    )\n\nregressor_importance.sort_values(by=\"Importance\", ascending=False) \\\n    .plot.bar(figsize=(20, 8), title='Regressor importance')\n\n\n\n\n\n\n\n\n\nFrom the graph above, we can see that the regressor bmp1l (black-market premium on foreign exchange) predominates. Interestingly, Belloni, Chernozhukov, and Hansen (2011) using squared-root lasso also find this regressor to be important."
  },
  {
    "objectID": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "href": "barrolee1994.html#testing-the-conditional-converge-hypothesis",
    "title": "Using gingado to understand economic growth",
    "section": "Testing the conditional converge hypothesis",
    "text": "Testing the conditional converge hypothesis\nNow we can leverage our automatic benchmark model to test the conditional converge hypothesis - ie, the preposition that countries with lower starting GDP tend to grow faster than other comparable countries. In other words, this hypothesis predicts that when GDP growth is regressed on the level of past GDP and on an adequate set of covariates \\(X\\), the coefficient on past GDP levels are negative.\nSince we have the results for the importance of each regressor in separating countries by their growth result, we can compare the estimated coefficient for GDP levels in regressions that include different regressors in the vector \\(X\\). To maintain this example a simple exercise, the following three models are estimated:\n\n\\(X\\) contains the five most important regressors, as estimated by the benchmark model (see the graph above)\n\\(X\\) contains the five least important regressors, from the same estimation as above\n\\(X\\) is the empty set - in other words, this is a simple equation on GDP growth on GDP levels\n\nA result that would be consistent with the conditionality of the conditional convergence hypothesis is the first equation resulting in a negative coefficient for starting GDP, while the following two equations may not necessarily be successful in identifying a negative coefficient. This is because the least important regressors are not likely to have sufficient predictive power to separate countries into comparable groups.\nThe five more and less important regressors are:\n\n\nCode\ntop_five = regressor_importance.sort_values(by=\"Importance\", ascending=False).head(5)\nbottom_five = regressor_importance.sort_values(by=\"Importance\", ascending=True).head(5)\n\ntop_five, bottom_five\n\n\n(            Importance\n bmp1l         0.163180\n pop6565       0.113292\n teasec65      0.058312\n Unnamed: 0    0.038863\n invsh41       0.031538,\n           Importance\n pm65        0.001660\n p65         0.002118\n humanm65    0.002158\n sf65        0.002263\n hyr65       0.002368)\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n\n\n\nCode\ngdp_level = 'gdpsh465'\n\n\n\n\nCode\nX_topfive = X[[gdp_level] + list(top_five.index)]\nX_topfive = sm.add_constant(X_topfive)\nX_topfive.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\nbmp1l\npop6565\nteasec65\nUnnamed: 0\ninvsh41\n\n\n\n\n0\n1.0\n6.591674\n0.2837\n0.027591\n17.3\n0\n0.11898\n\n\n1\n1.0\n6.829794\n0.6141\n0.035637\n18.0\n1\n0.12048\n\n\n2\n1.0\n8.895082\n0.0000\n0.076685\n20.7\n2\n0.23098\n\n\n3\n1.0\n7.565275\n0.1997\n0.031039\n22.7\n3\n0.12928\n\n\n4\n1.0\n7.162397\n0.1740\n0.026281\n17.6\n4\n0.07932\n\n\n\n\n\n\n\n\n\nCode\nX_bottomfive = X[[gdp_level] + list(bottom_five.index)]\nX_bottomfive = sm.add_constant(X_bottomfive)\nX_bottomfive.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\npm65\np65\nhumanm65\nsf65\nhyr65\n\n\n\n\n0\n1.0\n6.591674\n0.37\n0.29\n0.568\n0.02\n0.004\n\n\n1\n1.0\n6.829794\n1.00\n0.91\n1.138\n0.09\n0.027\n\n\n2\n1.0\n8.895082\n1.00\n1.00\n8.249\n0.51\n0.424\n\n\n3\n1.0\n7.565275\n1.00\n1.00\n3.860\n0.31\n0.104\n\n\n4\n1.0\n7.162397\n0.85\n0.82\n2.084\n0.13\n0.022\n\n\n\n\n\n\n\n\n\nCode\nX_onlyGDPlevel = sm.add_constant(X[gdp_level])\nX_onlyGDPlevel.head()\n\n\n\n\n\n\n\n\n\nconst\ngdpsh465\n\n\n\n\n0\n1.0\n6.591674\n\n\n1\n1.0\n6.829794\n\n\n2\n1.0\n8.895082\n\n\n3\n1.0\n7.565275\n\n\n4\n1.0\n7.162397\n\n\n\n\n\n\n\n\n\nCode\nmodels = dict(\n    topfive = sm.OLS(y, X_topfive).fit(),\n    bottomfive = sm.OLS(y, X_bottomfive).fit(),\n    onlyGDPlevel = sm.OLS(y, X_onlyGDPlevel).fit()\n)\n\n\n\n\nCode\ncoefs = pd.DataFrame({name: model.conf_int().loc[gdp_level] for name, model in models.items()})\ncoefs.loc[0.5] = [model.params[gdp_level] for _, model in models.items()]\ncoefs = coefs.sort_index().reset_index(drop=True)\ncoefs.index = ['[0.025', 'coef on GDP levels', '0.975]']\ncoefs\n\n\n\n\n\n\n\n\n\ntopfive\nbottomfive\nonlyGDPlevel\n\n\n\n\n[0.025\n-0.039422\n-0.041275\n-0.010810\n\n\ncoef on GDP levels\n-0.022210\n-0.018263\n0.001317\n\n\n0.975]\n-0.004998\n0.004749\n0.013444\n\n\n\n\n\n\n\nThe equation using the top five regressors in explanatory power yielded a coefficient that is statistically speaking negative under the usual confidence interval levels. In contrast, the regression using the bottom five regressors failed to maintain that level of statistical significance (although the coefficient point estimate was still negative). And finally the regression on GDP level solely resulted, as in the past literature, on a point estimate that is also statistically not different than zero.\nThese results above offer a different way to add evidence to the conditional convergence hypothesis. In particular, with the help of gingado’s RegressionBenchmark model, it is possible to identify which covariates can meaningfully serve as covariates in a growth equation from those that cannot. This is important because if the covariate selection for some reason included only variables with little explanatory power instead of the most relevant ones, an economist might erroneously reach a different conclusion."
  },
  {
    "objectID": "barrolee1994.html#model-documentation",
    "href": "barrolee1994.html#model-documentation",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nImportantly for model documentation, the benchmark already has some baseline documentation set up. If the user wishes, they can use that as a basis to document their model. Note that the output is in a raw format that is suitable for machine reading and writing. Intermediary and advanced users may wish to use that format to construct personalised forms, documents, etc.\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2025-03-06 11:35:53 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': RandomForestRegressor(max_features=None, n_estimators=250, oob_score=True),\n   'best_index_': np.int64(5),\n   'best_params_': {'max_features': None, 'n_estimators': 250},\n   'best_score_': np.float64(-0.09599158732753489),\n   'cv_results_': {'mean_fit_time': array([0.08163157, 0.19999509, 0.07509222, 0.18704247, 0.21416073,\n           0.51257398]),\n    'std_fit_time': array([0.00330253, 0.00479754, 0.00267975, 0.00242872, 0.00679633,\n           0.00389989]),\n    'mean_score_time': array([0.00225196, 0.00485158, 0.00205555, 0.00470035, 0.0024997 ,\n           0.00495148]),\n    'std_score_time': array([0.00040393, 0.00070862, 0.00015476, 0.0006419 , 0.00050143,\n           0.00078728]),\n    'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n                 mask=[False, False, False, False, False, False],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n                 mask=[False, False, False, False, False, False],\n           fill_value=999999),\n    'params': [{'max_features': 'sqrt', 'n_estimators': 100},\n     {'max_features': 'sqrt', 'n_estimators': 250},\n     {'max_features': 'log2', 'n_estimators': 100},\n     {'max_features': 'log2', 'n_estimators': 250},\n     {'max_features': None, 'n_estimators': 100},\n     {'max_features': None, 'n_estimators': 250}],\n    'split0_test_score': array([-0.68539151, -0.39172073, -0.94884314, -0.68387333, -0.62910558,\n           -0.4546978 ]),\n    'split1_test_score': array([-0.89546899, -0.77998868, -0.90098143, -0.93106863, -0.62369901,\n           -0.68959294]),\n    'split2_test_score': array([0.36759634, 0.43191685, 0.25654367, 0.31965689, 0.56696248,\n           0.64641681]),\n    'split3_test_score': array([-0.05711456,  0.00079545, -0.10207794, -0.08700329,  0.01203477,\n           -0.00819631]),\n    'split4_test_score': array([ 0.04337787,  0.04426984, -0.06195965, -0.01920524, -0.04059757,\n           -0.03995103]),\n    'split5_test_score': array([0.12644615, 0.3038631 , 0.32833905, 0.33456636, 0.24749989,\n           0.26603966]),\n    'split6_test_score': array([0.13472679, 0.08793907, 0.05928566, 0.05891631, 0.12938028,\n           0.28893914]),\n    'split7_test_score': array([-1.23576689, -1.31317686, -1.090751  , -1.12437927, -1.30536957,\n           -1.1420307 ]),\n    'split8_test_score': array([0.14910094, 0.14656088, 0.09380144, 0.14721524, 0.09056898,\n           0.10488812]),\n    'split9_test_score': array([ 0.11480895,  0.06606506,  0.12415132, -0.02143674,  0.15243006,\n            0.06826919]),\n    'mean_test_score': array([-0.19376849, -0.1403476 , -0.2242492 , -0.20066117, -0.13998953,\n           -0.09599159]),\n    'std_test_score': array([0.51313806, 0.5102656 , 0.5113256 , 0.49434884, 0.52205995,\n           0.49837956]),\n    'rank_test_score': array([4, 3, 6, 5, 2, 1], dtype=int32)},\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 10,\n   'refit_time_': 0.5833122730255127,\n   'scorer_': &lt;class 'sklearn.ensemble._forest.RandomForestRegressor'&gt;.score},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nSince there is some information in the model documentation that was automatically added, we might want to concentrate on the fields in the model card that are yet to be answered. Actually, this is the purpose of gingado’s automatic documentation: to afford users more time so they can invest, if they want, on model documentation.\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nLet’s fill some information:\n\n\nCode\nbenchmark.model_documentation.fill_info({\n    'intended_use': {\n        'primary_uses': 'This model is trained for pedagogical uses only.',\n        'primary_users': 'Everyone is welcome to follow the description showing the development of this benchmark.'\n    }\n})\n\n\nNote the format, based on a Python dictionary. In particular, the open_questions method results include keys divided by double underscores. As seen above, these should be interpreted as different levels of the documentation template, leading to a nested dictionary.\nNow when we confirm that the questions answered above are no longer “open questions”:\n\n\nCode\nbenchmark.model_documentation.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nIf we want, at any time we can save the documentation to a local JSON file, as well as read another document."
  },
  {
    "objectID": "barrolee1994.html#trying-out-model-alternatives",
    "href": "barrolee1994.html#trying-out-model-alternatives",
    "title": "Using gingado to understand economic growth",
    "section": "Trying out model alternatives",
    "text": "Trying out model alternatives\nThe benchmark model may be enough for some analyses, or maybe the user is interested in using the benchmark to explore the data and have an understanding of the importance of each regressor, to concentrate their work on data that can be meaningful for their purposes. But oftentimes a user will want to seek a machine learning model that performs as well as possible.\nFor users that want to manually create other models, gingado allows the possibility of comparing them with the benchmark. If the user model is better, it becomes the new benchmark!\nFor the following analyses, we will use K-fold as cross-validation, with 5 splits of the sample.\n\nFirst candidate: a gradient boosting tree\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n\n\nCode\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.25],\n    'max_depth': [3, 6, 9]\n}\n\nreg_gradbooster = GradientBoostingRegressor()\n\ngradboosterg_grid = GridSearchCV(\n    reg_gradbooster,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n\n\n\n\nCode\ny_pred = gradboosterg_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)\n\n\n\n\n\n\n\n\n\n\n\nSecond candidate: lasso\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nparam_grid = {\n    'alpha': [0.5, 1, 1.25],\n}\n\nreg_lasso = Lasso(fit_intercept=True)\n\nlasso_grid = GridSearchCV(\n    reg_lasso,\n    param_grid,\n    n_jobs=-1,\n    verbose=2\n).fit(X, y)\n\n\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n\n\n\n\nCode\ny_pred = lasso_grid.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)"
  },
  {
    "objectID": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "href": "barrolee1994.html#comparing-the-models-with-the-benchmark",
    "title": "Using gingado to understand economic growth",
    "section": "Comparing the models with the benchmark",
    "text": "Comparing the models with the benchmark\ngingado allows users to compare different candidate models with the existing benchmark in a very simple way: using the compare method.\n\n\nCode\ncandidates = [gradboosterg_grid, lasso_grid]\nbenchmark.compare(X, y, candidates)\n\n\nFitting 10 folds for each of 4 candidates, totalling 40 fits\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  13.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  14.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.2s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  13.5s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.4s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.4s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.4s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.3s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END candidate_estimator=GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n             estimator=RandomForestRegressor(oob_score=True),\n             param_grid={'max_features': ['sqrt', 'log2', None],\n                         'n_estimators': [100, 250]},\n             verbose=2), candidate_estimator__cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None), candidate_estimator__error_score=nan, candidate_estimator__estimator=RandomForestRegressor(oob_score=True), candidate_estimator__estimator__bootstrap=True, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=squared_error, candidate_estimator__estimator__max_depth=None, candidate_estimator__estimator__max_features=1.0, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__max_samples=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__monotonic_cst=None, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_jobs=None, candidate_estimator__estimator__oob_score=True, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=None, candidate_estimator__param_grid={'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=  12.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   1.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   1.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   1.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   1.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   1.0s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.9s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                         'max_depth': [3, 6, 9]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=GradientBoostingRegressor(), candidate_estimator__estimator__alpha=0.9, candidate_estimator__estimator__ccp_alpha=0.0, candidate_estimator__estimator__criterion=friedman_mse, candidate_estimator__estimator__init=None, candidate_estimator__estimator__learning_rate=0.1, candidate_estimator__estimator__loss=squared_error, candidate_estimator__estimator__max_depth=3, candidate_estimator__estimator__max_features=None, candidate_estimator__estimator__max_leaf_nodes=None, candidate_estimator__estimator__min_impurity_decrease=0.0, candidate_estimator__estimator__min_samples_leaf=1, candidate_estimator__estimator__min_samples_split=2, candidate_estimator__estimator__min_weight_fraction_leaf=0.0, candidate_estimator__estimator__n_estimators=100, candidate_estimator__estimator__n_iter_no_change=None, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__subsample=1.0, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__validation_fraction=0.1, candidate_estimator__estimator__verbose=0, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   1.1s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n             verbose=2), candidate_estimator__cv=None, candidate_estimator__error_score=nan, candidate_estimator__estimator=Lasso(), candidate_estimator__estimator__alpha=1.0, candidate_estimator__estimator__copy_X=True, candidate_estimator__estimator__fit_intercept=True, candidate_estimator__estimator__max_iter=1000, candidate_estimator__estimator__positive=False, candidate_estimator__estimator__precompute=False, candidate_estimator__estimator__random_state=None, candidate_estimator__estimator__selection=cyclic, candidate_estimator__estimator__tol=0.0001, candidate_estimator__estimator__warm_start=False, candidate_estimator__n_jobs=-1, candidate_estimator__param_grid={'alpha': [0.5, 1, 1.25]}, candidate_estimator__pre_dispatch=2*n_jobs, candidate_estimator__refit=True, candidate_estimator__return_train_score=False, candidate_estimator__scoring=None, candidate_estimator__verbose=2; total time=   0.0s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.7s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  14.1s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.2s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.5s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.9s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.7s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  13.6s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nFitting 5 folds for each of 9 candidates, totalling 45 fits\nFitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV] END candidate_estimator=VotingRegressor(estimators=[('candidate_1',\n                             GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                          estimator=RandomForestRegressor(oob_score=True),\n                                          param_grid={'max_features': ['sqrt',\n                                                                       'log2',\n                                                                       None],\n                                                      'n_estimators': [100,\n                                                                       250]},\n                                          verbose=2)),\n                            ('candidate_2',\n                             GridSearchCV(estimator=GradientBoostingRegressor(),\n                                          n_jobs=-1,\n                                          param_grid={'learning_rate': [0.01,\n                                                                        0.1,\n                                                                        0.25],\n                                                      'max_depth': [3, 6, 9]},\n                                          verbose=2)),\n                            ('candidate_3',\n                             GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                          param_grid={'alpha': [0.5, 1, 1.25]},\n                                          verbose=2))]); total time=  14.1s\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=sqrt, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=100; total time=   0.0s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=log2, n_estimators=250; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=100; total time=   0.1s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\n[CV] END ................max_features=None, n_estimators=250; total time=   0.4s\nBenchmark updated!\nNew benchmark:\nPipeline(steps=[('candidate_estimator',\n                 GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                              estimator=RandomForestRegressor(oob_score=True),\n                              param_grid={'max_features': ['sqrt', 'log2',\n                                                           None],\n                                          'n_estimators': [100, 250]},\n                              verbose=2))])\n\n\nThe output above clearly indicates that after evaluating the models - and their ensemble together with the existing benchmark - at least one of them was better than the current benchmark. Therefore, it will now be the new benchmark.\n\n\nCode\ny_pred = benchmark.predict(X)\npd.DataFrame({\n    'y': y,\n    'y_pred': y_pred\n    }).plot.scatter(x='y', y='y_pred', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(y - y_pred).plot.hist(bins=30)"
  },
  {
    "objectID": "barrolee1994.html#model-documentation-1",
    "href": "barrolee1994.html#model-documentation-1",
    "title": "Using gingado to understand economic growth",
    "section": "Model documentation",
    "text": "Model documentation\nAfter this process, we can now see how the model documentation was updated automatically:\n\n\nCode\nbenchmark.model_documentation.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2025-03-06 11:41:02 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'regressor',\n   'best_estimator_': Pipeline(steps=[('candidate_estimator',\n                    GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                 estimator=RandomForestRegressor(oob_score=True),\n                                 param_grid={'max_features': ['sqrt', 'log2',\n                                                              None],\n                                             'n_estimators': [100, 250]},\n                                 verbose=2))]),\n   'best_index_': np.int64(0),\n   'best_params_': {'candidate_estimator': GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                 estimator=RandomForestRegressor(oob_score=True),\n                 param_grid={'max_features': ['sqrt', 'log2', None],\n                             'n_estimators': [100, 250]},\n                 verbose=2),\n    'candidate_estimator__cv': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n    'candidate_estimator__error_score': nan,\n    'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n    'candidate_estimator__estimator__bootstrap': True,\n    'candidate_estimator__estimator__ccp_alpha': 0.0,\n    'candidate_estimator__estimator__criterion': 'squared_error',\n    'candidate_estimator__estimator__max_depth': None,\n    'candidate_estimator__estimator__max_features': 1.0,\n    'candidate_estimator__estimator__max_leaf_nodes': None,\n    'candidate_estimator__estimator__max_samples': None,\n    'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n    'candidate_estimator__estimator__min_samples_leaf': 1,\n    'candidate_estimator__estimator__min_samples_split': 2,\n    'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n    'candidate_estimator__estimator__monotonic_cst': None,\n    'candidate_estimator__estimator__n_estimators': 100,\n    'candidate_estimator__estimator__n_jobs': None,\n    'candidate_estimator__estimator__oob_score': True,\n    'candidate_estimator__estimator__random_state': None,\n    'candidate_estimator__estimator__verbose': 0,\n    'candidate_estimator__estimator__warm_start': False,\n    'candidate_estimator__n_jobs': None,\n    'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n     'max_features': ['sqrt', 'log2', None]},\n    'candidate_estimator__pre_dispatch': '2*n_jobs',\n    'candidate_estimator__refit': True,\n    'candidate_estimator__return_train_score': False,\n    'candidate_estimator__scoring': None,\n    'candidate_estimator__verbose': 2},\n   'best_score_': np.float64(0.07979283271309427),\n   'cv_results_': {'mean_fit_time': array([12.87956212,  1.06817105,  0.02526333, 13.83657551]),\n    'std_fit_time': array([5.93946204e-01, 4.43201264e-02, 4.63332885e-04, 2.68052365e-01]),\n    'mean_score_time': array([0.00410147, 0.00110042, 0.00099959, 0.00530269]),\n    'std_score_time': array([1.37550196e-03, 3.01004570e-04, 6.84390073e-07, 1.53778324e-03]),\n    'param_candidate_estimator': masked_array(data=[GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                    estimator=RandomForestRegressor(oob_score=True),\n                                    param_grid={'max_features': ['sqrt', 'log2', None],\n                                                'n_estimators': [100, 250]},\n                                    verbose=2)                                                                       ,\n                       GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                                    param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                                                'max_depth': [3, 6, 9]},\n                                    verbose=2)                                       ,\n                       GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                                    verbose=2)                                                         ,\n                       VotingRegressor(estimators=[('candidate_1',\n                                                    GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                                                 estimator=RandomForestRegressor(oob_score=True),\n                                                                 param_grid={'max_features': ['sqrt',\n                                                                                              'log2',\n                                                                                              None],\n                                                                             'n_estimators': [100,\n                                                                                              250]},\n                                                                 verbose=2)),\n                                                   ('candidate_2',\n                                                    GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                                 n_jobs=-1,\n                                                                 param_grid={'learning_rate': [0.01,\n                                                                                               0.1,\n                                                                                               0.25],\n                                                                             'max_depth': [3, 6, 9]},\n                                                                 verbose=2)),\n                                                   ('candidate_3',\n                                                    GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                                 param_grid={'alpha': [0.5, 1, 1.25]},\n                                                                 verbose=2))])                                                                    ],\n                 mask=[False, False, False, False],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__cv': masked_array(data=[ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                       None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__error_score': masked_array(data=[nan, nan, nan, --],\n                 mask=[False, False, False,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator': masked_array(data=[RandomForestRegressor(oob_score=True),\n                       GradientBoostingRegressor(), Lasso(), --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__bootstrap': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value=True),\n    'param_candidate_estimator__estimator__ccp_alpha': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__criterion': masked_array(data=['squared_error', 'friedman_mse', --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__max_depth': masked_array(data=[None, 3, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__max_features': masked_array(data=[1.0, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__max_leaf_nodes': masked_array(data=[None, None, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__max_samples': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__min_impurity_decrease': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__min_samples_leaf': masked_array(data=[1, 1, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=999999),\n    'param_candidate_estimator__estimator__min_samples_split': masked_array(data=[2, 2, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=999999),\n    'param_candidate_estimator__estimator__min_weight_fraction_leaf': masked_array(data=[0.0, 0.0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__monotonic_cst': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__n_estimators': masked_array(data=[100, 100, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=999999),\n    'param_candidate_estimator__estimator__n_jobs': masked_array(data=[None, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__oob_score': masked_array(data=[True, --, --, --],\n                 mask=[False,  True,  True,  True],\n           fill_value=True),\n    'param_candidate_estimator__estimator__random_state': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__verbose': masked_array(data=[0, 0, --, --],\n                 mask=[False, False,  True,  True],\n           fill_value=999999),\n    'param_candidate_estimator__estimator__warm_start': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__n_jobs': masked_array(data=[None, -1, -1, --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__param_grid': masked_array(data=[{'n_estimators': [100, 250], 'max_features': ['sqrt', 'log2', None]},\n                       {'learning_rate': [0.01, 0.1, 0.25], 'max_depth': [3, 6, 9]},\n                       {'alpha': [0.5, 1, 1.25]}, --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__pre_dispatch': masked_array(data=['2*n_jobs', '2*n_jobs', '2*n_jobs', --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__refit': masked_array(data=[True, True, True, --],\n                 mask=[False, False, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__return_train_score': masked_array(data=[False, False, False, --],\n                 mask=[False, False, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__scoring': masked_array(data=[None, None, None, --],\n                 mask=[False, False, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__verbose': masked_array(data=[2, 2, 2, --],\n                 mask=[False, False, False,  True],\n           fill_value=999999),\n    'param_candidate_estimator__estimator__alpha': masked_array(data=[--, 0.9, 1.0, --],\n                 mask=[ True, False, False,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__init': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__learning_rate': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__loss': masked_array(data=[--, 'squared_error', --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__n_iter_no_change': masked_array(data=[--, None, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'param_candidate_estimator__estimator__subsample': masked_array(data=[--, 1.0, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__tol': masked_array(data=[--, 0.0001, 0.0001, --],\n                 mask=[ True, False, False,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__validation_fraction': masked_array(data=[--, 0.1, --, --],\n                 mask=[ True, False,  True,  True],\n           fill_value=1e+20),\n    'param_candidate_estimator__estimator__copy_X': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__estimator__fit_intercept': masked_array(data=[--, --, True, --],\n                 mask=[ True,  True, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__estimator__max_iter': masked_array(data=[--, --, 1000, --],\n                 mask=[ True,  True, False,  True],\n           fill_value=999999),\n    'param_candidate_estimator__estimator__positive': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__estimator__precompute': masked_array(data=[--, --, False, --],\n                 mask=[ True,  True, False,  True],\n           fill_value=True),\n    'param_candidate_estimator__estimator__selection': masked_array(data=[--, --, 'cyclic', --],\n                 mask=[ True,  True, False,  True],\n           fill_value=np.str_('?'),\n                dtype=object),\n    'params': [{'candidate_estimator': GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                   estimator=RandomForestRegressor(oob_score=True),\n                   param_grid={'max_features': ['sqrt', 'log2', None],\n                               'n_estimators': [100, 250]},\n                   verbose=2),\n      'candidate_estimator__cv': ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': RandomForestRegressor(oob_score=True),\n      'candidate_estimator__estimator__bootstrap': True,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'squared_error',\n      'candidate_estimator__estimator__max_depth': None,\n      'candidate_estimator__estimator__max_features': 1.0,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__max_samples': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__monotonic_cst': None,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_jobs': None,\n      'candidate_estimator__estimator__oob_score': True,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': None,\n      'candidate_estimator__param_grid': {'n_estimators': [100, 250],\n       'max_features': ['sqrt', 'log2', None]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=GradientBoostingRegressor(), n_jobs=-1,\n                   param_grid={'learning_rate': [0.01, 0.1, 0.25],\n                               'max_depth': [3, 6, 9]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': GradientBoostingRegressor(),\n      'candidate_estimator__estimator__alpha': 0.9,\n      'candidate_estimator__estimator__ccp_alpha': 0.0,\n      'candidate_estimator__estimator__criterion': 'friedman_mse',\n      'candidate_estimator__estimator__init': None,\n      'candidate_estimator__estimator__learning_rate': 0.1,\n      'candidate_estimator__estimator__loss': 'squared_error',\n      'candidate_estimator__estimator__max_depth': 3,\n      'candidate_estimator__estimator__max_features': None,\n      'candidate_estimator__estimator__max_leaf_nodes': None,\n      'candidate_estimator__estimator__min_impurity_decrease': 0.0,\n      'candidate_estimator__estimator__min_samples_leaf': 1,\n      'candidate_estimator__estimator__min_samples_split': 2,\n      'candidate_estimator__estimator__min_weight_fraction_leaf': 0.0,\n      'candidate_estimator__estimator__n_estimators': 100,\n      'candidate_estimator__estimator__n_iter_no_change': None,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__subsample': 1.0,\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__validation_fraction': 0.1,\n      'candidate_estimator__estimator__verbose': 0,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'learning_rate': [0.01, 0.1, 0.25],\n       'max_depth': [3, 6, 9]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': GridSearchCV(estimator=Lasso(), n_jobs=-1, param_grid={'alpha': [0.5, 1, 1.25]},\n                   verbose=2),\n      'candidate_estimator__cv': None,\n      'candidate_estimator__error_score': nan,\n      'candidate_estimator__estimator': Lasso(),\n      'candidate_estimator__estimator__alpha': 1.0,\n      'candidate_estimator__estimator__copy_X': True,\n      'candidate_estimator__estimator__fit_intercept': True,\n      'candidate_estimator__estimator__max_iter': 1000,\n      'candidate_estimator__estimator__positive': False,\n      'candidate_estimator__estimator__precompute': False,\n      'candidate_estimator__estimator__random_state': None,\n      'candidate_estimator__estimator__selection': 'cyclic',\n      'candidate_estimator__estimator__tol': 0.0001,\n      'candidate_estimator__estimator__warm_start': False,\n      'candidate_estimator__n_jobs': -1,\n      'candidate_estimator__param_grid': {'alpha': [0.5, 1, 1.25]},\n      'candidate_estimator__pre_dispatch': '2*n_jobs',\n      'candidate_estimator__refit': True,\n      'candidate_estimator__return_train_score': False,\n      'candidate_estimator__scoring': None,\n      'candidate_estimator__verbose': 2},\n     {'candidate_estimator': VotingRegressor(estimators=[('candidate_1',\n                                   GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None),\n                                                estimator=RandomForestRegressor(oob_score=True),\n                                                param_grid={'max_features': ['sqrt',\n                                                                             'log2',\n                                                                             None],\n                                                            'n_estimators': [100,\n                                                                             250]},\n                                                verbose=2)),\n                                  ('candidate_2',\n                                   GridSearchCV(estimator=GradientBoostingRegressor(),\n                                                n_jobs=-1,\n                                                param_grid={'learning_rate': [0.01,\n                                                                              0.1,\n                                                                              0.25],\n                                                            'max_depth': [3, 6, 9]},\n                                                verbose=2)),\n                                  ('candidate_3',\n                                   GridSearchCV(estimator=Lasso(), n_jobs=-1,\n                                                param_grid={'alpha': [0.5, 1, 1.25]},\n                                                verbose=2))])}],\n    'split0_test_score': array([ 0.1888305 ,  0.08919633, -0.0736081 ,  0.06960472]),\n    'split1_test_score': array([-0.14865598,  0.08676878, -0.80881036, -0.21259937]),\n    'split2_test_score': array([-0.29366267, -0.37729154, -0.06675911, -0.13022047]),\n    'split3_test_score': array([ 0.08926385, -0.13746591, -0.53620398, -0.27760426]),\n    'split4_test_score': array([-0.22207806, -0.53202214, -0.05525841, -0.10433023]),\n    'split5_test_score': array([ 0.32588058,  0.27372852, -0.11987192,  0.18119212]),\n    'split6_test_score': array([ 0.12663873, -0.04815621,  0.01051812,  0.06797981]),\n    'split7_test_score': array([ 0.46322693,  0.51845522, -0.67022028,  0.24284036]),\n    'split8_test_score': array([ 0.44083091,  0.50011533, -0.05495854,  0.39559592]),\n    'split9_test_score': array([-0.17234646, -0.13057137,  0.00303366, -0.08304205]),\n    'mean_test_score': array([ 0.07979283,  0.0242757 , -0.23721389,  0.01494166]),\n    'std_test_score': array([0.26428412, 0.32666365, 0.29302181, 0.20310895]),\n    'rank_test_score': array([1, 2, 4, 3], dtype=int32)},\n   'feature_names_in_': array(['Unnamed: 0', 'gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65',\n          'hm65', 'hf65', 'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65',\n          'fert65', 'mort65', 'lifee065', 'gpop1', 'fert1', 'mort1',\n          'invsh41', 'geetot1', 'geerec1', 'gde1', 'govwb1', 'govsh41',\n          'gvxdxe41', 'high65', 'highm65', 'highf65', 'highc65', 'highcm65',\n          'highcf65', 'human65', 'humanm65', 'humanf65', 'hyr65', 'hyrm65',\n          'hyrf65', 'no65', 'nom65', 'nof65', 'pinstab1', 'pop65',\n          'worker65', 'pop1565', 'pop6565', 'sec65', 'secm65', 'secf65',\n          'secc65', 'seccm65', 'seccf65', 'syr65', 'syrm65', 'syrf65',\n          'teapri65', 'teasec65', 'ex1', 'im1', 'xr65', 'tot1'], dtype=object),\n   'multimetric_': False,\n   'n_features_in_': 62,\n   'n_splits_': 10,\n   'refit_time_': 13.403948068618774,\n   'scorer_': &lt;class 'sklearn.pipeline.Pipeline'&gt;.score},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nAnd as before, any remaining open questions can be viewed and answered using the same methods as above."
  },
  {
    "objectID": "barrolee1994.html#references",
    "href": "barrolee1994.html#references",
    "title": "Using gingado to understand economic growth",
    "section": "References",
    "text": "References\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37."
  },
  {
    "objectID": "estimators.html",
    "href": "estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "In many instances, economists are interested in using machine learning models for specific purposes that go beyond their ability to predict variables to a good accuracy. For example:\nThe gingado.estimators module contains machine learning algorithms adapted to enable the types of analyses described above. More estimators can be expected over time.\nFor more academic discussions of machine learning methods in economics covering a broad range of topics, see Athey and Imbens (2019)."
  },
  {
    "objectID": "estimators.html#clustering",
    "href": "estimators.html#clustering",
    "title": "Estimators",
    "section": "Clustering",
    "text": "Clustering\nThe clustering algorithms used below are not themselves adapted from the general use methods. Rather, the functions offer convenience functionalities to find and retain the other variables in the same cluster.\nThese variables are usually entities (individuals, countries, stocks, etc) in a larger population.\nThe gingado clustering routines are designed to allow users standalone usage, or a seamless integration as part of a pipeline.\nThere are three levels of sophistication that users can choose from:\n\nusing the off-the-shelf clustering routines provided by gingado, which were selected to be applied cross various use cases;\nselecting an existing clustering routine from the scikit-learn.cluster module; or\ndesigning their own clustering algorithm.\n\n\nFindCluster\n\nFindCluster (cluster_alg: '[BaseEstimator, ClusterMixin]' = AffinityPropagation(), auto_document: 'ggdModelDocumentation' = &lt;class 'gingado.model_documentation.ModelCard'&gt;, random_state: 'int | None' = None)\n\nRetain only the columns of `X` that are in the same cluster as `y`.\n\nArgs:\n    cluster_alg (BaseEstimator|ClusterMixin): An instance of the clustering algorithm to use.\n    auto_document (ggdModelDocumentation): gingado Documenter template to facilitate model documentation.\n    random_state (int|None): The random seed to be used by the algorithm, if relevant. Defaults to None.\n\nfit\n\nfit (self, X, y)\n\nFit `FindCluster`.\n\nArgs:\n    X: The population of entities, organized in columns.\n    y: The entity of interest.\n\n\ntransform\n\ntransform (self, X) -&gt; 'np.array'\n\nKeep only the entities in `X` that belong to the same cluster as `y`.\n\nArgs:\n    X: The population of entities, organized in columns.\n\nReturns:\n    np.array: Columns of `X` that are in the same cluster as `y`.\n\n\nfit_transform\n\nfit_transform (self, X, y) -&gt; 'np.array'\n\nFit a `FindCluster` object and keep only the entities in `X` that belong to the same cluster as `y`.\n\nArgs:\n    X: The population of entities, organized in columns.\n    y: The entity of interest.\n\nReturns:\n    np.array: Columns of `X` that are in the same cluster as `y`.\n\n\ndocument\n\ndocument (self, documenter: 'ggdModelDocumentation | None' = None)\n\nDocument the `FindCluster` model using the template in `documenter`.\n\nArgs:\n    documenter (ggdModelDocumentation|None): A gingado Documenter or the documenter set in `auto_document` if None.\n        Defaults to None.\n\n\nExample: finding similar countries\nThe Barro and Lee (1994) dataset is used to illustrate the use of FindCluster. It is a country-level dataset. Let’s use it to answer the following question: for some specific country, what other countries are the closest to it considering the data available?\nFirst, we import the data:\n\nfrom gingado.datasets import load_BarroLee_1994\n\nThe data is organized by rows: each row is a different country, and the variables are organised in columns.\nThe dataset is originally organised for a regression of GDP growth (here denoted y) on the covariates (X). This is not what we want to do in this case. So instead of keeping GDP as a separate variable, the next step is to include it in the X DataFrame.\n\nX, y = load_BarroLee_1994()\nX['gdp'] = y\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\ngdp\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n-0.024336\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n0.100473\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n0.067051\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n0.064089\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n0.027930\n\n\n\n\n5 rows × 63 columns\n\n\n\nNow we remove the first column (an identifier) and transpose the DataFrame, so that countries are organized in columns.\nEach country is identified by a number: 0, 1, …\n\nX = X.iloc[:, 1:]\ncountries = X.T\ncountries.columns = ['country_' + str(c) for c in countries.columns]\ncountries.head()\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n\n\n5 rows × 90 columns\n\n\n\nSuppose we are interested in country No 13. What other countries are similar to it?\nFirst, country No 13 needs to be carved out of the DataFrame with the other countries.\nSecond, we can now pass the larger DataFrame and country 13’s data separately to an instance of FindCluster.\n\ncountry_of_interest = countries.pop('country_13')\n\n\nsimilar = FindCluster(AffinityPropagation(convergence_iter=5000))\nsimilar\n\nFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FindClusteriNot fittedFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000)) cluster_alg: AffinityPropagationAffinityPropagation(convergence_iter=5000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(convergence_iter=5000) \n\n\n\nsame_cluster = similar.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster.equals(similar.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster\n\n\n\n\n\n\n\n\ncountry_2\ncountry_9\ncountry_41\ncountry_48\ncountry_49\ncountry_52\ncountry_60\ncountry_64\ncountry_66\n\n\n\n\ngdpsh465\n8.895082\n8.151910\n7.360740\n6.469250\n5.762051\n9.224933\n8.346168\n7.655864\n7.830028\n\n\nbmp1l\n0.000000\n0.148400\n0.418100\n0.538800\n0.600500\n0.000000\n0.319900\n0.134500\n0.488000\n\n\nfreeop\n0.204244\n0.110885\n0.218471\n0.153491\n0.151848\n0.204244\n0.110885\n0.164598\n0.136287\n\n\nfreetar\n0.009186\n0.028579\n0.027087\n0.043888\n0.024100\n0.009186\n0.028579\n0.044446\n0.046730\n\n\nh65\n0.260000\n0.145000\n0.032000\n0.015000\n0.002000\n0.393000\n0.272000\n0.080000\n0.146000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.174100\n0.052400\n0.190500\n0.069200\n0.148400\n0.255800\n0.062500\n0.052500\n0.076400\n\n\nim1\n0.175000\n0.052300\n0.225700\n0.074800\n0.186400\n0.241200\n0.057800\n0.057200\n0.086600\n\n\nxr65\n1.082000\n2.119000\n3.949000\n0.348000\n7.367000\n1.017000\n36.603000\n30.929000\n40.500000\n\n\ntot1\n-0.010040\n0.007584\n0.205768\n0.035226\n0.007548\n0.018636\n0.014286\n-0.004592\n-0.007018\n\n\ngdp\n0.067051\n0.039147\n0.016775\n-0.048712\n0.024477\n0.050757\n-0.034045\n0.046010\n-0.011384\n\n\n\n\n62 rows × 9 columns\n\n\n\nThe default clustering algorithm used by FindCluster is affinity propagation (Frey and Dueck 2007). It is the algorithm of choice because of it combines several desireable characteristics, in particular: - the number of clusters is data-driven instad of set by the user, - the number of entities in each cluster is also chosen by the model, - all entities are part of a cluster, and - each cluster might have a different number of entities.\nHowever, we may want to try different clustering algorithms. Let’s compare the result above with the same analyses using DBSCAN (Ester et al. 1996).\n\nfrom sklearn.cluster import DBSCAN\n\n\nsimilar_dbscan = FindCluster(cluster_alg=DBSCAN())\nsimilar_dbscan\n\nFindCluster(cluster_alg=DBSCAN())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. FindClusteriNot fittedFindCluster(cluster_alg=DBSCAN()) cluster_alg: DBSCANDBSCAN()  DBSCAN?Documentation for DBSCANDBSCAN() \n\n\n\nsame_cluster_dbscan = similar_dbscan.fit_transform(X=countries, y=country_of_interest)\n\nassert same_cluster_dbscan.equals(similar_dbscan.fit(X=countries, y=country_of_interest).transform(X=countries))\n\nsame_cluster_dbscan\n\n\n\n\n\n\n\n\ncountry_0\ncountry_1\ncountry_2\ncountry_3\ncountry_4\ncountry_5\ncountry_6\ncountry_7\ncountry_8\ncountry_9\n...\ncountry_80\ncountry_81\ncountry_82\ncountry_83\ncountry_84\ncountry_85\ncountry_86\ncountry_87\ncountry_88\ncountry_89\n\n\n\n\ngdpsh465\n6.591674\n6.829794\n8.895082\n7.565275\n7.162397\n7.218910\n7.853605\n7.703910\n9.063463\n8.151910\n...\n9.030974\n8.995537\n8.234830\n8.332549\n8.645586\n8.991064\n8.025189\n9.030137\n8.865312\n8.912339\n\n\nbmp1l\n0.283700\n0.614100\n0.000000\n0.199700\n0.174000\n0.000000\n0.000000\n0.277600\n0.000000\n0.148400\n...\n0.000000\n0.000000\n0.036300\n0.000000\n0.000000\n0.000000\n0.005000\n0.000000\n0.000000\n0.000000\n\n\nfreeop\n0.153491\n0.313509\n0.204244\n0.248714\n0.299252\n0.258865\n0.182525\n0.215275\n0.109614\n0.110885\n...\n0.293138\n0.304720\n0.288405\n0.345485\n0.288440\n0.371898\n0.296437\n0.265778\n0.282939\n0.150366\n\n\nfreetar\n0.043888\n0.061827\n0.009186\n0.036270\n0.037367\n0.020880\n0.014385\n0.029713\n0.002171\n0.028579\n...\n0.005517\n0.011658\n0.011589\n0.006503\n0.005995\n0.014586\n0.013615\n0.008629\n0.005048\n0.024377\n\n\nh65\n0.007000\n0.019000\n0.260000\n0.061000\n0.017000\n0.023000\n0.039000\n0.024000\n0.402000\n0.145000\n...\n0.245000\n0.246000\n0.183000\n0.188000\n0.256000\n0.255000\n0.108000\n0.288000\n0.188000\n0.257000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nex1\n0.072900\n0.094000\n0.174100\n0.126500\n0.121100\n0.063400\n0.034200\n0.086400\n0.059400\n0.052400\n...\n0.166200\n0.259700\n0.104400\n0.286600\n0.129600\n0.440700\n0.166900\n0.323800\n0.184500\n0.187600\n\n\nim1\n0.066700\n0.143800\n0.175000\n0.149600\n0.130800\n0.076200\n0.042800\n0.093100\n0.046000\n0.052300\n...\n0.161700\n0.228800\n0.179600\n0.350000\n0.145800\n0.425700\n0.220100\n0.313400\n0.194000\n0.200700\n\n\nxr65\n0.348000\n0.525000\n1.082000\n6.625000\n2.500000\n1.000000\n12.499000\n7.000000\n1.000000\n2.119000\n...\n4.286000\n2.460000\n32.051000\n0.452000\n652.850000\n2.529000\n25.553000\n4.152000\n0.452000\n0.886000\n\n\ntot1\n-0.014727\n0.005750\n-0.010040\n-0.002195\n0.003283\n-0.001747\n0.009092\n0.011630\n0.008169\n0.007584\n...\n-0.006642\n-0.003241\n-0.034352\n-0.001660\n-0.046278\n-0.011883\n-0.039080\n0.005175\n-0.029551\n-0.036482\n\n\ngdp\n-0.024336\n0.100473\n0.067051\n0.064089\n0.027930\n0.046407\n0.067332\n0.020978\n0.033551\n0.039147\n...\n0.038095\n0.034213\n0.052759\n0.038416\n0.031895\n0.031196\n0.034096\n0.046900\n0.039773\n0.040642\n\n\n\n\n62 rows × 89 columns\n\n\n\nAs illustrated above, the results can be quite different. In this case, affinity propagation converged to more tightly defined clusters, while DBSCAN selected a cluster that contains almost all other countries (therefore, not useful in this particular case).\nNote that model documentation is already jumpstarted when the cluster is fit. A glimpse of the current template, including the questions in the documentation template that have been automatically filled, are shown below.\n\nsimilar.model_documentation.show_json()\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2025-03-06 11:28:33 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'_estimator_type': 'clusterer',\n   'affinity_matrix_': array([[-4.23373922e+08, -5.97375771e+07, -5.35974361e+07, ...,\n           -1.92434215e+09, -8.60822083e+07, -3.77976931e+07],\n          [-5.97375771e+07, -4.23373922e+08, -2.26471602e+08, ...,\n           -2.66217555e+09, -2.43057326e+06, -1.92555486e+08],\n          [-5.35974361e+07, -2.26471602e+08, -4.23373922e+08, ...,\n           -1.33575671e+09, -2.75395788e+08, -1.37934978e+06],\n          ...,\n          [-1.92434215e+09, -2.66217555e+09, -1.33575671e+09, ...,\n           -4.23373922e+08, -2.82418157e+09, -1.42280304e+09],\n          [-8.60822083e+07, -2.43057326e+06, -2.75395788e+08, ...,\n           -2.82418157e+09, -4.23373922e+08, -2.37881124e+08],\n          [-3.77976931e+07, -1.92555486e+08, -1.37934978e+06, ...,\n           -1.42280304e+09, -2.37881124e+08, -4.23373922e+08]]),\n   'cluster_centers_': array([[ 6.82979374e+00,  6.14100000e-01,  3.13509000e-01, ...,\n            5.25000000e-01,  5.75000000e-03,  1.00472567e-01],\n          [ 8.89508153e+00,  0.00000000e+00,  2.04244000e-01, ...,\n            1.08200000e+00, -1.00400000e-02,  6.70514822e-02],\n          [ 7.56527528e+00,  1.99700000e-01,  2.48714000e-01, ...,\n            6.62500000e+00, -2.19500000e-03,  6.40891662e-02],\n          ...,\n          [ 8.33254894e+00,  0.00000000e+00,  3.45485000e-01, ...,\n            4.52000000e-01, -1.66000000e-03,  3.84156381e-02],\n          [ 8.86531163e+00,  0.00000000e+00,  2.82939000e-01, ...,\n            4.52000000e-01, -2.95510000e-02,  3.97733722e-02],\n          [ 8.91233857e+00,  0.00000000e+00,  1.50366000e-01, ...,\n            8.86000000e-01, -3.64820000e-02,  4.06415381e-02]]),\n   'cluster_centers_indices_': array([ 1,  2,  3,  4,  5,  7,  8, 10, 13, 14, 16, 18, 19, 25, 27, 32, 35,\n          39, 42, 45, 46, 49, 50, 52, 53, 55, 57, 58, 60, 62, 67, 68, 69, 71,\n          76, 82, 87, 88]),\n   'feature_names_in_': array(['gdpsh465', 'bmp1l', 'freeop', 'freetar', 'h65', 'hm65', 'hf65',\n          'p65', 'pm65', 'pf65', 's65', 'sm65', 'sf65', 'fert65', 'mort65',\n          'lifee065', 'gpop1', 'fert1', 'mort1', 'invsh41', 'geetot1',\n          'geerec1', 'gde1', 'govwb1', 'govsh41', 'gvxdxe41', 'high65',\n          'highm65', 'highf65', 'highc65', 'highcm65', 'highcf65', 'human65',\n          'humanm65', 'humanf65', 'hyr65', 'hyrm65', 'hyrf65', 'no65',\n          'nom65', 'nof65', 'pinstab1', 'pop65', 'worker65', 'pop1565',\n          'pop6565', 'sec65', 'secm65', 'secf65', 'secc65', 'seccm65',\n          'seccf65', 'syr65', 'syrm65', 'syrf65', 'teapri65', 'teasec65',\n          'ex1', 'im1', 'xr65', 'tot1', 'gdp'], dtype=object),\n   'labels_': array([29,  0,  1,  2,  3,  4, 18,  5,  6,  1,  7, 30, 14,  8,  9, 29, 10,\n          29, 11, 12, 12, 18, 29, 36, 18, 13, 18, 14, 29, 36, 36, 14, 15, 36,\n          29, 16, 18, 14, 36, 17,  1, 14, 18, 29, 29, 19, 20,  1,  1, 21, 22,\n           1, 23, 24, 21, 25, 36, 26, 27,  1, 28, 12, 29,  1, 14,  1, 29, 30,\n          31, 32, 12, 33, 18, 29, 30, 18, 34, 14, 18, 36, 36, 29, 35, 36, 29,\n          29, 14, 36, 37,  1]),\n   'n_features_in_': 62,\n   'n_iter_': 200},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': 'Information on training data'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'human_life': 'Is the model intended to inform decisions about matters central to human life or flourishing - e.g., health or safety? Or could it be used in such a way?',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nFindCluster can also be used as part of a pipeline. In this case, only the entities in the same cluster as the entity of interest will continue on to the next steps of the estimation.\n\nfrom gingado.benchmark import RegressionBenchmark\nfrom sklearn.pipeline import Pipeline\n\n\npipe = Pipeline([\n    ('cluster', FindCluster(AffinityPropagation(convergence_iter=5000))),\n    ('rf', RegressionBenchmark())\n])\n\n\npipe.fit(X=countries, y=country_of_interest)\n\nPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('cluster',\n                 FindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000))),\n                ('rf',\n                 RegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)))]) cluster: FindClusterFindCluster(cluster_alg=AffinityPropagation(convergence_iter=5000)) cluster_alg: AffinityPropagationAffinityPropagation(convergence_iter=5000)  AffinityPropagation?Documentation for AffinityPropagationAffinityPropagation(convergence_iter=5000) rf: RegressionBenchmarkRegressionBenchmark(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=None, train_size=None)) estimator: RandomForestRegressorRandomForestRegressor(oob_score=True)  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(oob_score=True)"
  },
  {
    "objectID": "estimators.html#comparative-case-studies",
    "href": "estimators.html#comparative-case-studies",
    "title": "Estimators",
    "section": "Comparative case studies",
    "text": "Comparative case studies\n\nMachineControl\n\nMachineControl (cluster_alg: '[BaseEstimator, ClusterMixin] | None' = AffinityPropagation(), estimator: 'BaseEstimator' = RegressionBenchmark(), manifold: 'BaseEstimator' = TSNE(), with_placebo: 'bool' = True, auto_document: 'ggdModelDocumentation' = &lt;class 'gingado.model_documentation.ModelCard'&gt;, random_state: 'int | None' = None)\n\nSynthetic controls with machine learning methods\n\nArgs:\n    cluster_alg (BaseEstimator | ClusterMixin | None): An instance of the clustering algorithm to use, or None to retain all entities.\n    estimator (BaseEstimator): Method to weight the control entities.\n    manifold (BaseEstimator): Algorithm for manifold learning.\n    with_placebo (bool): Include placebo estimations during prediction?\n    auto_document (ggdModelDocumentation): gingado Documenter template to facilitate model documentation.\n    random_state (int | None): The random seed to be used by the algorithm, if relevant.\n\nfit\n\nfit (self, X: 'pd.DataFrame', y: 'pd.DataFrame | pd.Series')\n\nFit the `MachineControl` model.\n\nArgs:\n    X (pd.DataFrame): A pandas DataFrame with pre-intervention data of shape (n_samples, n_control_entities).\n    y (pd.DataFrame | pd.Series): A pandas DataFrame or Series with pre-intervention data of shape (n_samples,).\n\n\npredict\n\npredict (self, X: 'pd.DataFrame', y: 'pd.DataFrame | pd.Series')\n\nCalculate the model predictions before and after the intervention.\n\nArgs:\n    X (pd.DataFrame): A pandas DataFrame with complete time series (pre- and post-intervention) of shape (n_samples, n_control_entities).\n    y (pd.DataFrame | pd.Series): A pandas DataFrame or Series with complete time series of shape (n_samples,).\n\n\nget_controls\n\nget_controls (self)\n\nGet the list of control entities\n\n\ndocument\n\ndocument (self, documenter: 'ggdModelDocumentation | None' = None)\n\nDocument the `MachineControl` model using the template in `documenter`.\n\nArgs:\n    documenter (ggdModelDocumentation | None): A gingado Documenter or the documenter set in `auto_document` if None.\n\n\nBrief econometric description\nThe goal of MachineControl is to estimate:\n\\[\n\\tau_t = Y_{1, t}^{I} - Y_{1, t}^{N}, t &gt; T0\n\\]\nwhere:\n\n\\(\\tau\\) is the effect on entity \\(i=1\\) of the intervention of interest\nwithout loss of generality, \\(i=1\\) is an entity that has undergone the intervention of interest, amongst \\(N\\) total entities\ntime period \\(T0\\) is a date in which the intervention occurred\nsuperscript \\(I\\) in an outcome variable denotes the occurence of the intervention, whereas superscript \\(N\\) is absence of intervention\nfor \\(t &gt; T0\\), \\(Y_{i, t}^{I}\\) is observed while \\(Y_{i, t}^{N}\\) must be estimated because it is a counterfacual.\n\n\\(Y_{i, t}^{N}\\) is calculated from the values of the other entities, \\(i \\neq 1\\). Collect this data in a vector \\(\\mathbb{Y}_{-1, t}^{N}\\). Then, following Doudchenko and Imbens (2016):\n\\[\n\\hat{Y}_{i, t}^{N} = f^*(\\mathbb{Y}_{-1, t}^{N}),\n\\]\nwith the star (\\(*\\)) superscript on the function \\(f(\\cdot)\\) representing that it was trained only with data up until the intervention date. The exact form of \\(f(\\cdot)\\) depends on the argument estimator. A general use estimator is the random forest (Breiman 2001).\nThe panel data itself might be the whole population in the data, or a subset when using the whole population might be too cumbersome to run analyses (eg, if the data contains too many entities). One way to select this subsample of control units without including subjective judgment in the data is quantitatilve. The control units are selected through a clustering algorithm (argument cluster_arg). One cluster algorithm that can be used is affinity propagation (Frey and Dueck 2007).\nTo finalise, the quality of the synthetic control can be assessed in many ways. One fully data-driven way to achieve this is by using manifold learning: lower-dimensional embeddings of a higher-dimensional data. A preferred manifold learning algorithm is t-SNE (Van der Maaten and Hinton 2008).\nThe relative distance between embeddings and the target centre, as well as the control and the target, represent the chance that a better feasible control (either from real or combined) will materialise. The intuition behind this test is:\n\nlet \\(d_{i,j}\\) be the Euclidean distance between the embeddings (2d points) of entities \\(i\\) and \\(j\\)\nif only a very small percentage of \\(d_{1, j \\in (2, ..., N)}\\) are lower than \\(d_{1, \\text{Synthetic control}}\\), than the synthetic control produced with \\(f(\\cdot)\\) is indeed a formula that provides one of the best alternative.\n\nMain references:\n\nAbadie and Gardeazabal (2003)\nAbadie, Diamond, and Hainmueller (2010)\nAbadie, Diamond, and Hainmueller (2015)\nDoudchenko and Imbens (2016)\nAbadie (2021)\n\n\n\nExample: impact of labour reform on productivity\nSee Machine controls: Synthetic controls with machine learning."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets for economic research",
    "section": "",
    "text": "load_BarroLee_1994 (return_tuple: 'bool' = True)\n\nLoads the dataset used in R. Barro and J.-W. Lee's \"Sources of Economic Growth\" (1994).\n\nArgs:\n    return_tuple (bool):  Whether to return the data in a tuple or jointly in a single pandas\n                          DataFrame.\n\nReturns:\n    pandas.DataFrame or tuple: If `return_tuple` is True, returns a tuple of (X, y), where `X` is a\n    DataFrame of independent variables and `y` is a Series of the dependent variable. If False,\n    returns a single DataFrame with both independent and dependent variables.\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\nIf you use this data in your work, please cite Barro and Lee (1994). A BibTeX code for convenience is below:\n@article{BARRO19941,\ntitle = {Sources of economic growth},\njournal = {Carnegie-Rochester Conference Series on Public Policy},\nvolume = {40},\npages = {1-46},\nyear = {1994},\nissn = {0167-2231},\ndoi = {10.1016/0167-2231(94)90002-7},\nurl = {https://www.sciencedirect.com/science/article/pii/0167223194900027},\nauthor = {Robert J. Barro and Jong-Wha Lee},\nabstract = {For 116 countries from 1965 to 1985, the lowest quintile had an average growth rate of real per capita GDP of - 1.3pct, whereas the highest quintile had an average of 4.8pct. We isolate five influences that discriminate reasonably well between the slow-and fast-growers: a conditional convergence effect, whereby a country grows faster if it begins with lower real per-capita GDP relative to its initial level of human capital in the forms of educational attainment and health; a positive effect on growth from a high ratio of investment to GDP (although this effect is weaker than that reported in some previous studies); a negative effect from overly large government; a negative effect from government-induced distortions of markets; and a negative effect from political instability. Overall, the fitted growth rates for 85 countries for 1965–1985 had a correlation of 0.8 with the actual values. We also find that female educational attainment has a pronounced negative effect on fertility, whereas female and male attainment are each positively related to life expectancy and negatively related to infant mortality. Male attainment plays a positive role in primary-school enrollment ratios, and male and female attainment relate positively to enrollment at the secondary level.}\n}\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n\n\n\n\n\n\n\n\n\n\n\n\nload_CB_speeches (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad Central Bankers speeches dataset from \nBank for International Settlements (2024). Central bank speeches, all years,\nhttps://www.bis.org/cbspeeches/download.htm.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n\nReturns:\n    A pandas DataFrame containing the speeches dataset.\n\nUsage:\n    &gt;&gt;&gt; load_CB_speeches()\n\n    &gt;&gt;&gt; load_CB_speeches('2020')\n\n    &gt;&gt;&gt; load_CB_speeches([2020, 2021, 2022])\nThis function downloads the Central bankers speeches dataset (2024) from the BIS website (www.bis.org). More information on the dataset can be found on the BIS website.\nIf you use this data in your work, please cite the BIS central bank speeches dataset, as follows (Please substitute YYYY for the relevant years):\n@misc{biscbspeeches\n    author = {{Bank for International Settlements}},\n    title = {Central bank speeches, YYYY-YYYY},\n    year = {2024},\n    url = {https://www.bis.org/cbspeeches/download.htm}\n}\n\n# Load speeches for 2020\nspeeches = load_CB_speeches(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\nurl\ntitle\ndescription\ndate\ntext\nauthor\n\n\n\n\n0\nhttps://www.bis.org/review/r200107a.htm\nShaktikanta Das: Journey towards inclusive gro...\nOpening remarks by Mr Shaktikanta Das, Governo...\n2020-01-07 00:00:00\nShaktikanta Das: Journey towards inclusive gro...\nShaktikanta Das\n\n\n1\nhttps://www.bis.org/review/r200108a.htm\nLuis de Guindos: Europe's role in the global f...\nKeynote speech by Mr Luis de Guindos, Vice-Pre...\n2020-01-08 00:00:00\nLuis de Guindos: Europe's role in the global f...\nLuis de Guindos\n\n\n2\nhttps://www.bis.org/review/r200108b.htm\nKlaas Knot: Interests and alliances\nOpening remarks by Mr Klaas Knot, President of...\n2020-01-08 00:00:00\n\"Interests and alliances\"\\nOpening remarks by ...\nKlaas Knot\n\n\n3\nhttps://www.bis.org/review/r200108c.htm\nLael Brainard: Strengthening the Community Rei...\nSpeech by Ms Lael Brainard, Member of the Boar...\n2020-01-08 00:00:00\nFor release on delivery\\n10:00 a.m. EST\\nJanua...\nLael Brainard\n\n\n4\nhttps://www.bis.org/review/r200108d.htm\nChristine Lagarde: Interview in \"Challenges\" m...\nInterview with Ms Christine Lagarde, President...\n2020-01-08 00:00:00\nChristine Lagarde: Interview in \"Challenges\" m...\nChristine Lagarde\n\n\n\n\n\n\n\n\n\n\n\n\nload_monpol_statements (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad monetary policy statements from multiple central banks.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n    \nReturns:\n    A pandas DataFrame containing the dataset.\n\nUsage:\n    &gt;&gt;&gt; load_monpol_statements()\n\n    &gt;&gt;&gt; load_monpol_statements('2020')\n\n    &gt;&gt;&gt; load_monpol_statements([2020, 2021, 2022])\nThis function downloads monetary policy statements from 26 emerging market central banks (Armenia, Brazil, Chile, Colombia, Czech Republic, Egypt, Georgia, Hungary, Israel, India, Kazakhstan, Malaysia, Mongolia, Mexico, Nigeria, Pakistan, Peru, Philippines, Poland, Romania, Russia, South Africa, South Korea, Thailand, Türkiye, Ukraine) as well as the Fed and the ECB (press-conference introductory statements). The dataset includes official English versions of statements for 1998-2023 (starting date varies depending on data availability). The original source is Evdokimova et al. (2023). If you use this data in your work, please cite the dataset, as follows:\n@article{emcbcom,\n    author = {Tatiana Evdokimova and Piroska Nagy Mohácsi and Olga Ponomarenko and Elina Ribakova},\n    title = {Central banks and policy communication: How emerging markets have outperformed the Fed and ECB},\n    year = {2023},\n    institution = {Peterson Institute for International Economics},\n    url = {https://www.piie.com/publications/working-papers/central-banks-and-policy-communication-how-emerging-markets-have}\n}\n\n# Load monpol statements for 2020\nspeeches = load_monpol_statements(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\ncountry\ndate\nstatement\n\n\n\n\n0\nPoland\n2020-01-08\nWarsaw, 8 January 2020 Information from the me...\n\n\n1\nRomania\n2020-01-08\nIn its meeting of 8 January 2020, the Board of...\n\n\n2\nIsrael\n2020-01-09\nThe inflation environment remains low. The Nov...\n\n\n3\nPeru\n2020-01-09\nPRESS RELEASE MONETARY POLICY STATEMENT JANUAR...\n\n\n4\nEgypt\n2020-01-16\nCentral Bank of Egypt Press Release January 16...\n\n\n\n\n\n\n\n\n\n\n\n\nload_lr_tanzania_data (wide_format: 'bool' = False) -&gt; 'dict[str, pd.DataFrame]'\n\nLoads liquidity risk data from CSV files.\n\nThis function loads monthly and weekly liquidity risk data. The data is sourced from the paper\n\"Using machine learning for detecting liquidity risk in banks\" by Rweyemamu Ignatius Barongo and\nJimmy Tibangayuka Mbelwa. The dataset includes data from 38 Tanzanian banks (2010-2021) provided\nby the Bank of Tanzania (BOT). Banks are identified by anonymous bank codes.\n\n\nParameters:\nwide_format (bool): If True, returns data in wide format with pivoted columns. Column names will\n    be in the format BANK_CODE__VAR_NAME.\n\nReturns:\ndict[str, pd.DataFrame]: A dictionary with keys 'w' for weekly data and 'm' for monthly data.\n    The values are pandas DataFrames containing the data for each frequency.\nThis function loads liquidity risk data from CSV files for 38 Tanzanian commercial banks spanning from 2010 to 2021. The dataset includes both monthly and weekly data provided by the Bank of Tanzania (BOT). The original source is Barongo and Mbelwa (2024).\nIf you use this data in your work, please cite the dataset, as follows:\n@article{BARONGO2024100511,\n    author = {Rweyemamu Ignatius Barongo and Jimmy Tibangayuka Mbelwa},\n    title = {Using machine learning for detecting liquidity risk in banks},\n    journal = {Machine Learning with Applications},\n    volume = {15},\n    year = {2024},\n    doi = {https://doi.org/10.1016/j.mlwa.2023.100511},\n    url = {https://www.sciencedirect.com/science/article/pii/S2666827023000646},\n}\n\n# Load liquidity risk data\nlr_data = load_lr_tanzania_data()\nlr_data[\"m\"].head()\n\n\n\n\n\n\n\n\nINSTITUTIONCODE\nREPORTINGDATE\nF001_ASSET_CASH\nF002_ASSET_BAL_BOT\nF003_ASSET_BAL_BOT_SMR\nF004_ASSET_BAL_BOT_CURRENT_ACCOUNT\nF005_ASSET_BAL_BOT_OTHERS\nF006_ASSET_BAL_OTHER_BANKS\nF007_ASSET_BAL_OTHER_BANKS_TZ\nF008_ASSET_BAL_OTHER_BANKS_ABROAD\n...\nEWAQ_Capital\nEWAQ_NPL\nEWAQ_GrossLoans\nEWAQ_LargeExposures\nEWAQ_NPLsNetOfProvisions\nEWAQ_NPLsNetOfProvisions2CoreCapital\nEWAQ_NPLs2GrossLoans\nEWAQ_AssetsQualityRating\nEWAQ_Loans\nMLA_CLASS\n\n\n\n\n0\nB5412\n2010-01-31\n9.570422e+09\n5.260077e+10\n3.700000e+10\n1.560077e+10\n0.0\n1.348740e+11\n1.622160e+10\n1.186520e+11\n...\n9.514540e+10\n1.951219e+09\n1.274810e+11\n7.137633e+10\n903220975.0\n0.0095\n0.0153\n1.0\nNaN\nhigh\n\n\n1\nB5412\n2010-02-28\n6.996645e+09\n3.554718e+10\n3.200000e+10\n3.547185e+09\n0.0\n1.482240e+11\n7.535208e+08\n1.474710e+11\n...\n9.595342e+10\n1.954516e+09\n1.251520e+11\n7.831967e+10\n904869349.0\n0.0094\n0.0156\n1.0\nNaN\nhigh\n\n\n2\nB5412\n2010-03-31\n8.689629e+09\n3.744132e+10\n3.350000e+10\n3.941316e+09\n0.0\n1.621370e+11\n5.570919e+09\n1.565660e+11\n...\n9.864602e+10\n1.979268e+09\n1.284300e+11\n7.846886e+10\n917245663.0\n0.0093\n0.0154\n1.0\nNaN\nhigh\n\n\n3\nB5412\n2010-04-30\n6.811311e+09\n3.527009e+10\n3.250000e+10\n2.770089e+09\n0.0\n2.244100e+11\n8.739334e+09\n2.156710e+11\n...\n9.960755e+10\n2.014398e+09\n1.286710e+11\n8.986596e+10\n934810301.0\n0.0094\n0.0157\n1.0\nNaN\nhigh\n\n\n4\nB5412\n2010-05-31\n9.160128e+09\n4.927299e+10\n3.982500e+10\n9.447986e+09\n0.0\n1.893920e+11\n1.524899e+09\n1.878670e+11\n...\n1.007330e+11\n2.081954e+09\n1.319650e+11\n8.567217e+10\n968588451.0\n0.0096\n0.0158\n1.0\nNaN\nhigh\n\n\n\n\n5 rows × 242 columns"
  },
  {
    "objectID": "datasets.html#real-datasets",
    "href": "datasets.html#real-datasets",
    "title": "Datasets for economic research",
    "section": "",
    "text": "load_BarroLee_1994 (return_tuple: 'bool' = True)\n\nLoads the dataset used in R. Barro and J.-W. Lee's \"Sources of Economic Growth\" (1994).\n\nArgs:\n    return_tuple (bool):  Whether to return the data in a tuple or jointly in a single pandas\n                          DataFrame.\n\nReturns:\n    pandas.DataFrame or tuple: If `return_tuple` is True, returns a tuple of (X, y), where `X` is a\n    DataFrame of independent variables and `y` is a Series of the dependent variable. If False,\n    returns a single DataFrame with both independent and dependent variables.\nRobert Barro and Jong-Wha Lee’s (1994) dataset has been used over time by other economists, such as by Belloni, Chernozhukov, and Hansen (2011) and Giannone, Lenza, and Primiceri (2021). This function uses the version available in their online annex. In that paper, this dataset corresponds to what the authors call “macro2”.\nThe original data, along with more information on the variables, can be found in this NBER website. A very helpful codebook is found in this repo.\nIf you use this data in your work, please cite Barro and Lee (1994). A BibTeX code for convenience is below:\n@article{BARRO19941,\ntitle = {Sources of economic growth},\njournal = {Carnegie-Rochester Conference Series on Public Policy},\nvolume = {40},\npages = {1-46},\nyear = {1994},\nissn = {0167-2231},\ndoi = {10.1016/0167-2231(94)90002-7},\nurl = {https://www.sciencedirect.com/science/article/pii/0167223194900027},\nauthor = {Robert J. Barro and Jong-Wha Lee},\nabstract = {For 116 countries from 1965 to 1985, the lowest quintile had an average growth rate of real per capita GDP of - 1.3pct, whereas the highest quintile had an average of 4.8pct. We isolate five influences that discriminate reasonably well between the slow-and fast-growers: a conditional convergence effect, whereby a country grows faster if it begins with lower real per-capita GDP relative to its initial level of human capital in the forms of educational attainment and health; a positive effect on growth from a high ratio of investment to GDP (although this effect is weaker than that reported in some previous studies); a negative effect from overly large government; a negative effect from government-induced distortions of markets; and a negative effect from political instability. Overall, the fitted growth rates for 85 countries for 1965–1985 had a correlation of 0.8 with the actual values. We also find that female educational attainment has a pronounced negative effect on fertility, whereas female and male attainment are each positively related to life expectancy and negatively related to infant mortality. Male attainment plays a positive role in primary-school enrollment ratios, and male and female attainment relate positively to enrollment at the secondary level.}\n}\n\nX, y = load_BarroLee_1994()\nX.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ngdpsh465\nbmp1l\nfreeop\nfreetar\nh65\nhm65\nhf65\np65\npm65\n...\nseccf65\nsyr65\nsyrm65\nsyrf65\nteapri65\nteasec65\nex1\nim1\nxr65\ntot1\n\n\n\n\n0\n0\n6.591674\n0.2837\n0.153491\n0.043888\n0.007\n0.013\n0.001\n0.29\n0.37\n...\n0.04\n0.033\n0.057\n0.010\n47.6\n17.3\n0.0729\n0.0667\n0.348\n-0.014727\n\n\n1\n1\n6.829794\n0.6141\n0.313509\n0.061827\n0.019\n0.032\n0.007\n0.91\n1.00\n...\n0.64\n0.173\n0.274\n0.067\n57.1\n18.0\n0.0940\n0.1438\n0.525\n0.005750\n\n\n2\n2\n8.895082\n0.0000\n0.204244\n0.009186\n0.260\n0.325\n0.201\n1.00\n1.00\n...\n18.14\n2.573\n2.478\n2.667\n26.5\n20.7\n0.1741\n0.1750\n1.082\n-0.010040\n\n\n3\n3\n7.565275\n0.1997\n0.248714\n0.036270\n0.061\n0.070\n0.051\n1.00\n1.00\n...\n2.63\n0.438\n0.453\n0.424\n27.8\n22.7\n0.1265\n0.1496\n6.625\n-0.002195\n\n\n4\n4\n7.162397\n0.1740\n0.299252\n0.037367\n0.017\n0.027\n0.007\n0.82\n0.85\n...\n2.11\n0.257\n0.287\n0.229\n34.5\n17.6\n0.1211\n0.1308\n2.500\n0.003283\n\n\n\n\n5 rows × 62 columns\n\n\n\n\ny.plot.hist(title='GDP growth', bins=30)\n\n\n\n\n\n\n\n\n\n\n\n\n\nload_CB_speeches (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad Central Bankers speeches dataset from \nBank for International Settlements (2024). Central bank speeches, all years,\nhttps://www.bis.org/cbspeeches/download.htm.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n\nReturns:\n    A pandas DataFrame containing the speeches dataset.\n\nUsage:\n    &gt;&gt;&gt; load_CB_speeches()\n\n    &gt;&gt;&gt; load_CB_speeches('2020')\n\n    &gt;&gt;&gt; load_CB_speeches([2020, 2021, 2022])\nThis function downloads the Central bankers speeches dataset (2024) from the BIS website (www.bis.org). More information on the dataset can be found on the BIS website.\nIf you use this data in your work, please cite the BIS central bank speeches dataset, as follows (Please substitute YYYY for the relevant years):\n@misc{biscbspeeches\n    author = {{Bank for International Settlements}},\n    title = {Central bank speeches, YYYY-YYYY},\n    year = {2024},\n    url = {https://www.bis.org/cbspeeches/download.htm}\n}\n\n# Load speeches for 2020\nspeeches = load_CB_speeches(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\nurl\ntitle\ndescription\ndate\ntext\nauthor\n\n\n\n\n0\nhttps://www.bis.org/review/r200107a.htm\nShaktikanta Das: Journey towards inclusive gro...\nOpening remarks by Mr Shaktikanta Das, Governo...\n2020-01-07 00:00:00\nShaktikanta Das: Journey towards inclusive gro...\nShaktikanta Das\n\n\n1\nhttps://www.bis.org/review/r200108a.htm\nLuis de Guindos: Europe's role in the global f...\nKeynote speech by Mr Luis de Guindos, Vice-Pre...\n2020-01-08 00:00:00\nLuis de Guindos: Europe's role in the global f...\nLuis de Guindos\n\n\n2\nhttps://www.bis.org/review/r200108b.htm\nKlaas Knot: Interests and alliances\nOpening remarks by Mr Klaas Knot, President of...\n2020-01-08 00:00:00\n\"Interests and alliances\"\\nOpening remarks by ...\nKlaas Knot\n\n\n3\nhttps://www.bis.org/review/r200108c.htm\nLael Brainard: Strengthening the Community Rei...\nSpeech by Ms Lael Brainard, Member of the Boar...\n2020-01-08 00:00:00\nFor release on delivery\\n10:00 a.m. EST\\nJanua...\nLael Brainard\n\n\n4\nhttps://www.bis.org/review/r200108d.htm\nChristine Lagarde: Interview in \"Challenges\" m...\nInterview with Ms Christine Lagarde, President...\n2020-01-08 00:00:00\nChristine Lagarde: Interview in \"Challenges\" m...\nChristine Lagarde\n\n\n\n\n\n\n\n\n\n\n\n\nload_monpol_statements (year: 'str | int | list' = 'all', cache: 'bool' = True, timeout: 'float | None' = 120, **kwargs) -&gt; 'pd.DataFrame'\n\nLoad monetary policy statements from multiple central banks.\n\nArgs:\n    year: Either 'all' to download all available central bank speeches or the year(s)\n        to download. Defaults to 'all'.\n    cache: If False, cached data will be ignored and the dataset will be downloaded again.\n        Defaults to True.\n    timeout: The timeout to for downloading each speeches file. Set to `None` to disable\n        timeout. Defaults to 120.\n    **kwargs. Additional keyword arguments which will be passed to pandas `read_csv` function.\n    \nReturns:\n    A pandas DataFrame containing the dataset.\n\nUsage:\n    &gt;&gt;&gt; load_monpol_statements()\n\n    &gt;&gt;&gt; load_monpol_statements('2020')\n\n    &gt;&gt;&gt; load_monpol_statements([2020, 2021, 2022])\nThis function downloads monetary policy statements from 26 emerging market central banks (Armenia, Brazil, Chile, Colombia, Czech Republic, Egypt, Georgia, Hungary, Israel, India, Kazakhstan, Malaysia, Mongolia, Mexico, Nigeria, Pakistan, Peru, Philippines, Poland, Romania, Russia, South Africa, South Korea, Thailand, Türkiye, Ukraine) as well as the Fed and the ECB (press-conference introductory statements). The dataset includes official English versions of statements for 1998-2023 (starting date varies depending on data availability). The original source is Evdokimova et al. (2023). If you use this data in your work, please cite the dataset, as follows:\n@article{emcbcom,\n    author = {Tatiana Evdokimova and Piroska Nagy Mohácsi and Olga Ponomarenko and Elina Ribakova},\n    title = {Central banks and policy communication: How emerging markets have outperformed the Fed and ECB},\n    year = {2023},\n    institution = {Peterson Institute for International Economics},\n    url = {https://www.piie.com/publications/working-papers/central-banks-and-policy-communication-how-emerging-markets-have}\n}\n\n# Load monpol statements for 2020\nspeeches = load_monpol_statements(2020)\nspeeches.head()\n\n\n\n\n\n\n\n\ncountry\ndate\nstatement\n\n\n\n\n0\nPoland\n2020-01-08\nWarsaw, 8 January 2020 Information from the me...\n\n\n1\nRomania\n2020-01-08\nIn its meeting of 8 January 2020, the Board of...\n\n\n2\nIsrael\n2020-01-09\nThe inflation environment remains low. The Nov...\n\n\n3\nPeru\n2020-01-09\nPRESS RELEASE MONETARY POLICY STATEMENT JANUAR...\n\n\n4\nEgypt\n2020-01-16\nCentral Bank of Egypt Press Release January 16...\n\n\n\n\n\n\n\n\n\n\n\n\nload_lr_tanzania_data (wide_format: 'bool' = False) -&gt; 'dict[str, pd.DataFrame]'\n\nLoads liquidity risk data from CSV files.\n\nThis function loads monthly and weekly liquidity risk data. The data is sourced from the paper\n\"Using machine learning for detecting liquidity risk in banks\" by Rweyemamu Ignatius Barongo and\nJimmy Tibangayuka Mbelwa. The dataset includes data from 38 Tanzanian banks (2010-2021) provided\nby the Bank of Tanzania (BOT). Banks are identified by anonymous bank codes.\n\n\nParameters:\nwide_format (bool): If True, returns data in wide format with pivoted columns. Column names will\n    be in the format BANK_CODE__VAR_NAME.\n\nReturns:\ndict[str, pd.DataFrame]: A dictionary with keys 'w' for weekly data and 'm' for monthly data.\n    The values are pandas DataFrames containing the data for each frequency.\nThis function loads liquidity risk data from CSV files for 38 Tanzanian commercial banks spanning from 2010 to 2021. The dataset includes both monthly and weekly data provided by the Bank of Tanzania (BOT). The original source is Barongo and Mbelwa (2024).\nIf you use this data in your work, please cite the dataset, as follows:\n@article{BARONGO2024100511,\n    author = {Rweyemamu Ignatius Barongo and Jimmy Tibangayuka Mbelwa},\n    title = {Using machine learning for detecting liquidity risk in banks},\n    journal = {Machine Learning with Applications},\n    volume = {15},\n    year = {2024},\n    doi = {https://doi.org/10.1016/j.mlwa.2023.100511},\n    url = {https://www.sciencedirect.com/science/article/pii/S2666827023000646},\n}\n\n# Load liquidity risk data\nlr_data = load_lr_tanzania_data()\nlr_data[\"m\"].head()\n\n\n\n\n\n\n\n\nINSTITUTIONCODE\nREPORTINGDATE\nF001_ASSET_CASH\nF002_ASSET_BAL_BOT\nF003_ASSET_BAL_BOT_SMR\nF004_ASSET_BAL_BOT_CURRENT_ACCOUNT\nF005_ASSET_BAL_BOT_OTHERS\nF006_ASSET_BAL_OTHER_BANKS\nF007_ASSET_BAL_OTHER_BANKS_TZ\nF008_ASSET_BAL_OTHER_BANKS_ABROAD\n...\nEWAQ_Capital\nEWAQ_NPL\nEWAQ_GrossLoans\nEWAQ_LargeExposures\nEWAQ_NPLsNetOfProvisions\nEWAQ_NPLsNetOfProvisions2CoreCapital\nEWAQ_NPLs2GrossLoans\nEWAQ_AssetsQualityRating\nEWAQ_Loans\nMLA_CLASS\n\n\n\n\n0\nB5412\n2010-01-31\n9.570422e+09\n5.260077e+10\n3.700000e+10\n1.560077e+10\n0.0\n1.348740e+11\n1.622160e+10\n1.186520e+11\n...\n9.514540e+10\n1.951219e+09\n1.274810e+11\n7.137633e+10\n903220975.0\n0.0095\n0.0153\n1.0\nNaN\nhigh\n\n\n1\nB5412\n2010-02-28\n6.996645e+09\n3.554718e+10\n3.200000e+10\n3.547185e+09\n0.0\n1.482240e+11\n7.535208e+08\n1.474710e+11\n...\n9.595342e+10\n1.954516e+09\n1.251520e+11\n7.831967e+10\n904869349.0\n0.0094\n0.0156\n1.0\nNaN\nhigh\n\n\n2\nB5412\n2010-03-31\n8.689629e+09\n3.744132e+10\n3.350000e+10\n3.941316e+09\n0.0\n1.621370e+11\n5.570919e+09\n1.565660e+11\n...\n9.864602e+10\n1.979268e+09\n1.284300e+11\n7.846886e+10\n917245663.0\n0.0093\n0.0154\n1.0\nNaN\nhigh\n\n\n3\nB5412\n2010-04-30\n6.811311e+09\n3.527009e+10\n3.250000e+10\n2.770089e+09\n0.0\n2.244100e+11\n8.739334e+09\n2.156710e+11\n...\n9.960755e+10\n2.014398e+09\n1.286710e+11\n8.986596e+10\n934810301.0\n0.0094\n0.0157\n1.0\nNaN\nhigh\n\n\n4\nB5412\n2010-05-31\n9.160128e+09\n4.927299e+10\n3.982500e+10\n9.447986e+09\n0.0\n1.893920e+11\n1.524899e+09\n1.878670e+11\n...\n1.007330e+11\n2.081954e+09\n1.319650e+11\n8.567217e+10\n968588451.0\n0.0096\n0.0158\n1.0\nNaN\nhigh\n\n\n\n\n5 rows × 242 columns"
  },
  {
    "objectID": "datasets.html#simulated-datasets",
    "href": "datasets.html#simulated-datasets",
    "title": "Datasets for economic research",
    "section": "Simulated datasets",
    "text": "Simulated datasets\n\n\n\n\n\n\nNote\n\n\n\nAll of the functions creating simulated datasets have a parameter random_state that allow for reproducible random numbers.\n\n\n\nfrom gingado.datasets import make_causal_effect\n\n\nmake_causal_effect\n\nmake_causal_effect (n_samples: 'int' = 100, n_features: 'int' = 100, pretreatment_outcome=&lt;function &lt;lambda&gt; at 0x000002592A8291C0&gt;, treatment_propensity=&lt;function &lt;lambda&gt; at 0x000002592A829260&gt;, treatment_assignment=&lt;function &lt;lambda&gt; at 0x000002592A829300&gt;, treatment=&lt;function &lt;lambda&gt; at 0x000002592A8293A0&gt;, treatment_effect=&lt;function &lt;lambda&gt; at 0x000002592A829440&gt;, bias: 'float' = 0, noise: 'float' = 0, random_state=None, return_propensity: 'bool' = False, return_assignment: 'bool' = False, return_treatment_value: 'bool' = False, return_treatment_effect: 'bool' = True, return_pretreatment_y: 'bool' = False, return_as_dict: 'bool' = False)\n\nGenerates a simulated dataset to analyze causal effects of a treatment on an outcome variable.\n\nArgs:\n    n_samples (int): Number of observations in the dataset.\n    n_features (int): Number of covariates for each observation.\n    pretreatment_outcome (function): Function to generate outcome variable before any treatment.\n    treatment_propensity (function or float): Function to generate treatment propensity or a fixed value for each observation.\n    treatment_assignment (function): Function to determine treatment assignment based on propensity.\n    treatment (function): Function to determine the magnitude of treatment for each treated observation.\n    treatment_effect (function): Function to calculate the effect of treatment on the outcome variable.\n    bias (float): Constant value added to the outcome variable.\n    noise (float): Standard deviation of the noise added to the outcome variable. If 0, no noise is added.\n    random_state (int, RandomState instance, or None): Seed or numpy random state instance for reproducibility.\n    return_propensity (bool): If True, returns the treatment propensity for each observation.\n    return_assignment (bool): If True, returns the treatment assignment status for each observation.\n    return_treatment_value (bool): If True, returns the treatment value for each observation.\n    return_treatment_effect (bool): If True, returns the treatment effect for each observation.\n    return_pretreatment_y (bool): If True, returns the outcome variable of each observation before treatment effect.\n    return_as_dict (bool): If True, returns the results as a dictionary; otherwise, returns as a list.\n    \nReturns:\n    A dictionary or list containing the simulated dataset components specified by the return flags.\nmake_causal_effect creates a dataset for when the question of interest is related to the causal effects of a treatment. For example, for a simulated dataset, we can check that \\(Y_i\\) corresponds to the sum of the treatment effects plus the component that does not depend on the treatment:\n\n causal_sim = make_causal_effect(\n    n_samples=2000,\n    n_features=100,\n    return_propensity=True,\n    return_treatment_effect=True, \n    return_pretreatment_y=True, \n    return_as_dict=True)\n\n assert not np.any(np.round(causal_sim['y'] - causal_sim['pretreatment_y'] - causal_sim['treatment_effect'], decimals=13))\n\n\nPre-treatment outcome\nThe pre-treatment outcome \\(Y_i|X_i\\) (the part of the outcome variable that is not dependent on the treatment) might be defined by the user. This corresponds to the value of the outcome for any untreated observations. The function should always take at least two arguments: X and bias, even if one of them is unused; bias is the constant. The argument is zero by default but can be set by the user to be another value.\n\ncausal_sim = make_causal_effect(\n    bias=0.123,\n    pretreatment_outcome=lambda X, bias: bias,\n    return_assignment=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim['y'][causal_sim['treatment_assignment'] == 0] == 0.123)\n\nIf the outcome depends on specific columns of \\(X\\), this can be implemented as shown below.\n\ncausal_sim = make_causal_effect(\n    pretreatment_outcome=lambda X, bias: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias\n)\n\nAnd of course, the outcome might also have a random component.\nIn these cases (and in other parts of this function), when the user wants to use the same random number generator as the other parts of the function, the function must have an argment rng for the NumPy random number generator used in other parts of the function.\n\ncausal_sim_1 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\ncausal_sim_2 = make_causal_effect(\n    pretreatment_outcome=lambda X, bias, rng: X[:, 1] + np.maximum(X[:,2], 0) + X[:,3] * X[:,4] + bias + rng.standard_normal(size=X.shape[0]),\n    random_state=42,\n    return_pretreatment_y=True,\n    return_as_dict=True\n)\n\nassert all(causal_sim_1['X'].reshape(-1, 1) == causal_sim_2['X'].reshape(-1, 1))\nassert all(causal_sim_1['y'] == causal_sim_2['y'])\nassert all(causal_sim_1['pretreatment_y'] == causal_sim_2['pretreatment_y'])\n\n\n\nTreatment propensity\nThe treatment propensity of observations may all be the same, in which case treatment_propensity is a floating number between 0 and 1.\n\nsame_propensity_sim = make_causal_effect(\n    n_samples=485,\n    treatment_propensity=0.3,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nassert np.unique(same_propensity_sim['propensity']) == 0.3\nassert len(same_propensity_sim['propensity']) == 485\n\nOr it might depend on the observation’s covariates, with the user passing a function with an argument ‘X’.\n\nheterogenous_propensities_sim = make_causal_effect(\n    n_samples=1000,\n    treatment_propensity=lambda X: 0.3 + (X[:, 0] &gt; 0) * 0.2,\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Heterogenously distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(heterogenous_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\n\n\n\n\nThe propensity can also be randomly allocated, together with covariate dependence or not. Note that even if the propensity is completely random and does not depend on covariates, the function must still use the argument X to calculate a random vector with the appropriate size.\n\nrandom_propensities_sim = make_causal_effect(\n    n_samples=50000,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    return_propensity=True,\n    return_as_dict=True\n)\n\nplt.title(\"Randomly distributed propensities\")\nplt.xlabel(\"Propensity\")\nplt.ylabel(\"No of observations\")\nplt.hist(random_propensities_sim['propensity'], bins=100)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTreatment assignment\nAs seen above, every observation has a given treatment propensity - the chance that they are treated. Users can define how this propensity translates into actual treatment with the argument treatment_assignment. This argument takes a function, which must have an argument called propensity.\nThe default value for this argument is a function returning 1s with probability propensity and 0s otherwise. Any other function should always return either 0s or 1s for the data simulator to work as expected.\n\ncausal_sim = make_causal_effect(\n    treatment_assignment=lambda propensity: np.random.binomial(1, propensity)\n)\n\nWhile the case above is likely to be the most useful in practice, this argument accepts more complex relationships between an observation’s propensity and the actual treatment assignment.\nFor example, if treatment is subject to rationing, then one could simulate data with 10 observations where only the samples with the highest (say, 3) propensity scores get treated, as below:\n\nrationed_treatment_sim = make_causal_effect(\n    n_samples=10,\n    treatment_propensity=lambda X: np.random.uniform(size=X.shape[0]),\n    treatment_assignment=lambda propensity: propensity &gt;= propensity[np.argsort(propensity)][-3],\n    return_propensity=True,\n    return_assignment=True,\n    return_as_dict=True\n)\n\n\nrationed_treatment = pd.DataFrame(\n    np.column_stack((rationed_treatment_sim['propensity'], rationed_treatment_sim['treatment_assignment'])),\n    columns = ['propensity', 'assignment']\n    )\n\n\nrationed_treatment.sort_values('propensity')\n\n\n\n\n\n\n\n\npropensity\nassignment\n\n\n\n\n2\n0.005718\n0.0\n\n\n0\n0.500951\n0.0\n\n\n7\n0.534899\n0.0\n\n\n1\n0.604894\n0.0\n\n\n6\n0.709831\n0.0\n\n\n5\n0.724756\n0.0\n\n\n3\n0.778845\n0.0\n\n\n8\n0.810066\n1.0\n\n\n9\n0.964335\n1.0\n\n\n4\n0.975238\n1.0\n\n\n\n\n\n\n\n\n\nTreatment value\nThe treatment argument indicates the magnitude of the treatment for each observation assigned for treatment. Its value is always a function that must have an argument called assignment, as in the first example below.\nIn the simplest case, the treatment is a binary variable indicating whether or not a variable was treated. In other words, the treatment is the same as the assignment, as in the default value.\nBut users can also simulate data with heterogenous treatment, conditional on assignment. This is done by including a pararemeter X in the function, as shown in the second example below.\n\nbinary_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment: assignment,\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nassert sum(binary_treatment_sim['treatment_assignment'] - binary_treatment_sim['treatment_value'][0]) == 0\n\nHeterogenous treatments may occur in settings where treatment intensity, conditional on assignment, varies across observations. Please note the following:\n\nthe heterogenous treatment amount may or may not depend on covariates, but either way, if treatment values are heterogenous, then X needs to be an argument of the function passed to treatment, if nothing else to make sure the shapes match; and\nif treatments are heterogenous, then it is important to multiply the treatment value with the assignment argument to ensure that observations that are not assigned to be treated are indeed not treated (the function will return an AssertionError otherwise).\n\n\nhetereogenous_treatment_sim = make_causal_effect(\n    n_samples=15,\n    treatment=lambda assignment, X: assignment * np.random.uniform(size=X.shape[0]),\n    return_assignment=True,\n    return_treatment_value=True,\n    return_as_dict=True\n)\n\nIn contrast to the function above, in the chunk below the function make_causal_effect fails because a treatment value is also assigned to observations that were not assigned for treatment.\n\ntry:\n    make_causal_effect(\n        treatment=lambda assignment, X: assignment + np.random.uniform(size=X.shape[0])\n    )\nexcept ValueError as e:\n    print(e)\n\nArgument `treatment` must be a function that returns 0 for observations with `assignment` == 0.\nOne suggestion is to multiply the desired treatment value with `assignment`.\n\n\n\n\nTreatment effect\nThe treatment effect can be homogenous, ie, is doesn’t depend on any other characteristic of the individual observations (in other words, does not depend on \\(X_i\\)), or heterogenous (where the treatment effect on \\(Y_i\\) does depend on each observation’s \\(X_i\\)). This can be done by specifying the causal relationship through a lambda function, as below:\n\nhomogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value: treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (homogenous_effects_sim['treatment_effect'] == homogenous_effects_sim['treatment_value']).all()\n\nheterogenous_effects_sim = make_causal_effect(\n        treatment_effect=lambda treatment_value, X: np.maximum(X[:, 1], 0) * treatment_value,\n        return_treatment_value=True,\n        return_as_dict=True\n)\n\nassert (heterogenous_effects_sim['treatment_effect'] != heterogenous_effects_sim['treatment_value']).any()"
  },
  {
    "objectID": "datasets.html#references",
    "href": "datasets.html#references",
    "title": "Datasets for economic research",
    "section": "References",
    "text": "References\n\n\nBarongo, Rweyemamu Ignatius, and Jimmy Tibangayuka Mbelwa. 2024. “Using Machine Learning for Detecting Liquidity Risk in Banks.” Machine Learning with Applications 15. https://doi.org/https://doi.org/10.1016/j.mlwa.2023.100511.\n\n\nBarro, Robert J., and Jong-Wha Lee. 1994. “Sources of Economic Growth.” Carnegie-Rochester Conference Series on Public Policy 40: 1–46. https://doi.org/10.1016/0167-2231(94)90002-7.\n\n\nBelloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2011. “Inference for High-Dimensional Sparse Econometric Models.” arXiv Preprint arXiv:1201.0220.\n\n\nEvdokimova, Tatiana, Piroska Nagy Mohácsi, Olga Ponomarenko, and Elina Ribakova. 2023. “Central Banks and Policy Communication: How Emerging Markets Have Outperformed the Fed and ECB.” https://www.piie.com/publications/working-papers/central-banks-and-policy-communication-how-emerging-markets-have.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E Primiceri. 2021. “Economic Predictions with Big Data: The Illusion of Sparsity.” Econometrica 89 (5): 2409–37.\n\n\nInternational Settlements, Bank for. 2024. “Central Bank Speeches.” https://www.bis.org/cbspeeches/download.htm."
  },
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Data augmentation",
    "section": "",
    "text": "gingado provides data augmentation functionalities that can help users to augment their datasets with a time series dimension. This can be done both on a stand-alone basis as the user incorporates new data on top of the original dataset, or as part of a scikit-learn Pipeline that also includes other steps like data transformation and model estimation."
  },
  {
    "objectID": "augmentation.html#compatibility-with-scikit-learn",
    "href": "augmentation.html#compatibility-with-scikit-learn",
    "title": "Data augmentation",
    "section": "Compatibility with scikit-learn",
    "text": "Compatibility with scikit-learn\nAs mentioned above, gingado’s transformers are built to be compatible with scikit-learn. The code below demonstrates this compatibility.\nFirst, we create the example dataset. In this case, it comprises the daily foreign exchange rate of selected currencies to the Euro. The Brazilian Real (BRL) is chosen for this example as the dependent variable.\n\nfrom gingado.utils import load_SDMX_data, Lag\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\nX = load_SDMX_data(\n    sources={'ECB': 'EXR'}, \n    keys={'FREQ': 'D', 'CURRENCY': ['EUR', 'AUD', 'BRL', 'CAD', 'CHF', 'GBP', 'JPY', 'SGD', 'USD']},\n    params={\"startPeriod\": 2003}\n    )\n# drop rows with empty values\nX.dropna(inplace=True)\n# adjust column names in this simple example for ease of understanding:\n# remove parts related to source and dataflow names\nX.columns = X.columns.str.replace(\"ECB__EXR_D__\", \"\").str.replace(\"__EUR__SP00__A\", \"\")\nX = Lag(lags=1, jump=0, keep_contemporaneous_X=True).fit_transform(X)\ny = X.pop('BRL')\n# retain only the lagged variables in the X variable\nX = X[X.columns[X.columns.str.contains('_lag_')]]\n\nQuerying data from ECB's dataflow 'EXR' - Exchange Rates...\n\n\n\nX_train, X_test = X.iloc[:-1], X.tail(1)\ny_train, y_test = y.iloc[:-1], y.tail(1)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n((5678, 8), (5678,), (1, 8), (1,))\n\n\nNext, the data augmentation object provided by gingado adds more data. In this case, for brevity only one dataflow from one source is listed. If users want to add more SDMX sources, simply add more keys to the dictionary. And if users want data from all dataflows from a given source provided the keys and parameters such as frequency and dates match, the value should be set to 'all', as in {'ECB': ['CISS'], 'BIS': 'all'}.\n\ntest_src = {'ECB': ['CISS'], 'BIS': ['WS_CBPOL_D']}\n\nX_train__fit_transform = AugmentSDMX(sources=test_src).fit_transform(X=X_train)\nX_train__fit_then_transform = AugmentSDMX(sources=test_src).fit(X=X_train).transform(X=X_train, training=True)\n\nassert X_train__fit_transform.shape == X_train__fit_then_transform.shape\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\nThis is the dataset now after this particular augmentation:\n\nprint(f\"No of columns: {len(X_train__fit_transform.columns)} {X_train__fit_transform.columns}\")\nX_train__fit_transform\n\nNo of columns: 30 Index(['AUD_lag_1', 'BRL_lag_1', 'CAD_lag_1', 'CHF_lag_1', 'GBP_lag_1',\n       'JPY_lag_1', 'SGD_lag_1', 'USD_lag_1',\n       'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nAUD_lag_1\nBRL_lag_1\nCAD_lag_1\nCHF_lag_1\nGBP_lag_1\nJPY_lag_1\nSGD_lag_1\nUSD_lag_1\nECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX\n...\nECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX\nECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON\nECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON\nECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX\nECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-01-03\n1.8554\n3.6770\n1.6422\n1.4528\n0.65200\n124.40\n1.8188\n1.0446\n0.021899\n0.043292\n...\n0.032973\n0.191545\n0.079942\n-0.243340\n0.150255\n0.139377\n0.036886\n0.075394\n0.245463\n0.134710\n\n\n2003-01-06\n1.8440\n3.6112\n1.6264\n1.4555\n0.65000\n124.56\n1.8132\n1.0392\n0.020801\n0.039924\n...\n0.032973\n0.191545\n0.086229\n-0.243340\n0.150255\n0.139377\n0.036886\n0.075394\n0.245463\n0.148688\n\n\n2003-01-07\n1.8281\n3.5145\n1.6383\n1.4563\n0.64950\n124.40\n1.8210\n1.0488\n0.019738\n0.038084\n...\n0.032973\n0.191545\n0.083881\n-0.243340\n0.150255\n0.139377\n0.036886\n0.075394\n0.245463\n0.156701\n\n\n2003-01-08\n1.8160\n3.5139\n1.6257\n1.4565\n0.64960\n124.82\n1.8155\n1.0425\n0.019947\n0.040338\n...\n0.032973\n0.191545\n0.087273\n-0.243340\n0.150255\n0.139377\n0.036886\n0.075394\n0.245463\n0.165586\n\n\n2003-01-09\n1.8132\n3.4405\n1.6231\n1.4586\n0.64950\n124.90\n1.8102\n1.0377\n0.017026\n0.040535\n...\n0.032973\n0.191545\n0.086138\n-0.243340\n0.150255\n0.139377\n0.036886\n0.075394\n0.245463\n0.184554\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2025-02-26\n1.6538\n6.0839\n1.4969\n0.9386\n0.82908\n157.19\n1.4047\n1.0497\n0.052334\n0.040623\n...\n0.022499\n0.069443\n0.003894\n-0.115521\n0.047985\n0.064308\n0.030268\n0.019904\n0.048344\n0.007904\n\n\n2025-02-27\n1.6622\n6.0329\n1.5054\n0.9392\n0.82868\n156.70\n1.4035\n1.0487\n0.050800\n0.038099\n...\n0.022499\n0.069443\n0.003472\n-0.115521\n0.047985\n0.064308\n0.030268\n0.019904\n0.048344\n0.009505\n\n\n2025-02-28\n1.6638\n6.0825\n1.5040\n0.9407\n0.82673\n156.73\n1.4049\n1.0477\n0.040189\n0.031683\n...\n0.022499\n0.069443\n0.003340\n-0.115521\n0.047985\n0.064308\n0.030268\n0.019904\n0.048344\n0.011010\n\n\n2025-03-03\n1.6741\n6.0712\n1.5019\n0.9394\n0.82608\n156.96\n1.4037\n1.0411\n0.048914\n0.040580\n...\n0.022499\n0.069443\n0.004810\n-0.115521\n0.047985\n0.064308\n0.030268\n0.019904\n0.048344\n0.014406\n\n\n2025-03-04\n1.6811\n6.1590\n1.5104\n0.9428\n0.82530\n158.33\n1.4106\n1.0465\n0.048034\n0.037239\n...\n0.022499\n0.069443\n0.035906\n-0.115521\n0.047985\n0.064308\n0.030268\n0.019904\n0.048344\n0.017931\n\n\n\n\n5678 rows × 30 columns\n\n\n\n\nPipeline\nAugmentSDMX can also be part of a Pipeline object, which minimises operational errors during modelling and avoids using testing data during training:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\npipeline = Pipeline([\n    ('augmentation', AugmentSDMX(sources={'BIS': 'WS_CBPOL_D'})),\n    ('imp', IterativeImputer(max_iter=10)),\n    ('forest', RandomForestRegressor())\n], verbose=True)\n\n\n\nTuning the data augmentation to enhance model performance\nAnd since AugmentSDMX can be included in a Pipeline, it can also be fine-tuned by parameter search techniques (such as grid search), further helping users make the best of available data to enhance performance of their models.\n\n\n\n\n\n\nTip\n\n\n\nUsers can cache the data augmentation step to avoid repeating potentially lengthy data downloads. See the memory argument in the sklearn.pipeline.Pipeline documentation.\n\n\n\ngrid = GridSearchCV(\n    estimator=pipeline,\n    param_grid={\n        'augmentation': ['passthrough', AugmentSDMX(sources={'ECB': 'CISS'})]\n    },\n    verbose=2,\n    cv=TimeSeriesSplit(n_splits=2)\n    )\n\ny_pred_grid = grid.fit(X_train, y_train).predict(X_test)\n\nFitting 2 folds for each of 2 candidates, totalling 4 fits\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   1.0s\n[CV] END ...........................augmentation=passthrough; total time=   0.9s\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   2.1s\n[CV] END ...........................augmentation=passthrough; total time=   2.1s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  19.8s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.4s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   3.1s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time=  43.7s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=  28.1s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.6s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   8.4s\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n[CV] END ..augmentation=AugmentSDMX(sources={'ECB': 'CISS'}); total time= 1.1min\n[Pipeline] ...... (step 1 of 3) Processing augmentation, total=   0.0s\n[Pipeline] ............... (step 2 of 3) Processing imp, total=   0.0s\n[Pipeline] ............ (step 3 of 3) Processing forest, total=   3.3s\n\n\n\ngrid.best_params_\n\n{'augmentation': 'passthrough'}\n\n\n\nprint(f\"In this particular case, the best model was achieved by {'not ' if grid.best_params_['augmentation'] == 'passthrough' else ''}using the data augmentation.\")\n\nIn this particular case, the best model was achieved by not using the data augmentation.\n\n\n\nprint(f\"The last value in the training dataset was {y_train.tail(1).to_numpy()}. The predicted value was {y_pred_grid}, and the actual value was {y_test.to_numpy()}.\")\n\nThe last value in the training dataset was [6.2132]. The predicted value was [6.202662], and the actual value was [6.2938]."
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Automatic benchmark model",
    "section": "",
    "text": "A Benchmark object has a similar API to a sciki-learn estimator: you build an instance with the desired arguments, and fit it to the data at a later moment. Benchmarks is a convenience wrapper for reading the training data, passing it through a simplified pipeline consisting of data imputation and a standard scalar, and then the benchmark function calibrated with a grid search.\nA gingado Benchmark object seeks to automatise a significant part of creating a benchmark model. Importantly, the Benchmark object also has a compare method that helps users evaluate if candidate models are better than the benchmark, and if one of them is, it becomes the new benchmark. This compare method takes as argument another fitted estimator (which could be itself a solo estimator or a whole pipeline) or a list of fitted estimators.\nBenchmarks start with default values that should perform reasonably well in most settings, but the user is also free to choose any of the benchmark’s components by passing as arguments the data split, pipeline, and/or a dictionary of parameters for the hyperparameter tuning."
  },
  {
    "objectID": "benchmark.html#scoring",
    "href": "benchmark.html#scoring",
    "title": "Automatic benchmark model",
    "section": "Scoring",
    "text": "Scoring\nClassificationBenchmark and RegressionBenchmark use the default scoring method for comparing model alternatives, both during estimation of the benchmark model and when comparing this benchmark with candidate models. Users are encouraged to consider if another scoring method is more suitable for their use case. More information on available scoring methods that are compatible with gingado Benchmark objects can be found here."
  },
  {
    "objectID": "benchmark.html#data-split",
    "href": "benchmark.html#data-split",
    "title": "Automatic benchmark model",
    "section": "Data split",
    "text": "Data split\ngingado benchmarks rely on hyperparameter tuning to discover the benchmark specification that is most likely to perform better with the user data. This tuning in turn depends on a data splitting strategy for the cross-validation. By default, gingado uses StratifiedShuffleSplit (in classification problems) or ShuffleSplit (in regression problems) if the data is not time series and TimeSeriesSplit otherwise.\nThe user may overrun these defaults either by directly setting the parameter cv or default_cv when instanciating the gingado benchmark class. The difference is that default_cv is only used after gingado checks that the data is not a time series (if a time dimension exists, then TimeSeriesSplit is used).\n\nX, y = make_classification()\nbm_cls = ClassificationBenchmark(cv=TimeSeriesSplit(n_splits=3)).fit(X, y)\nassert bm_cls.benchmark.n_splits_ == 3\n\nX, y = make_regression()\nbm_reg = RegressionBenchmark(default_cv=ShuffleSplit(n_splits=7)).fit(X, y)\nassert bm_reg.benchmark.n_splits_ == 7\n\nPlease refer to this page for more information on the different Splitter classes available on scikit-learn, and this page for practical advice on how to choose a splitter for data that are not time series. Any one of these objects (or a custom splitter that is compatible with them) can be passed to a Benchmark object.\nUsers that wish to use specific parameters should include the actual Splitter object as the parameter, as done with the n_splits parameter in the chunk above."
  },
  {
    "objectID": "benchmark.html#custom-benchmarks",
    "href": "benchmark.html#custom-benchmarks",
    "title": "Automatic benchmark model",
    "section": "Custom benchmarks",
    "text": "Custom benchmarks\ngingado provides users with two Benchmark objects out of the box: ClassificationBenchmark and RegressionBenchmark, to be used depending on the task at hand. Both classes derive from a base class ggdBenchmark, which implements methods that facilitate model comparison. Users that want to create a customised benchmark model for themselves have two options:\n\nthe simpler possibility is to train the estimator as usual, and then assign the fitted estimator to a Benchmark object.\nif the user wants more control over the fitting process of estimating the benchmark, they can create a class that subclasses from ggdBenchmark and either implements custom fit, predict and score methods, or also subclasses from scikit-learn’s BaseEstimator.\n\nIn any case, if the user wants the benchmark to automatically detect if the data is a time series and also to document the model right after fitting, the fit method should call self._fit on the data. Otherwise, the user can simply implement any consistent logic in fit as the user sees fit (pun intended)."
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Model documentation",
    "section": "",
    "text": "Each user has a specific documentation need, ranging from simply logging the model training to a more complex description of the model pipeline with a discusson of the model outcomes. gingado addresses this variety of needs by offering a class of objects, “Documenters”, that facilitate model documentation. A base class facilitates the creation of generic ways to document models, and gingado includes two specific model documentation templates off-the-shelf as described below.\nThe model documentation is performed by Documenters, objects that subclass from the base class ggdModelDocumentation. This base class offers code that can be used by any Documenter to read the model in question, format the information according to a template and save the resulting documentation in a JSON format. Documenters save the underlying information using the JSON format. With the JSON documentation file at hand, the user can then deploy existing third-party libraries to transform the information stored in JSON into a variety of formats (eg, HTML, PDF) as needed.\nOne current area of development is the automatic filing of some fields related to the model. The objective is to automatise documentation of the information that can be fetched automatically from the model, leaving time for the analyst to concentrate on other tasks, such as considering the ethical implications of the machine learning model being trained."
  },
  {
    "objectID": "documentation.html#modelcard",
    "href": "documentation.html#modelcard",
    "title": "Model documentation",
    "section": "ModelCard",
    "text": "ModelCard\nModelCard - the model documentation template inspired by the work of Mitchell et al. (2018) already comes with gingado. Its template can be used by users as is, or tweaked according to each need. The ModelCard template can also serve as inspiration for any custom documentation needs. Users with documentation needs beyond the out-of-the-box solutions provided by gingado can create their own class of Documenters (more information on that below), and compatibility with these custom documentation routines with the rest of the code is ensured. Users are encouraged to submit a pull request with their own documentation models subclassing ggdModelDocumentation if these custom templates can also benefit other users.\nLike all gingado Documenters, a ModelCard is can be easily created on a standalone basis as shown below, or as part of a gingado.ggdBenchmark object.\n\nmodel_doc = ModelCard()\n\nBy default, it autofills the template with the current date and time. Users can add other information to be automatically added by a customised Documenter object.\n\nmodel_doc_with_autofill = ModelCard(autofill=True)\nmodel_doc_no_autofill = ModelCard(autofill=False)\n\nBelow is a comparison of the model_details section of the model document, with and without the autofill.\n\nmodel_doc_with_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': '2025-03-06 11:28:04 ',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nmodel_doc_no_autofill.show_json()['model_details']\n\n{'developer': 'Person or organisation developing the model',\n 'datetime': 'Model date',\n 'version': 'Model version',\n 'type': 'Model type',\n 'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n 'paper': 'Paper or other resource for more information',\n 'citation': 'Citation details',\n 'license': 'License',\n 'contact': 'Where to send questions or comments about the model'}\n\n\n\nModelCard\n\nModelCard (file_path: 'str' = '', autofill: 'bool' = True, indent_level: 'int | None' = 2)\n\nA gingado Documenter based on @ModelCards\n\nautofill_template\n\nautofill_template (self)\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#forecastcard",
    "href": "documentation.html#forecastcard",
    "title": "Model documentation",
    "section": "ForecastCard",
    "text": "ForecastCard\nForecastCard is a model documentation template inspired by Mitchell et al. (2018), but with fields that are more specifically targeted towards forecasting or nowcasting use cases.\nBecause a ForecastCard Documenter object is targeted to forecasting and nowcasting models, it contains some specialised fields, as illustrated below.\n\nmodel_doc = ForecastCard()\n\nmodel_doc.show_template()\n\n{\n  \"model_details\": {\n    \"field_description\": \"Basic information about the model\",\n    \"variable\": \"Variable(s) being forecasted or nowcasted\",\n    \"jurisdiction\": \"Jurisdiction(s) of the variable being forecasted or nowcasted\",\n    \"developer\": \"Person or organisation developing the model\",\n    \"datetime\": \"Model date\",\n    \"version\": \"Model version\",\n    \"type\": \"Model type\",\n    \"pipeline\": \"Description of the pipeline steps being used\",\n    \"info\": \"Information about training algorithms, parameters, fairness constraints or other applied approaches, and features\",\n    \"econometric_model\": \"Information about the econometric model or technique\",\n    \"paper\": \"Paper or other resource for more information\",\n    \"citation\": \"Citation details\",\n    \"license\": \"License\",\n    \"contact\": \"Where to send questions or comments about the model\"\n  },\n  \"intended_use\": {\n    \"field_description\": \"Use cases that were envisioned during development\",\n    \"primary_uses\": \"Primary intended uses\",\n    \"primary_users\": \"Primary intended users\",\n    \"out_of_scope\": \"Out-of-scope use cases\"\n  },\n  \"factors\": {\n    \"field_description\": \"Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others\",\n    \"relevant\": \"Relevant factors\",\n    \"evaluation\": \"Evaluation factors\"\n  },\n  \"metrics\": {\n    \"field_description\": \"Metrics should be chosen to reflect potential real world impacts of the model\",\n    \"performance_measures\": \"Model performance measures\",\n    \"estimation_approaches\": \"How are the evaluation metrics calculated? Include information on the cross-validation approach, if used\"\n  },\n  \"data\": {\n    \"field_description\": \"Details on the dataset(s) used for the training and evaluation of the model\",\n    \"datasets\": \"Datasets\",\n    \"preprocessing\": \"Preprocessing\",\n    \"cutoff_date\": \"Cut-off date that separates training from evaluation data\"\n  },\n  \"ethical_considerations\": {\n    \"field_description\": \"Ethical considerations that went into model development, surfacing ethical challenges and solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work.\",\n    \"sensitive_data\": \"Does the model use any sensitive data (e.g., protected classes)?\",\n    \"risks_and_harms\": \"What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown\",\n    \"use_cases\": \"Are there any known model use cases that are especially fraught?\",\n    \"additional_information\": \"If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.\"\n  },\n  \"caveats_recommendations\": {\n    \"field_description\": \"Additional concerns that were not covered in the previous sections\",\n    \"caveats\": \"For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?\",\n    \"recommendations\": \"Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?\"\n  }\n}\n\n\n\nmodel_doc.show_json()\n\n{'model_details': {'variable': 'Variable(s) being forecasted or nowcasted',\n  'jurisdiction': 'Jurisdiction(s) of the variable being forecasted or nowcasted',\n  'developer': 'Person or organisation developing the model',\n  'datetime': '2025-03-06 11:28:05 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'pipeline': 'Description of the pipeline steps being used',\n  'info': 'Information about training algorithms, parameters, fairness constraints or other applied approaches, and features',\n  'econometric_model': 'Information about the econometric model or technique',\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'Primary intended uses',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'Out-of-scope use cases'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Model performance measures',\n  'estimation_approaches': 'How are the evaluation metrics calculated? Include information on the cross-validation approach, if used'},\n 'data': {'datasets': 'Datasets',\n  'preprocessing': 'Preprocessing',\n  'cutoff_date': 'Cut-off date that separates training from evaluation data'},\n 'ethical_considerations': {'sensitive_data': 'Does the model use any sensitive data (e.g., protected classes)?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\n\nForecastCard\n\nForecastCard (file_path: 'str' = '', autofill: 'bool' = True, indent_level: 'int | None' = 2)\n\nA gingado Documenter for forecasting or nowcasting use cases\n\nautofill_template\n\nautofill_template (self)\n\nCreate an empty model card template, then fills it with information that is automatically obtained from the system"
  },
  {
    "objectID": "documentation.html#preliminaries",
    "href": "documentation.html#preliminaries",
    "title": "Model documentation",
    "section": "Preliminaries",
    "text": "Preliminaries\nThe mock dataset below is used to construct models using different libraries, to demonstrate how they are read by Documenters.\n\nfrom sklearn.datasets import make_classification\n\n\n# some mock up data\nX, y = make_classification()\n\nX.shape, y.shape\n\n((100, 20), (100,))"
  },
  {
    "objectID": "documentation.html#gingado-benchmark",
    "href": "documentation.html#gingado-benchmark",
    "title": "Model documentation",
    "section": "gingado Benchmark",
    "text": "gingado Benchmark\n\nfrom gingado.benchmark import ClassificationBenchmark\n\n\n# the gingado benchmark\ngingado_clf = ClassificationBenchmark(verbose_grid=1).fit(X, y)\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\n\n\n\n# a new instance of ModelCard is created and used to document the model\nmodel_doc_gingado = ModelCard()\nmodel_doc_gingado.read_model(gingado_clf.benchmark)\nprint(model_doc_gingado.show_json()['model_details']['info'])\n\n# but given that gingado Benchmark objects already document the best model at every fit, we can check that they are equal:\nassert model_doc_gingado.show_json()['model_details']['info'] == gingado_clf.model_documentation.show_json()['model_details']['info']\n\n{'_estimator_type': 'classifier', 'best_estimator_': RandomForestClassifier(oob_score=True), 'best_index_': np.int64(0), 'best_params_': {'max_features': 'sqrt', 'n_estimators': 100}, 'best_score_': np.float64(0.9800000000000001), 'classes_': array([0, 1]), 'cv_results_': {'mean_fit_time': array([0.0758616 , 0.18149827, 0.07436633, 0.18540113, 0.09279556,\n       0.23015153]), 'std_fit_time': array([0.00501362, 0.00232287, 0.00205388, 0.00524391, 0.00214356,\n       0.00328967]), 'mean_score_time': array([0.00189848, 0.00470381, 0.00215042, 0.00440147, 0.00209916,\n       0.00445244]), 'std_score_time': array([0.00029969, 0.00039831, 0.00032119, 0.00049095, 0.00029719,\n       0.00047173]), 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'log2', 'log2', None, None],\n             mask=[False, False, False, False, False, False],\n       fill_value=np.str_('?'),\n            dtype=object), 'param_n_estimators': masked_array(data=[100, 250, 100, 250, 100, 250],\n             mask=[False, False, False, False, False, False],\n       fill_value=999999), 'params': [{'max_features': 'sqrt', 'n_estimators': 100}, {'max_features': 'sqrt', 'n_estimators': 250}, {'max_features': 'log2', 'n_estimators': 100}, {'max_features': 'log2', 'n_estimators': 250}, {'max_features': None, 'n_estimators': 100}, {'max_features': None, 'n_estimators': 250}], 'split0_test_score': array([1., 1., 1., 1., 1., 1.]), 'split1_test_score': array([1., 1., 1., 1., 1., 1.]), 'split2_test_score': array([1. , 1. , 1. , 1. , 0.9, 1. ]), 'split3_test_score': array([0.9, 0.8, 0.9, 0.9, 1. , 0.9]), 'split4_test_score': array([1. , 0.9, 1. , 0.9, 1. , 1. ]), 'split5_test_score': array([1. , 1. , 1. , 1. , 1. , 0.9]), 'split6_test_score': array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9]), 'split7_test_score': array([1., 1., 1., 1., 1., 1.]), 'split8_test_score': array([1. , 1. , 1. , 1. , 0.9, 1. ]), 'split9_test_score': array([1., 1., 1., 1., 1., 1.]), 'mean_test_score': array([0.98, 0.96, 0.98, 0.97, 0.97, 0.97]), 'std_test_score': array([0.04      , 0.0663325 , 0.04      , 0.04582576, 0.04582576,\n       0.04582576]), 'rank_test_score': array([1, 6, 1, 3, 3, 3], dtype=int32)}, 'multimetric_': False, 'n_features_in_': 20, 'n_splits_': 10, 'refit_time_': 0.07486987113952637, 'scorer_': &lt;class 'sklearn.ensemble._forest.RandomForestClassifier'&gt;.score}"
  },
  {
    "objectID": "documentation.html#scikit-learn",
    "href": "documentation.html#scikit-learn",
    "title": "Model documentation",
    "section": "scikit-learn",
    "text": "scikit-learn\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nsklearn_clf = RandomForestClassifier().fit(X, y)\n\n\nmodel_doc_sklearn = ModelCard()\nmodel_doc_sklearn.read_model(sklearn_clf)\nprint(model_doc_sklearn.show_json()['model_details']['info'])\n\n{'_estimator_type': 'classifier', 'classes_': array([0, 1]), 'estimator_': DecisionTreeClassifier(), 'estimators_': [DecisionTreeClassifier(max_features='sqrt', random_state=158589683), DecisionTreeClassifier(max_features='sqrt', random_state=1121224119), DecisionTreeClassifier(max_features='sqrt', random_state=1606839577), DecisionTreeClassifier(max_features='sqrt', random_state=1199551069), DecisionTreeClassifier(max_features='sqrt', random_state=835761063), DecisionTreeClassifier(max_features='sqrt', random_state=551036695), DecisionTreeClassifier(max_features='sqrt', random_state=480838901), DecisionTreeClassifier(max_features='sqrt', random_state=210015001), DecisionTreeClassifier(max_features='sqrt', random_state=1140110080), DecisionTreeClassifier(max_features='sqrt', random_state=463293203), DecisionTreeClassifier(max_features='sqrt', random_state=358800538), DecisionTreeClassifier(max_features='sqrt', random_state=1246567499), DecisionTreeClassifier(max_features='sqrt', random_state=606770221), DecisionTreeClassifier(max_features='sqrt', random_state=983251903), DecisionTreeClassifier(max_features='sqrt', random_state=1509122585), DecisionTreeClassifier(max_features='sqrt', random_state=1501143480), DecisionTreeClassifier(max_features='sqrt', random_state=879204444), DecisionTreeClassifier(max_features='sqrt', random_state=1321669283), DecisionTreeClassifier(max_features='sqrt', random_state=673246196), DecisionTreeClassifier(max_features='sqrt', random_state=1587488558), DecisionTreeClassifier(max_features='sqrt', random_state=1829036959), DecisionTreeClassifier(max_features='sqrt', random_state=1890132692), DecisionTreeClassifier(max_features='sqrt', random_state=812386331), DecisionTreeClassifier(max_features='sqrt', random_state=433554792), DecisionTreeClassifier(max_features='sqrt', random_state=1133782311), DecisionTreeClassifier(max_features='sqrt', random_state=1739908330), DecisionTreeClassifier(max_features='sqrt', random_state=1496062690), DecisionTreeClassifier(max_features='sqrt', random_state=112160128), DecisionTreeClassifier(max_features='sqrt', random_state=785554520), DecisionTreeClassifier(max_features='sqrt', random_state=1684130534), DecisionTreeClassifier(max_features='sqrt', random_state=784963937), DecisionTreeClassifier(max_features='sqrt', random_state=693718999), DecisionTreeClassifier(max_features='sqrt', random_state=650731226), DecisionTreeClassifier(max_features='sqrt', random_state=1336989461), DecisionTreeClassifier(max_features='sqrt', random_state=519161132), DecisionTreeClassifier(max_features='sqrt', random_state=1344627434), DecisionTreeClassifier(max_features='sqrt', random_state=877646592), DecisionTreeClassifier(max_features='sqrt', random_state=1296823208), DecisionTreeClassifier(max_features='sqrt', random_state=1255733197), DecisionTreeClassifier(max_features='sqrt', random_state=74713928), DecisionTreeClassifier(max_features='sqrt', random_state=979389987), DecisionTreeClassifier(max_features='sqrt', random_state=1045842057), DecisionTreeClassifier(max_features='sqrt', random_state=995024127), DecisionTreeClassifier(max_features='sqrt', random_state=568767557), DecisionTreeClassifier(max_features='sqrt', random_state=1339230260), DecisionTreeClassifier(max_features='sqrt', random_state=331632395), DecisionTreeClassifier(max_features='sqrt', random_state=69394979), DecisionTreeClassifier(max_features='sqrt', random_state=283351690), DecisionTreeClassifier(max_features='sqrt', random_state=1751579008), DecisionTreeClassifier(max_features='sqrt', random_state=1538998223), DecisionTreeClassifier(max_features='sqrt', random_state=338455217), DecisionTreeClassifier(max_features='sqrt', random_state=159425279), DecisionTreeClassifier(max_features='sqrt', random_state=1556727583), DecisionTreeClassifier(max_features='sqrt', random_state=2089589391), DecisionTreeClassifier(max_features='sqrt', random_state=807040305), DecisionTreeClassifier(max_features='sqrt', random_state=1161465658), DecisionTreeClassifier(max_features='sqrt', random_state=1592490432), DecisionTreeClassifier(max_features='sqrt', random_state=420251410), DecisionTreeClassifier(max_features='sqrt', random_state=1385745650), DecisionTreeClassifier(max_features='sqrt', random_state=1150615975), DecisionTreeClassifier(max_features='sqrt', random_state=1082502719), DecisionTreeClassifier(max_features='sqrt', random_state=909386985), DecisionTreeClassifier(max_features='sqrt', random_state=1070590481), DecisionTreeClassifier(max_features='sqrt', random_state=1508085041), DecisionTreeClassifier(max_features='sqrt', random_state=1527457544), DecisionTreeClassifier(max_features='sqrt', random_state=676163485), DecisionTreeClassifier(max_features='sqrt', random_state=249825268), DecisionTreeClassifier(max_features='sqrt', random_state=835520132), DecisionTreeClassifier(max_features='sqrt', random_state=354392376), DecisionTreeClassifier(max_features='sqrt', random_state=673996929), DecisionTreeClassifier(max_features='sqrt', random_state=1604391699), DecisionTreeClassifier(max_features='sqrt', random_state=1473956858), DecisionTreeClassifier(max_features='sqrt', random_state=342769397), DecisionTreeClassifier(max_features='sqrt', random_state=1415877102), DecisionTreeClassifier(max_features='sqrt', random_state=196268702), DecisionTreeClassifier(max_features='sqrt', random_state=1011750030), DecisionTreeClassifier(max_features='sqrt', random_state=1712420347), DecisionTreeClassifier(max_features='sqrt', random_state=1101808411), DecisionTreeClassifier(max_features='sqrt', random_state=841208608), DecisionTreeClassifier(max_features='sqrt', random_state=1023177493), DecisionTreeClassifier(max_features='sqrt', random_state=1181986780), DecisionTreeClassifier(max_features='sqrt', random_state=1591525177), DecisionTreeClassifier(max_features='sqrt', random_state=1990709224), DecisionTreeClassifier(max_features='sqrt', random_state=1971828731), DecisionTreeClassifier(max_features='sqrt', random_state=1916675552), DecisionTreeClassifier(max_features='sqrt', random_state=1404932948), DecisionTreeClassifier(max_features='sqrt', random_state=531126279), DecisionTreeClassifier(max_features='sqrt', random_state=1370892687), DecisionTreeClassifier(max_features='sqrt', random_state=1207344306), DecisionTreeClassifier(max_features='sqrt', random_state=1418587883), DecisionTreeClassifier(max_features='sqrt', random_state=2068437186), DecisionTreeClassifier(max_features='sqrt', random_state=444204463), DecisionTreeClassifier(max_features='sqrt', random_state=202687497), DecisionTreeClassifier(max_features='sqrt', random_state=1759753006), DecisionTreeClassifier(max_features='sqrt', random_state=1611915016), DecisionTreeClassifier(max_features='sqrt', random_state=585168231), DecisionTreeClassifier(max_features='sqrt', random_state=159559435), DecisionTreeClassifier(max_features='sqrt', random_state=1302822980), DecisionTreeClassifier(max_features='sqrt', random_state=1154647697), DecisionTreeClassifier(max_features='sqrt', random_state=1302050038)], 'estimators_samples_': [array([24, 58, 22, 78, 82, 80, 55, 21, 11, 16, 55, 15, 71, 78, 33, 70, 51,\n       11, 17, 32, 75, 66, 33, 69, 29, 53, 48, 84, 58, 97, 71, 24, 98, 56,\n       66, 54, 58, 14, 68, 66,  5, 87, 80, 26, 88, 51, 15, 65, 67, 53, 28,\n       39, 10, 61, 85, 52, 66, 60, 27, 83, 23, 93, 45, 78,  4, 32, 59, 18,\n       66, 92, 43,  0, 99,  4, 65, 37, 82, 68, 71, 61, 31,  9, 13, 24, 46,\n       42, 32, 57, 70, 36, 25, 68, 91, 82,  0, 21,  8, 67, 40, 26],\n      dtype=int32), array([29, 40,  0, 20, 92, 83, 69,  6, 33,  5,  3, 18, 84, 31,  0, 93, 61,\n       52, 29, 24, 72, 48, 91,  3, 43, 71, 52, 18, 91, 87, 23, 40, 25, 13,\n       44, 29, 83, 12, 10, 30, 83, 98, 81, 82, 82, 18, 53, 18, 40, 97, 72,\n        7, 81, 15, 42, 64, 15, 72, 90, 60, 93, 43,  4, 61, 99, 99, 80, 34,\n       73, 12, 49, 33, 82, 50, 12, 79, 55, 85, 27, 61, 10,  1,  2, 95, 85,\n       88, 94, 98, 88, 12, 99, 16,  5,  6, 72, 62, 12, 22, 39, 13],\n      dtype=int32), array([19, 91, 22, 59, 53, 37,  7,  4, 37, 32, 21, 23, 83, 94, 38, 71, 17,\n       96, 96, 64, 94, 85, 19, 74, 79, 57, 47, 90,  1, 39, 12, 80,  4, 53,\n       58, 19,  6, 12, 64, 49, 81, 66, 68, 93, 94, 36, 95, 44, 65, 22, 48,\n       60, 35, 65, 92, 94,  0, 31, 15, 16, 71,  3,  2, 25, 83, 47, 64, 36,\n        9, 45,  4, 78, 76, 83, 78, 53, 12, 47, 46, 95, 30, 45, 99, 63, 29,\n       73, 37, 66, 16, 92, 17, 92,  9, 37, 87, 88, 26, 94, 84, 98],\n      dtype=int32), array([22, 10,  9, 32, 81, 98, 20, 17, 88, 47, 80, 68, 82, 42, 38, 62, 13,\n       93, 61, 53, 99, 57, 74, 79,  5, 52, 79, 66, 51, 56, 62, 54, 93, 14,\n       75, 36,  0,  1, 71, 52, 15, 71, 62, 32, 93,  5, 27, 85, 34, 90, 24,\n       31, 95, 44, 45, 88, 79, 98, 66, 59,  5, 14, 57, 52, 98, 71, 48, 43,\n       99, 73,  3, 72, 68,  8, 94, 85, 59, 50, 10, 60, 89, 69, 81, 70, 71,\n       60, 77, 87, 62, 81,  9, 14, 54, 96, 37,  0, 41, 81,  6, 10],\n      dtype=int32), array([55, 32, 21, 88, 60, 19, 74, 29, 20, 50, 43,  0, 83, 87, 63, 45, 12,\n       82, 34, 43,  1, 23, 37, 88, 65, 74, 32, 62, 90, 43, 17, 27, 51, 91,\n       41, 29, 22,  1, 29, 58, 48,  7,  3, 81,  2, 73, 41, 28, 40, 12, 66,\n       88, 53, 50, 23, 83, 60, 30, 15, 27, 61, 45, 15, 46,  0, 21, 70, 57,\n       56,  6,  6, 24,  9, 55, 50,  7, 19, 75,  2, 11, 53, 40, 38, 51, 65,\n       60, 28,  1, 19, 32, 38, 86, 73,  1,  0, 25,  3, 73, 43, 79],\n      dtype=int32), array([ 0, 93, 77, 55, 28, 34, 89, 31, 59, 39,  7, 91, 72,  3, 15, 11, 54,\n       13, 99, 35,  3, 77, 33, 18, 25, 39, 41, 94,  0, 18, 69, 20, 24, 71,\n       95, 86, 42, 16, 42, 66, 80, 13, 37, 90,  5, 80, 41,  1, 45,  6, 43,\n       54, 91, 16, 27, 11, 40, 73, 88, 55, 89, 33, 21, 14, 81, 67, 79,  3,\n       87, 97, 88, 27, 39, 46, 76, 96, 31, 13, 46, 94, 26, 16, 12, 31, 36,\n       58, 10, 50, 64, 23, 57, 48, 92, 79, 92, 92, 43, 97, 50, 70],\n      dtype=int32), array([79, 29, 90, 43, 77,  0, 13, 42, 96, 77, 19, 84, 26, 94, 94, 50, 86,\n       26,  6, 22, 90, 47, 60, 13, 21, 35, 42, 16,  3, 19, 87, 68, 98, 17,\n       72, 52, 84, 23, 28, 72, 51,  7,  5,  3, 98, 19, 90, 84, 70, 58, 21,\n       51, 93, 60, 38, 49, 53, 37, 90, 91, 15, 40, 16, 24, 31, 17, 84, 16,\n        6, 28,  1, 22, 16, 71, 92, 16, 14, 81, 72, 93,  1, 19, 23, 99, 54,\n       89, 65, 85, 96, 79,  9, 28, 74, 26, 34, 40,  0, 64, 37, 18],\n      dtype=int32), array([41, 21, 95, 63,  3, 50, 28, 70, 68, 77, 57, 12, 62, 12, 58, 45,  8,\n       33, 44,  4, 43, 56, 12,  6, 98,  4,  7, 76,  6, 75, 93, 45,  5, 79,\n        0, 85,  5, 29,  6, 15, 24, 57, 23, 86, 13, 83, 78, 24, 69, 52, 30,\n       64, 57, 23,  7, 52, 49, 16, 37, 62,  7, 86, 21, 34, 64, 20,  4, 23,\n       95, 32, 74, 83, 62, 24, 76, 56, 85, 53, 97, 36, 89, 75, 41, 20, 53,\n       67, 62, 94,  1, 23, 71, 62, 65, 11, 20, 49,  3, 40, 59, 21],\n      dtype=int32), array([14, 30, 96, 74, 25, 40, 87,  3, 52, 19, 30, 30, 62, 43, 24, 38, 78,\n        6, 37, 87, 91, 63, 69, 34, 13, 54, 32, 82, 57, 18, 91, 23, 42, 97,\n       58, 80, 14, 22, 88, 93, 30, 66, 55, 63, 62, 92,  7, 14, 23, 75,  1,\n       23, 46, 81, 54, 51, 67, 54, 45, 18, 18, 18, 19, 29, 22, 69,  7, 25,\n       70, 58, 45, 77, 75, 18, 46, 26, 86,  1, 92, 10, 24, 38, 73, 94, 53,\n       84, 40,  8, 78, 58, 71, 77, 70, 26, 13, 83, 91, 11, 40, 84],\n      dtype=int32), array([49, 77, 16, 87, 98,  4, 95, 41, 79, 11, 84, 77, 66,  4, 34,  6, 44,\n       32, 85, 50, 21, 68, 95, 85, 10, 43, 46, 66, 67,  6, 60, 38,  1, 14,\n       29,  3, 84, 67, 59, 96, 36, 95,  1, 61, 69, 77, 57, 66, 94, 31, 92,\n       77, 62, 99, 99, 40, 37, 42, 31, 11, 27, 54, 35, 36, 84, 42, 77, 62,\n       75, 43, 70, 79, 27, 20, 49, 54, 38, 48, 48, 84, 74, 73, 79, 29, 15,\n       49, 49, 42,  6, 41, 45, 81,  8, 19, 95, 12, 93, 93, 34, 91],\n      dtype=int32), array([21, 90, 96,  4, 32, 68, 40, 26, 39, 92, 39, 78, 77, 43, 78, 87, 68,\n       78, 77, 74, 41, 37, 16, 41, 88, 95, 35, 82, 59, 49, 92, 63, 76, 99,\n       33, 15, 87, 21, 31,  0, 48, 82, 20, 24, 22, 19, 27, 55, 39, 82, 21,\n       28, 13, 30, 31, 42, 62, 59, 93, 97, 31, 36, 89, 96, 26, 63,  8, 30,\n        4, 86, 79, 50, 39, 52, 10, 68, 49, 43,  6, 74, 57, 40, 40, 30, 36,\n       92, 44, 23,  3, 18, 67,  6, 48, 64, 32,  2, 98, 11, 64, 43],\n      dtype=int32), array([33, 37, 22, 80, 12,  9, 50, 63, 29, 71,  2, 10, 87, 56, 40,  1, 81,\n       10, 67, 18, 86, 60,  1, 30, 63, 29, 82, 97, 20, 21, 47, 31, 60, 47,\n       18, 26, 36,  5, 30, 15, 48, 89, 88, 90, 41, 23, 16, 47, 15, 14, 39,\n       12, 85, 66, 52, 50, 35,  7, 93, 40, 87, 34, 15, 73, 80,  3, 11, 26,\n       44, 50,  0, 86, 91, 71, 58, 99,  2, 61, 53, 91, 56, 80, 70, 20, 80,\n       22, 89, 50, 72, 15, 24, 17, 61,  6, 86, 64, 62, 61, 83, 98],\n      dtype=int32), array([32, 45, 86, 90, 69, 81, 55, 60, 54, 61, 88, 55, 35, 61, 41,  6, 76,\n        9, 91, 38,  9, 92, 43, 60, 54, 96, 32, 17, 95, 22, 69, 10, 65, 74,\n       56, 71,  6, 44, 81, 43, 34, 83, 21, 12, 55, 56, 65,  3, 73, 38, 94,\n       60, 49, 40, 69, 98, 61, 82, 83, 72, 96, 63, 83, 60, 21, 84,  2, 43,\n       53, 16, 45,  9, 71, 79, 87, 89,  3, 92, 31, 65, 51, 75, 78, 51, 29,\n       32, 59,  3, 84, 35, 41, 27, 21, 60, 95, 16, 35, 73, 54, 75],\n      dtype=int32), array([55, 73, 59, 46, 79, 68,  9, 70, 78,  8, 37, 84, 48, 64, 80,  4,  4,\n        4, 55, 83, 57, 78, 29, 78, 77, 61, 98, 74, 67,  3, 76, 14, 19, 21,\n       73, 66, 43, 42, 77, 12, 18,  9, 55, 76, 75, 68, 62, 21, 11, 92, 76,\n       43, 94, 74, 92, 20, 27, 89, 24, 13, 80, 75, 58,  6, 14, 77, 35, 57,\n       90, 37, 38, 28, 12, 95, 91, 57, 59, 61, 28, 56,  4, 63, 84, 34, 29,\n       49, 14, 42, 45, 12, 80, 82, 59, 62, 92, 72, 21, 79, 97, 66],\n      dtype=int32), array([16, 66, 41, 28, 61, 14, 56, 63, 20, 14, 81, 63, 89, 10, 69, 18, 38,\n        2, 77,  2, 30, 38, 36, 20, 39, 27, 23, 68,  6, 47, 94, 96, 61, 67,\n       40, 26, 69, 58, 89, 79, 25, 29, 98,  5, 52,  2, 71, 99, 11, 19, 69,\n       21, 99, 15, 82, 37, 49, 63, 64, 28, 38, 67,  9, 75, 44, 48, 46, 71,\n       53, 28, 38, 99, 58, 19, 74, 98, 70, 16, 77,  2, 81, 34, 21, 78, 62,\n        8, 64, 84, 42, 22, 12, 71, 29, 78, 82, 13, 66, 51, 52,  9],\n      dtype=int32), array([43,  2, 65, 28, 47, 54, 97,  9, 99, 32, 31, 12, 59, 83, 59, 86, 63,\n       74, 87, 71, 95, 30,  7, 14, 39, 40, 67, 50, 12, 58, 74, 20, 42,  0,\n       27, 87, 84, 39, 18, 59, 12, 44, 21, 38, 87, 81, 82, 16, 44, 95,  0,\n       31, 84, 21, 96, 67, 76, 50, 79, 67, 67, 68, 90, 23, 24,  0, 50, 76,\n       48, 95,  8, 69, 25, 71, 95, 58, 83,  3,  0, 72, 43, 57, 11, 66, 30,\n       57, 42, 85, 18, 19, 94, 97, 75, 71, 15, 39, 72, 93, 63, 45],\n      dtype=int32), array([29,  0, 77, 40, 74, 52, 21, 73, 73, 87, 54, 10,  8, 60, 15, 31, 99,\n       79, 41, 50, 30, 64, 92, 73, 22, 26,  4, 60, 81, 13, 54, 10,  5, 96,\n       59, 56, 81, 95, 58, 88, 17, 73, 56, 10, 87, 41,  9, 12,  8, 55, 97,\n       14, 51, 36, 39, 28, 88, 50, 44, 77, 59, 56, 17, 93, 45, 54, 26, 63,\n       79, 68, 85, 79, 28, 53, 54, 56, 86, 19, 20, 85, 84, 81,  7, 90, 16,\n       32, 65, 80, 45, 81, 86, 99, 76, 83, 32, 55, 86, 52, 49, 31],\n      dtype=int32), array([11, 82, 41,  2, 48, 83, 76, 61, 21, 31, 65, 12, 18, 74, 82, 80, 48,\n       34, 36, 89, 26, 74, 34, 36, 22,  6, 13, 22, 85, 51, 56, 78, 29, 55,\n       63, 66,  1, 29, 13, 71, 39, 50, 37, 78, 68, 18, 35, 63, 36, 45,  6,\n       38, 94, 20, 91, 16, 47, 83, 86, 97, 10, 98, 98, 32, 53, 59, 18, 98,\n       42, 55, 58, 76, 78, 77, 65, 63, 61, 49, 81,  8, 52, 17, 28, 58, 11,\n        1, 21, 35, 95, 55, 67, 25,  6,  8, 45, 96, 15, 50, 68, 50],\n      dtype=int32), array([17,  1, 97, 19, 16, 77,  9, 15, 70, 84, 27, 33, 37, 76,  9, 98,  7,\n       12, 31, 79, 21, 42, 68, 90, 58, 98,  9, 47, 30, 97, 60, 84, 79, 10,\n       10, 19, 39, 96, 91, 28, 50, 39, 50, 75, 46, 84, 17, 93, 95, 48, 19,\n        1, 38, 96, 67,  6, 83, 57, 73, 51,  9, 28, 36, 31, 16, 38, 58,  7,\n       15, 25, 25, 98, 97, 76, 35, 85, 30, 54, 68, 57, 40, 28, 88, 88, 49,\n       48, 74, 98, 49, 81, 57, 48, 95, 45, 53, 75, 19, 30, 72, 43],\n      dtype=int32), array([70, 69, 37, 82, 77, 73,  0, 20, 16, 62, 89, 26, 69, 22, 65, 15, 93,\n       71, 62, 13, 58, 21, 81, 20, 23, 56, 32, 67, 71, 29, 58, 32, 20, 25,\n       42, 66, 40,  9, 16, 86, 47, 16, 92, 88, 82, 68, 40, 79, 52, 33, 74,\n       36, 93, 77, 44,  7, 81, 77, 61, 26, 48, 86, 76, 59, 26,  5, 77, 39,\n       80, 24,  1, 69, 36, 42, 27, 37, 72, 80,  9, 41, 60, 20, 69, 99, 74,\n       87,  5, 50, 86, 71, 74, 41, 24,  7, 88, 37, 32, 27, 97, 34],\n      dtype=int32), array([71, 29, 69, 84, 26, 21, 58, 96, 34, 93, 26, 61,  8, 33, 58, 25, 60,\n       12, 63, 16, 57, 25, 33, 68,  0, 31,  8, 47, 14, 92,  2, 63, 37, 57,\n       13,  3, 38, 77,  5,  2, 49, 17, 57, 27, 32, 80, 40, 11, 54, 21, 86,\n       18, 99, 84, 30, 39, 58, 62, 31,  9, 63,  5, 68, 41, 86, 32, 18, 59,\n       74, 81, 82, 49, 11, 59, 75, 65, 79, 15, 71, 55, 88, 37, 21, 34, 30,\n       11, 13, 17, 24,  7, 97, 82, 53, 52, 55, 30, 85, 65, 40, 54],\n      dtype=int32), array([12, 45, 64, 97, 34, 56, 21, 47, 45, 28, 59, 86, 44, 31, 19, 61, 42,\n       89, 34,  1, 67, 86, 75, 93, 39, 58, 55, 36, 55, 96, 90,  7, 55, 98,\n       26,  7, 44, 83, 59, 81, 27, 81, 71, 55, 47, 20, 33, 89, 99, 93, 43,\n       81, 26, 76,  4, 50, 14, 15,  0, 39, 43, 24, 93, 41, 49, 61, 14, 40,\n       12, 27,  3, 70, 75, 14, 29,  6, 22, 57, 19, 98, 98, 90, 30, 55, 94,\n       87, 78, 72, 34, 86, 27, 61, 85, 51, 65, 50, 92,  6, 66, 73],\n      dtype=int32), array([ 6,  4, 62, 65, 88, 51, 31, 98, 29, 29, 51, 99, 34, 80, 98,  7, 50,\n       83, 62, 97, 36, 55, 85, 67, 80, 56, 63, 53, 16, 42, 50,  4,  8, 40,\n        3, 28, 91, 33, 67, 95, 73, 89, 74,  7, 67, 80, 15, 66, 66, 20,  6,\n        1, 40, 46,  7, 77, 25,  0, 92,  0, 80, 67,  8, 53, 96, 25, 10, 39,\n       32, 61, 45, 48, 12, 91, 66, 31, 12,  3, 99, 98, 69, 25, 36, 78, 88,\n       66, 22, 62, 55, 27,  1, 10, 96, 24, 93, 24, 34, 58, 27, 62],\n      dtype=int32), array([52, 39, 87, 80, 65,  5, 47, 64, 20, 75, 63, 98, 64, 43, 93, 51, 31,\n       30, 12, 84, 30, 75, 17,  3, 91, 71, 85,  7, 29, 92, 13, 95, 76,  2,\n       90, 31,  9, 61, 93,  0, 40,  6, 56,  8, 35, 95, 22, 17, 59, 72, 52,\n       70, 27, 94, 55, 81,  9, 55,  5, 61, 37, 18,  9, 32, 52, 34, 68, 94,\n       64, 58, 94, 42, 36, 79, 88, 51, 16, 52, 11, 20, 97, 97, 96, 61,  4,\n       79, 56,  6, 50, 78, 57, 45,  7, 25, 79, 34, 62, 63, 14, 37],\n      dtype=int32), array([26, 48, 28, 85, 46, 75, 93, 85,  4, 34, 44,  9, 55, 53, 18, 64,  1,\n       48, 61, 62, 29, 54, 95,  0, 75, 35, 18, 83, 67, 44, 38, 69, 52, 78,\n       11, 65, 83, 60, 64, 65, 21, 66, 89, 43, 23,  3, 49, 39, 46, 29, 83,\n       23, 86, 79, 59, 41, 30, 78, 97, 81, 40, 67, 29, 83, 90, 15, 41, 96,\n       75, 35, 67, 29, 43, 11, 11, 44, 54, 67, 90, 16,  9, 42, 13, 93, 87,\n       76, 82, 12, 91, 37, 60,  0, 86, 66,  4,  2, 33, 41, 57, 35],\n      dtype=int32), array([22,  9, 99, 75, 63, 68, 75, 64, 52, 10, 75, 83, 56, 34, 84, 38, 85,\n       30, 21, 56, 86,  3, 33,  8, 44, 75, 24, 70, 81, 98, 64, 25,  2, 49,\n       43, 14, 64, 13, 62, 56, 47,  1,  4, 28, 36, 49, 77, 52, 49, 93, 22,\n       17, 90, 59, 15, 54, 51,  6, 38, 11, 93, 88, 83, 64, 25, 15, 44,  9,\n       64, 64, 46,  6, 42, 18,  9, 85, 48, 78, 20,  8, 72, 70, 96, 36, 34,\n       64, 53, 43, 82, 62, 46, 14, 64, 24, 90, 51, 36, 20, 85, 27],\n      dtype=int32), array([63, 19, 30, 71,  8,  2,  0, 14, 22, 72,  2, 24,  7, 38, 40, 83, 60,\n       67, 88, 28,  2, 81, 70, 78, 46, 17, 56, 20, 49, 88, 46,  7, 16, 73,\n       74, 66, 76, 78, 18, 16, 31, 86, 98, 18, 12, 60, 31, 92, 24, 85, 50,\n       15, 39, 97, 97,  5, 26, 52, 58, 20, 10, 66, 72, 87, 70, 71, 26, 96,\n       16, 24, 90, 37, 11, 98, 35, 86, 29, 68, 84,  8, 70, 56, 31, 15, 36,\n       43, 21, 62, 21, 94, 43, 54,  3, 33, 68, 99, 77, 52, 56, 38],\n      dtype=int32), array([50, 36, 29, 58, 13, 43, 17, 58, 85, 65,  5, 91, 92, 37, 51, 41, 40,\n       38, 90, 52, 83, 31, 32,  0, 22, 88, 53, 37, 41, 53, 63, 12, 66, 57,\n       64, 16, 97, 90, 87, 28, 87, 24,  9, 40, 28, 63,  2, 56, 27, 69, 74,\n       91, 98, 45,  4, 78, 83, 32, 70, 47, 36, 20,  7, 48, 51, 37, 55, 23,\n       77, 84, 38, 95, 12, 76, 26, 98, 44, 92, 43, 43, 85,  7, 93,  6, 10,\n        1, 64, 39,  8,  4, 69, 18, 17, 76, 85, 99,  9, 63, 94, 44],\n      dtype=int32), array([57, 97, 32, 88, 56, 90,  7, 19, 50, 21,  7,  1, 77, 31, 49, 99, 17,\n       11, 24, 37, 73, 89,  9, 57, 81, 50, 12, 77, 52, 85,  3, 20, 88,  3,\n       63,  6, 25, 68, 64, 50, 41, 28, 13, 50, 18, 29, 40, 53, 82, 75, 79,\n       36, 58, 17, 11,  9, 23, 94, 16, 77, 10, 66,  5, 32, 25, 15, 15, 97,\n        5, 89, 64,  2, 58, 76, 39,  0, 44, 94, 51, 17, 50, 88, 31,  5, 20,\n       10, 21, 76, 20, 70, 21, 56, 45,  0, 92, 82, 81, 48, 91, 61],\n      dtype=int32), array([17, 84, 73, 92, 15, 22, 90, 72, 23, 36, 57, 79, 65, 17, 58, 11, 30,\n       37, 88, 84, 24, 37,  1, 16, 20,  5, 57, 45, 86,  9, 85, 57, 29, 47,\n       56, 20, 58,  1, 26, 91, 56, 49, 67, 68, 71,  7, 10, 77, 52, 75, 52,\n       71, 81, 30,  9, 69, 67, 53, 19, 70, 92, 16, 76, 24, 83, 30, 28, 73,\n       36, 11, 23, 38, 32, 63, 63, 43, 63, 11, 31, 69, 86, 48, 13, 37, 33,\n       76, 27, 96, 46, 71, 93, 53, 43, 79, 15,  0, 78, 79,  6, 16],\n      dtype=int32), array([41, 32, 40, 79, 53, 74, 34, 85,  0, 45, 94, 27, 57, 51, 84, 38, 11,\n       13, 27, 23, 31, 36, 24, 88, 20, 37, 19, 62, 95, 94, 11, 49, 48, 73,\n       21, 13, 11, 22, 85, 90, 12, 53, 42, 71, 61, 71, 42, 88, 87, 31, 91,\n        6, 21, 17, 20, 99, 83, 77, 55, 19, 62, 28, 16, 69, 16, 42, 73, 57,\n       79, 31, 63, 59, 93, 84, 67,  5,  5, 15, 23,  8, 29, 47, 57, 72, 13,\n       85, 17, 45, 37, 99, 54, 63, 97, 33, 95, 69, 42, 54,  5, 50],\n      dtype=int32), array([85, 50, 40, 68, 72, 33, 45, 35, 64, 97, 89, 92, 59, 25, 11,  7, 65,\n       53, 14, 64, 65, 88, 96, 22, 90, 82, 30, 69, 62, 13,  9, 89, 93, 26,\n       89, 38, 13,  9, 16, 73, 75, 63, 17, 58, 51, 82, 74, 55, 99, 13, 82,\n        7, 25, 16, 66, 11, 18, 36,  0, 83, 82, 10, 67, 17, 98, 17, 29, 65,\n       93, 75, 82, 51, 13, 33,  9, 57, 36, 40, 72, 49, 21, 43, 30, 63, 49,\n       89, 64, 24, 72, 94, 93, 45, 73, 92, 43, 21, 49,  2, 89, 34],\n      dtype=int32), array([85, 66, 85, 19, 29, 87, 92,  7, 37,  1, 20, 84, 80, 96, 86, 24, 37,\n       47, 13, 39, 57, 11, 39, 76, 68, 60, 20, 81, 54, 92, 17, 92, 30, 99,\n       61, 48, 83, 34, 10,  8,  2, 64, 91, 95, 42, 85,  7,  0, 59, 95, 46,\n       73, 51, 39, 32, 97, 49,  6, 22, 43, 96, 92, 74, 39, 84, 66,  6, 15,\n       95, 30, 36, 96, 42, 20, 78,  5, 43, 37, 26, 66, 70, 92, 44, 37, 69,\n       63, 64,  6, 54, 34,  9, 18, 69, 20,  2, 38,  4, 27, 54, 82],\n      dtype=int32), array([87, 45, 79, 50, 77, 44, 83, 61, 95, 89, 39, 78, 62, 65, 37, 36, 35,\n       99, 34, 21, 99, 60, 62,  4, 78, 20, 76, 50, 92, 78, 31, 53, 85,  1,\n       34, 24, 50, 31, 73, 30, 67, 45, 38, 51, 69, 58, 98, 95, 96, 83, 38,\n        2, 24, 69, 49,  2, 73, 21, 55, 67, 47,  9, 49, 67, 32, 36, 51, 44,\n        9, 57, 44, 91, 67, 66, 34, 28, 10, 67, 66, 95, 55, 16, 94, 66, 12,\n        8, 58, 30, 91, 16, 59, 67, 20, 64,  8, 95, 18, 74, 84, 28],\n      dtype=int32), array([11, 43, 95, 15, 98, 65, 90, 48,  7, 50, 95, 75, 94, 27, 27, 35, 28,\n       59, 41, 37, 25, 64, 73, 64, 20, 20, 44, 38, 37, 96, 16, 75, 72, 67,\n       86, 50, 66, 52,  4, 28, 74, 84, 56, 78, 59, 25, 72, 67, 36, 68, 59,\n       90, 57, 61, 98, 80, 92, 15, 35, 72, 95, 23, 71, 95, 92, 59, 28, 45,\n        1, 21, 70, 38, 79, 46, 37,  2, 30, 43,  4, 87, 36, 98, 36, 84, 65,\n       11, 66, 40, 26, 44, 92, 60, 78, 20, 52, 22, 78, 82, 58, 36],\n      dtype=int32), array([10,  9, 81, 70, 37, 22, 34, 46, 87, 66, 13, 86, 14, 53, 24, 22, 93,\n       18, 59, 27, 54, 10, 79, 98, 44, 45, 29, 75, 26,  2, 75, 25, 84, 44,\n       69, 23,  4,  8, 91, 74, 57, 56, 36, 50, 89, 58, 79, 50, 99, 60, 94,\n       17, 82, 11, 16, 54, 96, 35, 65, 28, 46, 39, 26, 79, 71, 77, 46, 94,\n        3,  6,  7, 45, 34, 28, 29, 28,  2, 15, 18, 57, 66,  1, 41, 20, 42,\n       93, 61,  6, 23, 13, 14,  8, 90, 29, 48, 43, 46, 81, 39, 60],\n      dtype=int32), array([10, 43, 31, 24, 40,  7, 11, 74, 24,  4, 97, 11, 45, 86, 92, 50, 35,\n       42, 30, 23, 43, 40, 54, 54, 28, 24, 47, 19, 18, 38, 58, 28, 91, 58,\n       95, 71, 16, 12, 69,  3, 13, 47, 86, 82, 93, 92, 17, 20, 87, 75, 66,\n       95, 84, 23, 63, 64, 24, 47, 68, 95, 77, 13, 86,  0, 22, 87, 60, 83,\n       15, 75, 34,  1,  9, 18, 88, 95, 43, 69, 41, 12, 63,  3, 16, 41, 67,\n       39,  8, 32, 16, 88, 36, 26, 85, 23, 30, 12, 83, 97, 23, 70],\n      dtype=int32), array([37, 82, 31, 65, 99, 46, 19, 12, 59, 67, 35, 55, 63, 63, 44, 11, 62,\n       35, 25, 67, 73, 19, 53, 94, 13, 54, 25, 14, 75, 18, 35, 40, 71, 48,\n        2, 95, 12, 33,  0, 80, 29, 27, 49, 58, 66, 70, 59, 96, 15, 32, 71,\n       67, 73, 41, 58, 44, 50, 39, 68, 62, 17, 52, 27, 31, 21, 67, 80, 43,\n       78, 19, 69, 96,  4, 53, 66, 28, 21, 98, 48, 98, 32, 95, 66, 11, 41,\n       74, 69,  8,  3, 44, 11,  5, 23, 17, 66, 77, 38, 76, 96, 17],\n      dtype=int32), array([47, 78,  6, 43, 48, 83,  0, 19,  4, 92, 43, 67, 61, 23, 58, 71, 33,\n       71, 96, 75, 77,  0, 44, 54, 74, 74, 29, 63, 91, 73,  3, 25, 36, 76,\n       26, 25, 38, 13, 16, 56,  7, 61, 30,  2, 74, 68, 21, 83, 14, 14, 36,\n        7, 31, 62, 51, 26,  4, 29, 69, 15, 50,  3, 17,  5, 55, 85, 91, 45,\n        9, 91, 98, 22, 74,  1, 89, 73, 88,  0, 91,  6, 23,  3, 93, 32, 97,\n       96, 26, 45, 96, 56, 23, 67, 36, 75, 40, 83,  7, 45, 65, 60],\n      dtype=int32), array([ 4, 42, 43, 84, 10, 66, 80, 10, 39, 78,  7, 50, 12, 12, 60, 86, 25,\n       59, 24, 86, 33,  1, 83, 14, 94, 58, 78, 96, 77, 39, 55, 97, 77, 74,\n       50, 78, 41, 24, 52, 33,  5, 14, 85, 21, 34, 72, 58, 17, 80, 65, 97,\n       52, 43, 95, 71, 84, 99, 16, 60, 43, 18, 91, 74, 74, 57, 24, 56, 23,\n       40, 55, 27, 24, 25,  0, 57,  5, 69,  5, 58, 62, 18, 50, 71, 68, 77,\n       62, 97, 32, 38, 27, 25,  8, 49, 14, 98, 68, 24, 10, 24, 56],\n      dtype=int32), array([63,  8, 78, 96,  1, 82, 70, 24, 50, 77, 78, 46, 65, 58, 58, 28,  5,\n       17, 61, 36, 24, 39, 11, 70, 51, 88, 85,  4,  1, 19, 82, 24, 47, 60,\n       26, 85,  3, 21, 88, 63,  9, 24, 95, 88, 16, 45, 88, 50, 73, 38, 53,\n       18, 89, 10,  6, 52, 28,  0,  0, 87, 34, 86, 34, 63, 66, 61, 81, 53,\n       89, 16, 46, 15, 83, 52, 28, 47, 70, 61, 13, 47, 83,  5, 56, 72, 24,\n       69, 33, 26, 39, 86, 72, 91, 43, 51, 60, 68, 75, 23,  9, 89],\n      dtype=int32), array([71, 40, 79, 98, 31, 81, 99, 86, 39, 38, 56, 53, 36, 95, 70, 47, 98,\n       44, 95, 78, 66, 75,  7, 23, 49, 35,  5, 59, 20, 40, 57, 74, 57, 23,\n       50, 95, 76, 38, 73, 77, 72,  9, 98, 24,  2, 68, 54, 10, 81, 80, 44,\n       61, 30, 44, 48,  9, 32, 30, 32, 32, 65, 13, 65, 92,  8,  8, 83,  2,\n       61, 81, 72, 21, 92, 11, 26, 24, 48, 36, 38, 52, 44,  0, 53, 19, 23,\n        5, 49, 30, 93, 90,  8, 53, 70, 32, 84, 76, 91, 40, 55, 69],\n      dtype=int32), array([41, 66, 45, 61,  2, 18, 59, 97, 94, 78, 70, 42,  4, 79, 25, 70, 94,\n       61, 18, 70, 92, 66, 87,  2, 26, 95, 44, 51, 80, 44, 23, 83, 77,  8,\n       23, 77, 37, 68, 34,  9, 11, 56, 66, 10, 69, 94, 88, 37, 74,  2,  7,\n       16, 34, 15, 23, 88, 38, 86, 68, 28, 36, 61, 73, 41, 63, 52, 51,  4,\n       50, 73, 30, 55, 24, 17, 87, 15, 26, 69, 63, 11, 58, 58, 35, 62, 82,\n       72, 29, 90, 24, 22,  9, 59, 30, 71, 78, 81, 74, 17, 81, 97],\n      dtype=int32), array([13, 60, 60, 54, 26, 91, 19, 95, 18, 69, 25, 34,  6,  2, 52, 13, 83,\n       85, 62, 19, 87, 36, 54,  4, 10, 66, 33, 18, 56, 42, 15, 60, 59, 63,\n       94, 95, 83, 37, 30, 36,  5, 70, 63, 13, 67, 11, 19,  0, 35, 92, 70,\n       35, 73, 95, 68, 21, 14, 92,  1, 31,  2, 20, 49, 99, 38, 67, 28, 38,\n       48, 86, 31,  5, 72, 58, 35, 72, 68, 70, 90, 42, 21, 84, 61, 28,  2,\n       69, 25, 45, 54, 22, 72,  4, 94, 26, 52, 43, 52, 63, 79, 85],\n      dtype=int32), array([81, 11, 53, 42, 30, 65, 15, 96, 82, 79, 60, 75, 83,  9, 32, 32, 30,\n       56, 11, 35, 42, 61, 37, 35, 84, 55, 93, 28, 30, 38, 17, 44,  8,  0,\n       18, 53, 74, 23, 75, 82, 43, 37, 24, 33, 48, 21, 66, 14, 13, 85, 84,\n       72, 88,  0, 93, 93, 41, 94, 55,  4, 19, 38, 85, 35, 89, 77, 86, 16,\n       43, 89, 41, 52, 67, 90, 88, 20, 76, 96, 79,  5, 54, 66, 18, 13,  7,\n       52, 89, 67, 75, 24, 40, 79, 98, 31, 69, 63,  7, 18, 95, 51],\n      dtype=int32), array([73, 35, 25, 52, 65, 79, 61, 25, 73, 76, 98, 98, 86, 35, 28,  0, 76,\n        2, 39,  8, 42,  6, 89, 40, 45, 94, 76, 80, 19,  0, 40, 77, 93, 34,\n       95, 24, 38, 69, 64, 49, 85, 88, 23, 11,  1, 91, 38,  9, 57, 67, 80,\n       39,  4, 18, 68, 46, 64, 96, 74, 88,  5, 69, 77, 93, 74, 33, 37, 43,\n       81, 75, 43, 97, 32, 49, 23, 66, 90, 49, 33,  1, 12, 75, 63, 65, 58,\n       18, 56, 17, 68, 94, 93, 26, 28, 45, 89, 76, 51, 50, 39, 67],\n      dtype=int32), array([90, 33, 70, 45, 37, 83, 96,  3, 44, 75, 96, 68, 99, 43, 80,  0, 24,\n       76,  3, 15, 51, 67,  4, 17, 41, 13, 59,  2, 62, 39, 46, 26, 28, 22,\n       58, 17, 83, 96, 32, 30, 27, 64, 52, 93, 26, 10, 95, 22, 38, 84, 54,\n       61, 78, 24, 47, 70, 26, 83, 17, 22, 66, 83, 32, 55,  3, 93, 70, 74,\n       43, 44,  0, 44,  6, 69, 98, 47,  3, 84, 35,  1, 80, 45, 21, 21, 91,\n       17,  2, 52, 45, 92, 76, 25, 60, 66, 33, 81, 45, 36, 88, 66],\n      dtype=int32), array([34, 97, 77, 50, 38, 38, 61, 27, 86,  2, 90, 51, 21, 27, 37, 46, 46,\n       17, 90, 59, 25, 55, 38,  2, 57,  3, 26, 80, 53, 70, 14, 19, 29, 44,\n       52, 84, 98, 46, 86, 88, 55, 63, 74, 24,  2,  9, 23, 27, 83, 23, 69,\n       70, 92, 34, 23, 93, 22, 59, 73, 94, 36, 46, 20, 93, 94, 33, 12, 81,\n       99, 22, 99, 43, 57, 93,  5, 65, 27, 90, 70, 67, 53,  1, 27, 28, 91,\n       22, 55,  3, 17, 55,  3, 85, 28, 91, 92, 29,  7, 54, 64, 31],\n      dtype=int32), array([74, 45, 57, 48, 50, 10,  4, 48, 20,  0, 89,  8, 19, 68, 63, 50, 10,\n       17, 49, 35, 10, 81,  1, 62, 71, 14, 27,  4, 30, 73, 81, 48, 20, 90,\n       79, 96, 50, 35, 20, 68, 50, 50, 51, 68, 23,  5, 57, 75, 66, 14, 13,\n       68, 81, 55,  2,  8, 64, 12, 77, 78, 94, 96, 33, 80,  4, 15, 54, 99,\n        5, 90,  4, 28, 49, 69, 13, 65, 26, 55, 48, 10, 57, 43, 97, 26, 33,\n       75, 48, 72, 19, 97, 14, 35,  2,  3, 43, 83, 90, 37, 41, 89],\n      dtype=int32), array([54, 21, 26, 12, 84, 59, 67, 69,  6, 70, 28, 72, 58, 55, 87, 16, 98,\n       64, 66, 18, 43, 92, 98, 51, 75, 30, 54,  3, 10, 15, 60, 76, 81, 53,\n       47, 14, 26, 34,  2, 77, 59, 35, 77, 55, 47, 44, 93, 68, 68, 83, 49,\n       30, 37, 31, 60, 38, 81, 48, 77, 60,  4, 56, 49, 61, 44, 20, 18,  2,\n       30, 96, 81, 97, 89, 93, 38, 63, 93, 35, 90, 74, 14, 60, 30, 26, 94,\n       50, 85, 18, 81, 12, 51, 35, 97, 44, 50, 31, 14, 17, 45, 98],\n      dtype=int32), array([14, 89, 77, 54, 33, 23, 98, 97, 29, 46, 22, 39, 22, 99, 79, 59, 85,\n       69, 29, 74, 62, 36, 48, 21, 92, 63, 96, 70, 94, 94, 57, 14, 61, 65,\n       67, 94, 63, 41,  2, 99, 86, 74, 36, 78, 73, 47, 83, 80, 24, 29, 87,\n       93, 37, 94, 19,  9, 34, 18, 59, 85, 97, 61, 53, 19, 48, 78, 19, 36,\n       35, 26, 85, 65, 70, 33,  6, 54, 87, 81, 87, 96, 56, 91, 15, 22, 74,\n       47, 34, 93, 46, 80, 99, 14, 89, 63, 16, 88, 36,  8,  7,  1],\n      dtype=int32), array([21, 97, 87, 16, 11, 85, 80, 78, 73, 91,  2, 93, 30, 34, 55, 42, 28,\n       59, 93, 98, 60, 38, 12, 29, 51, 36, 96, 60, 24, 77, 80, 92, 86, 50,\n       39, 88, 68, 42, 65, 15, 31, 21, 63, 64, 87, 26, 76, 34, 94, 54, 29,\n       73, 40, 21, 72, 59, 88, 10, 46, 86, 20, 93, 25, 61, 77, 17, 55, 80,\n       28, 88, 10, 53, 96, 80, 59, 78, 40,  8, 32,  4, 29, 76,  4, 96,  0,\n       35, 67, 28, 76, 40, 52, 49,  2, 61, 47, 16, 20, 69, 26, 43],\n      dtype=int32), array([44,  2, 44, 64, 39,  9, 24, 35, 25, 73, 96, 11,  7, 49, 94,  1, 62,\n       39, 99,  3, 81, 65, 36, 59, 23, 16, 93, 78, 67, 70, 81, 72, 53, 52,\n       27, 64, 82, 73, 91, 19, 67,  6, 21, 66, 48, 99, 59, 60, 17, 70, 47,\n       66, 58, 74, 97, 95, 78, 45, 94, 20, 94, 48, 82, 66,  1, 91, 81, 54,\n       83, 54, 63, 63,  8, 56, 28, 63, 75, 34, 77, 50,  9, 26,  2, 63,  3,\n       33, 47, 16, 29, 37,  1, 22, 76, 74, 44, 91, 19, 32, 27, 81],\n      dtype=int32), array([48, 61, 21, 35, 86, 73, 41, 24, 73, 22, 84, 74, 96, 36,  1, 23,  9,\n       93, 47, 98, 56, 67,  2, 51, 51, 38, 46,  8, 60, 35, 18,  6, 18, 14,\n       74, 30, 63, 86, 78, 86, 65, 95, 59, 20, 44, 25, 16, 14, 81, 41, 44,\n       13, 55, 55, 74, 63, 34, 51, 62, 67, 85, 14, 68, 42, 97,  0, 74, 86,\n       73, 39, 30, 79, 10, 36,  9, 84, 67, 79, 53, 80,  3, 93, 79, 97,  3,\n       80, 87, 98, 99, 56, 14, 77, 18, 61, 36, 55, 54, 76, 64, 45],\n      dtype=int32), array([56, 85, 19, 27, 95, 70, 15, 12,  5,  8, 40,  8,  8, 20, 98, 35, 18,\n       99, 95, 28, 23,  8, 56, 70, 34,  8, 42, 68, 24, 27, 53, 12, 96, 90,\n       97, 69, 70, 86, 98,  7, 26, 46, 67, 70, 53, 65, 86, 20, 75, 39, 31,\n       20, 86, 31, 90, 37, 28, 77, 40, 14, 11, 45, 44, 30, 99, 54, 89, 74,\n       56, 70, 69, 80, 18, 82, 31, 97, 24, 87, 44, 82, 56, 17, 94, 68, 57,\n       20, 87, 47, 50, 82, 15, 75, 84, 17, 60, 46, 71, 30, 18, 24],\n      dtype=int32), array([49,  7, 42, 87, 24,  1, 95,  8, 97, 65, 21,  6, 82, 99, 28,  8, 69,\n       51, 27, 84, 26, 47, 20, 27, 15, 41, 95, 56, 81, 67, 83, 65, 38, 36,\n       64, 62, 19, 49, 78, 77, 83, 19, 91, 77, 66, 86, 87, 58, 24, 24, 42,\n       89, 49, 32,  3, 20, 81,  4, 61, 85, 40, 84, 79, 41, 22,  4, 55, 99,\n       69, 75, 78, 34, 60, 40, 90, 40, 41, 21, 10, 72, 67, 75, 40, 33, 37,\n       40, 94, 30, 14, 12, 62, 71, 61, 87,  5, 53, 53, 83, 21, 89],\n      dtype=int32), array([22, 12, 94, 96, 18, 55, 51, 40, 99,  0, 45,  1, 32, 87,  5, 76, 12,\n       11, 33, 42, 63, 96, 83, 53, 73, 96, 64, 62, 20, 35, 87, 19, 50, 58,\n       14, 79, 97, 90, 78, 34, 58, 68, 44, 17, 94, 41, 37, 16, 82, 45, 88,\n       23, 70, 35, 94, 11, 69, 25,  5, 13, 87, 91, 63, 60, 10, 37, 98, 74,\n       86, 89, 39, 54, 74, 70, 63, 81, 80, 59, 89,  5, 10, 33, 29, 75, 12,\n       78, 42, 52, 98, 59, 29, 40, 80, 49, 39, 89,  9, 15, 41, 73],\n      dtype=int32), array([13, 92, 26, 17, 39, 87, 88, 58, 45, 10, 97,  5, 68, 55, 19, 12, 73,\n       47, 59, 92, 30,  9, 36, 98, 34, 33, 16, 20, 69, 22, 80, 10, 54, 82,\n       66, 86, 64, 26, 75, 57, 85, 70, 69, 38, 63, 55, 74, 45, 67,  4, 27,\n       53, 35, 49, 20, 22, 94, 15, 68, 18, 15, 20, 79, 69, 11, 97, 90,  6,\n       55, 82, 60, 82, 61, 31,  2, 47, 10, 16, 97,  3,  9, 53, 15, 15, 81,\n       23, 32, 69, 96, 92, 81, 62,  4, 64, 64, 83, 32,  5, 32, 73],\n      dtype=int32), array([25, 68, 38, 49, 53, 67, 26, 98, 41,  0, 52, 20, 98, 25, 75, 33, 44,\n       80, 79, 41, 78, 46, 99, 29, 65, 43, 48, 18, 30, 98, 45,  1, 85, 69,\n       18, 63, 22,  8, 14, 13, 64, 88, 77, 65, 53, 18, 45, 26, 22, 54, 23,\n       80, 65, 95, 29, 33, 51, 13, 17, 59, 94, 74, 53, 40,  5,  0, 68, 26,\n       79, 77, 83, 15, 76, 10, 41, 25, 58, 63,  4, 59, 30, 23, 77,  7, 47,\n       37, 64, 60, 28, 45, 86, 60,  2, 98, 40, 72, 63, 79, 87, 84],\n      dtype=int32), array([83, 80, 83, 31, 42, 43, 20, 18,  6, 90, 35, 27, 93, 62, 11, 79, 34,\n       78, 66, 61, 28, 66, 89, 87, 79, 70, 11,  4, 90, 32, 81, 11, 23, 85,\n       48, 42, 21, 53, 77, 75, 92, 44,  8, 68, 63, 33, 50,  4, 83, 34, 41,\n       55, 35, 13, 86, 90, 76, 53,  6, 83, 14, 70, 64, 70, 14, 89,  3, 92,\n       40, 71, 68, 68, 23, 22, 63, 18, 53, 84, 85, 13,  1, 52, 83, 51, 16,\n       86, 49, 34, 17,  5, 22, 42, 32, 79,  6, 43, 16, 95, 52, 95],\n      dtype=int32), array([43, 33, 79,  7, 17, 94,  1, 37, 72, 52, 69, 40, 98,  8, 90, 42, 88,\n        3, 83,  0, 28, 70, 10, 91, 22, 88, 80, 40, 90, 86, 14, 96, 32, 25,\n       91, 26, 55, 91, 26, 98, 58,  1, 75, 88, 31, 92, 90, 82,  9, 41, 81,\n       96, 76, 26, 16, 44, 56, 35, 43, 79, 41, 70, 85, 82, 43, 34, 88, 79,\n       74,  7,  5, 30, 79, 70, 63, 14, 53, 65, 43, 10, 77, 76, 81, 35, 75,\n       18, 17, 31, 89, 89, 59, 57, 29,  6, 38, 80, 98, 18, 53,  5],\n      dtype=int32), array([58, 16, 87, 17, 93, 86, 21, 66, 96, 54, 65, 30, 37, 55, 17, 54, 28,\n       79, 73, 92, 21, 13, 32, 37, 43, 91, 88, 28, 66, 24, 29, 75, 99, 20,\n       38, 35, 12,  1, 59, 11, 71, 44, 27, 88,  4, 65, 63, 40, 16, 92,  3,\n       36, 90, 37, 90, 96, 69, 23, 97, 25, 52, 64, 37, 15, 78,  1, 39, 32,\n       95, 20,  8, 13, 39, 13, 52, 13, 74, 99, 68, 93, 41, 52, 89, 92, 28,\n       20, 76,  1, 66, 28, 67, 23, 15, 38, 76, 70, 49, 62, 62, 62],\n      dtype=int32), array([78, 22, 12, 96, 96, 43,  3, 75, 96, 94, 53, 69, 83, 19, 93, 61, 20,\n       97, 59, 53, 66,  6,  3,  8, 74, 99,  8, 16, 35, 70,  9, 43, 62, 19,\n       62, 60, 77, 42, 58,  5, 33,  4, 64, 42, 20, 68,  6, 27, 31, 19, 16,\n       96, 51, 21, 18,  9, 27, 83, 83, 89, 76, 32, 36, 62, 22,  3, 66, 11,\n       12, 91, 35, 36, 37, 41, 87, 99, 50, 66, 70, 36, 82, 10, 36, 95, 23,\n       50, 12, 39, 87, 57, 71, 15, 71, 57, 15, 24, 99, 13, 56,  6],\n      dtype=int32), array([61, 26, 29, 95, 40, 70,  4, 26, 54, 24, 44, 57, 11, 41,  4, 57, 79,\n       62, 75, 49, 28,  6, 44, 32, 25, 70, 77, 75,  2,  5, 54, 88, 12, 95,\n       14, 69, 71,  2, 14, 64, 66,  1, 71, 38, 86, 37, 17,  7, 50, 16, 83,\n       34, 61, 84, 96, 54,  6,  4, 52, 54, 48, 71,  2, 13, 38, 73,  0, 94,\n       89, 51, 93, 66, 35, 34, 12, 71, 11, 93, 80, 31, 21, 74, 67, 47, 96,\n       77, 70, 62, 66, 88, 12, 29, 88, 61, 71, 29, 10, 58, 30, 47],\n      dtype=int32), array([98, 65,  0, 15, 49, 97, 88, 26, 14, 66, 98,  1, 85, 98, 69, 90, 44,\n       73, 45, 32, 51, 65, 31, 96,  6, 97, 78, 27, 25, 17, 48, 10, 12, 87,\n       86, 66,  8, 28, 79, 58, 57, 43, 30,  1, 35, 72, 73, 83, 28, 23, 43,\n       87, 69, 64, 28, 53, 18, 32, 40, 80, 57, 33, 28, 70, 12, 29, 91, 10,\n       47, 89, 39, 22, 76, 59, 69, 39, 49,  5, 34, 52, 19, 36, 91, 74, 41,\n       31, 30, 55, 94, 75, 43, 81, 17, 20, 94,  9, 87, 34, 92, 42],\n      dtype=int32), array([56, 48, 61, 28, 66, 42, 48, 87,  8, 22, 43, 56, 56, 22, 42, 34, 40,\n       58, 44,  0, 46, 97, 65, 99, 93, 12, 80, 89, 38, 99, 48, 23, 75, 81,\n       54, 96, 99, 95, 93, 46, 79, 88, 13, 25,  2, 63,  4, 34, 99, 89,  8,\n       44,  6, 64, 34, 96,  9, 40,  7, 60, 34, 44, 88, 97, 51,  7, 20, 46,\n        1, 93, 79, 78, 38, 20, 84, 84, 31, 40, 94, 14, 31, 93, 24, 58, 92,\n       59, 11, 18, 12, 92, 17,  4, 67,  1, 98,  7,  4, 86, 93, 60],\n      dtype=int32), array([77, 84, 52,  9, 81, 80, 53, 51, 60, 92, 39, 98, 82, 10,  0, 10, 90,\n       94, 14, 81, 25, 95, 99, 22, 77, 91, 43, 92, 30, 38, 11, 85, 46, 67,\n       13, 87, 22, 16, 66, 21,  8, 78, 46, 46, 12, 16,  0, 51,  1, 60,  6,\n        5, 77, 54, 24, 67, 63, 34, 83,  5, 53, 96, 47, 93, 52, 21, 37, 44,\n       72, 25, 61, 69, 78, 21, 20, 27, 83, 18, 82, 65, 21,  4, 38, 75,  4,\n       74, 62, 36, 80, 96, 49, 21, 52, 63, 48,  6,  2, 26, 99, 47],\n      dtype=int32), array([79, 54,  9, 41, 94, 58, 37, 14, 47, 79, 32,  8, 55, 28, 54, 67, 87,\n       21,  7, 33,  1, 83, 87, 40, 17, 47, 81, 99,  4, 93, 66, 57, 17, 46,\n       30, 43, 35, 55,  0,  1, 37, 66, 57, 43, 98, 93, 30, 29, 29, 14,  8,\n       60, 75, 64, 68,  6, 28, 49, 43, 61, 73, 27, 17,  3, 57, 75, 13, 23,\n       65,  6, 88,  1, 72, 13, 98, 61, 88, 17, 85,  8, 73,  9, 98, 71, 66,\n        2,  5, 58, 17, 67, 37, 86, 41, 72, 18, 21, 57, 59, 40, 34],\n      dtype=int32), array([18,  7, 80, 93, 14, 55,  8, 64, 61, 50, 32, 39, 66, 17, 31, 53, 32,\n       18, 24, 71, 55, 71,  9, 81, 23, 41, 27, 63, 41, 88, 24, 15, 59, 95,\n       44, 86,  6, 50, 36,  4, 83, 13, 53, 61, 38, 68, 97,  7, 54, 22, 19,\n       88, 23, 47,  0, 50, 20, 40, 59, 68, 99, 88, 98,  0, 88, 55, 86,  4,\n       79, 21, 29,  0, 52, 14, 19, 21, 91, 31, 69, 76, 59, 12, 86, 96, 74,\n       98, 77, 39, 99, 72, 18, 86, 41, 58,  0, 51, 53, 74, 45, 26],\n      dtype=int32), array([80, 40, 84, 78,  7, 14, 67, 64, 15, 98, 37, 18, 48, 45, 99,  2, 26,\n       27, 66, 31, 93,  6, 38, 45, 38, 46, 60, 13, 19, 33, 49, 26, 35, 44,\n        3, 22, 40, 60, 37, 43, 81, 30, 97,  1, 47, 11, 99, 10, 12, 84,  8,\n       24, 41, 76, 49, 94, 89, 96, 19, 86, 49, 80, 38, 38,  1, 85, 66, 13,\n       49, 12, 13,  7, 20, 47,  2, 60, 11, 36, 67, 91, 44, 57, 56, 24, 12,\n        7,  7, 62, 27, 21, 64, 90, 33, 24, 24, 48, 61, 46, 16, 28],\n      dtype=int32), array([88, 42, 85, 51, 93, 94, 24, 59, 18, 45, 75, 10, 95, 21, 59, 70, 31,\n       26, 73, 10, 89, 58, 77, 40, 11, 32, 32, 81, 45, 44, 78, 58, 76, 44,\n       24, 51, 36, 66, 59, 95, 43, 73,  4, 30, 20, 32, 97, 78, 11, 76, 31,\n       41, 24, 77, 33,  1, 82,  2,  6,  8, 95, 72, 16, 13, 40, 21,  6, 47,\n       56, 27, 39, 17, 38, 32, 81, 69, 44, 93, 96, 79, 78, 58, 62, 42, 79,\n       30, 46, 31,  2, 75,  5, 32, 32, 51, 66, 70, 46, 58, 95,  3],\n      dtype=int32), array([92, 89, 43, 27, 33, 29, 45, 22, 65, 15, 82, 13, 33, 79, 89, 39, 92,\n       20, 90, 62,  2, 69, 23, 21, 65, 23, 68, 51, 53, 32, 56, 29, 34, 63,\n       35, 11,  8, 86, 46, 82, 70, 10, 48, 68, 91, 10, 27, 59, 97, 56, 23,\n       24, 18, 46, 26, 80, 32, 46,  2, 60, 47, 47, 59, 14,  9, 97, 14, 35,\n       29, 72, 81, 96, 43, 55, 44,  4, 68,  1, 14, 81, 89, 31, 81, 70, 62,\n       20, 24, 10, 29, 31, 41, 66, 26, 39, 50, 94,  0, 55, 18, 44],\n      dtype=int32), array([61, 61, 90, 61, 87, 30, 99, 14, 21, 52,  8, 82, 39, 42, 52, 49, 20,\n       17, 71, 84, 78, 98, 76, 52, 90,  8, 95, 18,  8, 58, 91, 12, 54, 81,\n       33, 10, 75, 37, 29, 39, 72, 15, 24, 58, 87,  0, 59, 26, 67, 79, 50,\n        0, 44, 91, 84, 92, 44, 95, 72, 46, 70, 71, 22, 85, 14, 43, 13, 89,\n       11, 54, 68, 18, 49, 29, 77, 90, 75, 66, 57, 84, 42,  2, 64, 35, 70,\n       22, 45,  8, 59, 76, 89,  2, 14, 51, 24,  1, 71, 47, 67, 76],\n      dtype=int32), array([23, 53, 16, 28, 32, 81, 75,  9, 80, 88, 85,  2, 74, 64, 57, 36, 41,\n        1, 33, 25, 16, 46,  8, 74, 40, 56, 91, 24, 52, 69, 95, 90,  0, 58,\n       37,  2, 11, 36, 18, 58,  4, 30, 60,  0, 16, 11, 49, 32, 15, 69,  8,\n       60, 43, 29, 34,  2, 15, 80, 57, 42, 96, 98, 74, 70, 57, 46,  2, 72,\n        1, 33, 65, 86, 93, 29, 47, 38,  5, 18, 29, 19, 17, 23,  8, 15, 48,\n       71, 56, 99, 35, 50, 94, 67, 87, 49, 31, 15, 77,  0,  0, 23],\n      dtype=int32), array([78, 68,  8, 50, 25, 14, 13, 85,  2, 25, 63, 45,  7, 74, 22, 44, 43,\n       38, 98, 86, 99, 31,  8, 95, 30, 88, 75, 67, 37, 96, 89, 84,  6, 29,\n       66, 39, 10, 87, 74, 62, 47,  4,  7, 69, 23, 34, 49,  8, 71, 76, 82,\n       75,  7, 48,  2, 96, 98, 73, 99,  2, 49, 28, 27, 74, 56, 13, 47, 43,\n        3, 99, 56, 46, 76, 12, 61, 98, 36, 62, 87, 47, 39, 76, 51, 19, 83,\n       67, 27, 48, 43,  3, 32, 95, 55, 54, 77, 78, 90, 92, 22, 98],\n      dtype=int32), array([75, 51, 83, 71, 61, 49,  4, 28, 39, 32, 42,  9, 97, 56, 30, 31, 89,\n        1, 89, 17, 61, 58, 34, 37, 57,  2, 13, 55, 83, 86, 61, 15, 13, 73,\n       77, 72, 50,  8, 41, 51, 29, 17, 30, 53, 91, 52, 70, 64, 18, 91, 69,\n       60, 56,  1, 22, 61, 67, 26, 26, 58, 62, 77, 32, 46, 60,  9,  7, 68,\n       37, 40, 81, 39, 35, 79, 65, 56, 76,  4, 39, 13, 12, 39, 23, 43,  9,\n       74, 33, 39, 41, 79, 23, 79, 71, 42, 95,  3, 66, 81, 43,  8],\n      dtype=int32), array([27, 17, 43,  0, 66, 33, 71,  2, 87,  7, 70, 12, 81, 79, 17,  2, 89,\n       66, 45, 75, 20, 20, 95, 35, 34, 99, 37, 21, 18, 38, 55, 31, 57, 67,\n       77, 48, 50, 33, 30, 89, 25,  9, 55, 53, 90, 10, 84, 29, 45, 19,  0,\n       42, 85, 11, 75, 89, 49, 49, 52, 82, 44, 13, 90, 97, 42, 97, 85, 28,\n       61, 77, 34, 99, 52, 54, 85, 82, 46, 60, 56, 58, 84, 53, 87, 26, 43,\n       61, 13, 74, 21, 33, 59, 20, 98, 30, 52, 51, 56, 13, 57, 80],\n      dtype=int32), array([15,  6, 13, 88, 33, 49, 28, 12, 21,  6, 74, 83, 56,  0, 17, 15, 27,\n       92, 36, 76, 94, 92, 25, 28, 82, 56, 36, 23, 92, 75, 41, 17, 84, 84,\n       20, 59, 42, 66, 84, 85, 63, 48,  1,  0,  1, 77, 77, 22, 97,  5,  6,\n       25, 53, 55, 18, 24, 77, 16, 16, 75,  7, 51, 63, 84, 70, 71, 99, 62,\n       41,  3, 79, 71,  9,  8, 97, 66, 90, 92, 70, 14, 14, 53, 63,  1, 20,\n       63, 42, 76, 12, 21, 25, 45, 76, 40, 91, 78, 58, 33, 31, 40],\n      dtype=int32), array([10,  3, 15, 50, 38, 13, 10, 51, 26, 65, 29, 79, 24, 96, 95, 10, 87,\n       24, 82, 82, 24, 86, 29,  4, 12, 34, 41,  2, 64, 39, 10, 97, 85, 25,\n        5, 16,  3, 39, 56, 84, 36,  4, 53, 57, 85, 54, 28, 40, 30, 57, 71,\n       38, 81, 51, 34, 51,  8, 76, 90, 69, 22, 90, 62, 14, 16, 56, 69, 67,\n       91, 63, 19, 82, 41, 57, 31, 47, 79, 11, 11, 35, 17, 13,  5, 97, 95,\n       76,  9, 28, 44, 11, 83, 85, 17, 52, 88, 27, 73, 77, 58, 38],\n      dtype=int32), array([ 2, 49, 16, 57, 55, 59, 98, 57, 17, 22, 65, 73, 64, 43, 79, 28,  1,\n       75, 42, 29, 76, 58, 71, 30, 85, 57, 51, 53, 82, 18, 80, 90, 36, 24,\n       99, 26, 69, 75, 26, 24, 56, 93, 33, 92, 37, 91, 22, 51, 60, 30, 88,\n       94, 85, 35, 99, 80, 82, 52, 40, 39, 93, 32, 62, 16, 71, 30, 26, 19,\n       92, 45, 41,  0, 83, 17, 49, 10, 93, 75, 60, 87, 54, 28,  9, 44, 58,\n        0, 16, 27, 87, 30, 23, 11, 54, 96, 25, 76, 35, 77, 99, 95],\n      dtype=int32), array([ 0, 75,  8, 90, 86, 69, 80, 32, 65, 39, 99, 40, 78, 94, 22, 29, 95,\n       51, 35, 12, 98, 82, 15, 92, 95, 66, 64, 82, 48, 64, 79, 55, 25, 26,\n       92, 21, 10, 59, 79, 67, 38, 48, 19,  7, 85, 13, 77, 90, 76, 67, 90,\n       67, 75, 20, 90, 76, 81, 39, 45,  7, 62, 88, 14, 70, 75, 99, 70, 17,\n       14, 32, 90, 93, 37, 72, 69, 27, 94, 47, 66, 10, 61, 37, 49, 25,  3,\n       86, 26, 82, 42, 11, 88, 47, 83, 25, 21, 24, 84, 90, 82,  5],\n      dtype=int32), array([96, 44, 82, 90,  2, 38, 62, 50, 47, 88, 52, 98,  7, 45, 98, 96, 28,\n       68, 32, 89, 95, 81, 36, 38, 10, 17, 86, 62, 46, 56,  3, 90, 33, 33,\n       81, 13, 88, 15, 84, 63, 94,  2, 60, 46, 15, 26, 29, 33, 15, 96, 32,\n       49, 26, 92, 34, 79, 27, 69, 96, 98, 22, 45, 95, 31, 57, 90, 52, 50,\n       39, 82, 73, 36, 93, 69, 78, 70, 47, 58,  4, 15, 75, 54, 46, 80, 99,\n        3, 78, 97,  4, 47, 70,  6,  3, 78, 70, 67, 69, 32, 77, 32],\n      dtype=int32), array([68, 32, 67, 94, 49,  4, 72, 68, 13, 52, 19, 80,  7, 88, 67, 55, 72,\n       64, 88, 38, 12, 18, 17, 50,  7, 39, 57, 16, 98, 57, 77,  9, 70, 91,\n       64, 85, 29, 78, 33, 24, 18, 31, 12, 46, 26, 79, 55, 98, 73, 32, 18,\n       65, 59, 48, 31, 97, 49, 40, 88, 82, 83,  6, 34, 11, 83, 85, 56, 84,\n       53, 53, 60, 21, 20, 79, 92,  8, 35, 91, 86, 41, 92, 16, 44, 97, 56,\n       48,  9, 20, 30, 12, 18, 23, 28, 91, 42,  8, 59, 98, 51, 70],\n      dtype=int32), array([13, 93, 94, 86, 75, 57, 32, 59, 51,  2, 17, 86, 60, 59,  5,  3, 73,\n       53, 46, 76,  6, 94, 23, 41, 53, 11, 53, 53, 76, 96, 51, 65, 96, 53,\n       12, 97, 81, 59, 14, 35, 51, 94, 20, 62, 11, 69, 73, 47, 47,  7, 30,\n       71, 78, 59, 34, 77, 19, 56, 69, 73, 88, 47, 72, 95, 83, 54, 64, 30,\n       45,  1, 70, 32, 49, 64, 30, 91,  0, 82, 52, 12, 90, 12, 58, 48, 42,\n       51, 37, 27, 37, 12, 52, 52, 48, 29, 94, 73, 30, 19, 93, 56],\n      dtype=int32), array([12, 72, 92, 93, 48, 32,  6, 14,  2, 51, 75, 83,  8,  7, 11, 26, 57,\n       47, 38, 57, 83,  2, 51, 17, 14,  4, 15, 82, 56, 74, 36, 91, 51, 33,\n       32, 25, 38, 61,  3, 77, 77, 42, 43, 11,  8, 74, 22, 28, 34, 47, 58,\n       66, 90, 67, 29, 77, 74, 84, 51, 80,  2, 67, 25, 11, 97, 54, 47, 55,\n       69, 50,  6, 68, 53, 93, 77, 56, 39, 74, 97, 56, 97, 24, 23, 12, 68,\n       93, 19, 87, 95, 20, 70, 46, 67, 60, 30,  1, 17, 71, 30, 99],\n      dtype=int32), array([37, 96, 58, 38, 99, 76, 14, 98, 32, 96, 76, 13, 29,  5, 41, 93, 33,\n       83, 67, 62, 41, 27,  8, 65, 16, 34, 45, 66, 67, 64, 58, 17,  7, 39,\n       95,  0, 15, 50, 26, 98, 26, 19, 76, 92, 26, 85, 53, 31, 83,  0, 18,\n       88, 80, 65,  7, 82, 60,  5, 95, 95, 11,  4,  7, 51, 40, 23, 58, 83,\n       81, 59, 23, 61, 85, 70, 70, 72, 48, 60, 98, 62, 83, 32, 49, 50, 80,\n       32, 50, 46, 25, 67, 94, 60, 14, 40, 96, 13, 50, 60, 44, 68],\n      dtype=int32), array([12, 19, 75, 30,  5, 51, 60, 82, 47, 22, 33, 90, 51, 92,  4, 40, 38,\n       89, 63, 92, 86, 21, 83,  5, 95, 17, 91,  1, 62, 11, 15, 76, 95, 72,\n        7, 66, 27, 61, 49,  3, 53, 24, 22, 12, 36, 53, 40,  7, 48, 56, 33,\n       47, 22, 82, 67, 46, 97, 31,  3, 85, 75, 18, 15, 49, 57, 34, 37, 67,\n       45, 11, 39, 48, 64, 96, 30, 96, 71, 30, 91, 68, 39, 13, 66, 33, 83,\n       52, 75, 19, 58, 78, 56, 38, 24, 46, 32, 39, 65, 78, 23, 79],\n      dtype=int32), array([35, 87, 51, 42, 76, 37, 71, 91, 87,  6, 85, 84, 81, 50, 49, 28, 32,\n       73, 43, 99, 88, 66, 46, 90, 19, 70, 46, 77, 40, 96, 83, 61, 90, 22,\n       21, 38,  1,  3,  2, 67, 35, 50, 81, 83, 91, 49, 92, 89, 78, 54, 33,\n       92, 76, 66,  7,  4, 59, 62, 21, 98, 35, 68, 85, 23,  1, 57, 13, 34,\n       91, 36, 28, 64, 98, 91, 26, 27,  3, 13, 64, 29, 86,  1, 91,  4, 80,\n       14, 14, 99, 48,  5, 64, 40, 43, 91, 34, 51, 60, 26, 90, 34],\n      dtype=int32), array([62,  9, 52, 40, 35, 16, 11, 92, 91, 45, 15, 26,  8, 85, 43, 37, 52,\n       58, 81, 47, 77, 61, 12, 19, 17, 45, 14, 48, 60, 57, 75, 75,  9, 80,\n       96, 66, 69, 64, 73, 54, 18, 67, 16, 98, 68, 94, 19, 38, 58, 21,  0,\n       33, 37, 66, 27, 51, 99, 94, 24, 77, 34, 96, 66, 98, 27, 54, 58, 48,\n       36, 25, 44, 33, 88, 99,  1, 72, 95, 80, 99,  5,  9, 16, 78, 76, 93,\n       79, 33, 99, 67, 39, 78, 69,  9, 35, 28, 47, 65, 12, 96, 27],\n      dtype=int32), array([81, 70, 46, 52, 41, 27, 20, 30, 70, 62, 29, 14, 94, 98, 46, 98, 17,\n       98, 52,  9, 35, 14, 34, 88, 70, 58, 66, 42, 13, 33, 22, 66, 91, 67,\n       30, 82, 38, 41, 48, 22, 67, 17, 57, 28, 10, 57, 15, 18,  6, 18, 25,\n       30, 82, 33, 74, 25, 74, 76, 91, 18, 14,  8, 20, 66,  6, 58, 51, 55,\n       51,  0, 98, 94, 46, 50, 97, 50, 78, 52, 19, 43, 85, 69,  2,  5, 58,\n       67,  3, 52,  4, 14, 94, 24, 49, 92, 74, 49, 55,  3, 73, 59],\n      dtype=int32), array([83, 45, 66, 78, 81, 39, 86, 20, 12, 65, 91,  0, 64, 42, 92, 37, 25,\n       62, 98, 17, 21, 77,  2, 14,  5, 55, 56, 24, 67, 11, 94, 39, 17,  5,\n       25, 82, 60, 27, 11, 69, 82, 96, 58, 91, 11, 66, 48, 78, 67, 74, 43,\n       58, 90, 60, 52, 77,  4, 85, 23, 59, 46, 86, 59, 56,  5, 33, 81,  1,\n       60, 97, 82, 26,  5, 36, 37, 11, 89, 40, 74, 37, 67, 88, 79, 46, 11,\n       29, 78, 45, 14, 66, 59, 46, 76,  7, 89, 36,  1, 66, 62, 31],\n      dtype=int32), array([92, 10, 43, 17, 89, 45, 28, 52, 11, 85, 75, 91, 73, 89, 40, 12, 84,\n       21, 88,  7, 72, 33, 88,  3, 16, 86,  3, 61, 58, 37, 95,  7, 46, 35,\n       20, 30, 27, 82, 87,  1, 45, 68, 18, 69, 93, 37, 13, 29, 63,  9,  4,\n       73, 64, 88, 23, 75, 56, 84, 61, 42, 25, 83, 73, 46, 64,  0, 80, 22,\n       63, 53, 91, 18, 31, 64, 62, 59, 47, 80, 98, 30, 21, 76, 92, 14,  5,\n       90, 53, 97,  4, 69, 15, 90, 65,  5,  4, 41, 52,  0, 37, 37],\n      dtype=int32), array([67, 15, 61, 11, 56, 42, 53, 47, 38,  4, 36, 34, 44,  9, 56, 80, 17,\n       67, 67, 17, 62, 43, 32, 91, 10,  1, 33, 25, 89, 18, 81, 66, 45, 51,\n       51, 75, 35,  7, 11, 79,  2, 91, 90, 24, 79, 58, 95, 16, 95, 76, 72,\n       82,  6, 68, 69, 68, 69,  7, 40, 91,  9, 35, 69, 40, 60,  0, 11, 53,\n       56, 18,  8, 59,  7, 46, 85, 49, 46, 50, 93, 33, 95, 75, 23, 81, 78,\n       27,  7, 91, 22, 75, 22, 10, 14,  2, 18, 62, 32, 10, 94, 31],\n      dtype=int32), array([86, 63,  6, 85, 11, 33, 89, 35,  4, 55, 61, 80, 72, 82,  6, 92, 48,\n       61, 64, 22, 14, 69, 53, 75, 26, 89, 75, 96, 64, 93, 88, 93, 85, 73,\n       56, 60, 75, 45, 28, 10, 70, 59, 92, 16, 66,  0, 46, 83, 63, 46, 95,\n       89, 26, 17, 88, 59,  6, 49, 67, 37, 60, 45, 48, 19, 81, 19, 35, 24,\n       34, 92, 42, 88, 96, 31, 91, 58, 11, 89,  4, 13, 44, 82, 82, 97, 84,\n       39, 45, 33, 59, 13, 31, 14, 13, 14, 39, 65, 90,  1,  0, 69],\n      dtype=int32), array([20, 61, 16, 43, 84,  3,  0,  4, 17, 26, 43, 13, 31, 88, 51, 78, 90,\n       54, 79, 61, 91, 90, 40,  5, 99, 43,  2, 66,  4, 70, 93, 70, 67, 50,\n       61, 52, 87,  0, 87, 31, 63,  8,  1,  8, 17, 57, 34, 21, 11,  7, 18,\n       90, 96, 65, 56,  3, 10, 31, 54, 52, 14, 20, 38, 56,  4, 63, 53,  7,\n       89, 60, 43, 20, 12, 71, 75, 81, 73, 20, 15, 26, 55, 92,  1,  0, 63,\n       43, 13,  9, 59, 37, 60, 77,  1, 78, 55, 45, 22, 92, 76, 79],\n      dtype=int32), array([76, 99,  1, 16, 88, 84, 94, 20, 43, 90, 34, 35, 33, 21, 29, 76, 62,\n       77, 45, 46, 65, 97, 49, 21, 46, 24,  9, 12, 71, 84, 76, 46, 83,  7,\n       36, 78, 83, 89, 67, 65,  5, 60,  5,  1, 82, 95, 56, 49, 78, 90, 23,\n       96,  8, 43, 12, 20, 14, 44, 42, 13, 96, 76, 43,  0, 51, 72, 73, 28,\n       18,  5, 50, 45, 93, 72, 66, 14, 66, 39, 25, 55, 97, 78, 16, 24, 70,\n       38,  7, 69, 23, 82, 78,  0, 56, 60, 84,  7, 47, 91, 69, 70],\n      dtype=int32), array([93, 58, 63, 91, 19, 52,  0,  5, 53,  5, 82, 69,  1, 76, 79, 68,  7,\n       76,  4, 88, 19, 86,  3, 33, 41, 62, 42, 26, 75, 40, 15, 87,  3, 17,\n       54, 67, 20, 82, 64, 64, 42, 68, 27,  5,  5, 82, 68, 38, 79,  6, 27,\n       77, 70, 68, 31, 51, 53, 17, 71, 98, 32, 42,  8, 90, 44, 17, 92, 33,\n       36, 71, 18, 18, 67, 98, 24, 80, 72, 19,  6, 48, 88, 47, 23, 96, 13,\n       14, 21, 83, 41, 44, 34, 93, 81,  8, 84, 35, 48, 77, 30, 70],\n      dtype=int32), array([76, 40, 86, 64, 12, 37, 27, 94, 57, 95, 22, 58,  1, 98, 94, 95, 92,\n       70, 92, 49, 85, 16,  9, 75, 40, 40, 48, 52, 86, 41, 61, 70, 26, 69,\n       29, 56,  3, 65, 79, 66, 41, 78, 73, 91, 58, 73, 10, 83, 51, 80, 61,\n       46, 79, 76, 66, 76, 53, 64,  0, 66, 18, 61, 89,  9,  5, 87, 76, 81,\n        6, 36, 50, 42, 35, 55, 22, 92, 14, 57, 69, 49, 82, 56, 47, 85, 41,\n       23, 35, 53, 50, 13, 34, 29, 26, 85, 82, 70, 21, 20, 16, 18],\n      dtype=int32), array([70, 88, 22, 15,  7, 62, 76,  5, 69, 64, 78, 45, 17, 28, 92, 29,  1,\n       32, 77, 37, 10, 99, 93, 82, 10, 90,  6, 62, 41, 15, 79, 32, 60, 92,\n       74,  3, 49,  9, 46, 30, 59, 26,  4,  8, 33, 52, 20,  0, 29, 32, 49,\n       14, 87, 60, 27,  1, 53,  4, 44, 43, 23, 28, 16, 35,  3, 95, 49, 28,\n       95, 23, 18, 16, 48, 80, 50, 95, 57, 50, 52, 91, 90, 96, 24, 63,  8,\n       96, 90, 29, 80, 24, 11, 79, 37, 52,  9, 25, 76, 29, 84, 19],\n      dtype=int32), array([96, 14, 19, 46, 78, 83, 37, 17,  1, 65,  2, 45, 30, 58, 81, 10, 85,\n       49, 19, 49, 36, 66, 75, 15, 11, 22, 15, 86, 64, 37, 85, 70, 73, 44,\n       42, 49, 54, 71, 55, 45, 20, 24, 22, 66, 61, 70, 74, 73, 32, 86, 64,\n       36, 62, 46, 94, 81, 75, 27, 10, 53, 20, 50,  2,  7, 87, 93, 55, 15,\n       21,  8, 19, 38, 35, 41, 56, 58, 88, 19, 11, 94, 74, 27, 33, 10, 56,\n       15, 46, 60,  0, 60, 42, 20, 98, 28, 82,  9, 76, 81, 75, 83],\n      dtype=int32)], 'feature_importances_': array([0.01306717, 0.2212353 , 0.02090709, 0.02030634, 0.02070127,\n       0.01021719, 0.02547474, 0.0250837 , 0.18530024, 0.02525172,\n       0.01522741, 0.24222554, 0.02086506, 0.01515128, 0.01613278,\n       0.01897662, 0.02422525, 0.02780539, 0.01099475, 0.04085118]), 'n_classes_': 2, 'n_features_in_': 20, 'n_outputs_': 1}"
  },
  {
    "objectID": "documentation.html#keras",
    "href": "documentation.html#keras",
    "title": "Model documentation",
    "section": "Keras",
    "text": "Keras\n\nfrom tensorflow import keras\n\n\nkeras_clf = keras.Sequential()\nkeras_clf.add(keras.layers.Dense(16, activation='relu', input_shape=(20,)))\nkeras_clf.add(keras.layers.Dense(8, activation='relu'))\nkeras_clf.add(keras.layers.Dense(1, activation='sigmoid'))\nkeras_clf.compile(optimizer='sgd', loss='binary_crossentropy')\nkeras_clf.fit(X, y, batch_size=10, epochs=10)\n\nEpoch 1/10\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 1s 166ms/step - loss: 0.7365\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.7342  \nEpoch 2/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - loss: 0.8063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.7228 \nEpoch 3/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.8151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.7261 \nEpoch 4/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6988 \nEpoch 5/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - loss: 0.6918\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6878 \nEpoch 6/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6611 \nEpoch 7/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - loss: 0.7063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6780 \nEpoch 8/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.6510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6459 \nEpoch 9/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.7107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6583 \nEpoch 10/10\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.5356\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.6039 \n\n\n&lt;keras.src.callbacks.history.History at 0x200a840a7d0&gt;\n\n\n\nmodel_doc_keras = ModelCard()\nmodel_doc_keras.read_model(keras_clf)\nmodel_doc_keras.show_json()['model_details']['info']\n\n'{\"module\": \"keras\", \"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"layers\": [{\"module\": \"keras.layers\", \"class_name\": \"InputLayer\", \"config\": {\"batch_shape\": [null, 20], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"input_layer\"}, \"registered_name\": null}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 16, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 20]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 8, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 16]}}, {\"module\": \"keras.layers\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": {\"module\": \"keras\", \"class_name\": \"DTypePolicy\", \"config\": {\"name\": \"float32\"}, \"registered_name\": null}, \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}, \"registered_name\": null}, \"bias_initializer\": {\"module\": \"keras.initializers\", \"class_name\": \"Zeros\", \"config\": {}, \"registered_name\": null}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 8]}}], \"build_input_shape\": [null, 20]}, \"registered_name\": null, \"build_config\": {\"input_shape\": [null, 20]}, \"compile_config\": {\"optimizer\": {\"module\": \"keras.optimizers\", \"class_name\": \"SGD\", \"config\": {\"name\": \"SGD\", \"learning_rate\": 0.009999999776482582, \"weight_decay\": null, \"clipnorm\": null, \"global_clipnorm\": null, \"clipvalue\": null, \"use_ema\": false, \"ema_momentum\": 0.99, \"ema_overwrite_frequency\": null, \"loss_scale_factor\": null, \"gradient_accumulation_steps\": null, \"momentum\": 0.0, \"nesterov\": false}, \"registered_name\": null}, \"loss\": \"binary_crossentropy\", \"loss_weights\": null, \"metrics\": null, \"weighted_metrics\": null, \"run_eagerly\": false, \"steps_per_execution\": 1, \"jit_compile\": false}}'"
  },
  {
    "objectID": "documentation.html#other-models",
    "href": "documentation.html#other-models",
    "title": "Model documentation",
    "section": "Other models",
    "text": "Other models\nNative support for automatic documentation of other model types, such as from fastai, pytorch is expected to be available in future versions. Until then, any models coded form scratch by the user as well as any other model can be documented by passing the information as an argument to the Documenter’s fill_model_info method. This can be done with a string or dictionary. For example:\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.astype(np.float32))\n        self.y = torch.from_numpy(y.astype(np.float32))\n        self.len = self.X.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass PytorchNet(torch.nn.Module):\n    def __init__(self):\n        super(PytorchNet, self).__init__()\n        self.layer1 = torch.nn.Linear(20, 16)\n        self.layer2 = torch.nn.Linear(16, 8)\n        self.layer3 = torch.nn.Linear(8, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        x = torch.sigmoid(self.layer3(x))\n        return x\n\npytorch_clf = PytorchNet()\n\ndataloader = MockDataset(X, y)\n\n\nloss_func = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(pytorch_clf.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(10):\n    running_loss = 0.0\n    for i, data in enumerate(dataloader, 0):\n        _X, _y = data\n        optimizer.zero_grad()\n        y_pred_epoch = pytorch_clf(_X)\n        loss = loss_func(y_pred_epoch, _y.reshape(1))\n        loss.backward()\n        optimizer.step()\n\n\nmodel_doc_pytorch = ModelCard()\nmodel_doc_pytorch.fill_model_info(\"This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation\")\nmodel_doc_pytorch.show_json()['model_details']['info']\n\n'This model is a neural network consisting of two fully connected layers and ending in a linear layer with a sigmoid activation'"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\n\n\n\nread_attr (obj)\n\nReads and yields the type and values of fitted attributes from an object.\n\nArgs:\n    obj: Object from which attributes will be read.\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=1247572020),\n    DecisionTreeRegressor(max_features=1.0, random_state=487026338),\n    DecisionTreeRegressor(max_features=1.0, random_state=1149396190)]},\n  {'estimators_samples_': [array([1, 1], dtype=int32),\n    array([0, 1], dtype=int32),\n    array([0, 0], dtype=int32)]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-model-documentation",
    "href": "utils.html#support-for-model-documentation",
    "title": "Utils",
    "section": "",
    "text": "get_datetime ()\n\nReturns the time now\n\nd = get_datetime()\nassert isinstance(d, str)\nassert len(d) &gt; 0\n\n\n\n\n\nread_attr (obj)\n\nReads and yields the type and values of fitted attributes from an object.\n\nArgs:\n    obj: Object from which attributes will be read.\nFunction read_attr helps gingado Documenters to read the object behind the scenes.\nIt collects the type of estimator, and any attributes resulting from fitting an object (in ie, those that end in “_” without being double underscores).\nFor example, the attributes of an untrained and a trained random forest are, in sequence:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf_unfit = RandomForestRegressor(n_estimators=3)\nrf_fit = RandomForestRegressor(n_estimators=3)\\\n    .fit([[1, 0], [0, 1]], [[0.5], [0.5]]) # random numbers\nlist(read_attr(rf_unfit)), list(read_attr(rf_fit))\n\n([{'_estimator_type': 'regressor'}],\n [{'_estimator_type': 'regressor'},\n  {'estimator_': DecisionTreeRegressor()},\n  {'estimators_': [DecisionTreeRegressor(max_features=1.0, random_state=1247572020),\n    DecisionTreeRegressor(max_features=1.0, random_state=487026338),\n    DecisionTreeRegressor(max_features=1.0, random_state=1149396190)]},\n  {'estimators_samples_': [array([1, 1], dtype=int32),\n    array([0, 1], dtype=int32),\n    array([0, 0], dtype=int32)]},\n  {'feature_importances_': array([0., 0.])},\n  {'n_features_in_': 2},\n  {'n_outputs_': 1}])"
  },
  {
    "objectID": "utils.html#support-for-time-series",
    "href": "utils.html#support-for-time-series",
    "title": "Utils",
    "section": "Support for time series",
    "text": "Support for time series\nObjects of the class Lag are similar to scikit-learn’s transformers.\n\nLag\n\nLag (lags=1, jump=0, keep_contemporaneous_X=False)\n\nA transformer for lagging variables.\n\nArgs:\n    lags (int): The number of lags to apply.\n    jump (int): The number of initial observations to skip before applying the lag.\n    keep_contemporaneous_X (bool): Whether to keep the contemporaneous values of X in the output.\n\n\nfit\n\nfit (self, X: numpy.ndarray, y=None)\n\nFits the Lag transformer.\n\nArgs:\n    X (np.ndarray): Array-like data of shape (n_samples, n_features).\n    y: Array-like data of shape (n_samples,) or (n_samples, n_targets) or None.\n    \nReturns:\n    self: A fitted version of the `Lag` instance.\n\n\ntransform\n\ntransform (self, X: numpy.ndarray)\n\nApplies the lag transformation to the dataset `X`.\n\nArgs:\n    X (np.ndarray): Array-like data of shape (n_samples, n_features).\n    \nReturns:\n    A lagged version of `X`.\n\n\nfit_transform\n\nfit_transform (self, X, y=None, **fit_params)\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params`\nand returns a transformed version of `X`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    Input samples.\n\ny :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n    Target values (None for unsupervised transformations).\n\n**fit_params : dict\n    Additional fit parameters.\n\nReturns\n-------\nX_new : ndarray array of shape (n_samples, n_features_new)\n    Transformed array.\nThe code below demonstrates how Lag works in practice. Note in particular that, because Lag is a transformer, it can be used as part of a scikit-learn’s Pipeline.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\nrandomX = np.random.rand(15, 2)\nrandomY = np.random.rand(15)\n\nlags = 3\njump = 2\n\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('lagger', Lag(lags=lags, jump=jump, keep_contemporaneous_X=False))\n]).fit_transform(randomX, randomY)\n\nBelow we confirm that the lagger removes the correct number of rows corresponding to the lagged observations:\n\nassert randomX.shape[0] - lags - jump == pipe.shape[0]\n\nAnd because Lag is a transformer, its parameters (lags and jump) can be calibrated using hyperparameter tuning to achieve the best performance for a model."
  },
  {
    "objectID": "utils.html#support-for-data-augmentation-with-sdmx",
    "href": "utils.html#support-for-data-augmentation-with-sdmx",
    "title": "Utils",
    "section": "Support for data augmentation with SDMX",
    "text": "Support for data augmentation with SDMX\n\n\n\n\n\n\nNote\n\n\n\nplease note that working with SDMX may take some minutes depending on the amount of information you are downloading.\n\n\n\nlist_SDMX_sources\n\nlist_SDMX_sources ()\n\nFetches the list of SDMX sources.\n\nReturns:\n    The list of codes representing the SDMX sources available for data download.\n\nsources = list_SDMX_sources()\nprint(sources)\n\nassert len(sources) &gt; 0\n# all elements are of type 'str'\nassert sum([isinstance(src, str) for src in sources]) == len(sources)\n\n['ABS', 'ABS_JSON', 'AR1', 'BBK', 'BIS', 'COMP', 'ECB', 'EMPL', 'ESTAT', 'ESTAT3', 'ESTAT_COMEXT', 'GROW', 'ILO', 'IMF', 'IMF_beta', 'IMF_beta3', 'INEGI', 'INSEE', 'ISTAT', 'LSD', 'NB', 'NBB', 'OECD', 'OECD_JSON', 'SGR', 'SPC', 'STAT_EE', 'StatCan', 'UNESCO', 'UNICEF', 'UNSD', 'UY110', 'WB', 'WB_WDI']\n\n\n\n\nlist_all_dataflows\n\nlist_all_dataflows (codes_only: bool = False, return_pandas: bool = True)\n\nLists all SDMX dataflows. Note: When using as a parameter to an `AugmentSDMX` object\nor to the `load_SDMX_data` function, set `codes_only=True`\"\n\nArgs:\n    codes_only (bool): Whether to return only the dataflow codes.\n    return_pandas (bool): Whether to return the result in a pandas DataFrame format.\n    \nReturns:\n    All available dataflows for all SDMX sources.\n\ndflows = list_all_dataflows(return_pandas=False)\n\nassert isinstance(dflows, dict)\nall_sources = list_SDMX_sources()\nassert len([s for s in dflows.keys() if s in all_sources]) == len(dflows.keys())\n\nlist_all_dataflows returns by default a pandas Series, facilitating data discovery by users like so:\n\ndflows = list_all_dataflows(return_pandas=True)\nassert type(dflows) == pd.core.series.Series\n\ndflows\n\nABS    ABORIGINAL_POP_PROJ                 Projected population, Aboriginal and Torres St...\n       ABORIGINAL_POP_PROJ_REMOTE          Projected population, Aboriginal and Torres St...\n       ABS_ABORIGINAL_POPPROJ_INDREGION    Projected population, Aboriginal and Torres St...\n       ABS_ACLD_LFSTATUS                   Australian Census Longitudinal Dataset (ACLD):...\n       ABS_ACLD_TENURE                     Australian Census Longitudinal Dataset (ACLD):...\n                                                                 ...                        \nUY110  DF_TTT_SEXO_AREA_REF                Tasa de Teletrabajo por sexo y area de referencia\nWB     DF_WITS_Tariff_TRAINS                                WITS - UNCTAD TRAINS Tariff Data\n       DF_WITS_TradeStats_Development                             WITS TradeStats Devlopment\n       DF_WITS_TradeStats_Tariff                                      WITS TradeStats Tariff\n       DF_WITS_TradeStats_Trade                                        WITS TradeStats Trade\nName: dataflow, Length: 33145, dtype: object\n\n\nThis format allows for more easily searching dflows by source:\n\nlist_all_dataflows(codes_only=True, return_pandas=True)\n\nABS    0                  ABORIGINAL_POP_PROJ\n       1           ABORIGINAL_POP_PROJ_REMOTE\n       2     ABS_ABORIGINAL_POPPROJ_INDREGION\n       3                    ABS_ACLD_LFSTATUS\n       4                      ABS_ACLD_TENURE\n                           ...               \nUY110  86                DF_TTT_SEXO_AREA_REF\nWB     0                DF_WITS_Tariff_TRAINS\n       1       DF_WITS_TradeStats_Development\n       2            DF_WITS_TradeStats_Tariff\n       3             DF_WITS_TradeStats_Trade\nName: dataflow, Length: 33145, dtype: object\n\n\n\ndflows['BIS']\n\nBIS_REL_CAL                                     BIS_RELEASE_CALENDAR\nWS_CBPOL                                   Central bank policy rates\nWS_CBS_PUB                                  BIS consolidated banking\nWS_CBTA                                    Central bank total assets\nWS_CPMI_CASHLESS                      CPMI cashless payments (T5,T6)\nWS_CPMI_CT1                           CPMI comparative tables type 1\nWS_CPMI_CT2                           CPMI comparative tables type 2\nWS_CPMI_DEVICES                            CPMI payment devices (T4)\nWS_CPMI_INSTITUT                              CPMI institutions (T3)\nWS_CPMI_MACRO                                     CPMI macro (T1,T2)\nWS_CPMI_PARTICIP                  CPMI participants (T7,T10,T12,T15)\nWS_CPMI_SYSTEMS     CPMI systems (T8,T9,T11,T13,T14,T16,T17,T18,T19)\nWS_CPP                                    Commercial property prices\nWS_CREDIT_GAP                                 BIS credit-to-GDP gaps\nWS_DEBT_SEC2_PUB    BIS international debt securities (BIS-compiled)\nWS_DER_OTC_TOV                              OTC derivatives turnover\nWS_DPP                          Detailed residential property prices\nWS_DSR                                        BIS debt service ratio\nWS_EER                                  BIS effective exchange rates\nWS_GLI                                   Global liquidity indicators\nWS_LBS_D_PUB                                  BIS locational banking\nWS_LONG_CPI                                 BIS long consumer prices\nWS_NA_SEC_C3                          BIS debt securities statistics\nWS_NA_SEC_DSS                         BIS Debt securities statistics\nWS_OTC_DERIV2                            OTC derivatives outstanding\nWS_SPP                          Selected residential property prices\nWS_TC                                BIS long series on total credit\nWS_XRU                                      US dollar exchange rates\nWS_XTD_DERIV                             Exchange traded derivatives\nName: dataflow, dtype: object\n\n\nOr the user can search dataflows by their human-readable name instead of their code. For example, this is one way to see if any dataflow has information on interest rates:\n\ndflows[dflows.str.contains('Interest rate', case=False)]\n\nECB     IRS                                            Interest rate statistics\n        MIR                                        MFI Interest Rate Statistics\n        RIR                                               Retail Interest Rates\nESTAT   IRT_ST_A                      Money market interest rates - annual data\n        IRT_ST_M                     Money market interest rates - monthly data\n        IRT_ST_Q                   Money market interest rates - quarterly data\n        EI_MFIR_M                                 Interest rates - monthly data\n        ENPE_IRT_LD                     Loan and deposit one year interest rate\n        ENPE_IRT_ST                                 Money market interest rates\n        TEIMF040                                          3-month-interest rate\n        TEIMF100                         Day-to-day money market interest rates\nESTAT3  IRT_ST_A                      Money market interest rates - annual data\n        IRT_ST_M                     Money market interest rates - monthly data\n        IRT_ST_Q                   Money market interest rates - quarterly data\n        EI_MFIR_M                                 Interest rates - monthly data\n        ENPE_IRT_LD                     Loan and deposit one year interest rate\n        ENPE_IRT_ST                                 Money market interest rates\n        TEIMF040                                          3-month-interest rate\n        TEIMF100                         Day-to-day money market interest rates\nIMF     6SR                   M&B: Interest Rates and Share Prices (6SR) for...\n        INR                                                      Interest rates\n        INR_NSTD                                    Interest rates_Non-Standard\nNB      GOVT_GENERIC_RATES                               Generic interest rates\n        GOVT_IRS                                            Interest rate swaps\nName: dataflow, dtype: object\n\n\n\n\ncodelists\n\ncodelists (dflow)\n\nRetrieves the codelist for specific SDMX dataflows.\n\nArgs:\n    dflow (dict): A dictionary specifying the source as the key and the dataflow (or list of dataflows) as the value.\n\nReturns:\n    dict: A dictionary with each source as a key and the values containing the codelist for the specified dataflow(s),detailing code information relevant to the SDMX data structure.\nOnce the user finds a dataflow of interest, the function codelists returns a dictionary where each key is a dimension of that dataflow, and each value is that dimension’s codelist.\nFor example, the dimensions and codelists of the BIS’ dataflow on OTC derivatives outstanding are the following:\n\nprint(dflows[dflows.str.contains(\"OTC\", case=False)])\n\ncl_OTC = codelists(dflow={\"BIS\": \"WS_OTC_DERIV2\"})\n\nBIS  WS_DER_OTC_TOV       OTC derivatives turnover\n     WS_OTC_DERIV2     OTC derivatives outstanding\nName: dataflow, dtype: object\n\n\nHere is a list of all dimensions for the OTC derivatives outstanding dataflow:\n\ncl_OTC_BIS = cl_OTC[\"BIS\"]\ncl_OTC_BIS.keys()\n\ndict_keys(['CL_AVAILABILITY', 'CL_BIS_IF_REF_AREA', 'CL_BIS_UNIT', 'CL_COLLECTION', 'CL_CONF_STATUS', 'CL_DECIMALS', 'CL_DER_BASIS', 'CL_DER_INSTR', '3', 'W', 'CL_EX_METHOD', 'CL_FREQ', 'CL_ISSUE_MAT', 'C', 'CL_MARKET_RISK', 'CL_OBS_STATUS', 'D', 'H', 'CL_OD_TYPE', 'CL_RATING', 'CL_SECTOR_CPY', 'CL_SECTOR_UDL', 'CL_SUB_CHANNEL', 'CL_TIME_FORMAT', 'CL_UNIT_MULT'])\n\n\nBelow are the codelists of the frequency dimension (“CL_FREQ”) and the counterparty sector (“CL_SECTOR_CPY”):\n\ncl_OTC_BIS[\"CL_FREQ\"]\n\nCL_FREQ\nA                                   Annual\nB    Daily - business week (not supported)\nD                                    Daily\nE                    Event (not supported)\nH                              Half-yearly\nM                                  Monthly\nQ                                Quarterly\nW                                   Weekly\nName: Code list for Frequency (FREQ), dtype: object\n\n\n\ncl_OTC_BIS[\"CL_SECTOR_CPY\"]\n\n\n\n\n\n\n\n\nname\nparent\n\n\nCL_SECTOR_CPY\n\n\n\n\n\n\nA\nTotal (all counterparties)\n\n\n\nB\nReporting dealers\nA\n\n\nC\nOther financial institutions\nA\n\n\nD\nNon-reporting banks\nC\n\n\nE\nInstitutional investors\nC\n\n\nF\nHedge funds and proprietary trading firms\nC\n\n\nG\nOfficial sector financial institutions\nC\n\n\nH\nUndistributed\nC\n\n\nK\nCentral Counterparties\nC\n\n\nL\nBanks and securities firms\nC\n\n\nM\nInsurance and financial guaranty firms\nC\n\n\nN\nSPVs, SPCs or SPEs\nC\n\n\nO\nHedge funds\nC\n\n\nP\nOther residual financial institutions\nC\n\n\nU\nNon-financial customers\nA\n\n\nV\nPrime brokered\nA\n\n\nW\nRetail-driven\nA\n\n\nX\nRelated Party Trades\nA\n\n\nY\nOwn branches and subsidiaries\nA\n\n\nZ\nNon-reporters\nA\n\n\nQ\nNon-bank electronic market-makers\nA\n\n\nR\nOther customers\nA\n\n\nI\nBack-to-back trades\nA\n\n\nJ\nCompression trades\nA\n\n\n0\nTechnical residual (total)\n\n\n\n1\nTechnical residual (other financial institutions)\n\n\n\n2\nTechnical residual (Prime brokered)\n\n\n\n\n\n\n\n\nYou can also get codelists for multiple dataflows from the same source:\n\n# Get codelists for both Exchange Rates and Consumer Prices\ncl_multiple = codelists({\"ECB\": [\"EXR\", \"ICP\"]})\n\n# Show dimensions for each dataflow\nfor dataflow, codelist in cl_multiple[\"ECB\"].items():\n    print(f\"\\nDimensions in {dataflow} dataflow:\")\n    print(codelist.keys())\n\n\nDimensions in EXR dataflow:\ndict_keys(['CL_COLLECTION', 'CL_CURRENCY', 'CL_DECIMALS', 'CL_EXR_SUFFIX', 'CL_EXR_TYPE', 'CL_FREQ', 'CL_OBS_CONF', 'CL_OBS_STATUS', 'CL_ORGANISATION', 'CL_UNIT', 'CL_UNIT_MULT'])\n\nDimensions in ICP dataflow:\ndict_keys(['CL_ADJUSTMENT', 'CL_AREA_EE', 'CL_COLLECTION', 'CL_DECIMALS', 'CL_FREQ', 'CL_ICP_ITEM', 'CL_ICP_SUFFIX', 'CL_OBS_CONF', 'CL_OBS_STATUS', 'CL_ORGANISATION', 'CL_STS_INSTITUTION', 'CL_UNIT', 'CL_UNIT_MULT'])\n\n\nIn addition, you can also get codelists from multiple sources:\n\ncl_sources = codelists({\"ECB\": \"EXR\", \"BIS\": \"WS_OTC_DERIV2\"})\n\nprint(\"Available sources:\", cl_sources.keys())\n\nAvailable sources: dict_keys(['ECB', 'BIS'])\n\n\nThe function load_SDMX_data is a convenience function that downloads data from SDMX sources (and any specific dataflows passed as arguments) if they match the key and parameters set by the user.\n\n\nload_SDMX_data\n\nload_SDMX_data (sources: dict, keys: dict, params: dict, verbose: bool = True)\n\nLoads datasets from SDMX.\n\nArgs:\n    sources (dict): A dictionary with the sources and dataflows per source.\n    keys (dict): The keys to be used in the SDMX query.\n    params (dict): The parameters to be used in the SDMX query.\n    verbose (bool): Whether to communicate download steps to the user.\n    \nReturns:\n    A pandas DataFrame with data from SDMX or None if no data matches the sources, keys, and parameters.\n\ndf = load_SDMX_data(sources={'ECB': 'CISS', 'BIS': 'WS_CBPOL_D'}, keys={'FREQ': 'D'}, params={'startPeriod': 2003})\n\nassert type(df) == pd.DataFrame\nassert df.shape[0] &gt; 0\nassert df.shape[1] &gt; 0\n\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\nQuerying data from BIS's dataflow 'WS_CBPOL' - Central bank policy rates..."
  },
  {
    "objectID": "utils.html#temporal-features",
    "href": "utils.html#temporal-features",
    "title": "Utils",
    "section": "Temporal features",
    "text": "Temporal features\nTemporal features, such as the day of the week, month, or hour, provide valuable information for time series data, helping to capture seasonality, trends, and cyclic patterns. These features are especially useful because they represent known future information that can enhance model predictions. The gingado library offers the get_timefeat method to extract these features from a time series:\n\nget_timefeat\n\nget_timefeat (df: pandas.core.frame.DataFrame | pandas.core.series.Series, freq: str | gingado.internals.Frequency, columns: list[str] | None = None, add_to_df: bool = True) -&gt; pandas.core.frame.DataFrame\n\nGenerate temporal features from a DataFrame with a DatetimeIndex.\n\nThis function creates various time-based features such as day of week,\nday of month, week of year, etc., based on the DatetimeIndex of the input DataFrame.\n\nArgs:\n    df (pd.DataFrame | pd.Series): Input DataFrame or Series with a DatetimeIndex.\n    freq (FrequencyLike): Frequency of the input DataFrame. Can either be a string which is\n        a supported pandas frequency alias or an gingado-interal Frequency object.\n    columns (list[str], optional): List of colums with temporal feature names that should be\n        kept. If None, all default temporal features are returned. Defaults to None.\n    add_to_df (bool, optional): If True, append the generated features to the input DataFrame.\n        If False, return only the generated features. Defaults to True.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the generated temporal features,\n        either appended to the input DataFrame or as a separate DataFrame.\n\nRaises:\n    ValueError: If the input DataFrame's index is not a DatetimeIndex.\nFor instance, using daily data from a DataFrame:\n\n# Display the first few rows of the DataFrame\ndisplay(df.head())\n\n# Extract temporal features for daily data\ntemporal = get_timefeat(df, freq=\"D\", add_to_df=False)\ndisplay(temporal.head())\n\n\n\n\n\n\n\n\nECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX\nECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX\n...\nBIS__WS_CBPOL_D__SA\nBIS__WS_CBPOL_D__SE\nBIS__WS_CBPOL_D__TH\nBIS__WS_CBPOL_D__TR\nBIS__WS_CBPOL_D__US\nBIS__WS_CBPOL_D__XM\nBIS__WS_CBPOL_D__ZA\nBIS__WS_CBPOL_D__AR\nBIS__WS_CBPOL_D__AU\nBIS__WS_CBPOL_D__BR\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-01-01\n0.017774\n0.042273\nNaN\n0.107753\n0.028552\n0.053814\n0.005528\n0.060809\n0.004191\n0.057108\n...\n2.0\nNaN\n1.75\nNaN\n1.25\n2.75\nNaN\nNaN\nNaN\n25.0\n\n\n2003-01-02\n0.023427\n0.047823\nNaN\n0.148028\n0.039988\n0.075186\n0.013415\n0.049041\n0.014820\n0.064289\n...\nNaN\n3.75\n1.75\n44.0\n1.25\n2.75\n13.5\n5.99\n4.75\n25.0\n\n\n2003-01-03\n0.021899\n0.043292\nNaN\n0.141700\n0.040378\n0.077400\n0.014249\n0.047883\n0.016874\n0.064880\n...\nNaN\n3.75\n1.75\n44.0\n1.25\n2.75\n13.5\n6.05\n4.75\n25.0\n\n\n2003-01-04\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.0\nNaN\n1.75\nNaN\n1.25\n2.75\n13.5\nNaN\nNaN\n25.0\n\n\n2003-01-05\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.0\nNaN\n1.75\nNaN\n1.25\n2.75\nNaN\nNaN\nNaN\n25.0\n\n\n\n\n5 rows × 62 columns\n\n\n\n\n\n\n\n\n\n\nday_of_week\nday_of_month\nday_of_quarter\nday_of_year\nweek_of_month\nweek_of_quarter\nweek_of_year\nmonth_of_quarter\nmonth_of_year\nquarter_of_year\nquarter_end\nyear_end\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2003-01-01\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n\n\n2003-01-02\n3\n2\n2\n2\n1\n1\n1\n1\n1\n1\n0\n0\n\n\n2003-01-03\n4\n3\n3\n3\n1\n1\n1\n1\n1\n1\n0\n0\n\n\n2003-01-04\n5\n4\n4\n4\n1\n1\n1\n1\n1\n1\n0\n0\n\n\n2003-01-05\n6\n5\n5\n5\n1\n1\n1\n1\n1\n1\n0\n0\n\n\n\n\n\n\n\nYou can also integrate the temporal features directly into the original DataFrame by setting the add_to_df parameter to True:\n\n# Generate a sample DataFrame with a weekly index\ndf_weekly = pd.DataFrame(\n    data={\"value\": rng.normal(size=100)},\n    index=pd.date_range('2000-01-01', periods=100, freq='W-MON')\n)\n\n# Add temporal features to the weekly data\ndf_with_timefeat = get_timefeat(df_weekly, freq=\"W\", add_to_df=True)\ndisplay(df_with_timefeat.head())\n\n\n\n\n\n\n\n\nvalue\nweek_of_month\nweek_of_quarter\nweek_of_year\nmonth_of_quarter\nmonth_of_year\nquarter_of_year\nquarter_end\nyear_end\n\n\n\n\n2000-01-03\n0.304717\n1\n1\n1\n1\n1\n1\n0\n0\n\n\n2000-01-10\n-1.039984\n2\n2\n2\n1\n1\n1\n0\n0\n\n\n2000-01-17\n0.750451\n3\n3\n3\n1\n1\n1\n0\n0\n\n\n2000-01-24\n0.940565\n4\n4\n4\n1\n1\n1\n0\n0\n\n\n2000-01-31\n-1.951035\n5\n5\n5\n1\n1\n1\n0\n0\n\n\n\n\n\n\n\nIf you only need a subset of the temporal features, you can specify the desired feature names:\n\n# Generate a new DataFrame with a monthly index\ndf_monthly = pd.DataFrame(\n    data={\"value\": rng.normal(size=24)},\n    index=pd.date_range(\"2023-01-01\", periods=24, freq='MS')\n)\n# Only select a subset of temporal features:\ndf_with_timefeat = get_timefeat(df_monthly, freq=\"MS\", columns=[\"month_of_year\", \"quarter_of_year\"])\ndisplay(df_with_timefeat.head())\n\n\n\n\n\n\n\n\nvalue\nmonth_of_year\nquarter_of_year\n\n\n\n\n2023-01-01\n-0.378163\n1\n1\n\n\n2023-02-01\n1.299228\n2\n1\n\n\n2023-03-01\n-0.356264\n3\n1\n\n\n2023-04-01\n0.737516\n4\n2\n\n\n2023-05-01\n-0.933618\n5\n2\n\n\n\n\n\n\n\nIn addition to get_timefeat, the gingado library provides the TemporalFeatureTransformer class, which can be used to transform a DataFrame with a temporal index into a DataFrame with additional features:\n\ntemp_trf = TemporalFeatureTransformer(freq=\"W\", features=[\"week_of_month\", \"week_of_year\", \"quarter_of_year\"])\ndf_with_timefeat = temp_trf.fit_transform(df_weekly)\ndisplay(df_with_timefeat.head())\n\n\n\n\n\n\n\n\nvalue\nweek_of_month\nweek_of_year\nquarter_of_year\n\n\n\n\n2000-01-03\n0.304717\n1\n1\n1\n\n\n2000-01-10\n-1.039984\n2\n2\n1\n\n\n2000-01-17\n0.750451\n3\n3\n1\n\n\n2000-01-24\n0.940565\n4\n4\n1\n\n\n2000-01-31\n-1.951035\n5\n5\n1"
  },
  {
    "objectID": "forecast.html",
    "href": "forecast.html",
    "title": "Using gingado to forecast financial series",
    "section": "",
    "text": "This notebook illustrates the use of gingado to build models for forecasting, using foreign exchange (FX) rate movements as an example. Please note that the results or the model should not be taken as investment advice.\nForecasting exchange rates is notoriously difficult (Rossi (2013) and references therein).\nThis exercise will illustrate various functionalities provided by gingado:\nUnlike most scripts that concentrate the package imports at the beginning, this walkthrough will import as needed, to better highlight where each contribution of gingado is used in the workflow.\nFirst, we will use gingado to run a simple example with the following characteristics:"
  },
  {
    "objectID": "forecast.html#downloading-fx-rates",
    "href": "forecast.html#downloading-fx-rates",
    "title": "Using gingado to forecast financial series",
    "section": "Downloading FX rates",
    "text": "Downloading FX rates\nIn this exercise, we will concentrate on the bilateral FX rates between the 🇺🇸 US Dollar (USD) and the 🇧🇷 Brazilian Real (BRL), 🇨🇦 Canadian Dollar (CAD), 🇨🇭 Swiss Franc (CHF), 🇪🇺 Euro (EUR), 🇬🇧 British Pound (GBP), 🇯🇵 Japanese Yen (JPY) and 🇲🇽 Mexican Peso (MXN).\nThe rates are standardised to measure the units in foreign currency bought by one USD. Therefore, positive returns represent USD is more valued compared to the other currency, and vice-versa.\n\n\nCode\nfrom gingado.utils import load_SDMX_data\n\n\n\n\nCode\ndf = load_SDMX_data(\n    sources={'BIS': 'WS_XRU_D'},\n    keys={\n        'FREQ': 'D', \n        'CURRENCY': ['BRL', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'MXN'],\n        'REF_AREA': ['BR', 'CA', 'CH', 'XM', 'GB', 'JP', 'MX']\n        },\n    params={'startPeriod': 2003}\n)\n\n\nQuerying data from BIS's dataflow 'WS_XRU' - US dollar exchange rates...\n\n\nThe code below simplifies the column names by removing the identification of the SDMX sources, dataflows and keys and replacing it with the usual code for the bilateral exchange rates.\n\n\nCode\nprint(\"Original column names:\")\nprint(df.columns)\n\ndf.columns = ['USD' + col.split('_')[8] for col in df.columns]\n\nprint(\"New column names:\")\nprint(df.columns)\n\n\nOriginal column names:\nIndex(['BIS__WS_XRU_D__BR__BRL__A', 'BIS__WS_XRU_D__GB__GBP__A',\n       'BIS__WS_XRU_D__CH__CHF__A', 'BIS__WS_XRU_D__XM__EUR__A',\n       'BIS__WS_XRU_D__CA__CAD__A', 'BIS__WS_XRU_D__JP__JPY__A',\n       'BIS__WS_XRU_D__MX__MXN__A'],\n      dtype='object')\nNew column names:\nIndex(['USDBRL', 'USDGBP', 'USDCHF', 'USDEUR', 'USDCAD', 'USDJPY', 'USDMXN'], dtype='object')\n\n\nThe dataset looks like this so far (most recent 5 rows displayed only):\n\n\nCode\ndf.tail()\n\n\n\n\n\n\n\n\n\nUSDBRL\nUSDGBP\nUSDCHF\nUSDEUR\nUSDCAD\nUSDJPY\nUSDMXN\n\n\nTIME_PERIOD\n\n\n\n\n\n\n\n\n\n\n\n2025-02-26\n5.752741\n0.790197\n0.895585\n0.953562\n1.435492\n149.423095\n20.471536\n\n\n2025-02-27\n5.805574\n0.789090\n0.897872\n0.954472\n1.435525\n149.594350\n20.378066\n\n\n2025-02-28\n5.831524\n0.793468\n0.902315\n0.960523\n1.442609\n150.763615\n20.381423\n\n\n2025-03-03\n5.885332\n0.788629\n0.900908\n0.955566\n1.443287\n151.294792\n20.473292\n\n\n2025-03-04\n5.885384\n0.784200\n0.887657\n0.947239\n1.442076\n148.242872\n20.784124\n\n\n\n\n\n\n\nWe are interested in the percentage change from the previous day.\n\n\nCode\nFX_rate_changes = df.pct_change(fill_method=None)\nFX_rate_changes.dropna(inplace=True)\n\n\n\n\nCode\nFX_rate_changes.plot(subplots=True, layout=(4, 2), figsize=(15, 15), sharex=True, title='Selected daily FX rate changes')\n\n\narray([[&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;],\n       [&lt;Axes: xlabel='TIME_PERIOD'&gt;, &lt;Axes: xlabel='TIME_PERIOD'&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "forecast.html#augmenting-the-dataset",
    "href": "forecast.html#augmenting-the-dataset",
    "title": "Using gingado to forecast financial series",
    "section": "Augmenting the dataset",
    "text": "Augmenting the dataset\nWe will complement the FX rates data with two other datasets:\n\ndaily central bank policy rates from the Bank for International Settlements (BIS) (2017), and\nthe daily Composite Indicator of Systemic Stress (CISS), created by Hollo, Kremer, and Lo Duca (2012) and updated by the European Central Bank (ECB).\n\n\n\nCode\nfrom gingado.augmentation import AugmentSDMX\n\n\n\n\nCode\nX = AugmentSDMX(sources={'BIS': 'WS_CBPOL_D', 'ECB': 'CISS'}).fit_transform(FX_rate_changes)\n\n\nQuerying data from BIS's dataflow 'WS_CBPOL' - Central bank policy rates...\nQuerying data from ECB's dataflow 'CISS' - Composite Indicator of Systemic Stress...\n\n\n\n\n\n\n\n\nNote\n\n\n\nit is acceptable in gingado to pass the variable of interest (the “y”, or in this case, FX_rate_changes) as the X argument in fit_transform. This is because this series will also be merged with the additional, augmented data and subsequently lagged along with it.\n\n\nYou can see below that the column names for the newly added columns reflect the source (BIS or ECB), the dataflow (separated from the source by a double underline), and then the specific keys to the series, which are specific to each dataflow.\n\n\nCode\nX.columns\n\n\nIndex(['USDBRL', 'USDGBP', 'USDCHF', 'USDEUR', 'USDCAD', 'USDJPY', 'USDMXN',\n       'BIS__WS_CBPOL_D__CA', 'BIS__WS_CBPOL_D__CH', 'BIS__WS_CBPOL_D__CL',\n       'BIS__WS_CBPOL_D__CN', 'BIS__WS_CBPOL_D__CO', 'BIS__WS_CBPOL_D__CZ',\n       'BIS__WS_CBPOL_D__DK', 'BIS__WS_CBPOL_D__GB', 'BIS__WS_CBPOL_D__HK',\n       'BIS__WS_CBPOL_D__HR', 'BIS__WS_CBPOL_D__HU', 'BIS__WS_CBPOL_D__ID',\n       'BIS__WS_CBPOL_D__IL', 'BIS__WS_CBPOL_D__IN', 'BIS__WS_CBPOL_D__IS',\n       'BIS__WS_CBPOL_D__JP', 'BIS__WS_CBPOL_D__KR', 'BIS__WS_CBPOL_D__KW',\n       'BIS__WS_CBPOL_D__MA', 'BIS__WS_CBPOL_D__MK', 'BIS__WS_CBPOL_D__MX',\n       'BIS__WS_CBPOL_D__MY', 'BIS__WS_CBPOL_D__NO', 'BIS__WS_CBPOL_D__NZ',\n       'BIS__WS_CBPOL_D__PE', 'BIS__WS_CBPOL_D__PH', 'BIS__WS_CBPOL_D__PL',\n       'BIS__WS_CBPOL_D__RO', 'BIS__WS_CBPOL_D__RS', 'BIS__WS_CBPOL_D__RU',\n       'BIS__WS_CBPOL_D__SA', 'BIS__WS_CBPOL_D__SE', 'BIS__WS_CBPOL_D__TH',\n       'BIS__WS_CBPOL_D__TR', 'BIS__WS_CBPOL_D__US', 'BIS__WS_CBPOL_D__XM',\n       'BIS__WS_CBPOL_D__ZA', 'BIS__WS_CBPOL_D__AR', 'BIS__WS_CBPOL_D__AU',\n       'BIS__WS_CBPOL_D__BR', 'ECB__CISS_D__AT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__BE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__CN__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__DE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__ES__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FI__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__FR__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__GB__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IE__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__IT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__NL__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__PT__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_BM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CIN__IDX',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_CO__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_EM__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FI__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_FX__CON',\n       'ECB__CISS_D__U2__Z0Z__4F__EC__SS_MM__CON',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CI__IDX',\n       'ECB__CISS_D__US__Z0Z__4F__EC__SS_CIN__IDX'],\n      dtype='object')\n\n\nBefore proceeding, we also include a differentiated version of the central bank policy data. It will be sparse, since these changes occur infrequently for most central banks, but it can help the model uncover how FX rate changes respond to central bank policy changes.\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nX_diff = X.loc[:, X.columns.str.contains(\"BIS__WS_CBPOL_D\", case=False)].diff()\nX_diff.columns = [col + \"_diff\" for col in X_diff.columns]\nX = pd.concat([X, X_diff], axis=1)\n\n\nThis is how the data looks like now. Note that the names of the added columns reflect the source, dataflow and keys, all separated by underlines (the source is separated from the dataflow by two underlines at all cases). For example, the last key is the jurisdiction of the central bank.\nWe will keep all the newly added variables - even those that are from countries not in the currency list. This is because the model may uncover any relationship of interest between central bank policies from other countries and each particular currency pair.\n\n\nCode\nX.describe().transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nUSDBRL\n5646.0\n0.000163\n0.010440\n-0.080226\n-0.005683\n-0.000105\n0.005456\n0.120503\n\n\nUSDGBP\n5646.0\n0.000064\n0.005993\n-0.038140\n-0.003257\n-0.000091\n0.003209\n0.085019\n\n\nUSDCHF\n5646.0\n-0.000050\n0.006320\n-0.139149\n-0.003201\n0.000082\n0.003198\n0.085326\n\n\nUSDEUR\n5646.0\n0.000026\n0.005666\n-0.039574\n-0.003107\n-0.000080\n0.003017\n0.048493\n\n\nUSDCAD\n5646.0\n0.000013\n0.005689\n-0.043367\n-0.003079\n-0.000129\n0.003018\n0.036864\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nBIS__WS_CBPOL_D__XM_diff\n5645.0\n0.000000\n0.033171\n-0.750000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D__ZA_diff\n5645.0\n-0.001063\n0.061711\n-1.500000\n0.000000\n0.000000\n0.000000\n0.750000\n\n\nBIS__WS_CBPOL_D__AR_diff\n5645.0\n0.004066\n0.869000\n-33.000000\n0.000000\n0.000000\n0.000000\n21.000000\n\n\nBIS__WS_CBPOL_D__AU_diff\n5645.0\n-0.000115\n0.036810\n-1.000000\n0.000000\n0.000000\n0.000000\n0.500000\n\n\nBIS__WS_CBPOL_D__BR_diff\n5645.0\n-0.002081\n0.107656\n-2.500000\n0.000000\n0.000000\n0.000000\n1.500000\n\n\n\n\n109 rows × 8 columns\n\n\n\nThe policy rates for some central banks have less observations than the others, as seen above.\nBecause some data are missing, we will impute data for the missing dates, by simply propagating the last valid observation, and when that is not possible, replacing the missing information with a “0”.\n\n\nCode\nX.fillna(method='pad', inplace=True)\nX.fillna(value=0, inplace=True)\n\n\nNow is a good time to start the model documentation. For this, we can use the standard model card that already comes with gingado.\nThe goal is to facilitate economists who want to make model documentation a part of their normal workflow.\n\n\nCode\nfrom gingado.model_documentation import ModelCard\n\n\n\n\nCode\nmodel_doc = ModelCard()\nmodel_doc.open_questions()\n\n\n['model_details__developer',\n 'model_details__version',\n 'model_details__type',\n 'model_details__info',\n 'model_details__paper',\n 'model_details__citation',\n 'model_details__license',\n 'model_details__contact',\n 'intended_use__primary_uses',\n 'intended_use__primary_users',\n 'intended_use__out_of_scope',\n 'factors__relevant',\n 'factors__evaluation',\n 'metrics__performance_measures',\n 'metrics__thresholds',\n 'metrics__variation_approaches',\n 'evaluation_data__datasets',\n 'evaluation_data__motivation',\n 'evaluation_data__preprocessing',\n 'training_data__training_data',\n 'quant_analyses__unitary',\n 'quant_analyses__intersectional',\n 'ethical_considerations__sensitive_data',\n 'ethical_considerations__human_life',\n 'ethical_considerations__mitigations',\n 'ethical_considerations__risks_and_harms',\n 'ethical_considerations__use_cases',\n 'ethical_considerations__additional_information',\n 'caveats_recommendations__caveats',\n 'caveats_recommendations__recommendations']\n\n\nAs an example, we can add the following information to the model:\n\n\nCode\nmodel_doc.fill_info({\n    'intended_use': {\n        'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n        'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'\n    },\n    'metrics': {\n        'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.'\n    },\n    'ethical_considerations': {\n        'sensitive_data': 'These models were not trained with sensitive data.',\n        'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.'\n    },\n    \n})"
  },
  {
    "objectID": "forecast.html#lagging-the-regressors",
    "href": "forecast.html#lagging-the-regressors",
    "title": "Using gingado to forecast financial series",
    "section": "Lagging the regressors",
    "text": "Lagging the regressors\nThis model will not include any contemporaneous variable. Therefore, all regresors must be lagged.\nFor illustration purposes, we use 5 lags in this exercise.\n\n\nCode\nfrom gingado.utils import Lag\n\n\n\n\nCode\nn_lags = 5\n\nX_lagged = Lag(lags=n_lags).fit_transform(X)\nX_lagged\n\ny = FX_rate_changes[n_lags:]\n\n\nNow is a good opportunity to check by how much we have increased our regressor space:\n\n\nCode\npd.Series({\n    \"FX rates only\": y.shape[1],\n    \"... with augmentation_\": X.shape[1],\n    \"... lagged\": X_lagged.shape[1]\n})\n\n\nFX rates only               7\n... with augmentation_    109\n... lagged                545\ndtype: int64"
  },
  {
    "objectID": "forecast.html#training-the-models",
    "href": "forecast.html#training-the-models",
    "title": "Using gingado to forecast financial series",
    "section": "Training the models",
    "text": "Training the models\nOur dataset is now complete. Before using it to train the models, we hold out the most recent data to serve as our testing dataset, so we can compare our models with real out-of-sample information.\nWe can choose, say, 1st January 2022.\n\n\nCode\ncutoff = '2020-01-01'\n\nX_train, X_test = X_lagged[:cutoff], X_lagged[cutoff:]\ny_train, y_test = y[:cutoff], y[cutoff:]\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'training_data': \n    {'training_data': \n        \"\"\"\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\n        * foreign exchange rates\n        * central bank policy rates\n        * an estimated indicator for systemic stress\n        The training and evaluation datasets are the same time series, only different windows in time.\"\"\"\n    }\n})\n\n\nThe current status of the documentation is:\n\n\nCode\npd.Series(model_doc.show_json())\n\n\nmodel_details              {'developer': 'Person or organisation developi...\nintended_use               {'primary_uses': 'These models are simplified ...\nfactors                    {'relevant': 'Relevant factors', 'evaluation':...\nmetrics                    {'performance_measures': 'Consistent with most...\nevaluation_data            {'datasets': 'Datasets', 'motivation': 'Motiva...\ntraining_data              {'training_data': '\n        The training data ...\nquant_analyses             {'unitary': 'Unitary results', 'intersectional...\nethical_considerations     {'sensitive_data': 'These models were not trai...\ncaveats_recommendations    {'caveats': 'For example, did the results sugg...\ndtype: object\n\n\n\nCreating a random walk benchmark\nRossi (2013) highlights that few predictors beat the random walk without drift model. This is a good opportunity to showcase how we can use gingado’s in-built base class ggdBenchmark to build our customised benchmark model, in this case a random walk.\nThe calculation of the random walk benchmark is very simple. Still, creating a gingado benchmark offers some advantages: it is easier to compare alternative models, and the model documentation is done more seamlessly.\nA custom benchmark model must implement the following steps:\n\nsub-class ggdBenchmark (or alternatively implement its methods)\ndefine an estimator that is compatible with scikit-learn’s API:\n\nat the very least, it has a fit method that returns self\n\n\nIf the user is relying on a custom estimator - like in this case, a random walk estimator to align with the literature - then this custom estimator also has some requirements:\n\nit should ideally subclass scikit-learn’s BaseEstimator (mostly for the get_params / set_params methods)\nthree methods are necessary:\n\nfit, which should at least create an attribute ending in an underline (“_“), so that gingado knows it is fitted\npredict\nscore\n\n\n\n\nCode\nimport numpy as np\nfrom gingado.benchmark import ggdBenchmark\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import TimeSeriesSplit\n\n\n\n\nCode\nclass RandomWalkEstimator(BaseEstimator):\n    def __init__(self, scoring='neg_root_mean_squared_error'):\n        self.scoring = scoring\n    \n    def fit(self, X, y=None):\n        self.n_samples_ = X.shape[0]\n        return self\n\n    def predict(self, X):\n        return np.zeros(X.shape[0])\n\n    def score(self, X, y, sample_weight=None):\n        from sklearn.metrics import mean_squared_error\n        y_pred = self.predict(X)\n        return mean_squared_error(y, y_pred, sample_weight=sample_weight, squared=False)\n\n    def forecast(self, forecast_horizon=1):\n        self.forecast_horizon = forecast_horizon\n        return np.zeros(self.forecast_horizon)\n\nclass RandomWalkBenchmark(ggdBenchmark):\n    def __init__(\n        self, \n        estimator=RandomWalkEstimator(), \n        auto_document=ModelCard,\n        cv=TimeSeriesSplit(n_splits=10, test_size=60), \n        ensemble_method=VotingRegressor, \n        verbose_grid=None):\n        self.estimator=estimator\n        self.auto_document=auto_document\n        self.cv=cv\n        self.ensemble_method=ensemble_method\n        self.verbose_grid=verbose_grid\n\n    def fit(self, X, y=None):\n        self.benchmark=self.estimator\n        self.benchmark.fit(X, y)\n        return self\n\n\n\n\nTraining the candidate models\nNow that we have a benchmark, we can create candidate models that will try to beat it.\nIn this simplified example, we will choose only two: a random forest, an AdaBoost regressor and a Lasso model. Their hyperparameters are not particularly important for the example, but of course they could be fine-tuned as well.\nIn the language of Rossi (2013), the models below are one “single-equation, lagged fundamental model” for each currency.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.linear_model import Lasso\n\n\n\n\nCode\nforest = RandomForestRegressor(n_estimators=250, max_features='log2').fit(X_train, y_train['USDBRL'])\nadaboost = AdaBoostRegressor(n_estimators=150).fit(X_train, y_train['USDBRL'])\nlasso = Lasso(alpha=0.1).fit(X_train, y_train['USDBRL'])\n\nrw = RandomWalkBenchmark().fit(X_train, y_train['USDBRL'])\n\n\nWe can now compare the model results, using the test dataset we held out previously.\nNote that we must pass the criterion against which we are comparing the forecasts.\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\n\n\n\nCode\nresults = rw.compare_fitted_candidates(\n    X_test, y_test['USDBRL'],\n    candidates=[forest, adaboost, lasso],\n    scoring_func=mean_squared_error)\n\npd.Series(results)\n\n\nRandomWalkEstimator()                                           0.000101\nRandomForestRegressor(max_features='log2', n_estimators=250)    0.000104\nAdaBoostRegressor(n_estimators=150)                             0.000109\nLasso(alpha=0.1)                                                0.000101\ndtype: float64\n\n\nAs mentioned above, benchmarks can facilitate the model documentation. In addition to the broader documentation that is already ongoing, each benchmark object create their own where they store model information. We can use that for the broader documentation.\nIn our case, the only parameter we created above during fit is the number of samples: not a particularly informative variable but it was included just for illustration purposes. In any case, the parameter appears in the “model_details” section, item “info”, of the benchmark’s rw documentation. Similarly, the parameters of more fully-fledged estimators also appear in that section.\n\n\nCode\nrw.document()\n\nrw.model_documentation.show_json()['model_details']['info']\n\n\n{'n_samples_': 4314}\n\n\n\n\nCode\nmodel_doc.fill_info({\n    'model_details': {'info': rw.model_documentation.show_json()['model_details']['info']}\n})\n\n\n\n\nCode\nmodel_doc.show_json()\n\n\n{'model_details': {'developer': 'Person or organisation developing the model',\n  'datetime': '2025-03-06 11:42:07 ',\n  'version': 'Model version',\n  'type': 'Model type',\n  'info': {'n_samples_': 4314},\n  'paper': 'Paper or other resource for more information',\n  'citation': 'Citation details',\n  'license': 'License',\n  'contact': 'Where to send questions or comments about the model'},\n 'intended_use': {'primary_uses': 'These models are simplified toy models made to illustrate the use of gingado',\n  'primary_users': 'Primary intended users',\n  'out_of_scope': 'These models were not constructed for decision-making and as such their use as predictors in real life decisions is strongly discouraged and out of scope.'},\n 'factors': {'relevant': 'Relevant factors',\n  'evaluation': 'Evaluation factors'},\n 'metrics': {'performance_measures': 'Consistent with most papers reviewed by Rossi (2013), these models were evaluated by their root mean squared error.',\n  'thresholds': 'Decision thresholds',\n  'variation_approaches': 'Variation approaches'},\n 'evaluation_data': {'datasets': 'Datasets',\n  'motivation': 'Motivation',\n  'preprocessing': 'Preprocessing'},\n 'training_data': {'training_data': '\\n        The training data comprise time series obtained from official sources (BIS and ECB) on:\\n        * foreign exchange rates\\n        * central bank policy rates\\n        * an estimated indicator for systemic stress\\n        The training and evaluation datasets are the same time series, only different windows in time.'},\n 'quant_analyses': {'unitary': 'Unitary results',\n  'intersectional': 'Intersectional results'},\n 'ethical_considerations': {'sensitive_data': 'These models were not trained with sensitive data.',\n  'human_life': 'The models do not involve the collection or use of individual-level data, and have no foreseen impact on human life.',\n  'mitigations': 'What risk mitigation strategies were used during model development?',\n  'risks_and_harms': 'What risks may be present in model usage? Try to identify the potential recipients,likelihood, and magnitude of harms. If these cannot be determined, note that they were considered but remain unknown',\n  'use_cases': 'Are there any known model use cases that are especially fraught?',\n  'additional_information': 'If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community.'},\n 'caveats_recommendations': {'caveats': 'For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset?',\n  'recommendations': 'Are there additional recommendations for model use? What are the ideal characteristics of an evaluation dataset for this model?'}}\n\n\nWe can save the documentation to disk in JSON format with model_doc.save_json(), or parse it to create other documents (eg, a PDF file) using third-party libraries."
  },
  {
    "objectID": "forecast.html#references",
    "href": "forecast.html#references",
    "title": "Using gingado to forecast financial series",
    "section": "References",
    "text": "References\n\n\nBank for International Settlements. 2017. “Recent Enhancements to the BIS Statistics.” BIS Quarterly Review. Vol. September. https://www.bis.org/publ/qtrpdf/r_qt1709c.htm.\n\n\nHollo, Daniel, Manfred Kremer, and Marco Lo Duca. 2012. “CISS-a Composite Indicator of Systemic Stress in the Financial System.”\n\n\nRossi, Barbara. 2013. “Exchange Rate Predictability.” Journal of Economic Literature 51 (4): 1063–1119."
  },
  {
    "objectID": "dataset_transformation.html",
    "href": "dataset_transformation.html",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('gingado/data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/data/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "dataset_transformation.html#barro-and-lee-1994",
    "href": "dataset_transformation.html#barro-and-lee-1994",
    "title": "Dataset transformation for uploading",
    "section": "",
    "text": "For more information on this data, consult the datasets page.\n\nimport pandas as pd\nfrom scipy import io\n\ngrowth_data = io.loadmat('gingado/data/GrowthData.mat')\ncolnames = [m[0].strip() for m in growth_data['Mnem'][0]]\ndf = pd.DataFrame(growth_data['data'], columns=colnames)\n\ndf.to_csv('gingado/data/dataset_BarroLee_1994.csv')\n\n\npd.set_option('display.max_rows', None)\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ngdpsh465\n90.0\n7.702907\n0.896179\n5.762051\n7.131539\n7.725700\n8.441914\n9.229849\n\n\nbmp1l\n90.0\n0.168747\n0.249116\n0.000000\n0.000000\n0.063800\n0.274550\n1.637800\n\n\nfreeop\n90.0\n0.220102\n0.074861\n0.078488\n0.166044\n0.203972\n0.286425\n0.416234\n\n\nfreetar\n90.0\n0.028334\n0.021855\n0.000000\n0.011589\n0.025426\n0.039745\n0.109921\n\n\nh65\n90.0\n0.111556\n0.101361\n0.002000\n0.032250\n0.089000\n0.147500\n0.573000\n\n\nhm65\n90.0\n0.137156\n0.116826\n0.004000\n0.042250\n0.114500\n0.181000\n0.635000\n\n\nhf65\n90.0\n0.082233\n0.091549\n0.000000\n0.014000\n0.055000\n0.113000\n0.527000\n\n\np65\n90.0\n0.893333\n0.164938\n0.290000\n0.832500\n0.985000\n1.000000\n1.000000\n\n\npm65\n90.0\n0.919556\n0.134632\n0.370000\n0.882500\n1.000000\n1.000000\n1.000000\n\n\npf65\n90.0\n0.849556\n0.210899\n0.210000\n0.762500\n0.970000\n1.000000\n1.000000\n\n\ns65\n90.0\n0.406556\n0.247219\n0.020000\n0.182500\n0.395000\n0.555000\n0.910000\n\n\nsm65\n90.0\n0.436222\n0.246543\n0.030000\n0.220000\n0.420000\n0.607500\n0.950000\n\n\nsf65\n90.0\n0.379778\n0.265800\n0.010000\n0.140000\n0.345000\n0.530000\n0.920000\n\n\nfert65\n90.0\n4.742000\n1.974886\n1.450000\n2.845000\n5.060000\n6.560000\n8.000000\n\n\nmort65\n90.0\n0.070467\n0.047166\n0.009000\n0.024000\n0.064500\n0.110750\n0.183000\n\n\nlifee065\n90.0\n4.102025\n0.167895\n3.693867\n3.994061\n4.110874\n4.258443\n4.317488\n\n\ngpop1\n90.0\n0.021301\n0.009851\n0.002600\n0.013150\n0.022900\n0.029900\n0.039000\n\n\nfert1\n90.0\n4.962444\n1.936886\n1.742000\n2.906500\n5.394000\n6.680000\n8.000000\n\n\nmort1\n90.0\n0.087211\n0.051937\n0.015000\n0.031500\n0.085000\n0.131000\n0.204000\n\n\ninvsh41\n90.0\n0.196663\n0.086981\n0.027133\n0.129175\n0.191680\n0.254760\n0.475500\n\n\ngeetot1\n90.0\n0.035594\n0.014543\n0.011700\n0.024325\n0.033900\n0.046400\n0.077000\n\n\ngeerec1\n90.0\n0.029782\n0.012673\n0.004100\n0.020925\n0.028350\n0.038250\n0.068700\n\n\ngde1\n90.0\n0.032133\n0.033750\n0.005000\n0.015250\n0.020500\n0.039750\n0.251000\n\n\ngovwb1\n90.0\n0.126661\n0.045588\n0.061500\n0.096000\n0.120950\n0.145125\n0.352300\n\n\ngovsh41\n90.0\n0.155161\n0.061317\n0.035500\n0.118350\n0.147250\n0.183850\n0.385900\n\n\ngvxdxe41\n90.0\n0.094915\n0.053846\n0.010000\n0.058965\n0.086585\n0.122365\n0.310460\n\n\nhigh65\n90.0\n4.203000\n5.252551\n0.120000\n1.302500\n2.735000\n4.952500\n30.900000\n\n\nhighm65\n90.0\n5.531556\n5.908117\n0.230000\n1.890000\n3.990000\n6.525000\n33.400000\n\n\nhighf65\n90.0\n2.946889\n4.781800\n0.010000\n0.667500\n1.320000\n2.992500\n28.500000\n\n\nhighc65\n90.0\n2.455556\n2.487163\n0.090000\n1.007500\n1.695000\n3.135000\n15.630000\n\n\nhighcm65\n90.0\n3.434222\n3.111632\n0.180000\n1.347500\n2.445000\n4.517500\n18.830000\n\n\nhighcf65\n90.0\n1.529667\n2.066383\n0.010000\n0.432500\n0.765000\n1.925000\n12.770000\n\n\nhuman65\n90.0\n4.214878\n2.530420\n0.301000\n2.212000\n3.803500\n5.674000\n11.158000\n\n\nhumanm65\n90.0\n4.698267\n2.423311\n0.568000\n2.888000\n4.244500\n6.030250\n11.535000\n\n\nhumanf65\n90.0\n3.745967\n2.692534\n0.043000\n1.605250\n3.194000\n5.178250\n10.798000\n\n\nhyr65\n90.0\n0.133178\n0.151704\n0.004000\n0.046500\n0.087000\n0.161500\n0.829000\n\n\nhyrm65\n90.0\n0.179300\n0.177153\n0.008000\n0.066250\n0.125500\n0.215250\n0.960000\n\n\nhyrf65\n90.0\n0.089533\n0.133602\n0.000000\n0.021250\n0.043500\n0.095000\n0.712000\n\n\nno65\n90.0\n34.723111\n29.101051\n0.000000\n4.527500\n31.110000\n59.075000\n89.460000\n\n\nnom65\n90.0\n28.999889\n25.307305\n0.000000\n4.227500\n25.180000\n49.545000\n82.350000\n\n\nnof65\n90.0\n40.327111\n33.446850\n0.000000\n4.275000\n36.400000\n69.550000\n98.610000\n\n\npinstab1\n90.0\n0.114160\n0.214415\n0.000000\n0.000000\n0.000706\n0.103919\n1.068500\n\n\npop65\n90.0\n39825.300000\n87794.312844\n1482.000000\n4961.000000\n12275.000000\n42263.500000\n620701.000000\n\n\nworker65\n90.0\n0.369268\n0.069487\n0.215600\n0.311050\n0.369400\n0.410075\n0.526000\n\n\npop1565\n90.0\n0.375158\n0.093359\n0.201800\n0.280600\n0.421100\n0.456325\n0.515800\n\n\npop6565\n90.0\n0.059175\n0.037660\n0.021548\n0.031189\n0.036967\n0.086604\n0.151105\n\n\nsec65\n90.0\n15.318000\n13.039270\n0.450000\n5.700000\n11.200000\n18.865000\n57.100000\n\n\nsecm65\n90.0\n16.622111\n12.421347\n0.750000\n7.730000\n12.780000\n22.307500\n56.370000\n\n\nsecf65\n90.0\n14.024111\n14.121763\n0.170000\n3.747500\n8.865000\n16.840000\n57.800000\n\n\nsecc65\n90.0\n6.444111\n6.351368\n0.130000\n2.352500\n4.005000\n7.870000\n34.090000\n\n\nseccm65\n90.0\n6.702889\n6.182263\n0.150000\n2.635000\n4.735000\n8.910000\n31.270000\n\n\nseccf65\n90.0\n6.171778\n6.857261\n0.040000\n1.500000\n3.870000\n8.042500\n36.610000\n\n\nsyr65\n90.0\n0.912422\n0.823222\n0.033000\n0.357750\n0.678500\n1.130750\n4.211000\n\n\nsyrm65\n90.0\n1.045444\n0.831756\n0.057000\n0.467500\n0.770500\n1.246750\n4.227000\n\n\nsyrf65\n90.0\n0.783767\n0.839486\n0.010000\n0.215250\n0.512500\n1.002000\n4.198000\n\n\nteapri65\n90.0\n33.203333\n9.818516\n18.200000\n27.425000\n32.200000\n37.475000\n62.400000\n\n\nteasec65\n90.0\n19.412222\n6.384194\n7.200000\n15.350000\n18.400000\n22.800000\n37.100000\n\n\nex1\n90.0\n0.133981\n0.118708\n0.017800\n0.063450\n0.092300\n0.169225\n0.747000\n\n\nim1\n90.0\n0.144356\n0.120937\n0.022200\n0.070625\n0.116300\n0.181475\n0.848900\n\n\nxr65\n90.0\n42.663856\n119.335089\n0.003000\n1.099000\n4.762000\n19.619000\n652.850000\n\n\ntot1\n90.0\n0.009236\n0.059182\n-0.156878\n-0.016877\n0.004890\n0.018526\n0.207492\n\n\nOutcome\n90.0\n0.045349\n0.051314\n-0.100990\n0.021045\n0.046209\n0.074029\n0.185526"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to gingado",
    "section": "",
    "text": "Welcome, and thank you for your interest in contributing to gingado! Whether it’s reporting issues, suggesting new features, or contributing to the code, documentation, or tests, your involvement is highly appreciated.\n\n\n\n\nTo get started with contributing to gingado, you need to set up your development environment. This includes installing necessary tools and configuring your system to work efficiently with our codebase.\nTo install the dependencies to work with gingado, you need to install the execution dependencies and the development dependencies of gingado:\npip install -r requirements.txt\npip install -r dev_requirements.txt\nTo work with the documentation an tests, we use quarto. You can either use RStudio or Visual Studio Code with the Quarto extension installed for editing .qmd files which are used for generating documentation. Here’s how you can set up your environment:\n\nInstall Quarto: If you haven’t already, install Quarto from Quarto’s official website. Follow the instructions for your operating system.\nConfigure Your Editor:\n\nFor RStudio: Quarto is integrated with RStudio. Ensure you have the latest version of RStudio to work with Quarto seamlessly.\nFor Visual Studio Code: Install the Quarto extension from the Visual Studio Code marketplace. This extension provides support for .qmd files, including syntax highlighting and preview capabilities.\n\n\n\n\n\nTo modify the index page or the README.md file in the repository root, please only edit the index.qmd file. All changes in the index.qmd file will be rendered to the README.md file and moved automatically to the repository root via a post-render script move_readme_to_root.py.\n\n\n\nIf you encounter a bug, have suggestions, or want to propose new functionalities:\n\nCheck Existing Issues: Ensure the bug or suggestion hasn’t been reported/mentioned before by searching under Issues on GitHub.\nCreate a New Issue: If no existing issue addresses the problem or suggestion, please create a new issue, providing a descriptive title, a clear description, and as much relevant information as possible. For bugs, include a code sample or an executable test case demonstrating the expected behavior that is not occurring, along with complete error messages.\n\n\n\n\n\n\nTo contribute changes to the codebase, including documentation and tests, follow these guidelines:\n\nDocument New Features: Clearly document new functions or classes in the .qmd files. Write clear descriptions, specify expected inputs and outputs, and include any relevant information to understand the functionality.\nInclude Tests: Implement tests for new functionalities as part of the .qmd files to ensure the integrity and reliability of the code. Make sure your tests cover the expected behavior and edge cases.\n\n\n\n\n\nFocused PRs: Each PR should be focused on a single topic. Avoid combining unrelated changes.\nSeparate Style and Functional Changes: Do not mix style changes with functional changes in the same PR.\nPreserve File Style: Avoid adding or removing vertical whitespace unnecessarily. Keep the original style of the files you edit.\nDevelopment Process: Do not use a submitted PR as a development playground. If additional work is needed, consider closing the PR, completing the work, and then submitting a new PR.\nResponding to Feedback: If your PR requires changes based on feedback, continue committing to the same PR unless the changes are substantial. In that case, it might be better to start a new PR.\n\n\n\n\n\nWhen contributing to the documentation, ensure your contributions are made within .qmd files. This is essential for the changes to be correctly reflected in the generated documentation through Quarto.\n\n\n\n\nBy contributing to gingado, you are part of a community that values collaboration, innovation, and learning. We look forward to your contributions and are excited to see what we can build together."
  },
  {
    "objectID": "CONTRIBUTING.html#getting-started",
    "href": "CONTRIBUTING.html#getting-started",
    "title": "Contributing to gingado",
    "section": "",
    "text": "To get started with contributing to gingado, you need to set up your development environment. This includes installing necessary tools and configuring your system to work efficiently with our codebase.\nTo install the dependencies to work with gingado, you need to install the execution dependencies and the development dependencies of gingado:\npip install -r requirements.txt\npip install -r dev_requirements.txt\nTo work with the documentation an tests, we use quarto. You can either use RStudio or Visual Studio Code with the Quarto extension installed for editing .qmd files which are used for generating documentation. Here’s how you can set up your environment:\n\nInstall Quarto: If you haven’t already, install Quarto from Quarto’s official website. Follow the instructions for your operating system.\nConfigure Your Editor:\n\nFor RStudio: Quarto is integrated with RStudio. Ensure you have the latest version of RStudio to work with Quarto seamlessly.\nFor Visual Studio Code: Install the Quarto extension from the Visual Studio Code marketplace. This extension provides support for .qmd files, including syntax highlighting and preview capabilities.\n\n\n\n\n\nTo modify the index page or the README.md file in the repository root, please only edit the index.qmd file. All changes in the index.qmd file will be rendered to the README.md file and moved automatically to the repository root via a post-render script move_readme_to_root.py.\n\n\n\nIf you encounter a bug, have suggestions, or want to propose new functionalities:\n\nCheck Existing Issues: Ensure the bug or suggestion hasn’t been reported/mentioned before by searching under Issues on GitHub.\nCreate a New Issue: If no existing issue addresses the problem or suggestion, please create a new issue, providing a descriptive title, a clear description, and as much relevant information as possible. For bugs, include a code sample or an executable test case demonstrating the expected behavior that is not occurring, along with complete error messages.\n\n\n\n\n\n\nTo contribute changes to the codebase, including documentation and tests, follow these guidelines:\n\nDocument New Features: Clearly document new functions or classes in the .qmd files. Write clear descriptions, specify expected inputs and outputs, and include any relevant information to understand the functionality.\nInclude Tests: Implement tests for new functionalities as part of the .qmd files to ensure the integrity and reliability of the code. Make sure your tests cover the expected behavior and edge cases.\n\n\n\n\n\nFocused PRs: Each PR should be focused on a single topic. Avoid combining unrelated changes.\nSeparate Style and Functional Changes: Do not mix style changes with functional changes in the same PR.\nPreserve File Style: Avoid adding or removing vertical whitespace unnecessarily. Keep the original style of the files you edit.\nDevelopment Process: Do not use a submitted PR as a development playground. If additional work is needed, consider closing the PR, completing the work, and then submitting a new PR.\nResponding to Feedback: If your PR requires changes based on feedback, continue committing to the same PR unless the changes are substantial. In that case, it might be better to start a new PR.\n\n\n\n\n\nWhen contributing to the documentation, ensure your contributions are made within .qmd files. This is essential for the changes to be correctly reflected in the generated documentation through Quarto."
  },
  {
    "objectID": "CONTRIBUTING.html#your-contributions-make-a-difference",
    "href": "CONTRIBUTING.html#your-contributions-make-a-difference",
    "title": "Contributing to gingado",
    "section": "",
    "text": "By contributing to gingado, you are part of a community that values collaboration, innovation, and learning. We look forward to your contributions and are excited to see what we can build together."
  },
  {
    "objectID": "NOTES.html",
    "href": "NOTES.html",
    "title": "gingado",
    "section": "",
    "text": "Our software is provided under the Apache 2.0 License, specifically applying to the code we have authored. Dependencies are listed as information only, to facilitate installation and operation by the end user. While we strive to ensure all our dependencies are compliant with the Apache 2.0 license, given the dynamic nature of software licensing—where packages may change licenses across versions—and particularly with the use of open-ended version ranges (&gt;=), we cannot ensure perpetual license compliance of these dependencies.\nUsers are responsible for verifying the compliance of each dependency’s license in relation to their project requirements. This includes conducting due diligence on the licensing of dependencies, especially considering potential changes in license terms with version updates. The onus to ensure full legal compliance with the licenses of all dependencies used lies with the user. We recommend consulting with legal expertise for any license compatibility concerns.\nWe commit to the open-source ethos of transparent and responsible software development. Should you have questions or need clarification regarding the licensing of dependencies, feel free to reach out."
  },
  {
    "objectID": "NOTES.html#license-disclaimer",
    "href": "NOTES.html#license-disclaimer",
    "title": "gingado",
    "section": "",
    "text": "Our software is provided under the Apache 2.0 License, specifically applying to the code we have authored. Dependencies are listed as information only, to facilitate installation and operation by the end user. While we strive to ensure all our dependencies are compliant with the Apache 2.0 license, given the dynamic nature of software licensing—where packages may change licenses across versions—and particularly with the use of open-ended version ranges (&gt;=), we cannot ensure perpetual license compliance of these dependencies.\nUsers are responsible for verifying the compliance of each dependency’s license in relation to their project requirements. This includes conducting due diligence on the licensing of dependencies, especially considering potential changes in license terms with version updates. The onus to ensure full legal compliance with the licenses of all dependencies used lies with the user. We recommend consulting with legal expertise for any license compatibility concerns.\nWe commit to the open-source ethos of transparent and responsible software development. Should you have questions or need clarification regarding the licensing of dependencies, feel free to reach out."
  }
]