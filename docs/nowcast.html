<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="A simple, mixed-frequency example">

<title>Nowcasting inflation with neural networks – gingado</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Nowcasting inflation with neural networks – gingado">
<meta property="og:description" content="A simple, mixed-frequency example">
<meta property="og:site_name" content="gingado">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">gingado</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-examples" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Examples</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-examples">    
        <li>
    <a class="dropdown-item" href="./barrolee1994.html">
 <span class="dropdown-text">Economic growth</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./forecast.html">
 <span class="dropdown-text">Forecasting FX rates</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./machine_controls.html">
 <span class="dropdown-text">Effects of labour reform</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-reference" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Reference</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-reference">    
        <li>
    <a class="dropdown-item" href="./augmentation.html">
 <span class="dropdown-text">gingado.augmentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./datasets.html">
 <span class="dropdown-text">gingado.datasets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./benchmark.html">
 <span class="dropdown-text">gingado.benchmark</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./estimators.html">
 <span class="dropdown-text">gingado.estimators</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./documentation.html">
 <span class="dropdown-text">gingado.model_documentation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./utils.html">
 <span class="dropdown-text">gingado.utils</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/bis-med-it/gingado"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.sdmx.io"> <i class="bi bi-house-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#roadmap" id="toc-roadmap" class="nav-link active" data-scroll-target="#roadmap">Roadmap</a></li>
  <li><a href="#sec-data" id="toc-sec-data" class="nav-link" data-scroll-target="#sec-data">Loading the data</a>
  <ul>
  <li><a href="#inflation" id="toc-inflation" class="nav-link" data-scroll-target="#inflation">Inflation</a></li>
  <li><a href="#oil-prices" id="toc-oil-prices" class="nav-link" data-scroll-target="#oil-prices">Oil prices</a></li>
  <li><a href="#temporal-features-not-implemented-for-the-time-being" id="toc-temporal-features-not-implemented-for-the-time-being" class="nav-link" data-scroll-target="#temporal-features-not-implemented-for-the-time-being">Temporal features (not implemented for the time being)</a></li>
  <li><a href="#splitting-the-dataset" id="toc-splitting-the-dataset" class="nav-link" data-scroll-target="#splitting-the-dataset">Splitting the dataset</a></li>
  </ul></li>
  <li><a href="#sec-fc" id="toc-sec-fc" class="nav-link" data-scroll-target="#sec-fc">First model: a fully connected neural network</a></li>
  <li><a href="#sec-gate" id="toc-sec-gate" class="nav-link" data-scroll-target="#sec-gate">A useful tool: a gate</a></li>
  <li><a href="#sec-lstm" id="toc-sec-lstm" class="nav-link" data-scroll-target="#sec-lstm">Second model: Long short-term memory</a></li>
  <li><a href="#sec-gates" id="toc-sec-gates" class="nav-link" data-scroll-target="#sec-gates">Introducing… the gatekeepers</a></li>
  <li><a href="#sec-transf" id="toc-sec-transf" class="nav-link" data-scroll-target="#sec-transf">Now is a(nother) good time to pay attention</a></li>
  <li><a href="#sec-tftmf" id="toc-sec-tftmf" class="nav-link" data-scroll-target="#sec-tftmf">Complete architecture</a></li>
  <li><a href="#sec-nowcast" id="toc-sec-nowcast" class="nav-link" data-scroll-target="#sec-nowcast">Nowcasting inflation with a simple model</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/bis-med-it/gingado/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Nowcasting inflation with neural networks</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>

<div>
  <div class="description">
    A simple, mixed-frequency example
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Douglas K. G. Araujo </p>
             <p>Johannes Damp </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>This notebook showcases how to set up neural networks to nowcast inflation using data measured in different frequencies. The goal here is to start with a very simple dataset containing only two variables, inflation (monthly) and oil prices (daily), to slowly build up a more complex neural network based nowcasting model, the TFT-MF available in <code>gingado</code> from its v0.3.0.</p>
<p>Nowcasting is essentially the use of the most current information possible to estimate in real time an economic series of interest such as inflation or GDP before it is actually released<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. For example, if you could measure all prices every day, you could create on the last day of the month a very accurate nowcast for the headline inflation for that month - which would only be officialy published a few days later. In the case of GDP, this lag between the end of the reference period and actual publication tends to be significant, around 6-10 weeks. For policymakers, investors and other decisionmakers, a lot can happen in this period.</p>
<p>A related use of nowcasting is to estimate what the current period’s reading will be as this period rolls out. In other words, estimating today what the inflation reading for this month (or GDP for this quarter) will likely be as new information is unveiled in real time.</p>
<p>The nowcasting model available in <code>gingado</code> from v0.3.0 onwards is an adjusted version of the Temporal Fusion Transformer (TFT) of <span class="citation" data-cites="lim2021temporal">Lim et al. (<a href="#ref-lim2021temporal" role="doc-biblioref">2021</a>)</span>. This architecture combines <em>flexibility</em> to take on multiple datasets while learning which information to focus on and <em>interpretability</em> to provide insights on the important variables in each case.</p>
<section id="roadmap" class="level2">
<h2 class="anchored" data-anchor-id="roadmap">Roadmap</h2>
<p>The TFT model can be a bit complex to understand at first, so we will build it up, step by step. After loading the data in <a href="#sec-data" class="quarto-xref">Section&nbsp;2</a>, the most basic neural network - a neuron layer - is presented in <a href="#sec-fc" class="quarto-xref">Section&nbsp;3</a>. This is followed by an architecture that is more suitable for time series in <a href="#sec-lstm" class="quarto-xref">Section&nbsp;5</a>. Next, these elements are combined in <a href="#sec-gates" class="quarto-xref">Section&nbsp;6</a> to show how the model knows what to focus on. The next individual element is the self-attention layer in <a href="#sec-transf" class="quarto-xref">Section&nbsp;7</a>. Finally, if you want to see the full picture directly, go to <a href="#sec-tftmf" class="quarto-xref">Section&nbsp;8</a> to see how these elements are put together. <a href="#sec-nowcast" class="quarto-xref">Section&nbsp;9</a> then trains the model and presents the results for this simple, illustrative nowcasting.</p>
</section>
<section id="sec-data" class="level2">
<h2 class="anchored" data-anchor-id="sec-data">Loading the data</h2>
<p>Let’s use our SDMX connectors to find and download data from official sources in a reproducible way.</p>
<p>To abstract from currency issues, we will use US inflation and oil prices, which are denominated in US dollars.</p>
<div id="load-packages" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sdmx</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gingado.utils <span class="im">import</span> load_SDMX_data</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> TimeSeriesSplit</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="inflation" class="level3">
<h3 class="anchored" data-anchor-id="inflation">Inflation</h3>
<p>Since this is a monthly nowcast of inflation, the best way to do this is to use a <em>monthly change in the consumer price index</em>, <span class="math inline">\(\pi_t^{(m)}=(\text{CPI}_t - \text{CPI}_{t-1})/\text{CPI}_{t-1}\)</span>, not the year-on-year rate, <span class="math inline">\(\pi_t^{(y)}=(\text{CPI}_t - \text{CPI}_{t-12})/\text{CPI}_{t-12}\)</span>, which is how people usually think of inflation. This is because we want to nowcast only the value at the margin; 11 twelths of <span class="math inline">\(\pi_t^{(y)}\)</span> are already known, since <span class="math inline">\(\pi_t^{(y)} = -1 + \prod_{l=0}^{11} (1+\pi_{t-l})\)</span>.</p>
<p>Then, only at the end we combine rolling windows of 12 consecutive monthly inflation rates, of which only the last one or two are estimated, to correctly create an annual inflation rate.</p>
<p>Formally, if we know all values except the current and last month’s, then: <span id="eq-finalnowcast"><span class="math display">\[
\hat{\pi}_t^{(y)}=(\prod_{l=0}^1 (1+\hat{\pi}_{t-l}^{(m)}) \prod_{l=2}^{11} (1+\pi_{t-l}^{(m)}) )-1,
\tag{1}\]</span></span></p>
<p>where the hat notation (<span class="math inline">\(\hat{ }\)</span>) means that a particular value was estimated.</p>
<p>For inflation, we take a dataflow from the <a href="https://data.bis.org/topics/CPI">BIS</a>, since we are looking for US data. Let’s explore it first and then choose the correct data specifications to download the time series.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div id="inflation-dataflow" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>BIS <span class="op">=</span> sdmx.Client(<span class="st">"BIS"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>cpi_msg <span class="op">=</span> BIS.dataflow(<span class="st">'WS_LONG_CPI'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>cpi_dsd <span class="op">=</span> cpi_msg.structure</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>These are all possible keys:</p>
<div id="cell-CPI_dimensions" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.components</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="cpi_dimensions" class="cell-output cell-output-display" data-execution_count="3">
<pre><code>[&lt;Dimension FREQ&gt;,
 &lt;Dimension REF_AREA&gt;,
 &lt;Dimension UNIT_MEASURE&gt;,
 &lt;TimeDimension TIME_PERIOD&gt;]</code></pre>
</div>
</div>
<p>For example, “FREQ” (frequency) takes in these values:</p>
<div id="cell-FREQ_codelist" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cl__FREQ <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"FREQ"</span>).local_representation.enumerated)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>cl__FREQ</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="freq_codelist" class="cell-output cell-output-display" data-execution_count="4">
<pre><code>CL_FREQ
A                                   Annual
B    Daily - business week (not supported)
D                                    Daily
E                    Event (not supported)
H                              Half-yearly
M                                  Monthly
Q                                Quarterly
W                                   Weekly
Name: Code list for Frequency (FREQ), dtype: object</code></pre>
</div>
</div>
<p>And “REF_AREA” (reference area) can be set to:</p>
<div id="cell-REF_AREA_codelist" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"REF_AREA"</span>).local_representation.enumerated)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="ref_area_codelist" class="cell-output cell-output-display" data-execution_count="5">
<pre><code>CL_AREA
1X                                      ECB
4T    Emerging market economies (aggregate)
5A                  All reporting economies
5R                       Advanced economies
AE                     United Arab Emirates
                      ...                  
VN                                  Vietnam
XM                                Euro area
XW                                    World
ZA                             South Africa
_Z                           Not applicable
Name: Reference area code list, Length: 101, dtype: object</code></pre>
</div>
</div>
<p>We can check that the US is amongst the reference areas:</p>
<p>::: {#cell-check US in REF_AREA codelist .cell execution_count=6}</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA[<span class="st">'US'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="check-us-in-ref_area-codelist" class="cell-output cell-output-display" data-execution_count="6">
<pre><code>'United States'</code></pre>
</div>
<p>:::</p>
<p>Finally, the “UNIT_MEASURE” values can be:</p>
<p>::: {#cell-codelist for UNIT_MEASURE in dataflow BIS__WS_LONG_CPI .cell execution_count=7}</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"UNIT_MEASURE"</span>).local_representation.enumerated)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="codelist-for-unit_measure-in-dataflow-bis__ws_long_cpi" class="cell-output cell-output-display" data-execution_count="7">
<pre><code>CL_BIS_UNIT
000                     Unknown
001                 100 - yield
002                   EUR / MWh
003     Index, 1996 Jan 2 = 100
004           Euro / troy ounce
                 ...           
ZAR                        Rand
ZMK              Zambian Kwacha
ZMW              Zambian Kwacha
ZWD             Zimbabwe Dollar
ZWL    Zimbabwe Dollar (fourth)
Name: BIS_Unit, Length: 1051, dtype: object</code></pre>
</div>
<p>:::</p>
<p>In the <a href="https://data.bis.org/topics/CPI#faq">BIS website for this data</a>, we can see that the unit in levels is <code>Index, 2010 = 100</code> (the other one is <code>Year-on-year changes, in per cent</code>, which as discussed above we don’t want for this case.)</p>
<p>::: {#cell-finding code for index .cell execution_count=8}</p>
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE[cl__UNIT_MEASURE.<span class="bu">str</span>.contains(<span class="st">"Index, 2010 = 100"</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="finding-code-for-index" class="cell-output cell-output-display" data-execution_count="8">
<pre><code>CL_BIS_UNIT
628    Index, 2010 = 100
Name: BIS_Unit, dtype: object</code></pre>
</div>
<p>:::</p>
<p>Armed with this knowledge, we can now download monthly consumer price index data for the US. Let’s start after 1985 so that we have a sufficiently long history but without too much influence of the tectonic shift of the US dollar devaluation in the early 1970s and ensuing high inflation:</p>
<div id="cell-fig-cpi" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df_infl <span class="op">=</span> load_SDMX_data(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    sources<span class="op">=</span>{<span class="st">"BIS"</span>: <span class="st">"'WS_LONG_CPI'"</span>},</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    keys<span class="op">=</span>{<span class="st">"FREQ"</span>: <span class="st">"M"</span>, <span class="st">"REF_AREA"</span>: <span class="st">"US"</span>, <span class="st">"UNIT_MEASURE"</span>: <span class="st">"628"</span>},</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>{<span class="st">"startPeriod"</span>: <span class="dv">1985</span>}</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>df_infl.plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Querying data from BIS's dataflow 'WS_LONG_CPI' - BIS long consumer prices...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-cpi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cpi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: US consumer price index, 2010 = 100
</figcaption>
<div aria-describedby="fig-cpi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-cpi-output-2.png" width="585" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>As you can see in <a href="#fig-cpi" class="quarto-xref">Figure&nbsp;1</a>, we downloaded the series <span class="math inline">\(\{\text{CPI}_t\}\)</span>. Transforming that into <span class="math inline">\(\{\pi_t\}\)</span>, defined above, we have:</p>
<div id="cell-fig-pi" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>df_infl_m <span class="op">=</span> df_infl.pct_change().dropna()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>df_infl_m.index <span class="op">=</span> df_infl_m.index <span class="op">+</span> pd.offsets.MonthEnd(<span class="dv">0</span>) <span class="co"># move to month end</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>df_infl_m.plot(ax<span class="op">=</span>ax)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-pi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-pi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: US monthly inflation rate
</figcaption>
<div aria-describedby="fig-pi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-pi-output-1.png" width="599" height="403" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="oil-prices" class="level3">
<h3 class="anchored" data-anchor-id="oil-prices">Oil prices</h3>
<p>Since the focus is on US inflation, below we get WTI oil prices. This data is downloaded from the <a href="https://fred.stlouisfed.org/series/DCOILWTICO">St Louis Fed’s FRED webpage</a>.</p>
<div id="cell-fig-oil" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>df_oil <span class="op">=</span> pd.read_csv(<span class="st">"docs/DCOILWTICO.csv"</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DCOILWTICO'</span>] <span class="op">=</span> pd.to_numeric(df_oil[<span class="st">'DCOILWTICO'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DATE'</span>] <span class="op">=</span> pd.to_datetime(df_oil[<span class="st">'DATE'</span>])</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>df_oil.set_index(<span class="st">'DATE'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>df_oil.plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-oil" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-oil-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: WTI oil prices
</figcaption>
<div aria-describedby="fig-oil-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-oil-output-1.png" width="577" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>For the nowcasting, we are interested in the daily variation, clipped because of the sharp movements during the onset of the Covid-19 pandemic:</p>
<div id="cell-fig-oilD" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>df_oil_d <span class="op">=</span> df_oil.pct_change().dropna()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>df_oil_d.plot(ax<span class="op">=</span>ax)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.25</span>, <span class="fl">0.25</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-oild" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-oild-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Daily change in WTI oil prices
</figcaption>
<div aria-describedby="fig-oild-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-oild-output-1.png" width="582" height="401" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="temporal-features-not-implemented-for-the-time-being" class="level3">
<h3 class="anchored" data-anchor-id="temporal-features-not-implemented-for-the-time-being">Temporal features (not implemented for the time being)</h3>
<p>There is a lot of information encoded in the temporal features of a time series: which day in the month it is, which month of the year, etc. For example, consider how consumers behave differently in response to oil prices over warmer months (when many decide or not to travel, and how far) compared to colder months (when energy prices factor in heating and is thus perhaps less elastic).</p>
<p>To simplify notation about time, instead of the usual subscript <span class="math inline">\(t\)</span> as above to denote a time period, for precision about the frequency, we will follow this convention:</p>
<ul>
<li><p>subscript <span class="math inline">\(m\)</span> denotes a given month;</p></li>
<li><p>subscript <span class="math inline">\(d\)</span> denotes a given day;</p></li>
<li><p>subscript <span class="math inline">\(d(m)\)</span> denotes a given day in a given month; example: <span class="math inline">\(d(m-1)\)</span> is a day in the previous month.</p></li>
</ul>
<p><code>gingado</code> offers a practical way to set up the temporal features that requires only the dates of the dataset.</p>
<div id="tempfeatures" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: to add documentation and tests, and later incorporate as a new function in gingado.utils</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TO-DO: return also vocab_sizes dictionary, required to set up embedding layer.</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_timefeat(df, freq<span class="op">=</span><span class="st">"d"</span>, features<span class="op">=</span><span class="va">None</span>, add_to_df<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the future documentation: the add_to_df argument should be True if the data will be fed to an algorithm that takes in all data at once. If, like neural networks, the inputs are fed through different "pipelines", then use False and then take the result from this function an feed it separately to a neural network.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the frequency is used to filter which features to add. For example, if monthly then no higher frequency features (day of ..., week of... ) are added because it doesn't make sense</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># None or list. if futures is None, then add all temporal features that the frequency above allows. Otherwise adds only the names ones</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    all_freqs <span class="op">=</span> [<span class="st">"q"</span>, <span class="st">"m"</span>, <span class="st">"w"</span>, <span class="st">"d"</span>]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> pd.api.types.is_datetime64_any_dtype(df.index):</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        df.index <span class="op">=</span> pd.to_datetime(df.index)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> i2s(index, df<span class="op">=</span>df):</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mini-helper func that transforms an index into a pandas Series with the index</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.Series(index, index<span class="op">=</span>df.index)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    dict_timefeat <span class="op">=</span> {}</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> all_freqs:</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'year_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_year_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_of_year'</span>] <span class="op">=</span> i2s(df.index.quarter)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_quarter_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>]]:</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_year'</span>] <span class="op">=</span> i2s(df.index.month)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>, <span class="st">"m"</span>]]:</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_month'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.day <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">7</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: ((x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>(x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">-01'</span>)).days <span class="op">//</span> <span class="dv">7</span>) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_year'</span>] <span class="op">=</span> i2s(df.index.isocalendar().week)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="op">==</span> <span class="st">"d"</span>:</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_week'</span>] <span class="op">=</span> i2s(df.index.dayofweek)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_month'</span>] <span class="op">=</span> i2s(df.index.day)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-01-01'</span>)).days <span class="op">%</span> <span class="dv">91</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_year'</span>] <span class="op">=</span> i2s(df.index.dayofyear)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the dictionary of columns to a DataFrame</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> pd.concat(dict_timefeat, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>    var_thresh <span class="op">=</span> VarianceThreshold(threshold<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> var_thresh.fit_transform(df_timefeat)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> pd.DataFrame(df_timefeat, columns<span class="op">=</span>var_thresh.get_feature_names_out(), index<span class="op">=</span>df.index).astype(<span class="bu">int</span>)</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features:</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>        df_timefeat[features]</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    vocab_sizes <span class="op">=</span> {col: df_timefeat[col].nunique() <span class="op">+</span> <span class="dv">1</span> <span class="cf">for</span> col <span class="kw">in</span> df_timefeat}</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> add_to_df:</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.concat([df, df_timefeat], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> df_timefeat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Specifically, temporal features are an excellent (and rare) type of <em>known future</em> input. Those are the data that we know will be like that during forecasting time, ie, at the time the observation <span class="math inline">\(y_t\)</span> takes place. For example, it is trivial to know the day of the week, of the month etc, for any date we are forecasting in.</p>
<p>For this reason, we now calculate the temporal features of inflation.</p>
<div id="temporal-features-for-the-inflation-series" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df_timefeat <span class="op">=</span> get_timefeat(df_infl_m, freq<span class="op">=</span><span class="st">"m"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="splitting-the-dataset" class="level3">
<h3 class="anchored" data-anchor-id="splitting-the-dataset">Splitting the dataset</h3>
<p>We will now split the dataset into training data up until end-2020 and validation data afterwards. The training data will be further split into 5 temporally sequential folds.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>To simplify, we will consider valid nowcasting <em>input</em> data for a given output in period <span class="math inline">\(m\)</span> as:</p>
<ul>
<li><p>all monthly data up to, and including, <span class="math inline">\(m-1\)</span>; and</p></li>
<li><p>all daily data up to, and including, <span class="math inline">\(d(m)\)</span>.</p></li>
</ul>
<div id="time-series-splits" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training date cutoff</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="st">"2020-12-31"</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> df_infl_m[:cutoff][<span class="dv">1</span>:], df_infl_m[cutoff:][<span class="dv">1</span>:]</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>Xm_train, Xm_test <span class="op">=</span> df_infl_m[:cutoff][:<span class="op">-</span><span class="dv">1</span>], df_infl_m[cutoff:][:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>Xd_train, Xd_test <span class="op">=</span> df_oil_d[:cutoff], df_oil_d[cutoff:]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> {<span class="st">"m"</span>: Xm_train, <span class="st">"d"</span>: Xd_train}</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> {<span class="st">"m"</span>: Xm_test, <span class="st">"d"</span>: Xd_test}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now for every month <span class="math inline">\(m\)</span> in the dependent variable, we can find all <span class="math inline">\(m_{t-l}, l\geq 1\)</span> and all <span class="math inline">\(d(m_{t-s}), s\geq 0\)</span>.</p>
<p>Note that in the example below, for each data point that we want to forecast (<code>y</code>), we take 12 lags of the monthly covariates and 250 lags of the daily covariates.</p>
<div id="create-data" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>maxlags <span class="op">=</span> {<span class="st">"m"</span>: <span class="dv">12</span>, <span class="st">"d"</span>: <span class="dv">250</span>}</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data(X, y, maxlags<span class="op">=</span>maxlags, tscv<span class="op">=</span>TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">5</span>), timedim<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If timeedim is true, then the dimensions of the tensors are n_samples/time dimension/features. If not then it is n_samples/time dimension (lag) * features, ie each lag is flattened as if it were a feature. Use True when passing to recurrent nets, use False for fully connected layers.</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    X_split <span class="op">=</span> {}</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    y_split <span class="op">=</span> {}</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    n_feat <span class="op">=</span> {k: v.shape[<span class="dv">1</span>] <span class="cf">for</span> k, v <span class="kw">in</span> X.items()}</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tscv:</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>        split_cv <span class="op">=</span> tscv.split(y)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        cv_dates <span class="op">=</span> [</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>            (y.index[m], y.index[n]) <span class="co"># (train, valid) for each fold</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> m, n <span class="kw">in</span> split_cv</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n_fold, split <span class="kw">in</span> <span class="bu">enumerate</span>(cv_dates):</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>            fold <span class="op">=</span> <span class="ss">f"fold_</span><span class="sc">{</span>n_fold<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>            dates_split <span class="op">=</span> {</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>                <span class="st">"train"</span>: split[<span class="dv">0</span>],</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>                <span class="st">"valid"</span>: split[<span class="dv">1</span>]</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>            X_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>            y_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> chunk, dates <span class="kw">in</span> dates_split.items():</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> ysample_date <span class="kw">in</span> tqdm(dates):</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>                    padded_x <span class="op">=</span> {}</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> f <span class="kw">in</span> X.keys():</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">try</span>:</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> X[f][:ysample_date][:<span class="op">-</span><span class="dv">1</span>].values</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> keras.utils.pad_sequences([to_pad], maxlen<span class="op">=</span>maxlags[f], dtype<span class="op">=</span>np.float32)</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>                        x_shape <span class="op">=</span> (<span class="dv">1</span>, maxlags[f], n_feat[f]) <span class="cf">if</span> timedim <span class="cf">else</span> (<span class="dv">1</span>, maxlags[f] <span class="op">*</span> n_feat[f])</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> padded_x[f].reshape(x_shape)</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>                    X_split[fold][chunk].append(padded_x)</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>                    y_split[fold][chunk].append(y_train.loc[ysample_date])</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X_split, y_split</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s see how this time series fold will be structured. Each fold is a sequentially longer window, so we get the following data points:</p>
<div id="create-batches-of-data" class="cell" data-execution_count="17">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> y_train_split.keys():</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'train'</span>])<span class="sc">}</span><span class="ss"> training X-y pairs"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'valid'</span>])<span class="sc">}</span><span class="ss"> validation X-y pairs"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>fold_0:
  75 training X-y pairs
  71 validation X-y pairs
fold_1:
  146 training X-y pairs
  71 validation X-y pairs
fold_2:
  217 training X-y pairs
  71 validation X-y pairs
fold_3:
  288 training X-y pairs
  71 validation X-y pairs
fold_4:
  359 training X-y pairs
  71 validation X-y pairs</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-fc" class="level2">
<h2 class="anchored" data-anchor-id="sec-fc">First model: a fully connected neural network</h2>
<p>The goal of this model is to nowcast <span class="math inline">\(\pi_t\)</span> based on its past values <span class="math inline">\(\pi_{t-1}\)</span> and on current oil prices <span class="math inline">\(o_{d(m-s)}, s \geq 0\)</span>. The first model we will train is a very simple neural network:</p>
<p><span id="eq-model0nnlayer"><span class="math display">\[
\begin{align}
\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) \\
y_t &amp;= \mathbf{W}_2 \xi + \mathbf{b}_2,
\end{align}
\tag{2}\]</span></span></p>
<p>where <span class="math inline">\(x_t\)</span> is the input data and the subscript of parameters relates to the “depth” of the layer they belong to. Using <span class="math inline">\(\lambda\)</span> as the dimensionality of the model, <span class="math inline">\(\mathbf{W}_2 \in \mathbb{R}^{1 \times \lambda}\)</span>, <span class="math inline">\(b_2 \in \mathbb{R}\)</span>, <span class="math inline">\(\mathbf{W}_1 \in \mathbb{R}^{\lambda \times |x_t|}\)</span>, <span class="math inline">\(b_1 \in \mathbb{R}^{d}\)</span>, <span class="math inline">\(\xi \in \mathbb{R}^\lambda\)</span> and <span class="math inline">\(\phi\)</span> is an activation function. For simplicity, we will use the ReLU activation function, which is simply: <span class="math inline">\(\phi(z) = \text{max}(z, 0)\)</span>.</p>
<p>For this neural network, we need a fix dimensionality of the input data. In other words, the network <em>needs</em> to know how much data it will take in at any given time, and this should not change throughout training or inference time.</p>
<p>For each data in our dependent variable, we simply stack the latest available monthly and daily data and their respective lags. Using the numbers above, this would be 12 lags for monthly data and 250 lags of daily oil data. Linking this to <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a> above, <span class="math inline">\(x_t = [\pi_{m-1}, \dots, \pi_{m-12}, o_d, \dots, o_{d-250}]\)</span>.</p>
<p>All of this data will be considered by the neural network at the same time. In a way, this is analogous to how a normal regression is run. However, the number of data points (12 + 250) used in this toy neural network is bigger than typical regressions.</p>
<div class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> createNN_fc(dim<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    nn_fc <span class="op">=</span> keras.Sequential([</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),)),</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span>activation, name<span class="op">=</span><span class="st">"SummariseInput"</span>),</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    ], <span class="st">"FullyConnected"</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    nn_fc.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn_fc</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>nn_fc <span class="op">=</span> createNN_fc()</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>nn_fc.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-nn_fc_summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="18">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nn_fc_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Summary of fully connected model
</figcaption>
<div aria-describedby="tbl-nn_fc_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ SummariseInput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)             │         <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ CalculateOutput (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              │            <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,225</span> (16.50 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">4,225</span> (16.50 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<p><a href="#fig-archfc" class="quarto-xref">Figure&nbsp;5</a> presents the architecture of this model.</p>
<div id="cell-fig-archfc" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<div id="fig-archfc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-archfc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Architecture of the model with fully connected layer
</figcaption>
<div aria-describedby="fig-archfc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-archfc-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>Note in <a href="#fig-archfc" class="quarto-xref">Figure&nbsp;5</a> that a first dense layer (ie, as in <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a>) takes in 262 data points, uses the activation function ReLU, and then outputs 16 data points for the next layer.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>Checking that it works. In the code below, we take the last fold as an example.</p>
<div id="preparing-fully-connected-nn-data" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>X_train_split_fc, y_train_split_fc <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, timedim<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>fold <span class="op">=</span> <span class="st">"fold_4"</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>X_train_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"train"</span>]])</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>y_train_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"train"</span>])</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>X_valid_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"valid"</span>]])</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>y_valid_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"valid"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we train the network.</p>
<div id="check-nn-model-works" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>history_fc <span class="op">=</span> nn_fc.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 28s 817ms/step - loss: 5.7030e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - loss: 4.8486e-04 - val_loss: 0.0119
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 3.6492e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.3872e-04 - val_loss: 0.0106
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 3.8234e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0045e-04 - val_loss: 0.0101
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 5.7896e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 5.9387e-05 - val_loss: 0.0096
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.2404e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.3620e-05 - val_loss: 0.0095
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 3.6844e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 3.2829e-05 - val_loss: 0.0095
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 4.1805e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.7816e-05 - val_loss: 0.0093
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 8.6974e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.0940e-05 - val_loss: 0.0090
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 9.1179e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6395e-05 - val_loss: 0.0089
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 5.9058e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4882e-05 - val_loss: 0.0089
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.1358e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4406e-05 - val_loss: 0.0090
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.3560e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2785e-05 - val_loss: 0.0087
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 7.3897e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.3165e-05 - val_loss: 0.0087
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.2359e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1627e-05 - val_loss: 0.0087
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.2261e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1698e-05 - val_loss: 0.0087
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.5949e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2177e-05 - val_loss: 0.0087
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.8901e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1370e-05 - val_loss: 0.0087
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 5.3817e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0311e-05 - val_loss: 0.0087
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.7860e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1705e-05 - val_loss: 0.0086
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 6.6659e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0265e-05 - val_loss: 0.0086</code></pre>
</div>
</div>
<p>You can see in <a href="#fig-history_fc" class="quarto-xref">Figure&nbsp;6</a> that the loss decreases with training, but the validation loss (ie, calculated on held-out data) is higher than the in-sample loss. This suggests the model is learning <em>too much</em> how to fit the data. In other words, it is also fitting some level of noise, which is not reproducible out-of-sample.</p>
<div id="cell-fig-history_fc" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_fc.history).plot()</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_fc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_fc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Losses calculated in a simple, fully-connected neural network.
</figcaption>
<div aria-describedby="fig-history_fc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_fc-output-1.png" width="606" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>We don’t need to bother training too much this very simple neural network; the goal here is to use it as a building block for a mathematical/econometric intuition of the broader nowcasting model.</p>
</section>
<section id="sec-gate" class="level2">
<h2 class="anchored" data-anchor-id="sec-gate">A useful tool: a gate</h2>
<p>The network trained in the previous section can learn how to map the input data to the output data. But there are ways to take advantage of the incredible flexibility in architecture (ie, how neural layers are stacked). One such way is to have the data inform which layers’s outputs are actually used downstream or not. This section describes how.</p>
<p>First, we replicate the same simple fully connected layer of <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a> two times. One of the neural networks will behave as before: learning to map the input data to the output data. The second one will also look at the same data, but with a different goal: it will learn how much data to let through. Its output is a value between 0 and 1, which is then multiplied to the “original” network. When this part of the neural network yields values closer to 0, the mainstream values are effectively shut down. Conversely, when the values are close to 1, the data proceeds as normal.</p>
<p>Adjusting <a href="#eq-model0nnlayer" class="quarto-xref">Equation&nbsp;2</a> to include a gate could be as in <a href="#eq-gated" class="quarto-xref">Equation&nbsp;3</a>:</p>
<p><span id="eq-gated"><span class="math display">\[
\begin{align}
\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) \\
G &amp;= \sigma(\mathbf{W}_G x_t + \mathbf{b}_G) \\
y_t &amp;= \mathbf{W}_2 (\xi \odot G) + \mathbf{b}_2,
\end{align}
\tag{3}\]</span></span></p>
<p>with <span class="math inline">\(\sigma\)</span> representing the sigmoid function and <span class="math inline">\(\odot\)</span> the Hadamard multiplication.</p>
<p>Note that the data that informs the gate does not necessarily need to be the same as the mainstream data.</p>
<div class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>mainstream <span class="op">=</span> createNN_fc(activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>gate <span class="op">=</span> createNN_fc(activation<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),), name<span class="op">=</span><span class="st">"FlattenedLaggedData"</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>mainstream <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(<span class="bu">input</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>gate <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"Gate"</span>)(<span class="bu">input</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>gated_data <span class="op">=</span> keras.layers.Multiply(name<span class="op">=</span><span class="st">"GatedData"</span>)([mainstream, gate])</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(gated_data)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>nn_fc_gated <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">input</span>, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GatedModel"</span>)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-nn_fc_gated_summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="23">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nn_fc_gated_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Summary of gated model
</figcaption>
<div aria-describedby="tbl-nn_fc_gated_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "GatedModel"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ FlattenedLaggedData │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">262</span>)       │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ SummariseInput      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │ FlattenedLaggedD… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ Gate (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">4,208</span> │ FlattenedLaggedD… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ GatedData           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ SummariseInput[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Multiply</span>)          │                   │            │ Gate[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]        │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ CalculateOutput     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ GatedData[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]   │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)             │                   │            │                   │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,433</span> (32.94 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">8,433</span> (32.94 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-arch_gatedmodel" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc_gated, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<div id="fig-arch_gatedmodel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-arch_gatedmodel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Architecture of gated neural network
</figcaption>
<div aria-describedby="fig-arch_gatedmodel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-arch_gatedmodel-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>The model can now be fit with the same input data: it is then used by two different branches (referred to above in the text as “mainstream” and “gate”).</p>
<div id="fitting-gated-model" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>history_fc_gated <span class="op">=</span> nn_fc_gated.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 35s 1s/step - loss: 3.1614e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - loss: 2.2415e-04 - val_loss: 0.0029
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.3705e-0436/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.0920e-04 - val_loss: 0.0025
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 4.0366e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 5.4068e-05 - val_loss: 0.0023
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 2.2917e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 4.7104e-05 - val_loss: 0.0020
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 1.4010e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.9384e-05 - val_loss: 0.0020
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.6888e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.9346e-05 - val_loss: 0.0018
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 1.2921e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.8997e-05 - val_loss: 0.0018
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 2.5205e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.9405e-05 - val_loss: 0.0017
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 8.9151e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4858e-05 - val_loss: 0.0017
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 1.6239e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.9639e-05 - val_loss: 0.0016
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 2.4281e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.7507e-05 - val_loss: 0.0017
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 1.0204e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4224e-05 - val_loss: 0.0016
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 7.5797e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 2.0016e-05 - val_loss: 0.0016
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 2.2701e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.6403e-05 - val_loss: 0.0016
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 6.9787e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.1833e-05 - val_loss: 0.0016
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 8.6075e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4064e-05 - val_loss: 0.0015
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 7.0776e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2361e-05 - val_loss: 0.0015
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 9.1139e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.4689e-05 - val_loss: 0.0015
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 8.8811e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.2408e-05 - val_loss: 0.0015
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 6.4103e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 1.7664e-05 - val_loss: 0.0015</code></pre>
</div>
</div>
<div id="cell-fig-history_fc_gated" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>gated_loss <span class="op">=</span> pd.DataFrame(history_fc_gated.history)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>gated_loss[<span class="st">"val_loss (no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> gated_loss.plot()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-history_fc_gated" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_fc_gated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Losses calculated in a simple, fully-connected neural network with gate.
</figcaption>
<div aria-describedby="fig-history_fc_gated-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_fc_gated-output-1.png" width="606" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-lstm" class="level2">
<h2 class="anchored" data-anchor-id="sec-lstm">Second model: Long short-term memory</h2>
<p>A marked improvement in how we can model time series data is the use of recurrent neural networks (RNNs). In essence, these are networks that learn to keep a stateful memory, which is updated as the network “visits” each sequential step in time, in turn using both the memory and the new data at that period to predict the output.</p>
<p>In contrast to the fully connected layer in <a href="#sec-fc" class="quarto-xref">Section&nbsp;3</a>, which need to look at different lags to pick up any history-dependent information, RNNs look at the observable variables at each period and learn a latent “state” (akin to Kalman filters, for example). The same network then slides up one step in time and uses that information and the previous state to update the state, and so on…</p>
<p>One particular type of RNN that has proven to be very successful in practice is the long short-term memory (LSTM) model, due to <span class="citation" data-cites="hochreiter1997long">Hochreiter (<a href="#ref-hochreiter1997long" role="doc-biblioref">1997</a>)</span>. It is actually a combination of four different layers, of which three are actually gates. These layers are built in a specific way. Here’s how:</p>
<p><span id="eq-model1lstm"><span class="math display">\[
\begin{align}
f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
\tilde{c}_t &amp;= \omega(W_c x_t + U_c h_{t-1} + b_c) \\
c_t &amp;= \underbrace{f_t \odot c_{t-1}}_{\text{Gated past data}} + \underbrace{i_t \odot \tilde{c}_t}_{\text{How much to learn}} \\
h_t &amp;= o_t \odot \omega(c_t),
\end{align}
\tag{4}\]</span></span></p>
<p>where <span class="math inline">\(\omega\)</span> is the hyperbolic function.</p>
<p>The basic intuition of the LSTM is that some of the individual component layers essentially learn to look at the current data and the past memory and then decide how much new information to let through. Note that, because their activation is a sigmoid, the output of layers <span class="math inline">\(f_t\)</span>, <span class="math inline">\(i_t\)</span> and <span class="math inline">\(o_t\)</span> is a number between 0 and 1. This idea is important to bear in mind because it will be used at a much bigger scale by the whole TFT model - and will be one key feature of its interpretability.</p>
<p>With LSTM networks, it is easier to incorporate mixed-frequency data in a meaningful way. This is done below by passing data of each frequency through their own LSTM layers, and combining their output.</p>
<div class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>] <span class="co"># using here the commonly-used frequency abbreviations</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> inputs.items():</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>nn_lstm <span class="op">=</span> keras.Model(</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"LSTMNetwork"</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>nn_lstm.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>nn_lstm.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-lstm_summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="27">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lstm_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Summary of LSTM model
</figcaption>
<div aria-describedby="tbl-lstm_summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "LSTMNetwork"</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)        </span>┃<span style="font-weight: bold"> Output Shape      </span>┃<span style="font-weight: bold">    Param # </span>┃<span style="font-weight: bold"> Connected to      </span>┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ m (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ d (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ -                 │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ not_equal_1         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">NotEqual</span>)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>)   │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ masking_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Masking</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)   │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]           │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ any_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Any</span>)         │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)      │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ not_equal_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>] │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_m (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">1,152</span> │ masking[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],    │
│                     │                   │            │ any[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]         │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ LSTM__freq_d (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │      <span style="color: #00af00; text-decoration-color: #00af00">1,152</span> │ masking_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],  │
│                     │                   │            │ any_1[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ encoded_series      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │          <span style="color: #00af00; text-decoration-color: #00af00">0</span> │ LSTM__freq_m[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">Average</span>)           │                   │            │ LSTM__freq_d[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">…</span> │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)       │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)        │        <span style="color: #00af00; text-decoration-color: #00af00">272</span> │ encoded_series[<span style="color: #00af00; text-decoration-color: #00af00">0</span>… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)     │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)         │         <span style="color: #00af00; text-decoration-color: #00af00">17</span> │ dense[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,593</span> (10.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,593</span> (10.13 KB)
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div>
</div>
</figure>
</div>
</div>
<div id="cell-fig-archlstm" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_lstm, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="28">
<div id="fig-archlstm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-archlstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Architecture of the network with LSTM layer
</figcaption>
<div aria-describedby="fig-archlstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-archlstm-output-1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
</div>
</div>
<p>The reason why the dimensions in the input layer are now <code>(None, None, 1)</code>, with one addition <code>None</code> compared to before(for example, <a href="#fig-archfc" class="quarto-xref">Figure&nbsp;5</a>) is due to the <em>time dimension</em>. Whereas before the model didn’t know how many samples it would be fed, now also the length of the time window can change because each time step will pass through exactly the same parameters.</p>
<p>Note that the input goes through a few steps before reaching the LSTM layer. This is due to a masking layer that effectively helps the model jumps time steps for which there is no data available.</p>
<p>To check that the LSTM-based neural network works, we need to feed this neural network a slightly different type of data. LSTM, as other recurrent neural networks, takes in time series data. So, unlike before, we now prepare a time series (or panel data) for each</p>
<p>This neural network will then take in the inputed time series data, and encode each frequency’s series separately through the different LSTM streams. The final result will no longer have a time dimension; it is then averaged, and this average embeddings of the different time series is used to forecast the variable of interest.</p>
<div id="train-the-lstm" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>history_lstm <span class="op">=</span> nn_lstm.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3:17 6s/step - loss: 2.6572e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 4.2969e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 4.7036e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 4.5863e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 4.3698e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 4.1586e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.9938e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.8327e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.6873e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.5532e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.4283e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.3117e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.2393e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.1720e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.1058e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 3.0415e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 2.9831e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 2.9270e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 2.8747e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 2.8261e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.7799e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.7348e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.6937e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.6572e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.6234e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.5901e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.5583e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.5302e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.5034e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.4774e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.4515e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.4264e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.4022e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.3788e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 2.3567e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 2.3352e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 9s 92ms/step - loss: 2.3149e-05 - val_loss: 8.0783e-06
Epoch 2/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 9.6110e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.9107e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.9947e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.7571e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 9.5154e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.3580e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.3702e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.4501e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.5891e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.7124e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.8069e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.9507e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0052e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0129e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0218e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0326e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0418e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0477e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0514e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0535e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0672e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0801e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0941e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1058e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1158e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1241e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1318e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1382e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1441e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1494e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1539e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1595e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1643e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1690e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1731e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.1767e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.1801e-05 - val_loss: 7.9454e-06
Epoch 3/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 1.2949e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1407e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.0700e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1320e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1313e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1344e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1195e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1229e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1322e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1420e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1545e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1696e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1809e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1908e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1978e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2005e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2005e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1997e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1998e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2015e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2034e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2044e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2056e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2060e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2062e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2129e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2188e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2237e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2296e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2351e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2396e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2439e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2485e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2530e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2579e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2618e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.2655e-05 - val_loss: 7.7286e-06
Epoch 4/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 9.7206e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 9.9624e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.0000e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 9.6002e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 9.8709e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.8821e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.9078e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.8389e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.8564e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.8686e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.8484e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.8084e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.7749e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.7287e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.6862e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.6535e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.6144e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.6175e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.6469e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.6649e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.7044e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.7548e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.8067e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 9.8606e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0022e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0186e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0332e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0458e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0569e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0670e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0763e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0843e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0909e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0973e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1028e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.1081e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.1132e-05 - val_loss: 6.9780e-06
Epoch 5/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 7.7863e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 8.7771e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 9.4440e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.3490e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.5864e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6962e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7449e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7744e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7832e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7815e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.7686e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.7529e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.7345e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.7136e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.6904e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.6690e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.6536e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6456e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6404e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6375e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6332e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6297e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6256e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6196e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6121e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6041e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5969e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5896e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5821e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5741e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5665e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5609e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5556e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5500e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5453e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5406e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.5361e-05 - val_loss: 7.8806e-06
Epoch 6/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 1.8470e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.8276e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.7616e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.6689e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.6109e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5880e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5725e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5813e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5699e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5508e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5361e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.5183e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.5060e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4911e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4757e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4614e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4458e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4298e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.4134e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3980e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3829e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3698e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3583e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3467e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3366e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3261e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3171e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3103e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.3036e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2965e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2899e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2836e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2779e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2731e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2749e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2777e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.2803e-05 - val_loss: 6.8712e-06
Epoch 7/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 6.0098e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 8.7903e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 8.9502e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.2266e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.6781e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.0018e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0130e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0140e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0366e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0544e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0749e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0851e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0899e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0923e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0929e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0903e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0866e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0844e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0856e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0854e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0837e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0813e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0783e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0765e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0758e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0758e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0752e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0753e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0749e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0749e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0745e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0747e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0745e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0762e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0811e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0853e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.0894e-05 - val_loss: 7.1221e-06
Epoch 8/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 4.2400e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.0292e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.4738e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.7231e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.8028e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8449e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8388e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8268e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8112e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7912e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7681e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7449e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7271e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7095e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6907e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6745e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6577e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6415e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6274e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.6135e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5996e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5859e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5718e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5594e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5492e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5411e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5345e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5277e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5205e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5130e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5062e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.4994e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.4927e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.4870e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.4825e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.4778e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.4734e-05 - val_loss: 7.0070e-06
Epoch 9/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 1.0680e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1906e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.7322e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.8506e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.8964e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.9170e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.9051e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8832e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8630e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8606e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8557e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8446e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8291e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.8091e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7942e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7771e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7576e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7387e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7219e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.7059e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6894e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6729e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6567e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6427e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6294e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6160e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.6034e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5919e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5807e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5699e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5608e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5522e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5436e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.5355e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5273e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5192e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.5114e-05 - val_loss: 6.7194e-06
Epoch 10/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 4s 125ms/step - loss: 1.3462e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 1.3687e-05  3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.6147e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.7218e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 69ms/step - loss: 1.7152e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 70ms/step - loss: 1.8124e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 2s 73ms/step - loss: 1.8525e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 2s 72ms/step - loss: 1.8572e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 72ms/step - loss: 1.8497e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.8435e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - loss: 1.8383e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.8271e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.8148e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7997e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7857e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7697e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7557e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7411e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7305e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7205e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.7096e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.6979e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6851e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6733e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6607e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6474e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6344e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6223e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.6104e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5988e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5892e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5802e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5712e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5620e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5535e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.5460e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.5389e-05 - val_loss: 2.3222e-05
Epoch 11/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 4.9088e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 4.3082e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 3.7709e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 3.3749e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 3.1008e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 2.8760e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.6959e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.5449e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.4219e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.3170e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.2283e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.1533e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.0893e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 2.0326e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.9854e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.9438e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.9046e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.8665e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.8305e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.7982e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.7685e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.7404e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.7134e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6893e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6673e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6470e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6326e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6211e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6111e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.6012e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5916e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5820e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5732e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5642e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5552e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.5466e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.5385e-05 - val_loss: 6.1064e-06
Epoch 12/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 1.1087e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.0473e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.0390e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.0016e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.6012e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 9.2648e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9227e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.7216e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7544e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.8127e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.8268e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.8132e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7720e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7508e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7710e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7782e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.7842e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.7776e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.8202e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.8463e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.8910e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.9272e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 8.9776e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.0147e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.0505e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.1427e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.2235e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.2904e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3648e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.4510e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.5364e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6098e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6783e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7374e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7877e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8294e-0636/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 9.8689e-06 - val_loss: 6.6415e-06
Epoch 13/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 94ms/step - loss: 1.5257e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.6261e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.5019e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.3893e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 1.2957e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.2288e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.1916e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1550e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1229e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 1.1016e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0843e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0659e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.0509e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.0430e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.0583e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.0700e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 1.0817e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.0927e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.1013e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.1080e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 1.1155e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1215e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1291e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1355e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1412e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1456e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1485e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1505e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1515e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1520e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 70ms/step - loss: 1.1520e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1514e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1506e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1499e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1491e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 69ms/step - loss: 1.1481e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 74ms/step - loss: 1.1472e-05 - val_loss: 6.0016e-06
Epoch 14/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 92ms/step - loss: 9.3578e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.1295e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.0971e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.0537e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.0327e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.0086e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.9355e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.7829e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.6630e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.5429e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.4197e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.3396e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.3429e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.3524e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.3286e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.3033e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.2661e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.2475e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.2135e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1697e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.2398e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3341e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.4381e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.5328e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.6092e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6806e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7413e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7911e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8349e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.8677e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.8956e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.9244e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.9533e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 9.9810e-0635/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0009e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0038e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.0067e-05 - val_loss: 1.2632e-05
Epoch 15/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 7.1417e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 6.1073e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 5.4213e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 6.2752e-06 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 7.2667e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 68ms/step - loss: 8.0478e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 8.5355e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.7835e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9168e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 8.9701e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.0168e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.0788e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1044e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1206e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1318e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1495e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1599e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.1951e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.2465e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.2956e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.3301e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.3560e-0623/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.4558e-0624/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.5490e-0625/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6253e-0626/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.6858e-0627/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7429e-0628/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.7900e-0629/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8355e-0630/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8691e-0631/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.8977e-0632/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9326e-0633/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 9.9693e-0634/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0003e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0036e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0068e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.0098e-05 - val_loss: 5.9148e-06
Epoch 16/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 2.0672e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.6987e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.5018e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.3753e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.2935e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 2s 67ms/step - loss: 1.2602e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2268e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.2031e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1760e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1487e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1637e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1758e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1800e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1824e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1808e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1807e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1804e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1801e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1797e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 1.1825e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1834e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1825e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1810e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1800e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1789e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1766e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1741e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1725e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1705e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1697e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1703e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1701e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1697e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1688e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1677e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.1670e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.1664e-05 - val_loss: 6.6857e-06
Epoch 17/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 91ms/step - loss: 1.2090e-05 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.1080e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.0689e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 64ms/step - loss: 1.0334e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 1.0051e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.9259e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.6960e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 9.5085e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.3876e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.3583e-0611/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.4360e-0612/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.5439e-0613/36 ━━━━━━━━━━━━━━━━━━━━ 1s 70ms/step - loss: 9.6222e-0614/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.6978e-0615/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.7688e-0616/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.8086e-0617/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.8193e-0618/36 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - loss: 9.8215e-0619/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.8337e-0620/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.8485e-0621/36 ━━━━━━━━━━━━━━━━━━━━ 1s 68ms/step - loss: 9.9751e-0622/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0091e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0185e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0281e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0373e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0447e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0509e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0556e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0595e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0640e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0676e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0703e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0725e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0739e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0748e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 68ms/step - loss: 1.0757e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.0766e-05 - val_loss: 5.9146e-06
Epoch 18/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 93ms/step - loss: 9.6248e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.0862e-05 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.2726e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.3249e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.3202e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3130e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2887e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2580e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2380e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.2160e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.1928e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.1711e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1551e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1392e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1234e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1077e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0932e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0800e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0699e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0725e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0760e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.0772e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0772e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0768e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0765e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0768e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0765e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0786e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0802e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0808e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0809e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0824e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0840e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0849e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0863e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.0872e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 72ms/step - loss: 1.0881e-05 - val_loss: 6.9073e-06
Epoch 19/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 90ms/step - loss: 6.7801e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 9.4129e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.0215e-05 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 1.0021e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 66ms/step - loss: 9.7207e-06 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.7206e-06 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.6291e-06 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - loss: 9.5881e-06 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 9.5655e-0610/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0232e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.0750e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1141e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1436e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1646e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1774e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1865e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.1954e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2012e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2053e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.2076e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2099e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2115e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2118e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2107e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2099e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2109e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2111e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2102e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2096e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2096e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2087e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2072e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2054e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2030e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.2012e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 65ms/step - loss: 1.1995e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 70ms/step - loss: 1.1979e-05 - val_loss: 6.7580e-06
Epoch 20/20
 1/36 ━━━━━━━━━━━━━━━━━━━━ 3s 89ms/step - loss: 8.8454e-06 2/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 8.6388e-06 3/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 8.2040e-06 4/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.1214e-05 5/36 ━━━━━━━━━━━━━━━━━━━━ 2s 65ms/step - loss: 1.2756e-05 6/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3337e-05 7/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3768e-05 8/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3988e-05 9/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.4048e-0510/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3988e-0511/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3902e-0512/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3814e-0513/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3708e-0514/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3578e-0515/36 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - loss: 1.3444e-0516/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3322e-0517/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3217e-0518/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3165e-0519/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3100e-0520/36 ━━━━━━━━━━━━━━━━━━━━ 1s 66ms/step - loss: 1.3029e-0521/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2964e-0522/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2903e-0523/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2848e-0524/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2793e-0525/36 ━━━━━━━━━━━━━━━━━━━━ 0s 67ms/step - loss: 1.2729e-0526/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2675e-0527/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2632e-0528/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2590e-0529/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2559e-0530/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2525e-0531/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2489e-0532/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2452e-0533/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2416e-0534/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2383e-0535/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2348e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 0s 66ms/step - loss: 1.2311e-0536/36 ━━━━━━━━━━━━━━━━━━━━ 3s 71ms/step - loss: 1.2277e-05 - val_loss: 6.1282e-06</code></pre>
</div>
</div>
<div class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>lstm_loss <span class="op">=</span> pd.DataFrame(history_lstm.history)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC with gate)"</span>] <span class="op">=</span> history_fc_gated.history[<span class="st">"val_loss"</span>]</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss.plot()</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss[[<span class="st">"loss"</span>, <span class="st">"val_loss"</span>]].plot()</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-history_lstm" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="30">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-history_lstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Losses calculated in an LSTM
</figcaption>
<div aria-describedby="fig-history_lstm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-history_lstm-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-history_lstm-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) All models so far
</figcaption>
<div aria-describedby="fig-history_lstm-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_lstm-output-1.png" data-ref-parent="fig-history_lstm" width="606" height="429" class="figure-img">
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-history_lstm-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-fig" id="fig-history_lstm-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) LSTM only
</figcaption>
<div aria-describedby="fig-history_lstm-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01_nowcast_files/figure-html/fig-history_lstm-output-2.png" data-ref-parent="fig-history_lstm" width="597" height="443" class="figure-img">
</div>
</figure>
</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="sec-gates" class="level2">
<h2 class="anchored" data-anchor-id="sec-gates">Introducing… the gatekeepers</h2>
</section>
<section id="sec-transf" class="level2">
<h2 class="anchored" data-anchor-id="sec-transf">Now is a(nother) good time to pay attention</h2>
</section>
<section id="sec-tftmf" class="level2">
<h2 class="anchored" data-anchor-id="sec-tftmf">Complete architecture</h2>
</section>
<section id="sec-nowcast" class="level2">
<h2 class="anchored" data-anchor-id="sec-nowcast">Nowcasting inflation with a simple model</h2>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bok2018nowcasting" class="csl-entry" role="listitem">
Bok, Brandyn, Daniele Caratelli, Domenico Giannone, Argia M. Sbordone, and Andrea Tambalotti. 2018. <span>“Macroeconomic Nowcasting and Forecasting with Big Data.”</span> Journal Article. <em>Annual Review of Economics</em> 10 (Volume 10, 2018): 615–43. https://doi.org/<a href="https://doi.org/10.1146/annurev-economics-080217-053214">https://doi.org/10.1146/annurev-economics-080217-053214</a>.
</div>
<div id="ref-giannone2008nowcasting" class="csl-entry" role="listitem">
Giannone, Domenico, Lucrezia Reichlin, and David Small. 2008. <span>“Nowcasting: The Real-Time Informational Content of Macroeconomic Data.”</span> <em>Journal of Monetary Economics</em> 55 (4): 665–76.
</div>
<div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, S. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation MIT-Press</em>.
</div>
<div id="ref-lim2021temporal" class="csl-entry" role="listitem">
Lim, Bryan, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. <span>“Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting.”</span> <em>International Journal of Forecasting</em> 37 (4): 1748–64.
</div>
</div>


<!-- -->

</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="giannone2008nowcasting">Giannone, Reichlin, and Small (<a href="#ref-giannone2008nowcasting" role="doc-biblioref">2008</a>)</span> pioneered nowcasting in macroeconomics. See <span class="citation" data-cites="bok2018nowcasting">Bok et al. (<a href="#ref-bok2018nowcasting" role="doc-biblioref">2018</a>)</span> for a review.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See <a href="https://sdmx1.readthedocs.io/en/latest/walkthrough.html">here</a> for a practical walkthrough showing how to explore data with SDMX.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See <a href="https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split">here</a> for more information on time series splitting.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Ignore the <code>None</code> in the shapes; this means it dependets on each case. Specifically, the first dimension of the shapes in keras are always the number of samples going in the model. Because this varies with every training, application, etc, it is not fixed as the other dimensions are. Another way of thinking about this is as follows: in a regression, you know exactly how many variables you need to have, but the number of data points can vary.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb42" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Nowcasting inflation with neural networks</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> A simple, mixed-frequency example</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="an">output-file:</span><span class="co"> nowcast.html</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="an">authors:</span><span class="co"> </span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - Douglas K. G. Araujo</span></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - Johannes Damp</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="an">code-fold:</span><span class="co"> show</span></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="an">code-tools:</span><span class="co"> true</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="an">warning:</span><span class="co"> false</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="an">fig-cap-location:</span><span class="co"> top</span></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>This notebook showcases how to set up neural networks to nowcast inflation using data measured in different frequencies. The goal here is to start with a very simple dataset containing only two variables, inflation (monthly) and oil prices (daily), to slowly build up a more complex neural network based nowcasting model, the TFT-MF available in <span class="in">`gingado`</span> from its v0.3.0.</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>Nowcasting is essentially the use of the most current information possible to estimate in real time an economic series of interest such as inflation or GDP before it is actually released<span class="ot">[^review]</span>. For example, if you could measure all prices every day, you could create on the last day of the month a very accurate nowcast for the headline inflation for that month - which would only be officialy published a few days later. In the case of GDP, this lag between the end of the reference period and actual publication tends to be significant, around 6-10 weeks. For policymakers, investors and other decisionmakers, a lot can happen in this period.</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a><span class="ot">[^review]: </span>@giannone2008nowcasting pioneered nowcasting in macroeconomics. See @bok2018nowcasting for a review.</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>A related use of nowcasting is to estimate what the current period's reading will be as this period rolls out. In other words, estimating today what the inflation reading for this month (or GDP for this quarter) will likely be as new information is unveiled in real time.</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>The nowcasting model available in <span class="in">`gingado`</span> from v0.3.0 onwards is an adjusted version of the Temporal Fusion Transformer (TFT) of @lim2021temporal. This architecture combines *flexibility* to take on multiple datasets while learning which information to focus on and *interpretability* to provide insights on the important variables in each case.</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="fu">## Roadmap</span></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>The TFT model can be a bit complex to understand at first, so we will build it up, step by step. After loading the data in @sec-data, the most basic neural network - a neuron layer - is presented in @sec-fc. This is followed by an architecture that is more suitable for time series in @sec-lstm. Next, these elements are combined in @sec-gates to show how the model knows what to focus on. The next individual element is the self-attention layer in @sec-transf. Finally, if you want to see the full picture directly, go to @sec-tftmf to see how these elements are put together. @sec-nowcast then trains the model and presents the results for this simple, illustrative nowcasting.</span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loading the data {#sec-data}</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>Let's use our SDMX connectors to find and download data from official sources in a reproducible way.</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>To abstract from currency issues, we will use US inflation and oil prices, which are denominated in US dollars.</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load packages</span></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sdmx</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gingado.utils <span class="im">import</span> load_SDMX_data</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> VarianceThreshold</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> TimeSeriesSplit</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inflation</span></span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>Since this is a monthly nowcast of inflation, the best way to do this is to use a *monthly change in the consumer price index*, $\pi_t^{(m)}=(\text{CPI}_t - \text{CPI}_{t-1})/\text{CPI}_{t-1}$, not the year-on-year rate, $\pi_t^{(y)}=(\text{CPI}_t - \text{CPI}_{t-12})/\text{CPI}_{t-12}$, which is how people usually think of inflation. This is because we want to nowcast only the value at the margin; 11 twelths of $\pi_t^{(y)}$ are already known, since $\pi_t^{(y)} = -1 + \prod_{l=0}^{11} (1+\pi_{t-l})$. </span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-58"><a href="#cb42-58" aria-hidden="true" tabindex="-1"></a>Then, only at the end we combine rolling windows of 12 consecutive monthly inflation rates, of which only the last one or two are estimated, to correctly create an annual inflation rate. </span>
<span id="cb42-59"><a href="#cb42-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-60"><a href="#cb42-60" aria-hidden="true" tabindex="-1"></a>Formally, if we know all values except the current and last month's, then: </span>
<span id="cb42-61"><a href="#cb42-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-62"><a href="#cb42-62" aria-hidden="true" tabindex="-1"></a>\hat{\pi}_t^{(y)}=(\prod_{l=0}^1 (1+\hat{\pi}_{t-l}^{(m)}) \prod_{l=2}^{11} (1+\pi_{t-l}^{(m)}) )-1,</span>
<span id="cb42-63"><a href="#cb42-63" aria-hidden="true" tabindex="-1"></a>$$ {#eq-finalnowcast}</span>
<span id="cb42-64"><a href="#cb42-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-65"><a href="#cb42-65" aria-hidden="true" tabindex="-1"></a>where the hat notation ($\hat{ }$) means that a particular value was estimated.</span>
<span id="cb42-66"><a href="#cb42-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-67"><a href="#cb42-67" aria-hidden="true" tabindex="-1"></a>For inflation, we take a dataflow from the <span class="co">[</span><span class="ot">BIS</span><span class="co">](https://data.bis.org/topics/CPI)</span>, since we are looking for US data. Let's explore it first and then choose the correct data specifications to download the time series.<span class="ot">[^sdmx]</span></span>
<span id="cb42-68"><a href="#cb42-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-69"><a href="#cb42-69" aria-hidden="true" tabindex="-1"></a><span class="ot">[^sdmx]: </span>See <span class="co">[</span><span class="ot">here</span><span class="co">](https://sdmx1.readthedocs.io/en/latest/walkthrough.html)</span> for a practical walkthrough showing how to explore data with SDMX.</span>
<span id="cb42-70"><a href="#cb42-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-73"><a href="#cb42-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-74"><a href="#cb42-74" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "inflation dataflow"</span></span>
<span id="cb42-75"><a href="#cb42-75" aria-hidden="true" tabindex="-1"></a>BIS <span class="op">=</span> sdmx.Client(<span class="st">"BIS"</span>)</span>
<span id="cb42-76"><a href="#cb42-76" aria-hidden="true" tabindex="-1"></a>cpi_msg <span class="op">=</span> BIS.dataflow(<span class="st">'WS_LONG_CPI'</span>)</span>
<span id="cb42-77"><a href="#cb42-77" aria-hidden="true" tabindex="-1"></a>cpi_dsd <span class="op">=</span> cpi_msg.structure</span>
<span id="cb42-78"><a href="#cb42-78" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-79"><a href="#cb42-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-80"><a href="#cb42-80" aria-hidden="true" tabindex="-1"></a>These are all possible keys:</span>
<span id="cb42-81"><a href="#cb42-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-84"><a href="#cb42-84" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-85"><a href="#cb42-85" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "CPI_dimensions"</span></span>
<span id="cb42-86"><a href="#cb42-86" aria-hidden="true" tabindex="-1"></a>cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.components</span>
<span id="cb42-87"><a href="#cb42-87" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-88"><a href="#cb42-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-89"><a href="#cb42-89" aria-hidden="true" tabindex="-1"></a>For example, "FREQ" (frequency) takes in these values:</span>
<span id="cb42-90"><a href="#cb42-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-93"><a href="#cb42-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-94"><a href="#cb42-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "FREQ_codelist"</span></span>
<span id="cb42-95"><a href="#cb42-95" aria-hidden="true" tabindex="-1"></a>cl__FREQ <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"FREQ"</span>).local_representation.enumerated)</span>
<span id="cb42-96"><a href="#cb42-96" aria-hidden="true" tabindex="-1"></a>cl__FREQ</span>
<span id="cb42-97"><a href="#cb42-97" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-98"><a href="#cb42-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-99"><a href="#cb42-99" aria-hidden="true" tabindex="-1"></a>And "REF_AREA" (reference area) can be set to:</span>
<span id="cb42-100"><a href="#cb42-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-103"><a href="#cb42-103" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-104"><a href="#cb42-104" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "REF_AREA_codelist"</span></span>
<span id="cb42-105"><a href="#cb42-105" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"REF_AREA"</span>).local_representation.enumerated)</span>
<span id="cb42-106"><a href="#cb42-106" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA</span>
<span id="cb42-107"><a href="#cb42-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-108"><a href="#cb42-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-109"><a href="#cb42-109" aria-hidden="true" tabindex="-1"></a>We can check that the US is amongst the reference areas:</span>
<span id="cb42-110"><a href="#cb42-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-113"><a href="#cb42-113" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-114"><a href="#cb42-114" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "check US in REF_AREA codelist"</span></span>
<span id="cb42-115"><a href="#cb42-115" aria-hidden="true" tabindex="-1"></a>cl__REF_AREA[<span class="st">'US'</span>]</span>
<span id="cb42-116"><a href="#cb42-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-117"><a href="#cb42-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-118"><a href="#cb42-118" aria-hidden="true" tabindex="-1"></a>Finally, the "UNIT_MEASURE" values can be:</span>
<span id="cb42-119"><a href="#cb42-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-122"><a href="#cb42-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-123"><a href="#cb42-123" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "codelist for UNIT_MEASURE in dataflow BIS__WS_LONG_CPI"</span></span>
<span id="cb42-124"><a href="#cb42-124" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE <span class="op">=</span> sdmx.to_pandas(cpi_dsd[<span class="st">'BIS_LONG_CPI'</span>].dimensions.get(<span class="st">"UNIT_MEASURE"</span>).local_representation.enumerated)</span>
<span id="cb42-125"><a href="#cb42-125" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE</span>
<span id="cb42-126"><a href="#cb42-126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-127"><a href="#cb42-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-128"><a href="#cb42-128" aria-hidden="true" tabindex="-1"></a>In the <span class="co">[</span><span class="ot">BIS website for this data</span><span class="co">](https://data.bis.org/topics/CPI#faq)</span>, we can see that the unit in levels is <span class="in">`Index, 2010 = 100`</span> (the other one is <span class="in">`Year-on-year changes, in per cent`</span>, which as discussed above we don't want for this case.)</span>
<span id="cb42-129"><a href="#cb42-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-132"><a href="#cb42-132" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-133"><a href="#cb42-133" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "finding code for index"</span></span>
<span id="cb42-134"><a href="#cb42-134" aria-hidden="true" tabindex="-1"></a>cl__UNIT_MEASURE[cl__UNIT_MEASURE.<span class="bu">str</span>.contains(<span class="st">"Index, 2010 = 100"</span>)]</span>
<span id="cb42-135"><a href="#cb42-135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-136"><a href="#cb42-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-137"><a href="#cb42-137" aria-hidden="true" tabindex="-1"></a>Armed with this knowledge, we can now download monthly consumer price index data for the US. Let's start after 1985 so that we have a sufficiently long history but without too much influence of the tectonic shift of the US dollar devaluation in the early 1970s and ensuing high inflation:</span>
<span id="cb42-138"><a href="#cb42-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-141"><a href="#cb42-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-142"><a href="#cb42-142" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cpi</span></span>
<span id="cb42-143"><a href="#cb42-143" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "US consumer price index, 2010 = 100"</span></span>
<span id="cb42-144"><a href="#cb42-144" aria-hidden="true" tabindex="-1"></a>df_infl <span class="op">=</span> load_SDMX_data(</span>
<span id="cb42-145"><a href="#cb42-145" aria-hidden="true" tabindex="-1"></a>    sources<span class="op">=</span>{<span class="st">"BIS"</span>: <span class="st">"'WS_LONG_CPI'"</span>},</span>
<span id="cb42-146"><a href="#cb42-146" aria-hidden="true" tabindex="-1"></a>    keys<span class="op">=</span>{<span class="st">"FREQ"</span>: <span class="st">"M"</span>, <span class="st">"REF_AREA"</span>: <span class="st">"US"</span>, <span class="st">"UNIT_MEASURE"</span>: <span class="st">"628"</span>},</span>
<span id="cb42-147"><a href="#cb42-147" aria-hidden="true" tabindex="-1"></a>    params<span class="op">=</span>{<span class="st">"startPeriod"</span>: <span class="dv">1985</span>}</span>
<span id="cb42-148"><a href="#cb42-148" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-149"><a href="#cb42-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-150"><a href="#cb42-150" aria-hidden="true" tabindex="-1"></a>df_infl.plot()</span>
<span id="cb42-151"><a href="#cb42-151" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-152"><a href="#cb42-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-153"><a href="#cb42-153" aria-hidden="true" tabindex="-1"></a>As you can see in @fig-cpi, we downloaded the series $<span class="sc">\{</span>\text{CPI}_t<span class="sc">\}</span>$. Transforming that into $<span class="sc">\{</span>\pi_t<span class="sc">\}</span>$, defined above, we have:</span>
<span id="cb42-154"><a href="#cb42-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-157"><a href="#cb42-157" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-158"><a href="#cb42-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pi</span></span>
<span id="cb42-159"><a href="#cb42-159" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: US monthly inflation rate</span></span>
<span id="cb42-160"><a href="#cb42-160" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb42-161"><a href="#cb42-161" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb42-162"><a href="#cb42-162" aria-hidden="true" tabindex="-1"></a>df_infl_m <span class="op">=</span> df_infl.pct_change().dropna()</span>
<span id="cb42-163"><a href="#cb42-163" aria-hidden="true" tabindex="-1"></a>df_infl_m.index <span class="op">=</span> df_infl_m.index <span class="op">+</span> pd.offsets.MonthEnd(<span class="dv">0</span>) <span class="co"># move to month end</span></span>
<span id="cb42-164"><a href="#cb42-164" aria-hidden="true" tabindex="-1"></a>df_infl_m.plot(ax<span class="op">=</span>ax)</span>
<span id="cb42-165"><a href="#cb42-165" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-166"><a href="#cb42-166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-167"><a href="#cb42-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-168"><a href="#cb42-168" aria-hidden="true" tabindex="-1"></a><span class="fu">### Oil prices</span></span>
<span id="cb42-169"><a href="#cb42-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-170"><a href="#cb42-170" aria-hidden="true" tabindex="-1"></a>Since the focus is on US inflation, below we get WTI oil prices. This data is downloaded from the <span class="co">[</span><span class="ot">St Louis Fed's FRED webpage</span><span class="co">](https://fred.stlouisfed.org/series/DCOILWTICO)</span>.</span>
<span id="cb42-171"><a href="#cb42-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-174"><a href="#cb42-174" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-175"><a href="#cb42-175" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-oil</span></span>
<span id="cb42-176"><a href="#cb42-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: WTI oil prices</span></span>
<span id="cb42-177"><a href="#cb42-177" aria-hidden="true" tabindex="-1"></a>df_oil <span class="op">=</span> pd.read_csv(<span class="st">"docs/DCOILWTICO.csv"</span>)</span>
<span id="cb42-178"><a href="#cb42-178" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DCOILWTICO'</span>] <span class="op">=</span> pd.to_numeric(df_oil[<span class="st">'DCOILWTICO'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb42-179"><a href="#cb42-179" aria-hidden="true" tabindex="-1"></a>df_oil[<span class="st">'DATE'</span>] <span class="op">=</span> pd.to_datetime(df_oil[<span class="st">'DATE'</span>])</span>
<span id="cb42-180"><a href="#cb42-180" aria-hidden="true" tabindex="-1"></a>df_oil.set_index(<span class="st">'DATE'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-181"><a href="#cb42-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-182"><a href="#cb42-182" aria-hidden="true" tabindex="-1"></a>df_oil.plot()</span>
<span id="cb42-183"><a href="#cb42-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-184"><a href="#cb42-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-185"><a href="#cb42-185" aria-hidden="true" tabindex="-1"></a>For the nowcasting, we are interested in the daily variation, clipped because of the sharp movements during the onset of the Covid-19 pandemic:</span>
<span id="cb42-186"><a href="#cb42-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-189"><a href="#cb42-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-190"><a href="#cb42-190" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-oilD</span></span>
<span id="cb42-191"><a href="#cb42-191" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Daily change in WTI oil prices</span></span>
<span id="cb42-192"><a href="#cb42-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-193"><a href="#cb42-193" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb42-194"><a href="#cb42-194" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linewidth<span class="op">=</span><span class="fl">1.5</span>, color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb42-195"><a href="#cb42-195" aria-hidden="true" tabindex="-1"></a>df_oil_d <span class="op">=</span> df_oil.pct_change().dropna()</span>
<span id="cb42-196"><a href="#cb42-196" aria-hidden="true" tabindex="-1"></a>df_oil_d.plot(ax<span class="op">=</span>ax)</span>
<span id="cb42-197"><a href="#cb42-197" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="fl">0.25</span>, <span class="fl">0.25</span>)</span>
<span id="cb42-198"><a href="#cb42-198" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-199"><a href="#cb42-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-200"><a href="#cb42-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-201"><a href="#cb42-201" aria-hidden="true" tabindex="-1"></a><span class="fu">### Temporal features (not implemented for the time being)</span></span>
<span id="cb42-202"><a href="#cb42-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-203"><a href="#cb42-203" aria-hidden="true" tabindex="-1"></a>There is a lot of information encoded in the temporal features of a time series: which day in the month it is, which month of the year, etc. For example, consider how consumers behave differently in response to oil prices over warmer months (when many decide or not to travel, and how far) compared to colder months (when energy prices factor in heating and is thus perhaps less elastic).</span>
<span id="cb42-204"><a href="#cb42-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-205"><a href="#cb42-205" aria-hidden="true" tabindex="-1"></a>To simplify notation about time, instead of the usual subscript $t$ as above to denote a time period, for precision about the frequency, we will follow this convention:</span>
<span id="cb42-206"><a href="#cb42-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-207"><a href="#cb42-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>subscript $m$ denotes a given month;</span>
<span id="cb42-208"><a href="#cb42-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-209"><a href="#cb42-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>subscript $d$ denotes a given day;</span>
<span id="cb42-210"><a href="#cb42-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-211"><a href="#cb42-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>subscript $d(m)$ denotes a given day in a given month; example: $d(m-1)$ is a day in the previous month.</span>
<span id="cb42-212"><a href="#cb42-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-213"><a href="#cb42-213" aria-hidden="true" tabindex="-1"></a><span class="in">`gingado`</span> offers a practical way to set up the temporal features that requires only the dates of the dataset.</span>
<span id="cb42-214"><a href="#cb42-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-217"><a href="#cb42-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-218"><a href="#cb42-218" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tempfeatures</span></span>
<span id="cb42-219"><a href="#cb42-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-220"><a href="#cb42-220" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: to add documentation and tests, and later incorporate as a new function in gingado.utils</span></span>
<span id="cb42-221"><a href="#cb42-221" aria-hidden="true" tabindex="-1"></a><span class="co"># TO-DO: return also vocab_sizes dictionary, required to set up embedding layer.</span></span>
<span id="cb42-222"><a href="#cb42-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-223"><a href="#cb42-223" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_timefeat(df, freq<span class="op">=</span><span class="st">"d"</span>, features<span class="op">=</span><span class="va">None</span>, add_to_df<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb42-224"><a href="#cb42-224" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For the future documentation: the add_to_df argument should be True if the data will be fed to an algorithm that takes in all data at once. If, like neural networks, the inputs are fed through different "pipelines", then use False and then take the result from this function an feed it separately to a neural network.</span></span>
<span id="cb42-225"><a href="#cb42-225" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the frequency is used to filter which features to add. For example, if monthly then no higher frequency features (day of ..., week of... ) are added because it doesn't make sense</span></span>
<span id="cb42-226"><a href="#cb42-226" aria-hidden="true" tabindex="-1"></a>    <span class="co"># None or list. if futures is None, then add all temporal features that the frequency above allows. Otherwise adds only the names ones</span></span>
<span id="cb42-227"><a href="#cb42-227" aria-hidden="true" tabindex="-1"></a>    all_freqs <span class="op">=</span> [<span class="st">"q"</span>, <span class="st">"m"</span>, <span class="st">"w"</span>, <span class="st">"d"</span>]</span>
<span id="cb42-228"><a href="#cb42-228" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb42-229"><a href="#cb42-229" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> pd.api.types.is_datetime64_any_dtype(df.index):</span>
<span id="cb42-230"><a href="#cb42-230" aria-hidden="true" tabindex="-1"></a>        df.index <span class="op">=</span> pd.to_datetime(df.index)</span>
<span id="cb42-231"><a href="#cb42-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-232"><a href="#cb42-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-233"><a href="#cb42-233" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> i2s(index, df<span class="op">=</span>df):</span>
<span id="cb42-234"><a href="#cb42-234" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mini-helper func that transforms an index into a pandas Series with the index</span></span>
<span id="cb42-235"><a href="#cb42-235" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.Series(index, index<span class="op">=</span>df.index)</span>
<span id="cb42-236"><a href="#cb42-236" aria-hidden="true" tabindex="-1"></a>    dict_timefeat <span class="op">=</span> {}</span>
<span id="cb42-237"><a href="#cb42-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-238"><a href="#cb42-238" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> all_freqs:</span>
<span id="cb42-239"><a href="#cb42-239" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'year_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_year_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb42-240"><a href="#cb42-240" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_of_year'</span>] <span class="op">=</span> i2s(df.index.quarter)</span>
<span id="cb42-241"><a href="#cb42-241" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'quarter_end'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x.is_quarter_end <span class="cf">else</span> <span class="dv">0</span>))</span>
<span id="cb42-242"><a href="#cb42-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-243"><a href="#cb42-243" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>]]:</span>
<span id="cb42-244"><a href="#cb42-244" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb42-245"><a href="#cb42-245" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'month_of_year'</span>] <span class="op">=</span> i2s(df.index.month)</span>
<span id="cb42-246"><a href="#cb42-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-247"><a href="#cb42-247" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="kw">in</span> [f <span class="cf">for</span> f <span class="kw">in</span> all_freqs <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> [<span class="st">"y"</span>, <span class="st">"q"</span>, <span class="st">"m"</span>]]:</span>
<span id="cb42-248"><a href="#cb42-248" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_month'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x.day <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">7</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb42-249"><a href="#cb42-249" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: ((x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>(x.month <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">3</span> <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">-01'</span>)).days <span class="op">//</span> <span class="dv">7</span>) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb42-250"><a href="#cb42-250" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'week_of_year'</span>] <span class="op">=</span> i2s(df.index.isocalendar().week)</span>
<span id="cb42-251"><a href="#cb42-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-252"><a href="#cb42-252" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> freq <span class="op">==</span> <span class="st">"d"</span>:</span>
<span id="cb42-253"><a href="#cb42-253" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_week'</span>] <span class="op">=</span> i2s(df.index.dayofweek)</span>
<span id="cb42-254"><a href="#cb42-254" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_month'</span>] <span class="op">=</span> i2s(df.index.day)</span>
<span id="cb42-255"><a href="#cb42-255" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_quarter'</span>] <span class="op">=</span> i2s(df.index.to_series().<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x <span class="op">-</span> pd.Timestamp(<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">.</span>year<span class="sc">}</span><span class="ss">-01-01'</span>)).days <span class="op">%</span> <span class="dv">91</span> <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb42-256"><a href="#cb42-256" aria-hidden="true" tabindex="-1"></a>        dict_timefeat[<span class="st">'day_of_year'</span>] <span class="op">=</span> i2s(df.index.dayofyear)</span>
<span id="cb42-257"><a href="#cb42-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-258"><a href="#cb42-258" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the dictionary of columns to a DataFrame</span></span>
<span id="cb42-259"><a href="#cb42-259" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> pd.concat(dict_timefeat, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-260"><a href="#cb42-260" aria-hidden="true" tabindex="-1"></a>    var_thresh <span class="op">=</span> VarianceThreshold(threshold<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-261"><a href="#cb42-261" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> var_thresh.fit_transform(df_timefeat)</span>
<span id="cb42-262"><a href="#cb42-262" aria-hidden="true" tabindex="-1"></a>    df_timefeat <span class="op">=</span> pd.DataFrame(df_timefeat, columns<span class="op">=</span>var_thresh.get_feature_names_out(), index<span class="op">=</span>df.index).astype(<span class="bu">int</span>)</span>
<span id="cb42-263"><a href="#cb42-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-264"><a href="#cb42-264" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features:</span>
<span id="cb42-265"><a href="#cb42-265" aria-hidden="true" tabindex="-1"></a>        df_timefeat[features]</span>
<span id="cb42-266"><a href="#cb42-266" aria-hidden="true" tabindex="-1"></a>    vocab_sizes <span class="op">=</span> {col: df_timefeat[col].nunique() <span class="op">+</span> <span class="dv">1</span> <span class="cf">for</span> col <span class="kw">in</span> df_timefeat}</span>
<span id="cb42-267"><a href="#cb42-267" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> add_to_df:</span>
<span id="cb42-268"><a href="#cb42-268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pd.concat([df, df_timefeat], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-269"><a href="#cb42-269" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb42-270"><a href="#cb42-270" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> df_timefeat</span>
<span id="cb42-271"><a href="#cb42-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-272"><a href="#cb42-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-273"><a href="#cb42-273" aria-hidden="true" tabindex="-1"></a>Specifically, temporal features are an excellent (and rare) type of *known future* input. Those are the data that we know will be like that during forecasting time, ie, at the time the observation $y_t$ takes place. For example, it is trivial to know the day of the week, of the month etc, for any date we are forecasting in.</span>
<span id="cb42-274"><a href="#cb42-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-275"><a href="#cb42-275" aria-hidden="true" tabindex="-1"></a>For this reason, we now calculate the temporal features of inflation.</span>
<span id="cb42-276"><a href="#cb42-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-279"><a href="#cb42-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-280"><a href="#cb42-280" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: temporal features for the inflation series</span></span>
<span id="cb42-281"><a href="#cb42-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-282"><a href="#cb42-282" aria-hidden="true" tabindex="-1"></a>df_timefeat <span class="op">=</span> get_timefeat(df_infl_m, freq<span class="op">=</span><span class="st">"m"</span>)</span>
<span id="cb42-283"><a href="#cb42-283" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-284"><a href="#cb42-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-285"><a href="#cb42-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-286"><a href="#cb42-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-287"><a href="#cb42-287" aria-hidden="true" tabindex="-1"></a><span class="fu">### Splitting the dataset</span></span>
<span id="cb42-288"><a href="#cb42-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-289"><a href="#cb42-289" aria-hidden="true" tabindex="-1"></a>We will now split the dataset into training data up until end-2020 and validation data afterwards. The training data will be further split into 5 temporally sequential folds.<span class="ot">[^tssplit]</span></span>
<span id="cb42-290"><a href="#cb42-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-291"><a href="#cb42-291" aria-hidden="true" tabindex="-1"></a><span class="ot">[^tssplit]: </span>See <span class="co">[</span><span class="ot">here</span><span class="co">](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split)</span> for more information on time series splitting.</span>
<span id="cb42-292"><a href="#cb42-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-293"><a href="#cb42-293" aria-hidden="true" tabindex="-1"></a>To simplify, we will consider valid nowcasting *input* data for a given output in period $m$ as:</span>
<span id="cb42-294"><a href="#cb42-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-295"><a href="#cb42-295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>all monthly data up to, and including, $m-1$; and</span>
<span id="cb42-296"><a href="#cb42-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-297"><a href="#cb42-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>all daily data up to, and including, $d(m)$.</span>
<span id="cb42-298"><a href="#cb42-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-301"><a href="#cb42-301" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-302"><a href="#cb42-302" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "time series splits"</span></span>
<span id="cb42-303"><a href="#cb42-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-304"><a href="#cb42-304" aria-hidden="true" tabindex="-1"></a><span class="co"># Training date cutoff</span></span>
<span id="cb42-305"><a href="#cb42-305" aria-hidden="true" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="st">"2020-12-31"</span></span>
<span id="cb42-306"><a href="#cb42-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-307"><a href="#cb42-307" aria-hidden="true" tabindex="-1"></a>y_train, y_test <span class="op">=</span> df_infl_m[:cutoff][<span class="dv">1</span>:], df_infl_m[cutoff:][<span class="dv">1</span>:]</span>
<span id="cb42-308"><a href="#cb42-308" aria-hidden="true" tabindex="-1"></a>Xm_train, Xm_test <span class="op">=</span> df_infl_m[:cutoff][:<span class="op">-</span><span class="dv">1</span>], df_infl_m[cutoff:][:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-309"><a href="#cb42-309" aria-hidden="true" tabindex="-1"></a>Xd_train, Xd_test <span class="op">=</span> df_oil_d[:cutoff], df_oil_d[cutoff:]</span>
<span id="cb42-310"><a href="#cb42-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-311"><a href="#cb42-311" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> {<span class="st">"m"</span>: Xm_train, <span class="st">"d"</span>: Xd_train}</span>
<span id="cb42-312"><a href="#cb42-312" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> {<span class="st">"m"</span>: Xm_test, <span class="st">"d"</span>: Xd_test}</span>
<span id="cb42-313"><a href="#cb42-313" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-314"><a href="#cb42-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-315"><a href="#cb42-315" aria-hidden="true" tabindex="-1"></a>Now for every month $m$ in the dependent variable, we can find all $m_{t-l}, l\geq 1$ and all $d(m_{t-s}), s\geq 0$.</span>
<span id="cb42-316"><a href="#cb42-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-317"><a href="#cb42-317" aria-hidden="true" tabindex="-1"></a>Note that in the example below, for each data point that we want to forecast (<span class="in">`y`</span>), we take 12 lags of the monthly covariates and 250 lags of the daily covariates.</span>
<span id="cb42-318"><a href="#cb42-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-321"><a href="#cb42-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-322"><a href="#cb42-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: create data</span></span>
<span id="cb42-323"><a href="#cb42-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-324"><a href="#cb42-324" aria-hidden="true" tabindex="-1"></a>maxlags <span class="op">=</span> {<span class="st">"m"</span>: <span class="dv">12</span>, <span class="st">"d"</span>: <span class="dv">250</span>}</span>
<span id="cb42-325"><a href="#cb42-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-326"><a href="#cb42-326" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_data(X, y, maxlags<span class="op">=</span>maxlags, tscv<span class="op">=</span>TimeSeriesSplit(n_splits<span class="op">=</span><span class="dv">5</span>), timedim<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb42-327"><a href="#cb42-327" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If timeedim is true, then the dimensions of the tensors are n_samples/time dimension/features. If not then it is n_samples/time dimension (lag) * features, ie each lag is flattened as if it were a feature. Use True when passing to recurrent nets, use False for fully connected layers.</span></span>
<span id="cb42-328"><a href="#cb42-328" aria-hidden="true" tabindex="-1"></a>    X_split <span class="op">=</span> {}</span>
<span id="cb42-329"><a href="#cb42-329" aria-hidden="true" tabindex="-1"></a>    y_split <span class="op">=</span> {}</span>
<span id="cb42-330"><a href="#cb42-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-331"><a href="#cb42-331" aria-hidden="true" tabindex="-1"></a>    n_feat <span class="op">=</span> {k: v.shape[<span class="dv">1</span>] <span class="cf">for</span> k, v <span class="kw">in</span> X.items()}</span>
<span id="cb42-332"><a href="#cb42-332" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> tscv:</span>
<span id="cb42-333"><a href="#cb42-333" aria-hidden="true" tabindex="-1"></a>        split_cv <span class="op">=</span> tscv.split(y)</span>
<span id="cb42-334"><a href="#cb42-334" aria-hidden="true" tabindex="-1"></a>        cv_dates <span class="op">=</span> [</span>
<span id="cb42-335"><a href="#cb42-335" aria-hidden="true" tabindex="-1"></a>            (y.index[m], y.index[n]) <span class="co"># (train, valid) for each fold</span></span>
<span id="cb42-336"><a href="#cb42-336" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> m, n <span class="kw">in</span> split_cv</span>
<span id="cb42-337"><a href="#cb42-337" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb42-338"><a href="#cb42-338" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n_fold, split <span class="kw">in</span> <span class="bu">enumerate</span>(cv_dates):</span>
<span id="cb42-339"><a href="#cb42-339" aria-hidden="true" tabindex="-1"></a>            fold <span class="op">=</span> <span class="ss">f"fold_</span><span class="sc">{</span>n_fold<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb42-340"><a href="#cb42-340" aria-hidden="true" tabindex="-1"></a>            dates_split <span class="op">=</span> {</span>
<span id="cb42-341"><a href="#cb42-341" aria-hidden="true" tabindex="-1"></a>                <span class="st">"train"</span>: split[<span class="dv">0</span>],</span>
<span id="cb42-342"><a href="#cb42-342" aria-hidden="true" tabindex="-1"></a>                <span class="st">"valid"</span>: split[<span class="dv">1</span>]</span>
<span id="cb42-343"><a href="#cb42-343" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb42-344"><a href="#cb42-344" aria-hidden="true" tabindex="-1"></a>            X_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb42-345"><a href="#cb42-345" aria-hidden="true" tabindex="-1"></a>            y_split[fold] <span class="op">=</span> {<span class="st">"train"</span>: [], <span class="st">"valid"</span>: []}</span>
<span id="cb42-346"><a href="#cb42-346" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> chunk, dates <span class="kw">in</span> dates_split.items():</span>
<span id="cb42-347"><a href="#cb42-347" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> ysample_date <span class="kw">in</span> tqdm(dates):</span>
<span id="cb42-348"><a href="#cb42-348" aria-hidden="true" tabindex="-1"></a>                    padded_x <span class="op">=</span> {}</span>
<span id="cb42-349"><a href="#cb42-349" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> f <span class="kw">in</span> X.keys():</span>
<span id="cb42-350"><a href="#cb42-350" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">try</span>:</span>
<span id="cb42-351"><a href="#cb42-351" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> X[f][:ysample_date][:<span class="op">-</span><span class="dv">1</span>].values</span>
<span id="cb42-352"><a href="#cb42-352" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">except</span> <span class="pp">KeyError</span>:</span>
<span id="cb42-353"><a href="#cb42-353" aria-hidden="true" tabindex="-1"></a>                            to_pad <span class="op">=</span> np.zeros((<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb42-354"><a href="#cb42-354" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> keras.utils.pad_sequences([to_pad], maxlen<span class="op">=</span>maxlags[f], dtype<span class="op">=</span>np.float32)</span>
<span id="cb42-355"><a href="#cb42-355" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb42-356"><a href="#cb42-356" aria-hidden="true" tabindex="-1"></a>                        x_shape <span class="op">=</span> (<span class="dv">1</span>, maxlags[f], n_feat[f]) <span class="cf">if</span> timedim <span class="cf">else</span> (<span class="dv">1</span>, maxlags[f] <span class="op">*</span> n_feat[f])</span>
<span id="cb42-357"><a href="#cb42-357" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb42-358"><a href="#cb42-358" aria-hidden="true" tabindex="-1"></a>                        padded_x[f] <span class="op">=</span> padded_x[f].reshape(x_shape)</span>
<span id="cb42-359"><a href="#cb42-359" aria-hidden="true" tabindex="-1"></a>                    X_split[fold][chunk].append(padded_x)</span>
<span id="cb42-360"><a href="#cb42-360" aria-hidden="true" tabindex="-1"></a>                    y_split[fold][chunk].append(y_train.loc[ysample_date])</span>
<span id="cb42-361"><a href="#cb42-361" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X_split, y_split</span>
<span id="cb42-362"><a href="#cb42-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-363"><a href="#cb42-363" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb42-364"><a href="#cb42-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-365"><a href="#cb42-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-366"><a href="#cb42-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-367"><a href="#cb42-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-368"><a href="#cb42-368" aria-hidden="true" tabindex="-1"></a>Let's see how this time series fold will be structured. Each fold is a sequentially longer window, so we get the following data points:</span>
<span id="cb42-369"><a href="#cb42-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-372"><a href="#cb42-372" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-373"><a href="#cb42-373" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: "Create batches of data"</span></span>
<span id="cb42-374"><a href="#cb42-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-375"><a href="#cb42-375" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> fold <span class="kw">in</span> y_train_split.keys():</span>
<span id="cb42-376"><a href="#cb42-376" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>fold<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb42-377"><a href="#cb42-377" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'train'</span>])<span class="sc">}</span><span class="ss"> training X-y pairs"</span>)</span>
<span id="cb42-378"><a href="#cb42-378" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span><span class="bu">len</span>(y_train_split[fold][<span class="st">'valid'</span>])<span class="sc">}</span><span class="ss"> validation X-y pairs"</span>)</span>
<span id="cb42-379"><a href="#cb42-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-380"><a href="#cb42-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-381"><a href="#cb42-381" aria-hidden="true" tabindex="-1"></a><span class="fu">## First model: a fully connected neural network {#sec-fc}</span></span>
<span id="cb42-382"><a href="#cb42-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-383"><a href="#cb42-383" aria-hidden="true" tabindex="-1"></a>The goal of this model is to nowcast $\pi_t$ based on its past values $\pi_{t-1}$ and on current oil prices $o_{d(m-s)}, s \geq 0$. The first model we will train is a very simple neural network:</span>
<span id="cb42-384"><a href="#cb42-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-385"><a href="#cb42-385" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-386"><a href="#cb42-386" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb42-387"><a href="#cb42-387" aria-hidden="true" tabindex="-1"></a>\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) <span class="sc">\\</span></span>
<span id="cb42-388"><a href="#cb42-388" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mathbf{W}_2 \xi + \mathbf{b}_2,</span>
<span id="cb42-389"><a href="#cb42-389" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb42-390"><a href="#cb42-390" aria-hidden="true" tabindex="-1"></a>$$ {#eq-model0nnlayer}</span>
<span id="cb42-391"><a href="#cb42-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-392"><a href="#cb42-392" aria-hidden="true" tabindex="-1"></a>where $x_t$ is the input data and the subscript of parameters relates to the "depth" of the layer they belong to. Using $\lambda$ as the dimensionality of the model, $\mathbf{W}_2 \in \mathbb{R}^{1 \times \lambda}$, $b_2 \in \mathbb{R}$, $\mathbf{W}_1 \in \mathbb{R}^{\lambda \times |x_t|}$, $b_1 \in \mathbb{R}^{d}$, $\xi \in \mathbb{R}^\lambda$ and $\phi$ is an activation function. For simplicity, we will use the ReLU activation function, which is simply: $\phi(z) = \text{max}(z, 0)$.</span>
<span id="cb42-393"><a href="#cb42-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-394"><a href="#cb42-394" aria-hidden="true" tabindex="-1"></a>For this neural network, we need a fix dimensionality of the input data. In other words, the network *needs* to know how much data it will take in at any given time, and this should not change throughout training or inference time.</span>
<span id="cb42-395"><a href="#cb42-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-396"><a href="#cb42-396" aria-hidden="true" tabindex="-1"></a>For each data in our dependent variable, we simply stack the latest available monthly and daily data and their respective lags. Using the numbers above, this would be 12 lags for monthly data  and 250 lags of daily oil data. Linking this to @eq-model0nnlayer above, $x_t = <span class="co">[</span><span class="ot">\pi_{m-1}, \dots, \pi_{m-12}, o_d, \dots, o_{d-250}</span><span class="co">]</span>$. </span>
<span id="cb42-397"><a href="#cb42-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-398"><a href="#cb42-398" aria-hidden="true" tabindex="-1"></a>All of this data will be considered by the neural network at the same time. In a way, this is analogous to how a normal regression is run. However, the number of data points (12 + 250) used in this toy neural network is bigger than typical regressions.</span>
<span id="cb42-399"><a href="#cb42-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-402"><a href="#cb42-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-403"><a href="#cb42-403" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-nn_fc_summary</span></span>
<span id="cb42-404"><a href="#cb42-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of fully connected model</span></span>
<span id="cb42-405"><a href="#cb42-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-406"><a href="#cb42-406" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> createNN_fc(dim<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">"relu"</span>):</span>
<span id="cb42-407"><a href="#cb42-407" aria-hidden="true" tabindex="-1"></a>    nn_fc <span class="op">=</span> keras.Sequential([</span>
<span id="cb42-408"><a href="#cb42-408" aria-hidden="true" tabindex="-1"></a>        keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),)),</span>
<span id="cb42-409"><a href="#cb42-409" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span>activation, name<span class="op">=</span><span class="st">"SummariseInput"</span>),</span>
<span id="cb42-410"><a href="#cb42-410" aria-hidden="true" tabindex="-1"></a>        keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)</span>
<span id="cb42-411"><a href="#cb42-411" aria-hidden="true" tabindex="-1"></a>    ], <span class="st">"FullyConnected"</span>)</span>
<span id="cb42-412"><a href="#cb42-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-413"><a href="#cb42-413" aria-hidden="true" tabindex="-1"></a>    nn_fc.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb42-414"><a href="#cb42-414" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nn_fc</span>
<span id="cb42-415"><a href="#cb42-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-416"><a href="#cb42-416" aria-hidden="true" tabindex="-1"></a>nn_fc <span class="op">=</span> createNN_fc()</span>
<span id="cb42-417"><a href="#cb42-417" aria-hidden="true" tabindex="-1"></a>nn_fc.summary()</span>
<span id="cb42-418"><a href="#cb42-418" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-419"><a href="#cb42-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-420"><a href="#cb42-420" aria-hidden="true" tabindex="-1"></a>@fig-archfc presents the architecture of this model.</span>
<span id="cb42-421"><a href="#cb42-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-424"><a href="#cb42-424" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-425"><a href="#cb42-425" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-archfc</span></span>
<span id="cb42-426"><a href="#cb42-426" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the model with fully connected layer</span></span>
<span id="cb42-427"><a href="#cb42-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-428"><a href="#cb42-428" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-429"><a href="#cb42-429" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-430"><a href="#cb42-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-431"><a href="#cb42-431" aria-hidden="true" tabindex="-1"></a>Note in @fig-archfc that a first dense layer (ie, as in @eq-model0nnlayer) takes in 262 data points, uses the activation function ReLU, and then outputs 16 data points for the next layer.<span class="ot">[^NoneDim]</span></span>
<span id="cb42-432"><a href="#cb42-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-433"><a href="#cb42-433" aria-hidden="true" tabindex="-1"></a><span class="ot">[^NoneDim]: </span>Ignore the <span class="in">`None`</span> in the shapes; this means it dependets on each case. Specifically, the first dimension of the shapes in keras are always the number of samples going in the model. Because this varies with every training, application, etc, it is not fixed as the other dimensions are. Another way of thinking about this is as follows: in a regression, you know exactly how many variables you need to have, but the number of data points can vary.</span>
<span id="cb42-434"><a href="#cb42-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-435"><a href="#cb42-435" aria-hidden="true" tabindex="-1"></a>Checking that it works. In the code below, we take the last fold as an example. </span>
<span id="cb42-436"><a href="#cb42-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-437"><a href="#cb42-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-440"><a href="#cb42-440" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-441"><a href="#cb42-441" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: preparing fully connected NN data</span></span>
<span id="cb42-442"><a href="#cb42-442" aria-hidden="true" tabindex="-1"></a>X_train_split_fc, y_train_split_fc <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, timedim<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-443"><a href="#cb42-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-444"><a href="#cb42-444" aria-hidden="true" tabindex="-1"></a>fold <span class="op">=</span> <span class="st">"fold_4"</span></span>
<span id="cb42-445"><a href="#cb42-445" aria-hidden="true" tabindex="-1"></a>X_train_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"train"</span>]])</span>
<span id="cb42-446"><a href="#cb42-446" aria-hidden="true" tabindex="-1"></a>y_train_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"train"</span>])</span>
<span id="cb42-447"><a href="#cb42-447" aria-hidden="true" tabindex="-1"></a>X_valid_fc <span class="op">=</span> np.array([np.concatenate([np.squeeze(v) <span class="cf">for</span> v <span class="kw">in</span> sample.values()]) <span class="cf">for</span> sample <span class="kw">in</span> X_train_split_fc[fold][<span class="st">"valid"</span>]])</span>
<span id="cb42-448"><a href="#cb42-448" aria-hidden="true" tabindex="-1"></a>y_valid_fc <span class="op">=</span> np.array(y_train_split_fc[fold][<span class="st">"valid"</span>])</span>
<span id="cb42-449"><a href="#cb42-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-450"><a href="#cb42-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-451"><a href="#cb42-451" aria-hidden="true" tabindex="-1"></a>Now we train the network.</span>
<span id="cb42-452"><a href="#cb42-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-455"><a href="#cb42-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-456"><a href="#cb42-456" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: check nn model works</span></span>
<span id="cb42-457"><a href="#cb42-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-458"><a href="#cb42-458" aria-hidden="true" tabindex="-1"></a>history_fc <span class="op">=</span> nn_fc.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-459"><a href="#cb42-459" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-460"><a href="#cb42-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-461"><a href="#cb42-461" aria-hidden="true" tabindex="-1"></a>You can see in @fig-history_fc that the loss decreases with training, but the validation loss (ie, calculated on held-out data) is higher than the in-sample loss. This suggests the model is learning *too much* how to fit the data. In other words, it is also fitting some level of noise, which is not reproducible out-of-sample.</span>
<span id="cb42-462"><a href="#cb42-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-465"><a href="#cb42-465" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-466"><a href="#cb42-466" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_fc</span></span>
<span id="cb42-467"><a href="#cb42-467" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a simple, fully-connected neural network.</span></span>
<span id="cb42-468"><a href="#cb42-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-469"><a href="#cb42-469" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> pd.DataFrame(history_fc.history).plot()</span>
<span id="cb42-470"><a href="#cb42-470" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb42-471"><a href="#cb42-471" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb42-472"><a href="#cb42-472" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-473"><a href="#cb42-473" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-474"><a href="#cb42-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-475"><a href="#cb42-475" aria-hidden="true" tabindex="-1"></a>We don't need to bother training too much this very simple neural network; the goal here is to use it as a building block for a mathematical/econometric intuition of the broader nowcasting model.</span>
<span id="cb42-476"><a href="#cb42-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-477"><a href="#cb42-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## A useful tool: a gate {#sec-gate}</span></span>
<span id="cb42-478"><a href="#cb42-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-479"><a href="#cb42-479" aria-hidden="true" tabindex="-1"></a>The network trained in the previous section can learn how to map the input data to the output data. But there are ways to take advantage of the incredible flexibility in architecture (ie, how neural layers are stacked). One such way is to have the data inform which layers's outputs are actually used downstream or not. This section describes how.</span>
<span id="cb42-480"><a href="#cb42-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-481"><a href="#cb42-481" aria-hidden="true" tabindex="-1"></a>First, we replicate the same simple fully connected layer of @eq-model0nnlayer two times. One of the neural networks will behave as before: learning to map the input data to the output data. The second one will also look at the same data, but with a different goal: it will learn how much data to let through. Its output is a value between 0 and 1, which is then multiplied to the "original" network. When this part of the neural network yields values closer to 0, the mainstream values are effectively shut down. Conversely, when the values are close to 1, the data proceeds as normal.</span>
<span id="cb42-482"><a href="#cb42-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-483"><a href="#cb42-483" aria-hidden="true" tabindex="-1"></a>Adjusting @eq-model0nnlayer to include a gate could be as in @eq-gated:</span>
<span id="cb42-484"><a href="#cb42-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-485"><a href="#cb42-485" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-486"><a href="#cb42-486" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb42-487"><a href="#cb42-487" aria-hidden="true" tabindex="-1"></a>\xi &amp;= \phi(\mathbf{W}_1 x_t + \mathbf{b}_1) <span class="sc">\\</span></span>
<span id="cb42-488"><a href="#cb42-488" aria-hidden="true" tabindex="-1"></a>G &amp;= \sigma(\mathbf{W}_G x_t + \mathbf{b}_G) <span class="sc">\\</span></span>
<span id="cb42-489"><a href="#cb42-489" aria-hidden="true" tabindex="-1"></a>y_t &amp;= \mathbf{W}_2 (\xi \odot G) + \mathbf{b}_2,</span>
<span id="cb42-490"><a href="#cb42-490" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb42-491"><a href="#cb42-491" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gated}</span>
<span id="cb42-492"><a href="#cb42-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-493"><a href="#cb42-493" aria-hidden="true" tabindex="-1"></a>with $\sigma$ representing the sigmoid function and $\odot$ the Hadamard multiplication.</span>
<span id="cb42-494"><a href="#cb42-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-495"><a href="#cb42-495" aria-hidden="true" tabindex="-1"></a>Note that the data that informs the gate does not necessarily need to be the same as the mainstream data.</span>
<span id="cb42-496"><a href="#cb42-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-499"><a href="#cb42-499" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-500"><a href="#cb42-500" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-nn_fc_gated_summary</span></span>
<span id="cb42-501"><a href="#cb42-501" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of gated model</span></span>
<span id="cb42-502"><a href="#cb42-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-503"><a href="#cb42-503" aria-hidden="true" tabindex="-1"></a>mainstream <span class="op">=</span> createNN_fc(activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb42-504"><a href="#cb42-504" aria-hidden="true" tabindex="-1"></a>gate <span class="op">=</span> createNN_fc(activation<span class="op">=</span><span class="st">"sigmoid"</span>)</span>
<span id="cb42-505"><a href="#cb42-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-506"><a href="#cb42-506" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb42-507"><a href="#cb42-507" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="bu">sum</span>([v <span class="cf">for</span> v <span class="kw">in</span> maxlags.values()]),), name<span class="op">=</span><span class="st">"FlattenedLaggedData"</span>)</span>
<span id="cb42-508"><a href="#cb42-508" aria-hidden="true" tabindex="-1"></a>mainstream <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"relu"</span>, name<span class="op">=</span><span class="st">"SummariseInput"</span>)(<span class="bu">input</span>)</span>
<span id="cb42-509"><a href="#cb42-509" aria-hidden="true" tabindex="-1"></a>gate <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span>dim, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"Gate"</span>)(<span class="bu">input</span>)</span>
<span id="cb42-510"><a href="#cb42-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-511"><a href="#cb42-511" aria-hidden="true" tabindex="-1"></a>gated_data <span class="op">=</span> keras.layers.Multiply(name<span class="op">=</span><span class="st">"GatedData"</span>)([mainstream, gate])</span>
<span id="cb42-512"><a href="#cb42-512" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>, name<span class="op">=</span><span class="st">"CalculateOutput"</span>)(gated_data)</span>
<span id="cb42-513"><a href="#cb42-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-514"><a href="#cb42-514" aria-hidden="true" tabindex="-1"></a>nn_fc_gated <span class="op">=</span> keras.Model(inputs<span class="op">=</span><span class="bu">input</span>, outputs<span class="op">=</span>output, name<span class="op">=</span><span class="st">"GatedModel"</span>)</span>
<span id="cb42-515"><a href="#cb42-515" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb42-516"><a href="#cb42-516" aria-hidden="true" tabindex="-1"></a>nn_fc_gated.summary()</span>
<span id="cb42-517"><a href="#cb42-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-518"><a href="#cb42-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-521"><a href="#cb42-521" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-522"><a href="#cb42-522" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-arch_gatedmodel</span></span>
<span id="cb42-523"><a href="#cb42-523" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of gated neural network</span></span>
<span id="cb42-524"><a href="#cb42-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-525"><a href="#cb42-525" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_fc_gated, show_layer_names<span class="op">=</span><span class="va">True</span>, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-526"><a href="#cb42-526" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-527"><a href="#cb42-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-528"><a href="#cb42-528" aria-hidden="true" tabindex="-1"></a>The model can now be fit with the same input data: it is then used by two different branches (referred to above in the text as "mainstream" and "gate").</span>
<span id="cb42-529"><a href="#cb42-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-532"><a href="#cb42-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-533"><a href="#cb42-533" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fitting gated model</span></span>
<span id="cb42-534"><a href="#cb42-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-535"><a href="#cb42-535" aria-hidden="true" tabindex="-1"></a>history_fc_gated <span class="op">=</span> nn_fc_gated.fit(x<span class="op">=</span>X_train_fc, y<span class="op">=</span>y_train_fc, validation_data<span class="op">=</span>(X_valid_fc, y_valid_fc), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-536"><a href="#cb42-536" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-537"><a href="#cb42-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-540"><a href="#cb42-540" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-541"><a href="#cb42-541" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_fc_gated</span></span>
<span id="cb42-542"><a href="#cb42-542" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in a simple, fully-connected neural network with gate.</span></span>
<span id="cb42-543"><a href="#cb42-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-544"><a href="#cb42-544" aria-hidden="true" tabindex="-1"></a>gated_loss <span class="op">=</span> pd.DataFrame(history_fc_gated.history)</span>
<span id="cb42-545"><a href="#cb42-545" aria-hidden="true" tabindex="-1"></a>gated_loss[<span class="st">"val_loss (no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb42-546"><a href="#cb42-546" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> gated_loss.plot()</span>
<span id="cb42-547"><a href="#cb42-547" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb42-548"><a href="#cb42-548" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb42-549"><a href="#cb42-549" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-550"><a href="#cb42-550" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-551"><a href="#cb42-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-552"><a href="#cb42-552" aria-hidden="true" tabindex="-1"></a><span class="fu">## Second model: Long short-term memory {#sec-lstm}</span></span>
<span id="cb42-553"><a href="#cb42-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-554"><a href="#cb42-554" aria-hidden="true" tabindex="-1"></a>A marked improvement in how we can model time series data is the use of recurrent neural networks (RNNs). In essence, these are networks that learn to keep a stateful memory, which is updated as the network "visits" each sequential step in time, in turn using both the memory and the new data at that period to predict the output.</span>
<span id="cb42-555"><a href="#cb42-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-556"><a href="#cb42-556" aria-hidden="true" tabindex="-1"></a>In contrast to the fully connected layer in @sec-fc, which need to look at different lags to pick up any history-dependent information, RNNs look at the observable variables at each period and learn a latent "state" (akin to Kalman filters, for example). The same network then slides up one step in time and uses that information and the previous state to update the state, and so on...</span>
<span id="cb42-557"><a href="#cb42-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-558"><a href="#cb42-558" aria-hidden="true" tabindex="-1"></a>One particular type of RNN that has proven to be very successful in practice is the long short-term memory (LSTM) model, due to @hochreiter1997long. It is actually a combination of four different layers, of which three are actually gates. These layers are built in a specific way. Here's how:</span>
<span id="cb42-559"><a href="#cb42-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-560"><a href="#cb42-560" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb42-561"><a href="#cb42-561" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb42-562"><a href="#cb42-562" aria-hidden="true" tabindex="-1"></a>f_t &amp;= \sigma(W_f x_t + U_f h_{t-1} + b_f) <span class="sc">\\</span></span>
<span id="cb42-563"><a href="#cb42-563" aria-hidden="true" tabindex="-1"></a>i_t &amp;= \sigma(W_i x_t + U_i h_{t-1} + b_i) <span class="sc">\\</span></span>
<span id="cb42-564"><a href="#cb42-564" aria-hidden="true" tabindex="-1"></a>o_t &amp;= \sigma(W_o x_t + U_o h_{t-1} + b_o) <span class="sc">\\</span></span>
<span id="cb42-565"><a href="#cb42-565" aria-hidden="true" tabindex="-1"></a>\tilde{c}_t &amp;= \omega(W_c x_t + U_c h_{t-1} + b_c) <span class="sc">\\</span></span>
<span id="cb42-566"><a href="#cb42-566" aria-hidden="true" tabindex="-1"></a>c_t &amp;= \underbrace{f_t \odot c_{t-1}}_{\text{Gated past data}} + \underbrace{i_t \odot \tilde{c}_t}_{\text{How much to learn}} <span class="sc">\\</span></span>
<span id="cb42-567"><a href="#cb42-567" aria-hidden="true" tabindex="-1"></a>h_t &amp;= o_t \odot \omega(c_t),</span>
<span id="cb42-568"><a href="#cb42-568" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb42-569"><a href="#cb42-569" aria-hidden="true" tabindex="-1"></a>$$ {#eq-model1lstm}</span>
<span id="cb42-570"><a href="#cb42-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-571"><a href="#cb42-571" aria-hidden="true" tabindex="-1"></a>where $\omega$ is the hyperbolic function.</span>
<span id="cb42-572"><a href="#cb42-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-573"><a href="#cb42-573" aria-hidden="true" tabindex="-1"></a>The basic intuition of the LSTM is that some of the individual component layers essentially learn to look at the current data and the past memory and then decide how much new information to let through. Note that, because their activation is a sigmoid, the output of layers $f_t$, $i_t$ and $o_t$ is a number between 0 and 1. This idea is important to bear in mind because it will be used at a much bigger scale by the whole TFT model - and will be one key feature of its interpretability.</span>
<span id="cb42-574"><a href="#cb42-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-575"><a href="#cb42-575" aria-hidden="true" tabindex="-1"></a>With LSTM networks, it is easier to incorporate mixed-frequency data in a meaningful way. This is done below by passing data of each frequency through their own LSTM layers, and combining their output.</span>
<span id="cb42-576"><a href="#cb42-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-579"><a href="#cb42-579" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-580"><a href="#cb42-580" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-lstm_summary</span></span>
<span id="cb42-581"><a href="#cb42-581" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: Summary of LSTM model</span></span>
<span id="cb42-582"><a href="#cb42-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-583"><a href="#cb42-583" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> <span class="dv">16</span> <span class="co"># arbitrary dimension</span></span>
<span id="cb42-584"><a href="#cb42-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-585"><a href="#cb42-585" aria-hidden="true" tabindex="-1"></a>freqs <span class="op">=</span> [<span class="st">"m"</span>, <span class="st">"d"</span>] <span class="co"># using here the commonly-used frequency abbreviations</span></span>
<span id="cb42-586"><a href="#cb42-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-587"><a href="#cb42-587" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> {f: keras.layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,<span class="dv">1</span>), name<span class="op">=</span>f) <span class="cf">for</span> f <span class="kw">in</span> freqs}</span>
<span id="cb42-588"><a href="#cb42-588" aria-hidden="true" tabindex="-1"></a>LSTMs <span class="op">=</span> []</span>
<span id="cb42-589"><a href="#cb42-589" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> inputs.items():</span>
<span id="cb42-590"><a href="#cb42-590" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.Masking(mask_value<span class="op">=</span><span class="fl">0.0</span>)(v)</span>
<span id="cb42-591"><a href="#cb42-591" aria-hidden="true" tabindex="-1"></a>    lstm <span class="op">=</span> keras.layers.LSTM(units<span class="op">=</span>dim, return_sequences<span class="op">=</span><span class="va">False</span>, name<span class="op">=</span><span class="ss">f"LSTM__freq_</span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)(lstm)</span>
<span id="cb42-592"><a href="#cb42-592" aria-hidden="true" tabindex="-1"></a>    LSTMs.append(lstm)</span>
<span id="cb42-593"><a href="#cb42-593" aria-hidden="true" tabindex="-1"></a>encoded_series <span class="op">=</span> keras.layers.Average(name<span class="op">=</span><span class="st">"encoded_series"</span>)(LSTMs)</span>
<span id="cb42-594"><a href="#cb42-594" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units <span class="op">=</span> dim, activation<span class="op">=</span><span class="st">"relu"</span>)(encoded_series)</span>
<span id="cb42-595"><a href="#cb42-595" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> keras.layers.Dense(units<span class="op">=</span><span class="dv">1</span>)(out)</span>
<span id="cb42-596"><a href="#cb42-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-597"><a href="#cb42-597" aria-hidden="true" tabindex="-1"></a>nn_lstm <span class="op">=</span> keras.Model(</span>
<span id="cb42-598"><a href="#cb42-598" aria-hidden="true" tabindex="-1"></a>    inputs<span class="op">=</span>inputs, </span>
<span id="cb42-599"><a href="#cb42-599" aria-hidden="true" tabindex="-1"></a>    outputs<span class="op">=</span>out,</span>
<span id="cb42-600"><a href="#cb42-600" aria-hidden="true" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"LSTMNetwork"</span></span>
<span id="cb42-601"><a href="#cb42-601" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-602"><a href="#cb42-602" aria-hidden="true" tabindex="-1"></a>nn_lstm.<span class="bu">compile</span>(loss<span class="op">=</span>keras.losses.MeanSquaredError())</span>
<span id="cb42-603"><a href="#cb42-603" aria-hidden="true" tabindex="-1"></a>nn_lstm.summary()</span>
<span id="cb42-604"><a href="#cb42-604" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-605"><a href="#cb42-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-608"><a href="#cb42-608" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-609"><a href="#cb42-609" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-archlstm</span></span>
<span id="cb42-610"><a href="#cb42-610" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Architecture of the network with LSTM layer</span></span>
<span id="cb42-611"><a href="#cb42-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-612"><a href="#cb42-612" aria-hidden="true" tabindex="-1"></a>keras.utils.plot_model(nn_lstm, show_layer_activations<span class="op">=</span><span class="va">True</span>, show_shapes<span class="op">=</span><span class="va">True</span>, show_layer_names<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-613"><a href="#cb42-613" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-614"><a href="#cb42-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-615"><a href="#cb42-615" aria-hidden="true" tabindex="-1"></a>The reason why the dimensions in the input layer are now <span class="in">`(None, None, 1)`</span>, with one addition <span class="in">`None`</span> compared to before(for example, @fig-archfc) is due to the *time dimension*. Whereas before the model didn't know how many samples it would be fed, now also the length of the time window can change because each time step will pass through exactly the same parameters. </span>
<span id="cb42-616"><a href="#cb42-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-617"><a href="#cb42-617" aria-hidden="true" tabindex="-1"></a>Note that the input goes through a few steps before reaching the LSTM layer. This is due to a masking layer that effectively helps the model jumps time steps for which there is no data available.</span>
<span id="cb42-618"><a href="#cb42-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-619"><a href="#cb42-619" aria-hidden="true" tabindex="-1"></a>To check that the LSTM-based neural network works, we need to feed this neural network a slightly different type of data. LSTM, as other recurrent neural networks, takes in time series data. So, unlike before, we now prepare a time series (or panel data) for each </span>
<span id="cb42-620"><a href="#cb42-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-621"><a href="#cb42-621" aria-hidden="true" tabindex="-1"></a>This neural network will then take in the inputed time series data, and encode each frequency's series separately through the different LSTM streams. The final result will no longer have a time dimension; it is then averaged, and this average embeddings of the different time series is used to forecast the variable of interest.</span>
<span id="cb42-622"><a href="#cb42-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-625"><a href="#cb42-625" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-626"><a href="#cb42-626" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: train the lstm</span></span>
<span id="cb42-627"><a href="#cb42-627" aria-hidden="true" tabindex="-1"></a>X_train_split, y_train_split <span class="op">=</span> create_data(X<span class="op">=</span>X_train, y<span class="op">=</span>y_train, maxlags<span class="op">=</span>maxlags)</span>
<span id="cb42-628"><a href="#cb42-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-629"><a href="#cb42-629" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_0"</span>, chunk<span class="op">=</span><span class="st">"train"</span>):</span>
<span id="cb42-630"><a href="#cb42-630" aria-hidden="true" tabindex="-1"></a>    X_lstm <span class="op">=</span> {}</span>
<span id="cb42-631"><a href="#cb42-631" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> d <span class="kw">in</span> X_train_split[fold][chunk]:</span>
<span id="cb42-632"><a href="#cb42-632" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, array <span class="kw">in</span> d.items():</span>
<span id="cb42-633"><a href="#cb42-633" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> key <span class="kw">not</span> <span class="kw">in</span> X_lstm:</span>
<span id="cb42-634"><a href="#cb42-634" aria-hidden="true" tabindex="-1"></a>                X_lstm[key] <span class="op">=</span> []  <span class="co"># Initialize an empty list if key is not present</span></span>
<span id="cb42-635"><a href="#cb42-635" aria-hidden="true" tabindex="-1"></a>            X_lstm[key].append(array)  <span class="co"># Append the array to the list for that key</span></span>
<span id="cb42-636"><a href="#cb42-636" aria-hidden="true" tabindex="-1"></a>    lstm_X <span class="op">=</span> {k: np.squeeze(np.array(v), axis<span class="op">=</span><span class="dv">1</span>) <span class="cf">for</span> k, v <span class="kw">in</span> X_lstm.items()}</span>
<span id="cb42-637"><a href="#cb42-637" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_X</span>
<span id="cb42-638"><a href="#cb42-638" aria-hidden="true" tabindex="-1"></a>lstm_X_train <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb42-639"><a href="#cb42-639" aria-hidden="true" tabindex="-1"></a>lstm_X_valid <span class="op">=</span> adjust_data_lstm(fold<span class="op">=</span><span class="st">"fold_4"</span>, chunk<span class="op">=</span><span class="st">"valid"</span>)</span>
<span id="cb42-640"><a href="#cb42-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-641"><a href="#cb42-641" aria-hidden="true" tabindex="-1"></a>history_lstm <span class="op">=</span> nn_lstm.fit(x<span class="op">=</span>lstm_X_train, y<span class="op">=</span>np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"train"</span>]), validation_data<span class="op">=</span>(lstm_X_valid, np.array(y_train_split[<span class="st">"fold_4"</span>][<span class="st">"valid"</span>])), epochs<span class="op">=</span><span class="dv">20</span>, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-642"><a href="#cb42-642" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-643"><a href="#cb42-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-646"><a href="#cb42-646" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb42-647"><a href="#cb42-647" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-history_lstm</span></span>
<span id="cb42-648"><a href="#cb42-648" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Losses calculated in an LSTM</span></span>
<span id="cb42-649"><a href="#cb42-649" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb42-650"><a href="#cb42-650" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - All models so far</span></span>
<span id="cb42-651"><a href="#cb42-651" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - LSTM only</span></span>
<span id="cb42-652"><a href="#cb42-652" aria-hidden="true" tabindex="-1"></a>lstm_loss <span class="op">=</span> pd.DataFrame(history_lstm.history)</span>
<span id="cb42-653"><a href="#cb42-653" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC with gate)"</span>] <span class="op">=</span> history_fc_gated.history[<span class="st">"val_loss"</span>]</span>
<span id="cb42-654"><a href="#cb42-654" aria-hidden="true" tabindex="-1"></a>lstm_loss[<span class="st">"val_loss (FC no gate)"</span>] <span class="op">=</span> history_fc.history[<span class="st">"val_loss"</span>]</span>
<span id="cb42-655"><a href="#cb42-655" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss.plot()</span>
<span id="cb42-656"><a href="#cb42-656" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb42-657"><a href="#cb42-657" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb42-658"><a href="#cb42-658" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-659"><a href="#cb42-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-660"><a href="#cb42-660" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> lstm_loss[[<span class="st">"loss"</span>, <span class="st">"val_loss"</span>]].plot()</span>
<span id="cb42-661"><a href="#cb42-661" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb42-662"><a href="#cb42-662" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Mean squared error"</span>)</span>
<span id="cb42-663"><a href="#cb42-663" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb42-664"><a href="#cb42-664" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb42-665"><a href="#cb42-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-666"><a href="#cb42-666" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introducing... the gatekeepers {#sec-gates}</span></span>
<span id="cb42-667"><a href="#cb42-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-668"><a href="#cb42-668" aria-hidden="true" tabindex="-1"></a><span class="fu">## Now is a(nother) good time to pay attention {#sec-transf}</span></span>
<span id="cb42-669"><a href="#cb42-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-670"><a href="#cb42-670" aria-hidden="true" tabindex="-1"></a><span class="fu">## Complete architecture {#sec-tftmf}</span></span>
<span id="cb42-671"><a href="#cb42-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-672"><a href="#cb42-672" aria-hidden="true" tabindex="-1"></a><span class="fu">## Nowcasting inflation with a simple model {#sec-nowcast}</span></span>
<span id="cb42-673"><a href="#cb42-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-674"><a href="#cb42-674" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb42-675"><a href="#cb42-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-676"><a href="#cb42-676" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb42-677"><a href="#cb42-677" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bis-med-it/gingado/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>